(ModelArguments(model_name_or_path='roberta-base', cache_dir='./cache', use_adapter=False, use_lora=True), DataTrainingArguments(dataset_name='restaurant', max_seq_length=64), TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.0005,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=./result/restaurant_roberta-base_seed0_lora/runs/Nov15_21-49-49_node010,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.EPOCH,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=5.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
output_dir=./result/restaurant_roberta-base_seed0_lora,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
resume_from_checkpoint=None,
run_name=./result/restaurant_roberta-base_seed0_lora,
save_on_each_node=False,
save_safetensors=False,
save_steps=500,
save_strategy=IntervalStrategy.NO,
save_total_limit=None,
seed=111,
sharded_ddp=[],
skip_memory_metrics=True,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
))
11/15/2023 21:49:49 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False
11/15/2023 21:49:49 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.0005,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=./result/restaurant_roberta-base_seed0_lora/runs/Nov15_21-49-49_node010,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.EPOCH,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=5.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
output_dir=./result/restaurant_roberta-base_seed0_lora,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
resume_from_checkpoint=None,
run_name=./result/restaurant_roberta-base_seed0_lora,
save_on_each_node=False,
save_safetensors=False,
save_steps=500,
save_strategy=IntervalStrategy.NO,
save_total_limit=None,
seed=111,
sharded_ddp=[],
skip_memory_metrics=True,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
wandb: Tracking run with wandb version 0.15.12
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
Map:   0%|          | 0/4722 [00:00<?, ? examples/s]Map:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4163/4722 [00:00<00:00, 41410.27 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4722/4722 [00:00<00:00, 40737.14 examples/s]
[INFO|configuration_utils.py:715] 2023-11-15 21:50:05,509 >> loading configuration file config.json from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/config.json
[INFO|configuration_utils.py:775] 2023-11-15 21:50:05,518 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.33.3",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_auto.py:535] 2023-11-15 21:50:15,529 >> Could not locate the tokenizer configuration file, will try to use the model config instead.
[INFO|configuration_utils.py:715] 2023-11-15 21:50:25,545 >> loading configuration file config.json from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/config.json
[INFO|configuration_utils.py:775] 2023-11-15 21:50:25,546 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.33.3",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:1852] 2023-11-15 21:50:45,591 >> loading file vocab.json from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/vocab.json
[INFO|tokenization_utils_base.py:1852] 2023-11-15 21:50:45,591 >> loading file merges.txt from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/merges.txt
[INFO|tokenization_utils_base.py:1852] 2023-11-15 21:50:45,592 >> loading file tokenizer.json from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/tokenizer.json
[INFO|tokenization_utils_base.py:1852] 2023-11-15 21:50:45,592 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1852] 2023-11-15 21:50:45,592 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1852] 2023-11-15 21:50:45,593 >> loading file tokenizer_config.json from cache at None
[INFO|configuration_utils.py:715] 2023-11-15 21:50:45,594 >> loading configuration file config.json from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/config.json
[INFO|configuration_utils.py:775] 2023-11-15 21:50:45,594 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.33.3",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:2862] 2023-11-15 21:51:05,753 >> loading weights file model.safetensors from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/model.safetensors
[INFO|modeling_utils.py:3638] 2023-11-15 21:51:07,860 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3650] 2023-11-15 21:51:07,863 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
trainable params: 1,284,867 || all params: 125,830,662 || trainable%: 1.0211080348603745
model_max_length: 512
Running tokenizer on dataset:   0%|          | 0/3777 [00:00<?, ? examples/s]Running tokenizer on dataset:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 3000/3777 [00:00<00:00, 23521.88 examples/s]Running tokenizer on dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3777/3777 [00:00<00:00, 23824.53 examples/s]
Running tokenizer on dataset:   0%|          | 0/945 [00:00<?, ? examples/s]Running tokenizer on dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 945/945 [00:00<00:00, 30986.58 examples/s]
11/15/2023 21:51:08 - INFO - __main__ - Sample 3388 of the training set: {'text': 'food <SEP> Had dinner here on a Friday and the food was great.', 'label': 0, 'input_ids': [0, 13193, 28696, 3388, 510, 15698, 7301, 3630, 259, 15, 10, 273, 8, 5, 689, 21, 372, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
11/15/2023 21:51:08 - INFO - __main__ - Sample 871 of the training set: {'text': 'drinks <SEP> You get what you pay for and with that logic in mind, Spice is a great place to grab some cheap eats and drinks in a beautiful setting.', 'label': 0, 'input_ids': [0, 10232, 12935, 28696, 3388, 510, 15698, 370, 120, 99, 47, 582, 13, 8, 19, 14, 14578, 11, 1508, 6, 21665, 16, 10, 372, 317, 7, 6895, 103, 6162, 24923, 8, 6696, 11, 10, 2721, 2749, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
11/15/2023 21:51:08 - INFO - __main__ - Sample 3773 of the training set: {'text': 'food <SEP> Good service, great food, good value, and never have to wait in line!', 'label': 0, 'input_ids': [0, 13193, 28696, 3388, 510, 15698, 2497, 544, 6, 372, 689, 6, 205, 923, 6, 8, 393, 33, 7, 2067, 11, 516, 328, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
11/15/2023 21:51:08 - WARNING - accelerate.utils.other - Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[INFO|trainer.py:755] 2023-11-15 21:51:09,453 >> The following columns in the training set don't have a corresponding argument in `PeftModelForTokenClassification.forward` and have been ignored: text. If text are not expected by `PeftModelForTokenClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:1715] 2023-11-15 21:51:09,463 >> ***** Running training *****
[INFO|trainer.py:1716] 2023-11-15 21:51:09,464 >>   Num examples = 3,777
[INFO|trainer.py:1717] 2023-11-15 21:51:09,464 >>   Num Epochs = 5
[INFO|trainer.py:1718] 2023-11-15 21:51:09,464 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1721] 2023-11-15 21:51:09,465 >>   Total train batch size (w. parallel, distributed & accumulation) = 32
[INFO|trainer.py:1722] 2023-11-15 21:51:09,465 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:1723] 2023-11-15 21:51:09,465 >>   Total optimization steps = 595
[INFO|trainer.py:1724] 2023-11-15 21:51:09,466 >>   Number of trainable parameters = 1,284,867
[INFO|integration_utils.py:716] 2023-11-15 21:51:09,468 >> Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"
  0%|          | 0/595 [00:00<?, ?it/s]  0%|          | 1/595 [00:01<13:57,  1.41s/it]  1%|          | 3/595 [00:01<04:06,  2.40it/s]  1%|          | 5/595 [00:01<02:20,  4.20it/s]  1%|          | 7/595 [00:01<01:37,  6.01it/s]  2%|â–         | 9/595 [00:01<01:15,  7.72it/s]  2%|â–         | 11/595 [00:02<01:03,  9.22it/s]  2%|â–         | 13/595 [00:02<00:55, 10.49it/s]  3%|â–Ž         | 15/595 [00:02<00:50, 11.52it/s]  3%|â–Ž         | 17/595 [00:02<00:46, 12.32it/s]  3%|â–Ž         | 19/595 [00:02<00:44, 12.94it/s]  4%|â–Ž         | 21/595 [00:02<00:42, 13.41it/s]  4%|â–         | 23/595 [00:02<00:41, 13.68it/s]  4%|â–         | 25/595 [00:03<00:40, 13.91it/s]  5%|â–         | 27/595 [00:03<00:40, 14.09it/s]  5%|â–         | 29/595 [00:03<00:39, 14.22it/s]  5%|â–Œ         | 31/595 [00:03<00:39, 14.32it/s]  6%|â–Œ         | 33/595 [00:03<00:39, 14.38it/s]  6%|â–Œ         | 35/595 [00:03<00:38, 14.42it/s]  6%|â–Œ         | 37/595 [00:03<00:38, 14.46it/s]  7%|â–‹         | 39/595 [00:04<00:38, 14.51it/s]  7%|â–‹         | 41/595 [00:04<00:38, 14.51it/s]  7%|â–‹         | 43/595 [00:04<00:38, 14.50it/s]  8%|â–Š         | 45/595 [00:04<00:37, 14.51it/s]  8%|â–Š         | 47/595 [00:04<00:37, 14.53it/s]  8%|â–Š         | 49/595 [00:04<00:37, 14.51it/s]  9%|â–Š         | 51/595 [00:04<00:37, 14.50it/s]  9%|â–‰         | 53/595 [00:04<00:37, 14.43it/s]  9%|â–‰         | 55/595 [00:05<00:37, 14.44it/s] 10%|â–‰         | 57/595 [00:05<00:37, 14.48it/s] 10%|â–‰         | 59/595 [00:05<00:36, 14.51it/s] 10%|â–ˆ         | 61/595 [00:05<00:36, 14.52it/s] 11%|â–ˆ         | 63/595 [00:05<00:36, 14.51it/s] 11%|â–ˆ         | 65/595 [00:05<00:36, 14.49it/s] 11%|â–ˆâ–        | 67/595 [00:05<00:36, 14.51it/s] 12%|â–ˆâ–        | 69/595 [00:06<00:36, 14.52it/s] 12%|â–ˆâ–        | 71/595 [00:06<00:36, 14.47it/s] 12%|â–ˆâ–        | 73/595 [00:06<00:36, 14.47it/s] 13%|â–ˆâ–Ž        | 75/595 [00:06<00:35, 14.47it/s] 13%|â–ˆâ–Ž        | 77/595 [00:06<00:35, 14.48it/s] 13%|â–ˆâ–Ž        | 79/595 [00:06<00:35, 14.51it/s] 14%|â–ˆâ–Ž        | 81/595 [00:06<00:35, 14.50it/s] 14%|â–ˆâ–        | 83/595 [00:07<00:35, 14.48it/s] 14%|â–ˆâ–        | 85/595 [00:07<00:35, 14.48it/s] 15%|â–ˆâ–        | 87/595 [00:07<00:35, 14.49it/s] 15%|â–ˆâ–        | 89/595 [00:07<00:34, 14.51it/s] 15%|â–ˆâ–Œ        | 91/595 [00:07<00:34, 14.49it/s] 16%|â–ˆâ–Œ        | 93/595 [00:07<00:34, 14.48it/s] 16%|â–ˆâ–Œ        | 95/595 [00:07<00:34, 14.48it/s] 16%|â–ˆâ–‹        | 97/595 [00:08<00:34, 14.48it/s] 17%|â–ˆâ–‹        | 99/595 [00:08<00:34, 14.49it/s] 17%|â–ˆâ–‹        | 101/595 [00:08<00:34, 14.52it/s] 17%|â–ˆâ–‹        | 103/595 [00:08<00:33, 14.50it/s] 18%|â–ˆâ–Š        | 105/595 [00:08<00:33, 14.49it/s] 18%|â–ˆâ–Š        | 107/595 [00:08<00:33, 14.49it/s] 18%|â–ˆâ–Š        | 109/595 [00:08<00:33, 14.46it/s] 19%|â–ˆâ–Š        | 111/595 [00:08<00:33, 14.47it/s] 19%|â–ˆâ–‰        | 113/595 [00:09<00:33, 14.47it/s] 19%|â–ˆâ–‰        | 115/595 [00:09<00:33, 14.52it/s] 20%|â–ˆâ–‰        | 117/595 [00:09<00:32, 14.51it/s]                                                  20%|â–ˆâ–ˆ        | 119/595 [00:09<00:32, 14.51it/s][INFO|trainer.py:755] 2023-11-15 21:51:18,983 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModelForTokenClassification.forward` and have been ignored: text. If text are not expected by `PeftModelForTokenClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 21:51:18,985 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 21:51:18,985 >>   Num examples = 945
[INFO|trainer.py:3134] 2023-11-15 21:51:18,985 >>   Batch size = 8
{'loss': 0.6717, 'learning_rate': 0.0004, 'epoch': 1.0}

  0%|          | 0/119 [00:00<?, ?it/s][A
 10%|â–ˆ         | 12/119 [00:00<00:00, 118.67it/s][A
 20%|â–ˆâ–ˆ        | 24/119 [00:00<00:00, 112.13it/s][A
 30%|â–ˆâ–ˆâ–ˆ       | 36/119 [00:00<00:00, 110.24it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 48/119 [00:00<00:00, 109.33it/s][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 59/119 [00:00<00:00, 108.80it/s][A
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 70/119 [00:00<00:00, 108.42it/s][A
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 81/119 [00:00<00:00, 108.09it/s][A
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 92/119 [00:00<00:00, 108.05it/s][A
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 103/119 [00:00<00:00, 108.01it/s][A
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 114/119 [00:01<00:00, 107.98it/s][A                                                 
                                                  [A 20%|â–ˆâ–ˆ        | 119/595 [00:10<00:32, 14.51it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 119/119 [00:01<00:00, 107.98it/s][A
                                                  [A 20%|â–ˆâ–ˆ        | 120/595 [00:10<01:41,  4.68it/s] 21%|â–ˆâ–ˆ        | 122/595 [00:10<01:22,  5.73it/s] 21%|â–ˆâ–ˆ        | 124/595 [00:11<01:08,  6.89it/s] 21%|â–ˆâ–ˆ        | 126/595 [00:11<00:57,  8.09it/s] 22%|â–ˆâ–ˆâ–       | 128/595 [00:11<00:50,  9.26it/s] 22%|â–ˆâ–ˆâ–       | 130/595 [00:11<00:45, 10.33it/s] 22%|â–ˆâ–ˆâ–       | 132/595 [00:11<00:41, 11.26it/s] 23%|â–ˆâ–ˆâ–Ž       | 134/595 [00:11<00:38, 12.03it/s] 23%|â–ˆâ–ˆâ–Ž       | 136/595 [00:11<00:36, 12.64it/s] 23%|â–ˆâ–ˆâ–Ž       | 138/595 [00:11<00:34, 13.13it/s] 24%|â–ˆâ–ˆâ–Ž       | 140/595 [00:12<00:33, 13.49it/s] 24%|â–ˆâ–ˆâ–       | 142/595 [00:12<00:32, 13.75it/s] 24%|â–ˆâ–ˆâ–       | 144/595 [00:12<00:32, 13.93it/s] 25%|â–ˆâ–ˆâ–       | 146/595 [00:12<00:31, 14.06it/s] 25%|â–ˆâ–ˆâ–       | 148/595 [00:12<00:31, 14.14it/s] 25%|â–ˆâ–ˆâ–Œ       | 150/595 [00:12<00:31, 14.19it/s] 26%|â–ˆâ–ˆâ–Œ       | 152/595 [00:12<00:31, 14.23it/s] 26%|â–ˆâ–ˆâ–Œ       | 154/595 [00:13<00:30, 14.26it/s] 26%|â–ˆâ–ˆâ–Œ       | 156/595 [00:13<00:30, 14.29it/s] 27%|â–ˆâ–ˆâ–‹       | 158/595 [00:13<00:30, 14.31it/s] 27%|â–ˆâ–ˆâ–‹       | 160/595 [00:13<00:30, 14.33it/s] 27%|â–ˆâ–ˆâ–‹       | 162/595 [00:13<00:30, 14.35it/s] 28%|â–ˆâ–ˆâ–Š       | 164/595 [00:13<00:30, 14.34it/s] 28%|â–ˆâ–ˆâ–Š       | 166/595 [00:13<00:29, 14.36it/s] 28%|â–ˆâ–ˆâ–Š       | 168/595 [00:14<00:29, 14.38it/s] 29%|â–ˆâ–ˆâ–Š       | 170/595 [00:14<00:29, 14.39it/s] 29%|â–ˆâ–ˆâ–‰       | 172/595 [00:14<00:29, 14.40it/s] 29%|â–ˆâ–ˆâ–‰       | 174/595 [00:14<00:29, 14.35it/s] 30%|â–ˆâ–ˆâ–‰       | 176/595 [00:14<00:29, 14.35it/s] 30%|â–ˆâ–ˆâ–‰       | 178/595 [00:14<00:29, 14.36it/s] 30%|â–ˆâ–ˆâ–ˆ       | 180/595 [00:14<00:28, 14.35it/s] 31%|â–ˆâ–ˆâ–ˆ       | 182/595 [00:15<00:28, 14.34it/s] 31%|â–ˆâ–ˆâ–ˆ       | 184/595 [00:15<00:28, 14.33it/s] 31%|â–ˆâ–ˆâ–ˆâ–      | 186/595 [00:15<00:28, 14.34it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 188/595 [00:15<00:28, 14.33it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 190/595 [00:15<00:28, 14.34it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 192/595 [00:15<00:28, 14.35it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 194/595 [00:15<00:27, 14.35it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 196/595 [00:16<00:27, 14.36it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 198/595 [00:16<00:27, 14.36it/s] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 200/595 [00:16<00:27, 14.37it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 202/595 [00:16<00:27, 14.39it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 204/595 [00:16<00:27, 14.39it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 206/595 [00:16<00:27, 14.37it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 208/595 [00:16<00:26, 14.36it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 210/595 [00:16<00:26, 14.35it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 212/595 [00:17<00:26, 14.34it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 214/595 [00:17<00:26, 14.32it/s] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 216/595 [00:17<00:26, 14.32it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 218/595 [00:17<00:26, 14.32it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 220/595 [00:17<00:26, 14.33it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 222/595 [00:17<00:26, 14.33it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 224/595 [00:17<00:25, 14.33it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 226/595 [00:18<00:25, 14.34it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 228/595 [00:18<00:25, 14.35it/s] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 230/595 [00:18<00:25, 14.39it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 232/595 [00:18<00:25, 14.40it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 234/595 [00:18<00:25, 14.44it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 236/595 [00:18<00:24, 14.45it/s]                                                  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 238/595 [00:18<00:24, 14.45it/s][INFO|trainer.py:755] 2023-11-15 21:51:28,370 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModelForTokenClassification.forward` and have been ignored: text. If text are not expected by `PeftModelForTokenClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 21:51:28,372 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 21:51:28,372 >>   Num examples = 945
[INFO|trainer.py:3134] 2023-11-15 21:51:28,373 >>   Batch size = 8
{'eval_loss': 0.49943652749061584, 'eval_accuracy': 0.7851851851851852, 'eval_micro_f1': 0.7851851851851852, 'eval_macro_f1': 0.6805383869762552, 'eval_runtime': 1.1378, 'eval_samples_per_second': 830.578, 'eval_steps_per_second': 104.591, 'epoch': 1.0}
{'loss': 0.5187, 'learning_rate': 0.0003, 'epoch': 2.0}

  0%|          | 0/119 [00:00<?, ?it/s][A
 10%|â–ˆ         | 12/119 [00:00<00:00, 119.61it/s][A
 20%|â–ˆâ–ˆ        | 24/119 [00:00<00:00, 112.70it/s][A
 30%|â–ˆâ–ˆâ–ˆ       | 36/119 [00:00<00:00, 110.71it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 48/119 [00:00<00:00, 109.63it/s][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 59/119 [00:00<00:00, 109.16it/s][A
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 70/119 [00:00<00:00, 108.76it/s][A
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 81/119 [00:00<00:00, 108.52it/s][A
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 92/119 [00:00<00:00, 108.44it/s][A
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 103/119 [00:00<00:00, 108.36it/s][A
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 114/119 [00:01<00:00, 108.35it/s][A                                                 
                                                  [A 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 238/595 [00:20<00:24, 14.45it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 119/119 [00:01<00:00, 108.35it/s][A
                                                  [A 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 239/595 [00:20<01:15,  4.71it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 241/595 [00:20<01:01,  5.76it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 243/595 [00:20<00:50,  6.92it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 245/595 [00:20<00:42,  8.14it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 247/595 [00:20<00:37,  9.32it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 249/595 [00:20<00:33, 10.40it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 251/595 [00:20<00:30, 11.34it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 253/595 [00:21<00:28, 12.11it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 255/595 [00:21<00:26, 12.73it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 257/595 [00:21<00:25, 13.20it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 259/595 [00:21<00:24, 13.54it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 261/595 [00:21<00:24, 13.78it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 263/595 [00:21<00:23, 13.96it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 265/595 [00:21<00:23, 14.10it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 267/595 [00:22<00:23, 14.20it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 269/595 [00:22<00:22, 14.29it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 271/595 [00:22<00:22, 14.36it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 273/595 [00:22<00:22, 14.38it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 275/595 [00:22<00:22, 14.38it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 277/595 [00:22<00:22, 14.40it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 279/595 [00:22<00:21, 14.41it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 281/595 [00:23<00:21, 14.43it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 283/595 [00:23<00:21, 14.44it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 285/595 [00:23<00:21, 14.44it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 287/595 [00:23<00:21, 14.43it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 289/595 [00:23<00:21, 14.42it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 291/595 [00:23<00:21, 14.42it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 293/595 [00:23<00:20, 14.42it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 295/595 [00:23<00:20, 14.43it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 297/595 [00:24<00:20, 14.46it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 299/595 [00:24<00:20, 14.47it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 301/595 [00:24<00:20, 14.45it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 303/595 [00:24<00:20, 14.44it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 305/595 [00:24<00:20, 14.43it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 307/595 [00:24<00:19, 14.43it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 309/595 [00:24<00:19, 14.43it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 311/595 [00:25<00:19, 14.45it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 313/595 [00:25<00:19, 14.44it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 315/595 [00:25<00:19, 14.43it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 317/595 [00:25<00:19, 14.41it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 319/595 [00:25<00:19, 14.41it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 321/595 [00:25<00:19, 14.41it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 323/595 [00:25<00:18, 14.42it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 325/595 [00:26<00:18, 14.45it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 327/595 [00:26<00:18, 14.47it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 329/595 [00:26<00:18, 14.46it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 331/595 [00:26<00:18, 14.44it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 333/595 [00:26<00:18, 14.43it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 335/595 [00:26<00:18, 14.43it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 337/595 [00:26<00:17, 14.43it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 339/595 [00:27<00:17, 14.44it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 341/595 [00:27<00:17, 14.44it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 343/595 [00:27<00:17, 14.42it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 345/595 [00:27<00:17, 14.41it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 347/595 [00:27<00:17, 14.40it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 349/595 [00:27<00:17, 14.40it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 351/595 [00:27<00:16, 14.41it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 353/595 [00:28<00:16, 14.41it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 355/595 [00:28<00:16, 14.43it/s]                                                  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 357/595 [00:28<00:16, 14.43it/s][INFO|trainer.py:755] 2023-11-15 21:51:37,711 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModelForTokenClassification.forward` and have been ignored: text. If text are not expected by `PeftModelForTokenClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 21:51:37,712 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 21:51:37,712 >>   Num examples = 945
[INFO|trainer.py:3134] 2023-11-15 21:51:37,713 >>   Batch size = 8
{'eval_loss': 0.447017103433609, 'eval_accuracy': 0.8402116402116402, 'eval_micro_f1': 0.8402116402116402, 'eval_macro_f1': 0.7770287160527974, 'eval_runtime': 1.1313, 'eval_samples_per_second': 835.356, 'eval_steps_per_second': 105.193, 'epoch': 2.0}
{'loss': 0.4176, 'learning_rate': 0.0002, 'epoch': 3.0}

  0%|          | 0/119 [00:00<?, ?it/s][A
 10%|â–ˆ         | 12/119 [00:00<00:00, 119.31it/s][A
 20%|â–ˆâ–ˆ        | 24/119 [00:00<00:00, 112.53it/s][A
 30%|â–ˆâ–ˆâ–ˆ       | 36/119 [00:00<00:00, 110.47it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 48/119 [00:00<00:00, 109.58it/s][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 59/119 [00:00<00:00, 109.00it/s][A
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 70/119 [00:00<00:00, 108.49it/s][A
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 81/119 [00:00<00:00, 108.26it/s][A
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 92/119 [00:00<00:00, 108.05it/s][A
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 103/119 [00:00<00:00, 107.89it/s][A
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 114/119 [00:01<00:00, 107.71it/s][A                                                 
                                                  [A 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 357/595 [00:29<00:16, 14.43it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 119/119 [00:01<00:00, 107.71it/s][A
                                                  [A 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 358/595 [00:29<00:50,  4.70it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 360/595 [00:29<00:40,  5.75it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 362/595 [00:29<00:33,  6.91it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 364/595 [00:29<00:28,  8.11it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 366/595 [00:30<00:24,  9.28it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 368/595 [00:30<00:21, 10.35it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 370/595 [00:30<00:19, 11.28it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 372/595 [00:30<00:18, 12.05it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 374/595 [00:30<00:17, 12.67it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 376/595 [00:30<00:16, 13.13it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 378/595 [00:30<00:16, 13.49it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 380/595 [00:30<00:15, 13.76it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 382/595 [00:31<00:15, 13.96it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 384/595 [00:31<00:14, 14.08it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 386/595 [00:31<00:14, 14.16it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 388/595 [00:31<00:14, 14.21it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 390/595 [00:31<00:14, 14.25it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 392/595 [00:31<00:14, 14.28it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 394/595 [00:31<00:14, 14.30it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 396/595 [00:32<00:13, 14.31it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 398/595 [00:32<00:13, 14.31it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 400/595 [00:32<00:13, 14.33it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 402/595 [00:32<00:13, 14.35it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 404/595 [00:32<00:13, 14.37it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 406/595 [00:32<00:13, 14.39it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 408/595 [00:32<00:12, 14.39it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 410/595 [00:33<00:12, 14.38it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 412/595 [00:33<00:12, 14.37it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 414/595 [00:33<00:12, 14.36it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 416/595 [00:33<00:12, 14.35it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 418/595 [00:33<00:12, 14.35it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 420/595 [00:33<00:12, 14.35it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 422/595 [00:33<00:12, 14.35it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 424/595 [00:34<00:11, 14.36it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 426/595 [00:34<00:11, 14.37it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 428/595 [00:34<00:11, 14.38it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 430/595 [00:34<00:11, 14.39it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 432/595 [00:34<00:11, 14.40it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 434/595 [00:34<00:11, 14.41it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 436/595 [00:34<00:11, 14.38it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 438/595 [00:35<00:10, 14.37it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 440/595 [00:35<00:10, 14.36it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 442/595 [00:35<00:10, 14.34it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 444/595 [00:35<00:10, 14.34it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 446/595 [00:35<00:10, 14.34it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 448/595 [00:35<00:10, 14.34it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 450/595 [00:35<00:10, 14.34it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 452/595 [00:35<00:09, 14.34it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 454/595 [00:36<00:09, 14.35it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 456/595 [00:36<00:09, 14.35it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 458/595 [00:36<00:09, 14.36it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 460/595 [00:36<00:09, 14.38it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 462/595 [00:36<00:09, 14.39it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 464/595 [00:36<00:09, 14.39it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 466/595 [00:36<00:08, 14.38it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 468/595 [00:37<00:08, 14.36it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 470/595 [00:37<00:08, 14.34it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 472/595 [00:37<00:08, 14.32it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 474/595 [00:37<00:08, 14.33it/s]                                                  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 476/595 [00:37<00:08, 14.33it/s][INFO|trainer.py:755] 2023-11-15 21:51:47,095 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModelForTokenClassification.forward` and have been ignored: text. If text are not expected by `PeftModelForTokenClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 21:51:47,097 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 21:51:47,097 >>   Num examples = 945
[INFO|trainer.py:3134] 2023-11-15 21:51:47,097 >>   Batch size = 8
{'eval_loss': 0.41954129934310913, 'eval_accuracy': 0.8391534391534392, 'eval_micro_f1': 0.8391534391534392, 'eval_macro_f1': 0.7879874683979118, 'eval_runtime': 1.1336, 'eval_samples_per_second': 833.593, 'eval_steps_per_second': 104.971, 'epoch': 3.0}
{'loss': 0.3598, 'learning_rate': 0.0001, 'epoch': 4.0}

  0%|          | 0/119 [00:00<?, ?it/s][A
 10%|â–ˆ         | 12/119 [00:00<00:00, 118.82it/s][A
 20%|â–ˆâ–ˆ        | 24/119 [00:00<00:00, 111.95it/s][A
 30%|â–ˆâ–ˆâ–ˆ       | 36/119 [00:00<00:00, 109.95it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 48/119 [00:00<00:00, 109.01it/s][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 59/119 [00:00<00:00, 108.46it/s][A
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 70/119 [00:00<00:00, 108.12it/s][A
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 81/119 [00:00<00:00, 107.91it/s][A
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 92/119 [00:00<00:00, 107.80it/s][A
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 103/119 [00:00<00:00, 107.70it/s][A
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 114/119 [00:01<00:00, 107.66it/s][A                                                 
                                                  [A 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 476/595 [00:38<00:08, 14.33it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 119/119 [00:01<00:00, 107.66it/s][A
                                                  [A 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 477/595 [00:38<00:25,  4.68it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 479/595 [00:38<00:20,  5.73it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 481/595 [00:39<00:16,  6.89it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 483/595 [00:39<00:13,  8.09it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 485/595 [00:39<00:11,  9.26it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 487/595 [00:39<00:10, 10.33it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 489/595 [00:39<00:09, 11.25it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 491/595 [00:39<00:08, 12.00it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 493/595 [00:39<00:08, 12.61it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 495/595 [00:40<00:07, 13.07it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 497/595 [00:40<00:07, 13.41it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 499/595 [00:40<00:07, 13.66it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 501/595 [00:40<00:06, 13.85it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 503/595 [00:40<00:06, 13.98it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 505/595 [00:40<00:06, 14.08it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 507/595 [00:40<00:06, 14.15it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 509/595 [00:41<00:06, 14.19it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 511/595 [00:41<00:05, 14.22it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 513/595 [00:41<00:05, 14.25it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 515/595 [00:41<00:05, 14.27it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 517/595 [00:41<00:05, 14.30it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 519/595 [00:41<00:05, 14.31it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 521/595 [00:41<00:05, 14.33it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 523/595 [00:42<00:05, 14.33it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 525/595 [00:42<00:04, 14.34it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 527/595 [00:42<00:04, 14.36it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 529/595 [00:42<00:04, 14.36it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 531/595 [00:42<00:04, 14.37it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 533/595 [00:42<00:04, 14.36it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 535/595 [00:42<00:04, 14.35it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 537/595 [00:43<00:04, 14.34it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 539/595 [00:43<00:03, 14.33it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 541/595 [00:43<00:03, 14.32it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 543/595 [00:43<00:03, 14.31it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 545/595 [00:43<00:03, 14.31it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 547/595 [00:43<00:03, 14.30it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 549/595 [00:43<00:03, 14.30it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 551/595 [00:44<00:03, 14.29it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 553/595 [00:44<00:02, 14.29it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 555/595 [00:44<00:02, 14.29it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 557/595 [00:44<00:02, 14.29it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 559/595 [00:44<00:02, 14.29it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 561/595 [00:44<00:02, 14.29it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 563/595 [00:44<00:02, 14.29it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 565/595 [00:44<00:02, 14.29it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 567/595 [00:45<00:01, 14.29it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 569/595 [00:45<00:01, 14.29it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 571/595 [00:45<00:01, 14.29it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 573/595 [00:45<00:01, 14.29it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 575/595 [00:45<00:01, 14.29it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 577/595 [00:45<00:01, 14.29it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 579/595 [00:45<00:01, 14.29it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 581/595 [00:46<00:00, 14.29it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 583/595 [00:46<00:00, 14.29it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 585/595 [00:46<00:00, 14.29it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 587/595 [00:46<00:00, 14.29it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 589/595 [00:46<00:00, 14.30it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 591/595 [00:46<00:00, 14.29it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 593/595 [00:46<00:00, 14.29it/s]                                                 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 595/595 [00:47<00:00, 14.29it/s][INFO|trainer.py:755] 2023-11-15 21:51:56,513 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModelForTokenClassification.forward` and have been ignored: text. If text are not expected by `PeftModelForTokenClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 21:51:56,514 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 21:51:56,514 >>   Num examples = 945
[INFO|trainer.py:3134] 2023-11-15 21:51:56,514 >>   Batch size = 8
{'eval_loss': 0.3874734342098236, 'eval_accuracy': 0.8582010582010582, 'eval_micro_f1': 0.8582010582010582, 'eval_macro_f1': 0.7964035038543483, 'eval_runtime': 1.1371, 'eval_samples_per_second': 831.035, 'eval_steps_per_second': 104.649, 'epoch': 4.0}
{'loss': 0.3162, 'learning_rate': 0.0, 'epoch': 5.0}

  0%|          | 0/119 [00:00<?, ?it/s][A
 10%|â–ˆ         | 12/119 [00:00<00:00, 118.36it/s][A
 20%|â–ˆâ–ˆ        | 24/119 [00:00<00:00, 111.77it/s][A
 30%|â–ˆâ–ˆâ–ˆ       | 36/119 [00:00<00:00, 109.84it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 48/119 [00:00<00:00, 108.90it/s][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 59/119 [00:00<00:00, 108.43it/s][A
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 70/119 [00:00<00:00, 108.10it/s][A
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 81/119 [00:00<00:00, 107.80it/s][A
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 92/119 [00:00<00:00, 107.68it/s][A
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 103/119 [00:00<00:00, 107.62it/s][A
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 114/119 [00:01<00:00, 107.57it/s][A                                                 
                                                  [A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 595/595 [00:48<00:00, 14.29it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 119/119 [00:01<00:00, 107.57it/s][A
                                                  [A[INFO|trainer.py:1963] 2023-11-15 21:51:57,654 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 595/595 [00:48<00:00, 14.29it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 595/595 [00:48<00:00, 12.35it/s]
[INFO|trainer.py:2855] 2023-11-15 21:51:57,658 >> Saving model checkpoint to ./result/restaurant_roberta-base_seed0_lora
[INFO|tokenization_utils_base.py:2235] 2023-11-15 21:51:57,781 >> tokenizer config file saved in ./result/restaurant_roberta-base_seed0_lora/tokenizer_config.json
[INFO|tokenization_utils_base.py:2242] 2023-11-15 21:51:57,783 >> Special tokens file saved in ./result/restaurant_roberta-base_seed0_lora/special_tokens_map.json
{'eval_loss': 0.3978428542613983, 'eval_accuracy': 0.8592592592592593, 'eval_micro_f1': 0.8592592592592592, 'eval_macro_f1': 0.8000389559954572, 'eval_runtime': 1.1365, 'eval_samples_per_second': 831.479, 'eval_steps_per_second': 104.705, 'epoch': 5.0}
{'train_runtime': 48.188, 'train_samples_per_second': 391.903, 'train_steps_per_second': 12.347, 'train_loss': 0.4568096641732865, 'epoch': 5.0}
***** train metrics *****
  epoch                    =        5.0
  train_loss               =     0.4568
  train_runtime            = 0:00:48.18
  train_samples            =       3777
  train_samples_per_second =    391.903
  train_steps_per_second   =     12.347
11/15/2023 21:51:57 - INFO - __main__ - *** Evaluate ***
[INFO|trainer.py:755] 2023-11-15 21:51:57,879 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModelForTokenClassification.forward` and have been ignored: text. If text are not expected by `PeftModelForTokenClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 21:51:57,880 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 21:51:57,881 >>   Num examples = 945
[INFO|trainer.py:3134] 2023-11-15 21:51:57,881 >>   Batch size = 8
  0%|          | 0/119 [00:00<?, ?it/s] 10%|â–ˆ         | 12/119 [00:00<00:00, 118.67it/s] 20%|â–ˆâ–ˆ        | 24/119 [00:00<00:00, 112.04it/s] 30%|â–ˆâ–ˆâ–ˆ       | 36/119 [00:00<00:00, 110.19it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 48/119 [00:00<00:00, 109.35it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 59/119 [00:00<00:00, 108.96it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 70/119 [00:00<00:00, 108.62it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 81/119 [00:00<00:00, 108.41it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 92/119 [00:00<00:00, 108.25it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 103/119 [00:00<00:00, 108.13it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 114/119 [00:01<00:00, 108.11it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 119/119 [00:01<00:00, 106.33it/s]
***** eval metrics *****
  epoch                   =        5.0
  eval_accuracy           =     0.8593
  eval_loss               =     0.3978
  eval_macro_f1           =        0.8
  eval_micro_f1           =     0.8593
  eval_runtime            = 0:00:01.13
  eval_samples            =        945
  eval_samples_per_second =    834.717
  eval_steps_per_second   =    105.113
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                  eval/accuracy â–â–†â–†â–ˆâ–ˆâ–ˆ
wandb:                      eval/loss â–ˆâ–…â–ƒâ–â–‚â–‚
wandb:                  eval/macro_f1 â–â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:                  eval/micro_f1 â–â–†â–†â–ˆâ–ˆâ–ˆ
wandb:                   eval/runtime â–ˆâ–â–ƒâ–‡â–‡â–‚
wandb:        eval/samples_per_second â–â–ˆâ–…â–‚â–‚â–‡
wandb:          eval/steps_per_second â–â–ˆâ–…â–‚â–‚â–‡
wandb:                    train/epoch â–â–â–ƒâ–ƒâ–…â–…â–†â–†â–ˆâ–ˆâ–ˆâ–ˆ
wandb:              train/global_step â–â–â–ƒâ–ƒâ–…â–…â–†â–†â–ˆâ–ˆâ–ˆâ–ˆ
wandb:            train/learning_rate â–ˆâ–†â–…â–ƒâ–
wandb:                     train/loss â–ˆâ–…â–ƒâ–‚â–
wandb:               train/total_flos â–
wandb:               train/train_loss â–
wandb:            train/train_runtime â–
wandb: train/train_samples_per_second â–
wandb:   train/train_steps_per_second â–
wandb: 
wandb: Run summary:
wandb:                  eval/accuracy 0.85926
wandb:                      eval/loss 0.39784
wandb:                  eval/macro_f1 0.80004
wandb:                  eval/micro_f1 0.85926
wandb:                   eval/runtime 1.1321
wandb:        eval/samples_per_second 834.717
wandb:          eval/steps_per_second 105.113
wandb:                    train/epoch 5.0
wandb:              train/global_step 595
wandb:            train/learning_rate 0.0
wandb:                     train/loss 0.3162
wandb:               train/total_flos 629689029684480.0
wandb:               train/train_loss 0.45681
wandb:            train/train_runtime 48.188
wandb: train/train_samples_per_second 391.903
wandb:   train/train_steps_per_second 12.347
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /ceph/home/wangyifei/a3/wandb/offline-run-20231115_214951-u7bfbl98
wandb: Find logs at: ./wandb/offline-run-20231115_214951-u7bfbl98/logs
(ModelArguments(model_name_or_path='roberta-base', cache_dir='./cache', use_adapter=False, use_lora=True), DataTrainingArguments(dataset_name='acl', max_seq_length=64), TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.0005,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=./result/acl_roberta-base_seed0_lora/runs/Nov15_21-52-08_node010,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.EPOCH,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=5.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
output_dir=./result/acl_roberta-base_seed0_lora,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
resume_from_checkpoint=None,
run_name=./result/acl_roberta-base_seed0_lora,
save_on_each_node=False,
save_safetensors=False,
save_steps=500,
save_strategy=IntervalStrategy.NO,
save_total_limit=None,
seed=111,
sharded_ddp=[],
skip_memory_metrics=True,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
))
11/15/2023 21:52:09 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False
11/15/2023 21:52:09 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.0005,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=./result/acl_roberta-base_seed0_lora/runs/Nov15_21-52-08_node010,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.EPOCH,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=5.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
output_dir=./result/acl_roberta-base_seed0_lora,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
resume_from_checkpoint=None,
run_name=./result/acl_roberta-base_seed0_lora,
save_on_each_node=False,
save_safetensors=False,
save_steps=500,
save_strategy=IntervalStrategy.NO,
save_total_limit=None,
seed=111,
sharded_ddp=[],
skip_memory_metrics=True,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
wandb: Tracking run with wandb version 0.15.12
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
Map:   0%|          | 0/11020 [00:00<?, ? examples/s]Map:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 4164/11020 [00:00<00:00, 40551.12 examples/s]Map:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 8238/11020 [00:00<00:00, 40648.06 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11020/11020 [00:00<00:00, 40396.36 examples/s]
[INFO|configuration_utils.py:715] 2023-11-15 21:52:24,845 >> loading configuration file config.json from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/config.json
[INFO|configuration_utils.py:775] 2023-11-15 21:52:24,857 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.33.3",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_auto.py:535] 2023-11-15 21:52:34,873 >> Could not locate the tokenizer configuration file, will try to use the model config instead.
[INFO|configuration_utils.py:715] 2023-11-15 21:52:44,890 >> loading configuration file config.json from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/config.json
[INFO|configuration_utils.py:775] 2023-11-15 21:52:44,891 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.33.3",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:1852] 2023-11-15 21:53:04,944 >> loading file vocab.json from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/vocab.json
[INFO|tokenization_utils_base.py:1852] 2023-11-15 21:53:04,945 >> loading file merges.txt from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/merges.txt
[INFO|tokenization_utils_base.py:1852] 2023-11-15 21:53:04,945 >> loading file tokenizer.json from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/tokenizer.json
[INFO|tokenization_utils_base.py:1852] 2023-11-15 21:53:04,945 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1852] 2023-11-15 21:53:04,945 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1852] 2023-11-15 21:53:04,946 >> loading file tokenizer_config.json from cache at None
[INFO|configuration_utils.py:715] 2023-11-15 21:53:04,947 >> loading configuration file config.json from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/config.json
[INFO|configuration_utils.py:775] 2023-11-15 21:53:04,947 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.33.3",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:2862] 2023-11-15 21:53:25,126 >> loading weights file model.safetensors from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/model.safetensors
[INFO|modeling_utils.py:3638] 2023-11-15 21:53:25,851 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3650] 2023-11-15 21:53:25,853 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
trainable params: 1,284,867 || all params: 125,830,662 || trainable%: 1.0211080348603745
model_max_length: 512
Running tokenizer on dataset:   0%|          | 0/8816 [00:00<?, ? examples/s]Running tokenizer on dataset:  34%|â–ˆâ–ˆâ–ˆâ–      | 3000/8816 [00:00<00:00, 19853.28 examples/s]Running tokenizer on dataset:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 7000/8816 [00:00<00:00, 20581.43 examples/s]Running tokenizer on dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8816/8816 [00:00<00:00, 20500.36 examples/s]
Running tokenizer on dataset:   0%|          | 0/2204 [00:00<?, ? examples/s]Running tokenizer on dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2204/2204 [00:00<00:00, 21845.64 examples/s]
11/15/2023 21:53:26 - INFO - __main__ - Sample 3485 of the training set: {'text': 'ERGMs are particularly useful for testing hypotheses about network relations, and they have started to be applied more widely in public health [27].', 'label': 0, 'input_ids': [0, 39042, 13123, 32, 1605, 5616, 13, 3044, 44850, 59, 1546, 3115, 6, 8, 51, 33, 554, 7, 28, 5049, 55, 3924, 11, 285, 474, 646, 2518, 8174, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
11/15/2023 21:53:26 - INFO - __main__ - Sample 5176 of the training set: {'text': 'Consistent with previous results (Koroch et al. 2002; Liu et al. 2002; Staniszewska et al. 2003; Washida et al. 2004), the addition of IBA at optimum levels enhanced the growth of HRC of Echinacea but had no effect on the production of secondary metabolites.', 'label': 2, 'input_ids': [0, 24514, 21464, 19, 986, 775, 36, 530, 368, 4306, 4400, 1076, 4, 5241, 131, 13768, 4400, 1076, 4, 5241, 131, 8995, 354, 329, 10269, 2348, 4400, 1076, 4, 4999, 131, 13852, 4347, 4400, 1076, 4, 4482, 238, 5, 1285, 9, 38, 3813, 23, 33771, 1389, 9094, 5, 434, 9, 43204, 9, 381, 16682, 38937, 53, 56, 117, 1683, 15, 5, 931, 9, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.
11/15/2023 21:53:26 - INFO - __main__ - Sample 8092 of the training set: {'text': 'Syllables with a voiced onset developed a low tone, and those with a voiceless initial induced a high tone, resulting in a six-way tonal contrast (2 pitch heights x 3 contours).1\n(Kang 2014, Kim 2000, Oh 2011, Silva 2006, Wright 2007).', 'label': 0, 'input_ids': [0, 35615, 890, 6058, 19, 10, 12559, 23808, 2226, 10, 614, 6328, 6, 8, 167, 19, 10, 30118, 13802, 2557, 26914, 10, 239, 6328, 6, 5203, 11, 10, 411, 12, 1970, 4866, 337, 5709, 36, 176, 3242, 16889, 3023, 155, 8541, 5634, 322, 134, 50118, 1640, 530, 1097, 777, 6, 1636, 3788, 6, 5534, 1466, 6, 9392, 3503, 6, 5825, 3010, 322, 2, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]}.
11/15/2023 21:53:26 - WARNING - accelerate.utils.other - Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[INFO|trainer.py:755] 2023-11-15 21:53:27,732 >> The following columns in the training set don't have a corresponding argument in `PeftModelForTokenClassification.forward` and have been ignored: text. If text are not expected by `PeftModelForTokenClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:1715] 2023-11-15 21:53:27,749 >> ***** Running training *****
[INFO|trainer.py:1716] 2023-11-15 21:53:27,750 >>   Num examples = 8,816
[INFO|trainer.py:1717] 2023-11-15 21:53:27,750 >>   Num Epochs = 5
[INFO|trainer.py:1718] 2023-11-15 21:53:27,750 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1721] 2023-11-15 21:53:27,751 >>   Total train batch size (w. parallel, distributed & accumulation) = 32
[INFO|trainer.py:1722] 2023-11-15 21:53:27,751 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:1723] 2023-11-15 21:53:27,751 >>   Total optimization steps = 1,380
[INFO|trainer.py:1724] 2023-11-15 21:53:27,752 >>   Number of trainable parameters = 1,284,867
[INFO|integration_utils.py:716] 2023-11-15 21:53:27,753 >> Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"
  0%|          | 0/1380 [00:00<?, ?it/s]  0%|          | 1/1380 [00:01<23:15,  1.01s/it]  0%|          | 3/1380 [00:01<07:12,  3.19it/s]  0%|          | 5/1380 [00:01<04:18,  5.32it/s]  1%|          | 7/1380 [00:01<03:09,  7.26it/s]  1%|          | 9/1380 [00:01<02:33,  8.93it/s]  1%|          | 11/1380 [00:01<02:13, 10.29it/s]  1%|          | 13/1380 [00:01<02:00, 11.35it/s]  1%|          | 15/1380 [00:01<01:52, 12.16it/s]  1%|          | 17/1380 [00:02<01:46, 12.78it/s]  1%|â–         | 19/1380 [00:02<01:42, 13.22it/s]  2%|â–         | 21/1380 [00:02<01:40, 13.54it/s]  2%|â–         | 23/1380 [00:02<01:38, 13.77it/s]  2%|â–         | 25/1380 [00:02<01:37, 13.94it/s]  2%|â–         | 27/1380 [00:02<01:36, 14.06it/s]  2%|â–         | 29/1380 [00:02<01:35, 14.15it/s]  2%|â–         | 31/1380 [00:03<01:34, 14.21it/s]  2%|â–         | 33/1380 [00:03<01:34, 14.25it/s]  3%|â–Ž         | 35/1380 [00:03<01:34, 14.29it/s]  3%|â–Ž         | 37/1380 [00:03<01:33, 14.32it/s]  3%|â–Ž         | 39/1380 [00:03<01:33, 14.34it/s]  3%|â–Ž         | 41/1380 [00:03<01:33, 14.36it/s]  3%|â–Ž         | 43/1380 [00:03<01:33, 14.36it/s]  3%|â–Ž         | 45/1380 [00:04<01:33, 14.34it/s]  3%|â–Ž         | 47/1380 [00:04<01:33, 14.32it/s]  4%|â–Ž         | 49/1380 [00:04<01:32, 14.32it/s]  4%|â–Ž         | 51/1380 [00:04<01:32, 14.32it/s]  4%|â–         | 53/1380 [00:04<01:32, 14.32it/s]  4%|â–         | 55/1380 [00:04<01:32, 14.31it/s]  4%|â–         | 57/1380 [00:04<01:32, 14.30it/s]  4%|â–         | 59/1380 [00:05<01:32, 14.30it/s]  4%|â–         | 61/1380 [00:05<01:32, 14.28it/s]  5%|â–         | 63/1380 [00:05<01:32, 14.28it/s]  5%|â–         | 65/1380 [00:05<01:32, 14.28it/s]  5%|â–         | 67/1380 [00:05<01:31, 14.29it/s]  5%|â–Œ         | 69/1380 [00:05<01:31, 14.28it/s]  5%|â–Œ         | 71/1380 [00:05<01:31, 14.28it/s]  5%|â–Œ         | 73/1380 [00:06<01:31, 14.29it/s]  5%|â–Œ         | 75/1380 [00:06<01:31, 14.30it/s]  6%|â–Œ         | 77/1380 [00:06<01:31, 14.30it/s]  6%|â–Œ         | 79/1380 [00:06<01:31, 14.29it/s]  6%|â–Œ         | 81/1380 [00:06<01:30, 14.29it/s]  6%|â–Œ         | 83/1380 [00:06<01:30, 14.30it/s]  6%|â–Œ         | 85/1380 [00:06<01:30, 14.29it/s]  6%|â–‹         | 87/1380 [00:07<01:30, 14.29it/s]  6%|â–‹         | 89/1380 [00:07<01:30, 14.29it/s]  7%|â–‹         | 91/1380 [00:07<01:30, 14.28it/s]  7%|â–‹         | 93/1380 [00:07<01:30, 14.29it/s]  7%|â–‹         | 95/1380 [00:07<01:29, 14.29it/s]  7%|â–‹         | 97/1380 [00:07<01:29, 14.27it/s]  7%|â–‹         | 99/1380 [00:07<01:29, 14.27it/s]  7%|â–‹         | 101/1380 [00:07<01:29, 14.29it/s]  7%|â–‹         | 103/1380 [00:08<01:29, 14.30it/s]  8%|â–Š         | 105/1380 [00:08<01:29, 14.29it/s]  8%|â–Š         | 107/1380 [00:08<01:29, 14.27it/s]  8%|â–Š         | 109/1380 [00:08<01:28, 14.29it/s]  8%|â–Š         | 111/1380 [00:08<01:28, 14.29it/s]  8%|â–Š         | 113/1380 [00:08<01:28, 14.29it/s]  8%|â–Š         | 115/1380 [00:08<01:28, 14.29it/s]  8%|â–Š         | 117/1380 [00:09<01:28, 14.29it/s]  9%|â–Š         | 119/1380 [00:09<01:28, 14.29it/s]  9%|â–‰         | 121/1380 [00:09<01:28, 14.28it/s]  9%|â–‰         | 123/1380 [00:09<01:27, 14.30it/s]  9%|â–‰         | 125/1380 [00:09<01:27, 14.29it/s]  9%|â–‰         | 127/1380 [00:09<01:27, 14.33it/s]  9%|â–‰         | 129/1380 [00:09<01:27, 14.34it/s]  9%|â–‰         | 131/1380 [00:10<01:26, 14.36it/s] 10%|â–‰         | 133/1380 [00:10<01:26, 14.36it/s] 10%|â–‰         | 135/1380 [00:10<01:26, 14.38it/s] 10%|â–‰         | 137/1380 [00:10<01:26, 14.40it/s] 10%|â–ˆ         | 139/1380 [00:10<01:26, 14.41it/s] 10%|â–ˆ         | 141/1380 [00:10<01:25, 14.42it/s] 10%|â–ˆ         | 143/1380 [00:10<01:25, 14.41it/s] 11%|â–ˆ         | 145/1380 [00:11<01:25, 14.40it/s] 11%|â–ˆ         | 147/1380 [00:11<01:25, 14.40it/s] 11%|â–ˆ         | 149/1380 [00:11<01:25, 14.40it/s] 11%|â–ˆ         | 151/1380 [00:11<01:25, 14.40it/s] 11%|â–ˆ         | 153/1380 [00:11<01:25, 14.40it/s] 11%|â–ˆ         | 155/1380 [00:11<01:24, 14.42it/s] 11%|â–ˆâ–        | 157/1380 [00:11<01:24, 14.43it/s] 12%|â–ˆâ–        | 159/1380 [00:12<01:24, 14.44it/s] 12%|â–ˆâ–        | 161/1380 [00:12<01:24, 14.41it/s] 12%|â–ˆâ–        | 163/1380 [00:12<01:24, 14.40it/s] 12%|â–ˆâ–        | 165/1380 [00:12<01:24, 14.39it/s] 12%|â–ˆâ–        | 167/1380 [00:12<01:24, 14.39it/s] 12%|â–ˆâ–        | 169/1380 [00:12<01:24, 14.39it/s] 12%|â–ˆâ–        | 171/1380 [00:12<01:24, 14.39it/s] 13%|â–ˆâ–Ž        | 173/1380 [00:13<01:23, 14.40it/s] 13%|â–ˆâ–Ž        | 175/1380 [00:13<01:23, 14.42it/s] 13%|â–ˆâ–Ž        | 177/1380 [00:13<01:23, 14.41it/s] 13%|â–ˆâ–Ž        | 179/1380 [00:13<01:23, 14.40it/s] 13%|â–ˆâ–Ž        | 181/1380 [00:13<01:23, 14.39it/s] 13%|â–ˆâ–Ž        | 183/1380 [00:13<01:23, 14.38it/s] 13%|â–ˆâ–Ž        | 185/1380 [00:13<01:23, 14.35it/s] 14%|â–ˆâ–Ž        | 187/1380 [00:13<01:23, 14.35it/s] 14%|â–ˆâ–Ž        | 189/1380 [00:14<01:22, 14.36it/s] 14%|â–ˆâ–        | 191/1380 [00:14<01:22, 14.37it/s] 14%|â–ˆâ–        | 193/1380 [00:14<01:22, 14.34it/s] 14%|â–ˆâ–        | 195/1380 [00:14<01:22, 14.36it/s] 14%|â–ˆâ–        | 197/1380 [00:14<01:22, 14.38it/s] 14%|â–ˆâ–        | 199/1380 [00:14<01:22, 14.39it/s] 15%|â–ˆâ–        | 201/1380 [00:14<01:21, 14.42it/s] 15%|â–ˆâ–        | 203/1380 [00:15<01:21, 14.41it/s] 15%|â–ˆâ–        | 205/1380 [00:15<01:21, 14.40it/s] 15%|â–ˆâ–Œ        | 207/1380 [00:15<01:21, 14.38it/s] 15%|â–ˆâ–Œ        | 209/1380 [00:15<01:21, 14.37it/s] 15%|â–ˆâ–Œ        | 211/1380 [00:15<01:21, 14.37it/s] 15%|â–ˆâ–Œ        | 213/1380 [00:15<01:21, 14.37it/s] 16%|â–ˆâ–Œ        | 215/1380 [00:15<01:21, 14.38it/s] 16%|â–ˆâ–Œ        | 217/1380 [00:16<01:20, 14.39it/s] 16%|â–ˆâ–Œ        | 219/1380 [00:16<01:20, 14.39it/s] 16%|â–ˆâ–Œ        | 221/1380 [00:16<01:20, 14.39it/s] 16%|â–ˆâ–Œ        | 223/1380 [00:16<01:20, 14.40it/s] 16%|â–ˆâ–‹        | 225/1380 [00:16<01:20, 14.39it/s] 16%|â–ˆâ–‹        | 227/1380 [00:16<01:20, 14.37it/s] 17%|â–ˆâ–‹        | 229/1380 [00:16<01:20, 14.35it/s] 17%|â–ˆâ–‹        | 231/1380 [00:17<01:20, 14.35it/s] 17%|â–ˆâ–‹        | 233/1380 [00:17<01:19, 14.35it/s] 17%|â–ˆâ–‹        | 235/1380 [00:17<01:19, 14.34it/s] 17%|â–ˆâ–‹        | 237/1380 [00:17<01:19, 14.35it/s] 17%|â–ˆâ–‹        | 239/1380 [00:17<01:19, 14.36it/s] 17%|â–ˆâ–‹        | 241/1380 [00:17<01:19, 14.36it/s] 18%|â–ˆâ–Š        | 243/1380 [00:17<01:19, 14.37it/s] 18%|â–ˆâ–Š        | 245/1380 [00:18<01:18, 14.38it/s] 18%|â–ˆâ–Š        | 247/1380 [00:18<01:18, 14.40it/s] 18%|â–ˆâ–Š        | 249/1380 [00:18<01:18, 14.42it/s] 18%|â–ˆâ–Š        | 251/1380 [00:18<01:18, 14.42it/s] 18%|â–ˆâ–Š        | 253/1380 [00:18<01:18, 14.39it/s] 18%|â–ˆâ–Š        | 255/1380 [00:18<01:18, 14.36it/s] 19%|â–ˆâ–Š        | 257/1380 [00:18<01:18, 14.35it/s] 19%|â–ˆâ–‰        | 259/1380 [00:18<01:18, 14.35it/s] 19%|â–ˆâ–‰        | 261/1380 [00:19<01:17, 14.35it/s] 19%|â–ˆâ–‰        | 263/1380 [00:19<01:17, 14.36it/s] 19%|â–ˆâ–‰        | 265/1380 [00:19<01:17, 14.36it/s] 19%|â–ˆâ–‰        | 267/1380 [00:19<01:17, 14.36it/s] 19%|â–ˆâ–‰        | 269/1380 [00:19<01:17, 14.37it/s] 20%|â–ˆâ–‰        | 271/1380 [00:19<01:17, 14.38it/s] 20%|â–ˆâ–‰        | 273/1380 [00:19<01:16, 14.39it/s] 20%|â–ˆâ–‰        | 275/1380 [00:20<01:16, 14.40it/s]                                                   20%|â–ˆâ–ˆ        | 276/1380 [00:20<01:16, 14.40it/s][INFO|trainer.py:755] 2023-11-15 21:53:47,903 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModelForTokenClassification.forward` and have been ignored: text. If text are not expected by `PeftModelForTokenClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 21:53:47,905 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 21:53:47,905 >>   Num examples = 2204
[INFO|trainer.py:3134] 2023-11-15 21:53:47,905 >>   Batch size = 8
{'loss': 0.522, 'learning_rate': 0.0004, 'epoch': 1.0}

  0%|          | 0/276 [00:00<?, ?it/s][A
  4%|â–         | 12/276 [00:00<00:02, 118.27it/s][A
  9%|â–Š         | 24/276 [00:00<00:02, 111.57it/s][A
 13%|â–ˆâ–Ž        | 36/276 [00:00<00:02, 109.69it/s][A
 17%|â–ˆâ–‹        | 47/276 [00:00<00:02, 108.75it/s][A
 21%|â–ˆâ–ˆ        | 58/276 [00:00<00:02, 108.12it/s][A
 25%|â–ˆâ–ˆâ–Œ       | 69/276 [00:00<00:01, 107.71it/s][A
 29%|â–ˆâ–ˆâ–‰       | 80/276 [00:00<00:01, 107.51it/s][A
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 91/276 [00:00<00:01, 107.27it/s][A
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 102/276 [00:00<00:01, 107.13it/s][A
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 113/276 [00:01<00:01, 107.12it/s][A
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 124/276 [00:01<00:01, 107.12it/s][A
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 135/276 [00:01<00:01, 107.11it/s][A
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 146/276 [00:01<00:01, 107.05it/s][A
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 157/276 [00:01<00:01, 107.02it/s][A
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 168/276 [00:01<00:01, 107.07it/s][A
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 179/276 [00:01<00:00, 107.09it/s][A
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 190/276 [00:01<00:00, 106.98it/s][A
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 201/276 [00:01<00:00, 106.96it/s][A
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 212/276 [00:01<00:00, 106.95it/s][A
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 223/276 [00:02<00:00, 106.93it/s][A
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 234/276 [00:02<00:00, 106.95it/s][A
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 245/276 [00:02<00:00, 106.95it/s][A
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 256/276 [00:02<00:00, 106.91it/s][A
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 267/276 [00:02<00:00, 106.89it/s][A                                                  
                                                  [A 20%|â–ˆâ–ˆ        | 276/1380 [00:22<01:16, 14.40it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 276/276 [00:02<00:00, 106.89it/s][A
                                                  [A 20%|â–ˆâ–ˆ        | 277/1380 [00:22<08:27,  2.17it/s] 20%|â–ˆâ–ˆ        | 279/1380 [00:22<06:17,  2.92it/s] 20%|â–ˆâ–ˆ        | 281/1380 [00:23<04:46,  3.83it/s] 21%|â–ˆâ–ˆ        | 283/1380 [00:23<03:43,  4.92it/s] 21%|â–ˆâ–ˆ        | 285/1380 [00:23<02:58,  6.12it/s] 21%|â–ˆâ–ˆ        | 287/1380 [00:23<02:27,  7.39it/s] 21%|â–ˆâ–ˆ        | 289/1380 [00:23<02:06,  8.64it/s] 21%|â–ˆâ–ˆ        | 291/1380 [00:23<01:51,  9.80it/s] 21%|â–ˆâ–ˆ        | 293/1380 [00:23<01:40, 10.82it/s] 21%|â–ˆâ–ˆâ–       | 295/1380 [00:24<01:32, 11.67it/s] 22%|â–ˆâ–ˆâ–       | 297/1380 [00:24<01:27, 12.35it/s] 22%|â–ˆâ–ˆâ–       | 299/1380 [00:24<01:23, 12.88it/s] 22%|â–ˆâ–ˆâ–       | 301/1380 [00:24<01:21, 13.27it/s] 22%|â–ˆâ–ˆâ–       | 303/1380 [00:24<01:19, 13.56it/s] 22%|â–ˆâ–ˆâ–       | 305/1380 [00:24<01:18, 13.77it/s] 22%|â–ˆâ–ˆâ–       | 307/1380 [00:24<01:17, 13.92it/s] 22%|â–ˆâ–ˆâ–       | 309/1380 [00:25<01:16, 14.03it/s] 23%|â–ˆâ–ˆâ–Ž       | 311/1380 [00:25<01:15, 14.11it/s] 23%|â–ˆâ–ˆâ–Ž       | 313/1380 [00:25<01:15, 14.17it/s] 23%|â–ˆâ–ˆâ–Ž       | 315/1380 [00:25<01:15, 14.20it/s] 23%|â–ˆâ–ˆâ–Ž       | 317/1380 [00:25<01:14, 14.22it/s] 23%|â–ˆâ–ˆâ–Ž       | 319/1380 [00:25<01:14, 14.24it/s] 23%|â–ˆâ–ˆâ–Ž       | 321/1380 [00:25<01:14, 14.26it/s] 23%|â–ˆâ–ˆâ–Ž       | 323/1380 [00:26<01:14, 14.26it/s] 24%|â–ˆâ–ˆâ–Ž       | 325/1380 [00:26<01:13, 14.28it/s] 24%|â–ˆâ–ˆâ–Ž       | 327/1380 [00:26<01:13, 14.28it/s] 24%|â–ˆâ–ˆâ–       | 329/1380 [00:26<01:13, 14.28it/s] 24%|â–ˆâ–ˆâ–       | 331/1380 [00:26<01:13, 14.29it/s] 24%|â–ˆâ–ˆâ–       | 333/1380 [00:26<01:13, 14.29it/s] 24%|â–ˆâ–ˆâ–       | 335/1380 [00:26<01:13, 14.29it/s] 24%|â–ˆâ–ˆâ–       | 337/1380 [00:27<01:12, 14.29it/s] 25%|â–ˆâ–ˆâ–       | 339/1380 [00:27<01:12, 14.29it/s] 25%|â–ˆâ–ˆâ–       | 341/1380 [00:27<01:12, 14.29it/s] 25%|â–ˆâ–ˆâ–       | 343/1380 [00:27<01:12, 14.29it/s] 25%|â–ˆâ–ˆâ–Œ       | 345/1380 [00:27<01:12, 14.29it/s] 25%|â–ˆâ–ˆâ–Œ       | 347/1380 [00:27<01:12, 14.29it/s] 25%|â–ˆâ–ˆâ–Œ       | 349/1380 [00:27<01:12, 14.29it/s] 25%|â–ˆâ–ˆâ–Œ       | 351/1380 [00:28<01:12, 14.28it/s] 26%|â–ˆâ–ˆâ–Œ       | 353/1380 [00:28<01:11, 14.29it/s] 26%|â–ˆâ–ˆâ–Œ       | 355/1380 [00:28<01:11, 14.29it/s] 26%|â–ˆâ–ˆâ–Œ       | 357/1380 [00:28<01:11, 14.29it/s] 26%|â–ˆâ–ˆâ–Œ       | 359/1380 [00:28<01:11, 14.29it/s] 26%|â–ˆâ–ˆâ–Œ       | 361/1380 [00:28<01:11, 14.29it/s] 26%|â–ˆâ–ˆâ–‹       | 363/1380 [00:28<01:11, 14.29it/s] 26%|â–ˆâ–ˆâ–‹       | 365/1380 [00:28<01:11, 14.29it/s] 27%|â–ˆâ–ˆâ–‹       | 367/1380 [00:29<01:10, 14.27it/s] 27%|â–ˆâ–ˆâ–‹       | 369/1380 [00:29<01:10, 14.28it/s] 27%|â–ˆâ–ˆâ–‹       | 371/1380 [00:29<01:10, 14.30it/s] 27%|â–ˆâ–ˆâ–‹       | 373/1380 [00:29<01:10, 14.29it/s] 27%|â–ˆâ–ˆâ–‹       | 375/1380 [00:29<01:10, 14.29it/s] 27%|â–ˆâ–ˆâ–‹       | 377/1380 [00:29<01:10, 14.30it/s] 27%|â–ˆâ–ˆâ–‹       | 379/1380 [00:29<01:10, 14.30it/s] 28%|â–ˆâ–ˆâ–Š       | 381/1380 [00:30<01:09, 14.29it/s] 28%|â–ˆâ–ˆâ–Š       | 383/1380 [00:30<01:09, 14.29it/s] 28%|â–ˆâ–ˆâ–Š       | 385/1380 [00:30<01:09, 14.29it/s] 28%|â–ˆâ–ˆâ–Š       | 387/1380 [00:30<01:09, 14.29it/s] 28%|â–ˆâ–ˆâ–Š       | 389/1380 [00:30<01:09, 14.29it/s] 28%|â–ˆâ–ˆâ–Š       | 391/1380 [00:30<01:09, 14.29it/s] 28%|â–ˆâ–ˆâ–Š       | 393/1380 [00:30<01:09, 14.29it/s] 29%|â–ˆâ–ˆâ–Š       | 395/1380 [00:31<01:08, 14.29it/s] 29%|â–ˆâ–ˆâ–‰       | 397/1380 [00:31<01:08, 14.28it/s] 29%|â–ˆâ–ˆâ–‰       | 399/1380 [00:31<01:08, 14.29it/s] 29%|â–ˆâ–ˆâ–‰       | 401/1380 [00:31<01:08, 14.30it/s] 29%|â–ˆâ–ˆâ–‰       | 403/1380 [00:31<01:08, 14.30it/s] 29%|â–ˆâ–ˆâ–‰       | 405/1380 [00:31<01:08, 14.30it/s] 29%|â–ˆâ–ˆâ–‰       | 407/1380 [00:31<01:08, 14.29it/s] 30%|â–ˆâ–ˆâ–‰       | 409/1380 [00:32<01:07, 14.29it/s] 30%|â–ˆâ–ˆâ–‰       | 411/1380 [00:32<01:07, 14.29it/s] 30%|â–ˆâ–ˆâ–‰       | 413/1380 [00:32<01:07, 14.27it/s] 30%|â–ˆâ–ˆâ–ˆ       | 415/1380 [00:32<01:07, 14.26it/s] 30%|â–ˆâ–ˆâ–ˆ       | 417/1380 [00:32<01:07, 14.28it/s] 30%|â–ˆâ–ˆâ–ˆ       | 419/1380 [00:32<01:07, 14.29it/s] 31%|â–ˆâ–ˆâ–ˆ       | 421/1380 [00:32<01:07, 14.29it/s] 31%|â–ˆâ–ˆâ–ˆ       | 423/1380 [00:33<01:06, 14.31it/s] 31%|â–ˆâ–ˆâ–ˆ       | 425/1380 [00:33<01:06, 14.30it/s] 31%|â–ˆâ–ˆâ–ˆ       | 427/1380 [00:33<01:06, 14.30it/s] 31%|â–ˆâ–ˆâ–ˆ       | 429/1380 [00:33<01:06, 14.29it/s] 31%|â–ˆâ–ˆâ–ˆ       | 431/1380 [00:33<01:06, 14.29it/s] 31%|â–ˆâ–ˆâ–ˆâ–      | 433/1380 [00:33<01:06, 14.29it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 435/1380 [00:33<01:06, 14.29it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 437/1380 [00:34<01:05, 14.29it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 439/1380 [00:34<01:05, 14.29it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 441/1380 [00:34<01:05, 14.29it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 443/1380 [00:34<01:05, 14.29it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 445/1380 [00:34<01:05, 14.29it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 447/1380 [00:34<01:05, 14.29it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 449/1380 [00:34<01:05, 14.29it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 451/1380 [00:35<01:05, 14.29it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 453/1380 [00:35<01:04, 14.29it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 455/1380 [00:35<01:04, 14.29it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 457/1380 [00:35<01:04, 14.29it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 459/1380 [00:35<01:04, 14.29it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 461/1380 [00:35<01:04, 14.29it/s] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 463/1380 [00:35<01:04, 14.28it/s] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 465/1380 [00:35<01:04, 14.29it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 467/1380 [00:36<01:03, 14.29it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 469/1380 [00:36<01:03, 14.29it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 471/1380 [00:36<01:03, 14.29it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 473/1380 [00:36<01:03, 14.29it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 475/1380 [00:36<01:03, 14.29it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 477/1380 [00:36<01:03, 14.29it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 479/1380 [00:36<01:03, 14.29it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 481/1380 [00:37<01:02, 14.29it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 483/1380 [00:37<01:02, 14.28it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 485/1380 [00:37<01:02, 14.29it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 487/1380 [00:37<01:02, 14.29it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 489/1380 [00:37<01:02, 14.29it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 491/1380 [00:37<01:02, 14.29it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 493/1380 [00:37<01:02, 14.29it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 495/1380 [00:38<01:01, 14.29it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 497/1380 [00:38<01:01, 14.29it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 499/1380 [00:38<01:01, 14.29it/s] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 501/1380 [00:38<01:01, 14.30it/s] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 503/1380 [00:38<01:01, 14.29it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 505/1380 [00:38<01:01, 14.29it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 507/1380 [00:38<01:01, 14.29it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 509/1380 [00:39<01:00, 14.29it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 511/1380 [00:39<01:00, 14.29it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 513/1380 [00:39<01:00, 14.29it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 515/1380 [00:39<01:00, 14.29it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 517/1380 [00:39<01:00, 14.29it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 519/1380 [00:39<01:00, 14.29it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 521/1380 [00:39<01:00, 14.29it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 523/1380 [00:40<00:59, 14.29it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 525/1380 [00:40<00:59, 14.29it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 527/1380 [00:40<00:59, 14.29it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 529/1380 [00:40<00:59, 14.29it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 531/1380 [00:40<00:59, 14.29it/s] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 533/1380 [00:40<00:59, 14.28it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 535/1380 [00:40<00:59, 14.29it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 537/1380 [00:41<00:59, 14.29it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 539/1380 [00:41<00:58, 14.29it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 541/1380 [00:41<00:58, 14.29it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 543/1380 [00:41<00:58, 14.29it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 545/1380 [00:41<00:58, 14.29it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 547/1380 [00:41<00:58, 14.29it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 549/1380 [00:41<00:58, 14.30it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 551/1380 [00:42<00:57, 14.30it/s]                                                   40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 552/1380 [00:42<00:57, 14.30it/s][INFO|trainer.py:755] 2023-11-15 21:54:09,813 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModelForTokenClassification.forward` and have been ignored: text. If text are not expected by `PeftModelForTokenClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 21:54:09,815 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 21:54:09,815 >>   Num examples = 2204
[INFO|trainer.py:3134] 2023-11-15 21:54:09,815 >>   Batch size = 8
{'eval_loss': 0.4375453293323517, 'eval_accuracy': 0.8366606170598911, 'eval_micro_f1': 0.8366606170598911, 'eval_macro_f1': 0.8189734582500278, 'eval_runtime': 2.6222, 'eval_samples_per_second': 840.517, 'eval_steps_per_second': 105.255, 'epoch': 1.0}
{'loss': 0.3839, 'learning_rate': 0.0003, 'epoch': 2.0}

  0%|          | 0/276 [00:00<?, ?it/s][A
  4%|â–         | 12/276 [00:00<00:02, 117.58it/s][A
  9%|â–Š         | 24/276 [00:00<00:02, 110.93it/s][A
 13%|â–ˆâ–Ž        | 36/276 [00:00<00:02, 109.03it/s][A
 17%|â–ˆâ–‹        | 47/276 [00:00<00:02, 108.08it/s][A
 21%|â–ˆâ–ˆ        | 58/276 [00:00<00:02, 107.49it/s][A
 25%|â–ˆâ–ˆâ–Œ       | 69/276 [00:00<00:01, 107.19it/s][A
 29%|â–ˆâ–ˆâ–‰       | 80/276 [00:00<00:01, 107.02it/s][A
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 91/276 [00:00<00:01, 106.68it/s][A
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 102/276 [00:00<00:01, 106.66it/s][A
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 113/276 [00:01<00:01, 106.56it/s][A
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 124/276 [00:01<00:01, 106.53it/s][A
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 135/276 [00:01<00:01, 106.50it/s][A
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 146/276 [00:01<00:01, 106.59it/s][A
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 157/276 [00:01<00:01, 106.60it/s][A
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 168/276 [00:01<00:01, 106.52it/s][A
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 179/276 [00:01<00:00, 106.56it/s][A
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 190/276 [00:01<00:00, 106.53it/s][A
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 201/276 [00:01<00:00, 106.40it/s][A
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 212/276 [00:01<00:00, 106.44it/s][A
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 223/276 [00:02<00:00, 106.52it/s][A
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 234/276 [00:02<00:00, 106.46it/s][A
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 245/276 [00:02<00:00, 106.55it/s][A
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 256/276 [00:02<00:00, 106.52it/s][A
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 267/276 [00:02<00:00, 106.48it/s][A                                                  
                                                  [A 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 552/1380 [00:44<00:57, 14.30it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 276/276 [00:02<00:00, 106.48it/s][A
                                                  [A 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 553/1380 [00:44<06:22,  2.16it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 555/1380 [00:44<04:44,  2.90it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 557/1380 [00:45<03:36,  3.81it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 559/1380 [00:45<02:48,  4.88it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 561/1380 [00:45<02:14,  6.07it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 563/1380 [00:45<01:51,  7.34it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 565/1380 [00:45<01:34,  8.59it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 567/1380 [00:45<01:23,  9.75it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 569/1380 [00:45<01:15, 10.78it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 571/1380 [00:46<01:09, 11.61it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 573/1380 [00:46<01:06, 12.21it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 575/1380 [00:46<01:03, 12.69it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 577/1380 [00:46<01:01, 13.13it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 579/1380 [00:46<00:59, 13.39it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 581/1380 [00:46<00:58, 13.60it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 583/1380 [00:46<00:57, 13.79it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 585/1380 [00:47<00:57, 13.92it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 587/1380 [00:47<00:56, 13.99it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 589/1380 [00:47<00:56, 14.02it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 591/1380 [00:47<00:56, 14.08it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 593/1380 [00:47<00:55, 14.13it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 595/1380 [00:47<00:55, 14.16it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 597/1380 [00:47<00:55, 14.19it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 599/1380 [00:48<00:55, 14.19it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 601/1380 [00:48<00:54, 14.22it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 603/1380 [00:48<00:54, 14.23it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 605/1380 [00:48<00:54, 14.24it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 607/1380 [00:48<00:54, 14.22it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 609/1380 [00:48<00:54, 14.18it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 611/1380 [00:48<00:54, 14.18it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 613/1380 [00:49<00:54, 14.10it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 615/1380 [00:49<00:54, 14.11it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 617/1380 [00:49<00:54, 13.96it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 619/1380 [00:49<00:54, 14.01it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 621/1380 [00:49<00:53, 14.10it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 623/1380 [00:49<00:54, 13.94it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 625/1380 [00:49<00:53, 14.00it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 627/1380 [00:50<00:53, 14.05it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 629/1380 [00:50<00:53, 14.13it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 631/1380 [00:50<00:53, 14.01it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 633/1380 [00:50<00:53, 14.08it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 635/1380 [00:50<00:52, 14.11it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 637/1380 [00:50<00:53, 13.93it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 639/1380 [00:50<00:53, 13.91it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 641/1380 [00:51<00:52, 14.01it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 643/1380 [00:51<00:52, 14.10it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 645/1380 [00:51<00:51, 14.14it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 647/1380 [00:51<00:51, 14.17it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 649/1380 [00:51<00:51, 14.20it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 651/1380 [00:51<00:51, 14.22it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 653/1380 [00:51<00:51, 14.22it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 655/1380 [00:51<00:50, 14.24it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 657/1380 [00:52<00:50, 14.25it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 659/1380 [00:52<00:50, 14.26it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 661/1380 [00:52<00:50, 14.26it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 663/1380 [00:52<00:50, 14.27it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 665/1380 [00:52<00:50, 14.23it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 667/1380 [00:52<00:50, 14.23it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 669/1380 [00:52<00:49, 14.24it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 671/1380 [00:53<00:49, 14.21it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 673/1380 [00:53<00:49, 14.19it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 675/1380 [00:53<00:49, 14.19it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 677/1380 [00:53<00:49, 14.20it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 679/1380 [00:53<00:49, 14.21it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 681/1380 [00:53<00:49, 14.22it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 683/1380 [00:53<00:48, 14.24it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 685/1380 [00:54<00:48, 14.24it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 687/1380 [00:54<00:48, 14.23it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 689/1380 [00:54<00:48, 14.23it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 691/1380 [00:54<00:48, 14.25it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 693/1380 [00:54<00:48, 14.23it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 695/1380 [00:54<00:48, 14.22it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 697/1380 [00:54<00:48, 14.19it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 699/1380 [00:55<00:47, 14.20it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 701/1380 [00:55<00:48, 14.13it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 703/1380 [00:55<00:47, 14.15it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 705/1380 [00:55<00:47, 14.19it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 707/1380 [00:55<00:47, 14.20it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 709/1380 [00:55<00:47, 14.22it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 711/1380 [00:55<00:46, 14.24it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 713/1380 [00:56<00:46, 14.25it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 715/1380 [00:56<00:46, 14.26it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 717/1380 [00:56<00:46, 14.26it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 719/1380 [00:56<00:46, 14.27it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 721/1380 [00:56<00:46, 14.27it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 723/1380 [00:56<00:46, 14.28it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 725/1380 [00:56<00:45, 14.27it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 727/1380 [00:57<00:45, 14.25it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 729/1380 [00:57<00:45, 14.24it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 731/1380 [00:57<00:45, 14.25it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 733/1380 [00:57<00:45, 14.25it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 735/1380 [00:57<00:45, 14.25it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 737/1380 [00:57<00:45, 14.22it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 739/1380 [00:57<00:45, 14.22it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 741/1380 [00:58<00:44, 14.22it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 743/1380 [00:58<00:44, 14.21it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 745/1380 [00:58<00:44, 14.22it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 747/1380 [00:58<00:44, 14.21it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 749/1380 [00:58<00:44, 14.23it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 751/1380 [00:58<00:44, 14.24it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 753/1380 [00:58<00:44, 14.25it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 755/1380 [00:59<00:43, 14.25it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 757/1380 [00:59<00:43, 14.22it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 759/1380 [00:59<00:43, 14.23it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 761/1380 [00:59<00:43, 14.24it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 763/1380 [00:59<00:43, 14.23it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 765/1380 [00:59<00:43, 14.24it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 767/1380 [00:59<00:43, 14.18it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 769/1380 [00:59<00:43, 14.19it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 771/1380 [01:00<00:42, 14.20it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 773/1380 [01:00<00:42, 14.21it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 775/1380 [01:00<00:42, 14.22it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 777/1380 [01:00<00:42, 14.22it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 779/1380 [01:00<00:42, 14.24it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 781/1380 [01:00<00:42, 14.25it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 783/1380 [01:00<00:41, 14.26it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 785/1380 [01:01<00:41, 14.24it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 787/1380 [01:01<00:41, 14.24it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 789/1380 [01:01<00:41, 14.24it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 791/1380 [01:01<00:41, 14.26it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 793/1380 [01:01<00:41, 14.27it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 795/1380 [01:01<00:40, 14.27it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 797/1380 [01:01<00:40, 14.28it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 799/1380 [01:02<00:40, 14.25it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 801/1380 [01:02<00:40, 14.24it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 803/1380 [01:02<00:40, 14.25it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 805/1380 [01:02<00:40, 14.25it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 807/1380 [01:02<00:40, 14.24it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 809/1380 [01:02<00:40, 14.22it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 811/1380 [01:02<00:40, 14.22it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 813/1380 [01:03<00:39, 14.21it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 815/1380 [01:03<00:39, 14.22it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 817/1380 [01:03<00:39, 14.22it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 819/1380 [01:03<00:39, 14.21it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 821/1380 [01:03<00:39, 14.22it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 823/1380 [01:03<00:39, 14.22it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 825/1380 [01:03<00:39, 14.21it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 827/1380 [01:04<00:38, 14.23it/s]                                                   60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 828/1380 [01:04<00:38, 14.23it/s][INFO|trainer.py:755] 2023-11-15 21:54:31,870 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModelForTokenClassification.forward` and have been ignored: text. If text are not expected by `PeftModelForTokenClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 21:54:31,872 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 21:54:31,872 >>   Num examples = 2204
[INFO|trainer.py:3134] 2023-11-15 21:54:31,872 >>   Batch size = 8
{'eval_loss': 0.3739018142223358, 'eval_accuracy': 0.8620689655172413, 'eval_micro_f1': 0.8620689655172413, 'eval_macro_f1': 0.8442977186241274, 'eval_runtime': 2.636, 'eval_samples_per_second': 836.118, 'eval_steps_per_second': 104.704, 'epoch': 2.0}
{'loss': 0.3444, 'learning_rate': 0.0002, 'epoch': 3.0}

  0%|          | 0/276 [00:00<?, ?it/s][A
  4%|â–         | 12/276 [00:00<00:02, 116.86it/s][A
  9%|â–Š         | 24/276 [00:00<00:02, 109.92it/s][A
 13%|â–ˆâ–Ž        | 36/276 [00:00<00:02, 108.02it/s][A
 17%|â–ˆâ–‹        | 47/276 [00:00<00:02, 107.45it/s][A
 21%|â–ˆâ–ˆ        | 58/276 [00:00<00:02, 106.47it/s][A
 25%|â–ˆâ–ˆâ–Œ       | 69/276 [00:00<00:01, 106.41it/s][A
 29%|â–ˆâ–ˆâ–‰       | 80/276 [00:00<00:01, 106.30it/s][A
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 91/276 [00:00<00:01, 105.42it/s][A
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 102/276 [00:00<00:01, 105.77it/s][A
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 113/276 [00:01<00:01, 105.28it/s][A
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 124/276 [00:01<00:01, 105.58it/s][A
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 135/276 [00:01<00:01, 105.38it/s][A
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 146/276 [00:01<00:01, 105.58it/s][A
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 157/276 [00:01<00:01, 105.53it/s][A
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 168/276 [00:01<00:01, 105.66it/s][A
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 179/276 [00:01<00:00, 105.88it/s][A
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 190/276 [00:01<00:00, 105.97it/s][A
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 201/276 [00:01<00:00, 105.49it/s][A
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 212/276 [00:01<00:00, 105.74it/s][A
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 223/276 [00:02<00:00, 105.66it/s][A
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 234/276 [00:02<00:00, 105.35it/s][A
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 245/276 [00:02<00:00, 104.39it/s][A
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 256/276 [00:02<00:00, 104.93it/s][A
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 267/276 [00:02<00:00, 105.33it/s][A                                                  
                                                  [A 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 828/1380 [01:06<00:38, 14.23it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 276/276 [00:02<00:00, 105.33it/s][A
                                                  [A 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 829/1380 [01:06<04:17,  2.14it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 831/1380 [01:06<03:11,  2.87it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 833/1380 [01:07<02:24,  3.78it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 835/1380 [01:07<01:52,  4.85it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 837/1380 [01:07<01:29,  6.04it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 839/1380 [01:07<01:14,  7.30it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 841/1380 [01:07<01:03,  8.54it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 843/1380 [01:07<00:55,  9.69it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 845/1380 [01:07<00:50, 10.70it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 847/1380 [01:08<00:46, 11.55it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 849/1380 [01:08<00:43, 12.24it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 851/1380 [01:08<00:41, 12.74it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 853/1380 [01:08<00:40, 13.13it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 855/1380 [01:08<00:39, 13.42it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 857/1380 [01:08<00:38, 13.63it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 859/1380 [01:08<00:37, 13.79it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 861/1380 [01:09<00:37, 13.87it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 863/1380 [01:09<00:36, 13.98it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 865/1380 [01:09<00:36, 14.04it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 867/1380 [01:09<00:36, 14.10it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 869/1380 [01:09<00:36, 14.12it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 871/1380 [01:09<00:35, 14.15it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 873/1380 [01:09<00:35, 14.18it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 875/1380 [01:10<00:35, 14.13it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 877/1380 [01:10<00:35, 14.11it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 879/1380 [01:10<00:35, 14.14it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 881/1380 [01:10<00:35, 14.17it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 883/1380 [01:10<00:35, 14.19it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 885/1380 [01:10<00:34, 14.21it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 887/1380 [01:10<00:34, 14.21it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 889/1380 [01:11<00:34, 14.23it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 891/1380 [01:11<00:34, 14.24it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 893/1380 [01:11<00:34, 14.25it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 895/1380 [01:11<00:34, 14.22it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 897/1380 [01:11<00:33, 14.23it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 899/1380 [01:11<00:33, 14.20it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 901/1380 [01:11<00:33, 14.20it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 903/1380 [01:12<00:33, 14.20it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 905/1380 [01:12<00:33, 14.13it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 907/1380 [01:12<00:33, 14.18it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 909/1380 [01:12<00:33, 14.19it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 911/1380 [01:12<00:33, 14.19it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 913/1380 [01:12<00:32, 14.20it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 915/1380 [01:12<00:32, 14.19it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 917/1380 [01:13<00:32, 14.20it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 919/1380 [01:13<00:32, 14.20it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 921/1380 [01:13<00:32, 14.21it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 923/1380 [01:13<00:32, 14.20it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 925/1380 [01:13<00:32, 14.18it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 927/1380 [01:13<00:31, 14.19it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 929/1380 [01:13<00:31, 14.21it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 931/1380 [01:14<00:31, 14.22it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 933/1380 [01:14<00:31, 14.21it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 935/1380 [01:14<00:31, 14.20it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 937/1380 [01:14<00:31, 14.21it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 939/1380 [01:14<00:30, 14.23it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 941/1380 [01:14<00:30, 14.21it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 943/1380 [01:14<00:30, 14.19it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 945/1380 [01:15<00:30, 14.19it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 947/1380 [01:15<00:30, 14.19it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 949/1380 [01:15<00:30, 14.19it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 951/1380 [01:15<00:30, 14.20it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 953/1380 [01:15<00:30, 14.21it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 955/1380 [01:15<00:29, 14.18it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 957/1380 [01:15<00:29, 14.20it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 959/1380 [01:16<00:29, 14.20it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 961/1380 [01:16<00:29, 14.19it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 963/1380 [01:16<00:29, 14.13it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 965/1380 [01:16<00:29, 14.14it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 967/1380 [01:16<00:29, 14.17it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 969/1380 [01:16<00:28, 14.19it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 971/1380 [01:16<00:28, 14.22it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 973/1380 [01:17<00:28, 14.23it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 975/1380 [01:17<00:28, 14.22it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 977/1380 [01:17<00:28, 14.19it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 979/1380 [01:17<00:28, 14.19it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 981/1380 [01:17<00:28, 14.20it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 983/1380 [01:17<00:27, 14.19it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 985/1380 [01:17<00:27, 14.18it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 987/1380 [01:17<00:27, 14.19it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 989/1380 [01:18<00:27, 14.19it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 991/1380 [01:18<00:27, 14.19it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 993/1380 [01:18<00:27, 14.17it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 995/1380 [01:18<00:27, 14.19it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 997/1380 [01:18<00:26, 14.21it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 999/1380 [01:18<00:26, 14.21it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1001/1380 [01:18<00:26, 14.18it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1003/1380 [01:19<00:26, 14.18it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1005/1380 [01:19<00:26, 14.15it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1007/1380 [01:19<00:26, 14.16it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1009/1380 [01:19<00:26, 14.18it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1011/1380 [01:19<00:25, 14.21it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1013/1380 [01:19<00:25, 14.21it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1015/1380 [01:19<00:25, 14.22it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1017/1380 [01:20<00:25, 14.23it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1019/1380 [01:20<00:25, 14.23it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1021/1380 [01:20<00:25, 14.24it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1023/1380 [01:20<00:25, 14.22it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1025/1380 [01:20<00:24, 14.22it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1027/1380 [01:20<00:24, 14.22it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1029/1380 [01:20<00:24, 14.20it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1031/1380 [01:21<00:24, 14.20it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1033/1380 [01:21<00:24, 14.13it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1035/1380 [01:21<00:24, 14.17it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1037/1380 [01:21<00:24, 14.21it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1039/1380 [01:21<00:23, 14.23it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1041/1380 [01:21<00:23, 14.24it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1043/1380 [01:21<00:23, 14.24it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1045/1380 [01:22<00:23, 14.24it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1047/1380 [01:22<00:23, 14.24it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1049/1380 [01:22<00:23, 14.25it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1051/1380 [01:22<00:23, 14.25it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1053/1380 [01:22<00:22, 14.24it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1055/1380 [01:22<00:22, 14.24it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1057/1380 [01:22<00:22, 14.24it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1059/1380 [01:23<00:22, 14.23it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1061/1380 [01:23<00:22, 14.22it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1063/1380 [01:23<00:22, 14.18it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1065/1380 [01:23<00:22, 13.85it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1067/1380 [01:23<00:22, 13.97it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1069/1380 [01:23<00:22, 14.05it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1071/1380 [01:23<00:21, 14.10it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1073/1380 [01:24<00:21, 14.14it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1075/1380 [01:24<00:21, 14.18it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1077/1380 [01:24<00:21, 14.20it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1079/1380 [01:24<00:21, 14.21it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1081/1380 [01:24<00:21, 14.21it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1083/1380 [01:24<00:20, 14.20it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1085/1380 [01:24<00:20, 14.19it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1087/1380 [01:25<00:20, 14.16it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1089/1380 [01:25<00:20, 14.16it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1091/1380 [01:25<00:20, 14.18it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1093/1380 [01:25<00:20, 14.15it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1095/1380 [01:25<00:20, 14.16it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1097/1380 [01:25<00:19, 14.17it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1099/1380 [01:25<00:19, 14.18it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1101/1380 [01:26<00:19, 14.17it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1103/1380 [01:26<00:19, 14.19it/s]                                                    80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1104/1380 [01:26<00:19, 14.19it/s][INFO|trainer.py:755] 2023-11-15 21:54:53,968 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModelForTokenClassification.forward` and have been ignored: text. If text are not expected by `PeftModelForTokenClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 21:54:53,970 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 21:54:53,970 >>   Num examples = 2204
[INFO|trainer.py:3134] 2023-11-15 21:54:53,970 >>   Batch size = 8
{'eval_loss': 0.377132773399353, 'eval_accuracy': 0.8652450090744102, 'eval_micro_f1': 0.8652450090744102, 'eval_macro_f1': 0.8448884043431718, 'eval_runtime': 2.6661, 'eval_samples_per_second': 826.673, 'eval_steps_per_second': 103.522, 'epoch': 3.0}
{'loss': 0.3144, 'learning_rate': 0.0001, 'epoch': 4.0}

  0%|          | 0/276 [00:00<?, ?it/s][A
  4%|â–         | 12/276 [00:00<00:02, 116.52it/s][A
  9%|â–Š         | 24/276 [00:00<00:02, 109.62it/s][A
 13%|â–ˆâ–Ž        | 35/276 [00:00<00:02, 106.92it/s][A
 17%|â–ˆâ–‹        | 46/276 [00:00<00:02, 106.37it/s][A
 21%|â–ˆâ–ˆ        | 57/276 [00:00<00:02, 106.27it/s][A
 25%|â–ˆâ–ˆâ–       | 68/276 [00:00<00:01, 106.13it/s][A
 29%|â–ˆâ–ˆâ–Š       | 79/276 [00:00<00:01, 105.93it/s][A
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 90/276 [00:00<00:01, 105.91it/s][A
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 101/276 [00:00<00:01, 106.03it/s][A
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 112/276 [00:01<00:01, 105.40it/s][A
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 123/276 [00:01<00:01, 105.64it/s][A
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 134/276 [00:01<00:01, 105.48it/s][A
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 145/276 [00:01<00:01, 105.29it/s][A
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 156/276 [00:01<00:01, 105.25it/s][A
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 167/276 [00:01<00:01, 105.49it/s][A
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 178/276 [00:01<00:00, 105.44it/s][A
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 189/276 [00:01<00:00, 105.42it/s][A
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 200/276 [00:01<00:00, 105.56it/s][A
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 211/276 [00:01<00:00, 105.72it/s][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 222/276 [00:02<00:00, 105.62it/s][A
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 233/276 [00:02<00:00, 105.78it/s][A
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 244/276 [00:02<00:00, 105.31it/s][A
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 255/276 [00:02<00:00, 105.52it/s][A
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 266/276 [00:02<00:00, 105.61it/s][A                                                   
                                                  [A 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1104/1380 [01:28<00:19, 14.19it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 276/276 [00:02<00:00, 105.61it/s][A
                                                  [A 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1105/1380 [01:28<02:08,  2.15it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1107/1380 [01:29<01:34,  2.88it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1109/1380 [01:29<01:11,  3.78it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1111/1380 [01:29<00:55,  4.85it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1113/1380 [01:29<00:44,  6.04it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1115/1380 [01:29<00:36,  7.31it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1117/1380 [01:29<00:30,  8.55it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1119/1380 [01:29<00:26,  9.70it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1121/1380 [01:30<00:24, 10.72it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1123/1380 [01:30<00:22, 11.57it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1125/1380 [01:30<00:20, 12.25it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1127/1380 [01:30<00:19, 12.76it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1129/1380 [01:30<00:19, 13.14it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1131/1380 [01:30<00:18, 13.45it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1133/1380 [01:30<00:18, 13.67it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1135/1380 [01:31<00:17, 13.82it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1137/1380 [01:31<00:17, 13.94it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1139/1380 [01:31<00:17, 13.96it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1141/1380 [01:31<00:17, 14.00it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1143/1380 [01:31<00:16, 14.05it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1145/1380 [01:31<00:16, 14.10it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1147/1380 [01:31<00:16, 14.12it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1149/1380 [01:32<00:16, 14.12it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1151/1380 [01:32<00:16, 14.16it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1153/1380 [01:32<00:16, 14.15it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1155/1380 [01:32<00:15, 14.17it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1157/1380 [01:32<00:15, 14.19it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1159/1380 [01:32<00:15, 14.15it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1161/1380 [01:32<00:15, 14.17it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1163/1380 [01:33<00:15, 14.18it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1165/1380 [01:33<00:15, 14.17it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1167/1380 [01:33<00:15, 14.19it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1169/1380 [01:33<00:14, 14.20it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1171/1380 [01:33<00:14, 14.19it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1173/1380 [01:33<00:14, 14.21it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1175/1380 [01:33<00:14, 14.20it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1177/1380 [01:34<00:14, 14.20it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1179/1380 [01:34<00:14, 14.19it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1181/1380 [01:34<00:14, 14.17it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1183/1380 [01:34<00:13, 14.18it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1185/1380 [01:34<00:13, 14.20it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1187/1380 [01:34<00:13, 14.23it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1189/1380 [01:34<00:13, 14.19it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1191/1380 [01:35<00:13, 14.22it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1193/1380 [01:35<00:13, 14.23it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1195/1380 [01:35<00:13, 14.10it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1197/1380 [01:35<00:12, 14.11it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1199/1380 [01:35<00:12, 14.13it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1201/1380 [01:35<00:12, 14.15it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1203/1380 [01:35<00:12, 14.17it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1205/1380 [01:36<00:12, 14.18it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1207/1380 [01:36<00:12, 14.17it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1209/1380 [01:36<00:12, 14.17it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1211/1380 [01:36<00:11, 14.18it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1213/1380 [01:36<00:11, 14.19it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1215/1380 [01:36<00:11, 14.19it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1217/1380 [01:36<00:11, 14.15it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1219/1380 [01:36<00:11, 14.17it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1221/1380 [01:37<00:11, 14.19it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1223/1380 [01:37<00:11, 14.21it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1225/1380 [01:37<00:10, 14.21it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1227/1380 [01:37<00:10, 14.22it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1229/1380 [01:37<00:10, 14.22it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1231/1380 [01:37<00:10, 14.23it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1233/1380 [01:37<00:10, 14.22it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1235/1380 [01:38<00:10, 14.22it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1237/1380 [01:38<00:10, 14.21it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1239/1380 [01:38<00:09, 14.20it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1241/1380 [01:38<00:09, 14.19it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1243/1380 [01:38<00:09, 14.19it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1245/1380 [01:38<00:09, 14.19it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1247/1380 [01:38<00:09, 14.18it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1249/1380 [01:39<00:09, 14.19it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1251/1380 [01:39<00:09, 14.18it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1253/1380 [01:39<00:08, 14.19it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1255/1380 [01:39<00:08, 14.19it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1257/1380 [01:39<00:08, 14.17it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1259/1380 [01:39<00:08, 14.16it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1261/1380 [01:39<00:08, 14.12it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1263/1380 [01:40<00:08, 14.15it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1265/1380 [01:40<00:08, 14.18it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1267/1380 [01:40<00:07, 14.20it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1269/1380 [01:40<00:07, 14.21it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1271/1380 [01:40<00:07, 14.21it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1273/1380 [01:40<00:07, 14.21it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1275/1380 [01:40<00:07, 14.21it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1277/1380 [01:41<00:07, 14.20it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1279/1380 [01:41<00:07, 14.16it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1281/1380 [01:41<00:06, 14.17it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1283/1380 [01:41<00:06, 14.19it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1285/1380 [01:41<00:06, 14.20it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1287/1380 [01:41<00:06, 14.19it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1289/1380 [01:41<00:06, 14.19it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1291/1380 [01:42<00:06, 14.20it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1293/1380 [01:42<00:06, 14.19it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1295/1380 [01:42<00:05, 14.19it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1297/1380 [01:42<00:05, 14.18it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1299/1380 [01:42<00:05, 14.18it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1301/1380 [01:42<00:05, 14.18it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1303/1380 [01:42<00:05, 14.20it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1305/1380 [01:43<00:05, 14.21it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1307/1380 [01:43<00:05, 14.22it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1309/1380 [01:43<00:04, 14.22it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1311/1380 [01:43<00:04, 14.23it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1313/1380 [01:43<00:04, 14.23it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1315/1380 [01:43<00:04, 14.23it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1317/1380 [01:43<00:04, 14.16it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1319/1380 [01:44<00:04, 14.17it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1321/1380 [01:44<00:04, 14.15it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1323/1380 [01:44<00:04, 14.18it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1325/1380 [01:44<00:03, 14.20it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1327/1380 [01:44<00:03, 14.20it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1329/1380 [01:44<00:03, 14.21it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1331/1380 [01:44<00:03, 14.19it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1333/1380 [01:45<00:03, 14.19it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1335/1380 [01:45<00:03, 14.19it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1337/1380 [01:45<00:03, 14.18it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1339/1380 [01:45<00:02, 14.17it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1341/1380 [01:45<00:02, 14.18it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1343/1380 [01:45<00:02, 14.19it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1345/1380 [01:45<00:02, 14.20it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1347/1380 [01:46<00:02, 14.16it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1349/1380 [01:46<00:02, 14.18it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1351/1380 [01:46<00:02, 14.19it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1353/1380 [01:46<00:01, 14.20it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1355/1380 [01:46<00:01, 14.20it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1357/1380 [01:46<00:01, 14.19it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1359/1380 [01:46<00:01, 14.19it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1361/1380 [01:46<00:01, 14.18it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1363/1380 [01:47<00:01, 14.19it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1365/1380 [01:47<00:01, 14.20it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1367/1380 [01:47<00:00, 14.17it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1369/1380 [01:47<00:00, 14.18it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1371/1380 [01:47<00:00, 14.14it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1373/1380 [01:47<00:00, 14.13it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1375/1380 [01:47<00:00, 14.08it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1377/1380 [01:48<00:00, 14.13it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1379/1380 [01:48<00:00, 14.19it/s]                                                   100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1380/1380 [01:48<00:00, 14.19it/s][INFO|trainer.py:755] 2023-11-15 21:55:16,065 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModelForTokenClassification.forward` and have been ignored: text. If text are not expected by `PeftModelForTokenClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 21:55:16,067 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 21:55:16,067 >>   Num examples = 2204
[INFO|trainer.py:3134] 2023-11-15 21:55:16,067 >>   Batch size = 8
{'eval_loss': 0.3852865993976593, 'eval_accuracy': 0.8598003629764065, 'eval_micro_f1': 0.8598003629764064, 'eval_macro_f1': 0.8414583499746482, 'eval_runtime': 2.6596, 'eval_samples_per_second': 828.696, 'eval_steps_per_second': 103.775, 'epoch': 4.0}
{'loss': 0.2875, 'learning_rate': 0.0, 'epoch': 5.0}

  0%|          | 0/276 [00:00<?, ?it/s][A
  4%|â–         | 12/276 [00:00<00:02, 116.93it/s][A
  9%|â–Š         | 24/276 [00:00<00:02, 110.37it/s][A
 13%|â–ˆâ–Ž        | 36/276 [00:00<00:02, 108.49it/s][A
 17%|â–ˆâ–‹        | 47/276 [00:00<00:02, 107.27it/s][A
 21%|â–ˆâ–ˆ        | 58/276 [00:00<00:02, 106.86it/s][A
 25%|â–ˆâ–ˆâ–Œ       | 69/276 [00:00<00:01, 105.93it/s][A
 29%|â–ˆâ–ˆâ–‰       | 80/276 [00:00<00:01, 105.29it/s][A
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 91/276 [00:00<00:01, 105.08it/s][A
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 102/276 [00:00<00:01, 105.43it/s][A
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 113/276 [00:01<00:01, 105.46it/s][A
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 124/276 [00:01<00:01, 105.29it/s][A
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 135/276 [00:01<00:01, 105.48it/s][A
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 146/276 [00:01<00:01, 105.47it/s][A
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 157/276 [00:01<00:01, 105.70it/s][A
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 168/276 [00:01<00:01, 105.84it/s][A
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 179/276 [00:01<00:00, 105.52it/s][A
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 190/276 [00:01<00:00, 105.57it/s][A
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 201/276 [00:01<00:00, 105.41it/s][A
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 212/276 [00:01<00:00, 105.50it/s][A
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 223/276 [00:02<00:00, 105.15it/s][A
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 234/276 [00:02<00:00, 105.39it/s][A
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 245/276 [00:02<00:00, 104.91it/s][A
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 256/276 [00:02<00:00, 105.33it/s][A
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 267/276 [00:02<00:00, 105.45it/s][A                                                   
                                                  [A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1380/1380 [01:50<00:00, 14.19it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 276/276 [00:02<00:00, 105.45it/s][A
                                                  [A[INFO|trainer.py:1963] 2023-11-15 21:55:18,730 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                   100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1380/1380 [01:50<00:00, 14.19it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1380/1380 [01:50<00:00, 12.44it/s]
[INFO|trainer.py:2855] 2023-11-15 21:55:18,734 >> Saving model checkpoint to ./result/acl_roberta-base_seed0_lora
[INFO|tokenization_utils_base.py:2235] 2023-11-15 21:55:18,842 >> tokenizer config file saved in ./result/acl_roberta-base_seed0_lora/tokenizer_config.json
[INFO|tokenization_utils_base.py:2242] 2023-11-15 21:55:18,844 >> Special tokens file saved in ./result/acl_roberta-base_seed0_lora/special_tokens_map.json
{'eval_loss': 0.3932718336582184, 'eval_accuracy': 0.8570780399274047, 'eval_micro_f1': 0.8570780399274048, 'eval_macro_f1': 0.8380893735595091, 'eval_runtime': 2.6595, 'eval_samples_per_second': 828.713, 'eval_steps_per_second': 103.777, 'epoch': 5.0}
{'train_runtime': 110.9782, 'train_samples_per_second': 397.195, 'train_steps_per_second': 12.435, 'train_loss': 0.37044418998386547, 'epoch': 5.0}
***** train metrics *****
  epoch                    =        5.0
  train_loss               =     0.3704
  train_runtime            = 0:01:50.97
  train_samples            =       8816
  train_samples_per_second =    397.195
  train_steps_per_second   =     12.435
11/15/2023 21:55:18 - INFO - __main__ - *** Evaluate ***
[INFO|trainer.py:755] 2023-11-15 21:55:18,936 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModelForTokenClassification.forward` and have been ignored: text. If text are not expected by `PeftModelForTokenClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 21:55:18,937 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 21:55:18,938 >>   Num examples = 2204
[INFO|trainer.py:3134] 2023-11-15 21:55:18,938 >>   Batch size = 8
  0%|          | 0/276 [00:00<?, ?it/s]  4%|â–         | 12/276 [00:00<00:02, 117.28it/s]  9%|â–Š         | 24/276 [00:00<00:02, 110.68it/s] 13%|â–ˆâ–Ž        | 36/276 [00:00<00:02, 108.11it/s] 17%|â–ˆâ–‹        | 47/276 [00:00<00:02, 107.60it/s] 21%|â–ˆâ–ˆ        | 58/276 [00:00<00:02, 107.28it/s] 25%|â–ˆâ–ˆâ–Œ       | 69/276 [00:00<00:01, 106.91it/s] 29%|â–ˆâ–ˆâ–‰       | 80/276 [00:00<00:01, 106.92it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 91/276 [00:00<00:01, 106.42it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 102/276 [00:00<00:01, 106.16it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 113/276 [00:01<00:01, 106.33it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 124/276 [00:01<00:01, 106.53it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 135/276 [00:01<00:01, 106.38it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 146/276 [00:01<00:01, 106.54it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 157/276 [00:01<00:01, 106.62it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 168/276 [00:01<00:01, 106.42it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 179/276 [00:01<00:00, 106.52it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 190/276 [00:01<00:00, 106.40it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 201/276 [00:01<00:00, 106.09it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 212/276 [00:01<00:00, 105.92it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 223/276 [00:02<00:00, 105.93it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 234/276 [00:02<00:00, 106.07it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 245/276 [00:02<00:00, 105.91it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 256/276 [00:02<00:00, 106.08it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 267/276 [00:02<00:00, 106.23it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 276/276 [00:02<00:00, 105.12it/s]
***** eval metrics *****
  epoch                   =        5.0
  eval_accuracy           =     0.8571
  eval_loss               =     0.3933
  eval_macro_f1           =     0.8381
  eval_micro_f1           =     0.8571
  eval_runtime            = 0:00:02.63
  eval_samples            =       2204
  eval_samples_per_second =    835.181
  eval_steps_per_second   =    104.587
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                  eval/accuracy â–â–‡â–ˆâ–‡â–†â–†
wandb:                      eval/loss â–ˆâ–â–â–‚â–ƒâ–ƒ
wandb:                  eval/macro_f1 â–â–ˆâ–ˆâ–‡â–†â–†
wandb:                  eval/micro_f1 â–â–‡â–ˆâ–‡â–†â–†
wandb:                   eval/runtime â–â–ƒâ–ˆâ–‡â–‡â–„
wandb:        eval/samples_per_second â–ˆâ–†â–â–‚â–‚â–…
wandb:          eval/steps_per_second â–ˆâ–†â–â–‚â–‚â–…
wandb:                    train/epoch â–â–â–ƒâ–ƒâ–…â–…â–†â–†â–ˆâ–ˆâ–ˆâ–ˆ
wandb:              train/global_step â–â–â–ƒâ–ƒâ–…â–…â–†â–†â–ˆâ–ˆâ–ˆâ–ˆ
wandb:            train/learning_rate â–ˆâ–†â–…â–ƒâ–
wandb:                     train/loss â–ˆâ–„â–ƒâ–‚â–
wandb:               train/total_flos â–
wandb:               train/train_loss â–
wandb:            train/train_runtime â–
wandb: train/train_samples_per_second â–
wandb:   train/train_steps_per_second â–
wandb: 
wandb: Run summary:
wandb:                  eval/accuracy 0.85708
wandb:                      eval/loss 0.39327
wandb:                  eval/macro_f1 0.83809
wandb:                  eval/micro_f1 0.85708
wandb:                   eval/runtime 2.6389
wandb:        eval/samples_per_second 835.181
wandb:          eval/steps_per_second 104.587
wandb:                    train/epoch 5.0
wandb:              train/global_step 1380
wandb:            train/learning_rate 0.0
wandb:                     train/loss 0.2875
wandb:               train/total_flos 1469774552739840.0
wandb:               train/train_loss 0.37044
wandb:            train/train_runtime 110.9782
wandb: train/train_samples_per_second 397.195
wandb:   train/train_steps_per_second 12.435
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /ceph/home/wangyifei/a3/wandb/offline-run-20231115_215210-jizl8mtx
wandb: Find logs at: ./wandb/offline-run-20231115_215210-jizl8mtx/logs
(ModelArguments(model_name_or_path='roberta-base', cache_dir='./cache', use_adapter=False, use_lora=True), DataTrainingArguments(dataset_name='agnews_sup', max_seq_length=64), TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.0005,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=./result/agnews_sup_roberta-base_seed0_lora/runs/Nov15_21-55-32_node010,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.EPOCH,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=5.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
output_dir=./result/agnews_sup_roberta-base_seed0_lora,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
resume_from_checkpoint=None,
run_name=./result/agnews_sup_roberta-base_seed0_lora,
save_on_each_node=False,
save_safetensors=False,
save_steps=500,
save_strategy=IntervalStrategy.NO,
save_total_limit=None,
seed=111,
sharded_ddp=[],
skip_memory_metrics=True,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
))
11/15/2023 21:55:32 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False
11/15/2023 21:55:32 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.0005,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=./result/agnews_sup_roberta-base_seed0_lora/runs/Nov15_21-55-31_node010,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.EPOCH,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=5.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
output_dir=./result/agnews_sup_roberta-base_seed0_lora,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
resume_from_checkpoint=None,
run_name=./result/agnews_sup_roberta-base_seed0_lora,
save_on_each_node=False,
save_safetensors=False,
save_steps=500,
save_strategy=IntervalStrategy.NO,
save_total_limit=None,
seed=111,
sharded_ddp=[],
skip_memory_metrics=True,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
wandb: Tracking run with wandb version 0.15.12
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[INFO|configuration_utils.py:715] 2023-11-15 21:55:47,696 >> loading configuration file config.json from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/config.json
[INFO|configuration_utils.py:775] 2023-11-15 21:55:47,705 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.33.3",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_auto.py:535] 2023-11-15 21:55:57,720 >> Could not locate the tokenizer configuration file, will try to use the model config instead.
[INFO|configuration_utils.py:715] 2023-11-15 21:56:07,737 >> loading configuration file config.json from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/config.json
[INFO|configuration_utils.py:775] 2023-11-15 21:56:07,738 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.33.3",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:1852] 2023-11-15 21:56:27,785 >> loading file vocab.json from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/vocab.json
[INFO|tokenization_utils_base.py:1852] 2023-11-15 21:56:27,786 >> loading file merges.txt from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/merges.txt
[INFO|tokenization_utils_base.py:1852] 2023-11-15 21:56:27,786 >> loading file tokenizer.json from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/tokenizer.json
[INFO|tokenization_utils_base.py:1852] 2023-11-15 21:56:27,786 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1852] 2023-11-15 21:56:27,787 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1852] 2023-11-15 21:56:27,787 >> loading file tokenizer_config.json from cache at None
[INFO|configuration_utils.py:715] 2023-11-15 21:56:27,788 >> loading configuration file config.json from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/config.json
[INFO|configuration_utils.py:775] 2023-11-15 21:56:27,789 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.33.3",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:2862] 2023-11-15 21:56:47,951 >> loading weights file model.safetensors from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/model.safetensors
[INFO|modeling_utils.py:3638] 2023-11-15 21:56:48,651 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3650] 2023-11-15 21:56:48,652 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
trainable params: 1,285,636 || all params: 125,832,200 || trainable%: 1.0217066855701482
model_max_length: 512
Running tokenizer on dataset:   0%|          | 0/6840 [00:00<?, ? examples/s]Running tokenizer on dataset:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 4000/6840 [00:00<00:00, 21964.91 examples/s]Running tokenizer on dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6840/6840 [00:00<00:00, 22479.98 examples/s]Running tokenizer on dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6840/6840 [00:00<00:00, 22250.78 examples/s]
Running tokenizer on dataset:   0%|          | 0/760 [00:00<?, ? examples/s]Running tokenizer on dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 760/760 [00:00<00:00, 24515.83 examples/s]
11/15/2023 21:56:49 - INFO - __main__ - Sample 6776 of the training set: {'text': 'Cup chase lands in Dover When the green flag drops for today #39;s MBNA America 400 at Dover International Speedway, 43 drivers will be lined up to cross the start/finish line.', 'label': 0, 'input_ids': [0, 347, 658, 7859, 8952, 11, 21860, 520, 5, 2272, 3794, 9305, 13, 452, 849, 3416, 131, 29, 17025, 4444, 730, 3675, 23, 21860, 1016, 13243, 6, 3557, 2377, 40, 28, 9321, 62, 7, 2116, 5, 386, 73, 13597, 1173, 516, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
11/15/2023 21:56:49 - INFO - __main__ - Sample 1742 of the training set: {'text': 'Louisiana Tech Bulldogs RUSTON, Louisiana (Ticker) -- No. 17 Fresno State could not overcome a dominant performance by Ryan Moats or a poor one by Paul Pinegar.', 'label': 0, 'input_ids': [0, 29923, 8878, 4569, 10135, 248, 11120, 2191, 6, 5993, 36, 565, 13917, 43, 480, 440, 4, 601, 18084, 331, 115, 45, 6647, 10, 7353, 819, 30, 1774, 3713, 2923, 50, 10, 2129, 65, 30, 1206, 11542, 6276, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
11/15/2023 21:56:49 - INFO - __main__ - Sample 2588 of the training set: {'text': 'Rossi:  #39;I #39;m fairly happy #39; Valentino Rossi, who on Thursday pledged his future to Yamaha, entered the final qualifying session with the fastest time to date, but with the morning rain having washed the circuit clean, the Italian was unable to challenge Makoto Tamada for the pole.', 'label': 0, 'input_ids': [0, 35978, 118, 35, 1437, 849, 3416, 131, 100, 849, 3416, 131, 119, 5342, 1372, 849, 3416, 131, 18352, 1696, 21873, 6, 54, 15, 296, 7114, 39, 499, 7, 25297, 6, 2867, 5, 507, 7310, 1852, 19, 5, 6273, 86, 7, 1248, 6, 53, 19, 5, 662, 1895, 519, 15158, 5, 9326, 2382, 6, 5, 3108, 21, 3276, 7, 1539, 46383, 7736, 2095, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.
11/15/2023 21:56:49 - WARNING - accelerate.utils.other - Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[INFO|trainer.py:755] 2023-11-15 21:56:50,178 >> The following columns in the training set don't have a corresponding argument in `PeftModelForTokenClassification.forward` and have been ignored: text. If text are not expected by `PeftModelForTokenClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:1715] 2023-11-15 21:56:50,188 >> ***** Running training *****
[INFO|trainer.py:1716] 2023-11-15 21:56:50,188 >>   Num examples = 6,840
[INFO|trainer.py:1717] 2023-11-15 21:56:50,189 >>   Num Epochs = 5
[INFO|trainer.py:1718] 2023-11-15 21:56:50,189 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1721] 2023-11-15 21:56:50,189 >>   Total train batch size (w. parallel, distributed & accumulation) = 32
[INFO|trainer.py:1722] 2023-11-15 21:56:50,189 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:1723] 2023-11-15 21:56:50,190 >>   Total optimization steps = 1,070
[INFO|trainer.py:1724] 2023-11-15 21:56:50,191 >>   Number of trainable parameters = 1,285,636
[INFO|integration_utils.py:716] 2023-11-15 21:56:50,192 >> Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"
  0%|          | 0/1070 [00:00<?, ?it/s]  0%|          | 1/1070 [00:01<18:07,  1.02s/it]  0%|          | 3/1070 [00:01<05:37,  3.16it/s]  0%|          | 5/1070 [00:01<03:22,  5.26it/s]  1%|          | 7/1070 [00:01<02:27,  7.20it/s]  1%|          | 9/1070 [00:01<01:59,  8.87it/s]  1%|          | 11/1070 [00:01<01:43, 10.25it/s]  1%|          | 13/1070 [00:01<01:33, 11.34it/s]  1%|â–         | 15/1070 [00:01<01:26, 12.17it/s]  2%|â–         | 17/1070 [00:02<01:22, 12.80it/s]  2%|â–         | 19/1070 [00:02<01:19, 13.25it/s]  2%|â–         | 21/1070 [00:02<01:17, 13.57it/s]  2%|â–         | 23/1070 [00:02<01:15, 13.87it/s]  2%|â–         | 25/1070 [00:02<01:14, 14.03it/s]  3%|â–Ž         | 27/1070 [00:02<01:14, 14.06it/s]  3%|â–Ž         | 29/1070 [00:02<01:13, 14.14it/s]  3%|â–Ž         | 31/1070 [00:03<01:13, 14.18it/s]  3%|â–Ž         | 33/1070 [00:03<01:12, 14.23it/s]  3%|â–Ž         | 35/1070 [00:03<01:12, 14.28it/s]  3%|â–Ž         | 37/1070 [00:03<01:12, 14.31it/s]  4%|â–Ž         | 39/1070 [00:03<01:11, 14.34it/s]  4%|â–         | 41/1070 [00:03<01:11, 14.35it/s]  4%|â–         | 43/1070 [00:03<01:11, 14.36it/s]  4%|â–         | 45/1070 [00:04<01:11, 14.34it/s]  4%|â–         | 47/1070 [00:04<01:11, 14.33it/s]  5%|â–         | 49/1070 [00:04<01:11, 14.33it/s]  5%|â–         | 51/1070 [00:04<01:11, 14.31it/s]  5%|â–         | 53/1070 [00:04<01:11, 14.30it/s]  5%|â–Œ         | 55/1070 [00:04<01:11, 14.26it/s]  5%|â–Œ         | 57/1070 [00:04<01:10, 14.29it/s]  6%|â–Œ         | 59/1070 [00:05<01:10, 14.30it/s]  6%|â–Œ         | 61/1070 [00:05<01:10, 14.30it/s]  6%|â–Œ         | 63/1070 [00:05<01:10, 14.31it/s]  6%|â–Œ         | 65/1070 [00:05<01:10, 14.32it/s]  6%|â–‹         | 67/1070 [00:05<01:10, 14.29it/s]  6%|â–‹         | 69/1070 [00:05<01:09, 14.30it/s]  7%|â–‹         | 71/1070 [00:05<01:10, 14.25it/s]  7%|â–‹         | 73/1070 [00:06<01:09, 14.29it/s]  7%|â–‹         | 75/1070 [00:06<01:09, 14.30it/s]  7%|â–‹         | 77/1070 [00:06<01:09, 14.29it/s]  7%|â–‹         | 79/1070 [00:06<01:09, 14.28it/s]  8%|â–Š         | 81/1070 [00:06<01:09, 14.28it/s]  8%|â–Š         | 83/1070 [00:06<01:09, 14.30it/s]  8%|â–Š         | 85/1070 [00:06<01:08, 14.30it/s]  8%|â–Š         | 87/1070 [00:07<01:08, 14.30it/s]  8%|â–Š         | 89/1070 [00:07<01:08, 14.31it/s]  9%|â–Š         | 91/1070 [00:07<01:08, 14.29it/s]  9%|â–Š         | 93/1070 [00:07<01:08, 14.28it/s]  9%|â–‰         | 95/1070 [00:07<01:08, 14.30it/s]  9%|â–‰         | 97/1070 [00:07<01:07, 14.31it/s]  9%|â–‰         | 99/1070 [00:07<01:07, 14.31it/s]  9%|â–‰         | 101/1070 [00:08<01:07, 14.32it/s] 10%|â–‰         | 103/1070 [00:08<01:07, 14.31it/s] 10%|â–‰         | 105/1070 [00:08<01:07, 14.32it/s] 10%|â–ˆ         | 107/1070 [00:08<01:07, 14.32it/s] 10%|â–ˆ         | 109/1070 [00:08<01:07, 14.32it/s] 10%|â–ˆ         | 111/1070 [00:08<01:06, 14.33it/s] 11%|â–ˆ         | 113/1070 [00:08<01:06, 14.35it/s] 11%|â–ˆ         | 115/1070 [00:08<01:06, 14.34it/s] 11%|â–ˆ         | 117/1070 [00:09<01:06, 14.36it/s] 11%|â–ˆ         | 119/1070 [00:09<01:06, 14.40it/s] 11%|â–ˆâ–        | 121/1070 [00:09<01:05, 14.44it/s] 11%|â–ˆâ–        | 123/1070 [00:09<01:05, 14.43it/s] 12%|â–ˆâ–        | 125/1070 [00:09<01:05, 14.38it/s] 12%|â–ˆâ–        | 127/1070 [00:09<01:05, 14.37it/s] 12%|â–ˆâ–        | 129/1070 [00:09<01:05, 14.37it/s] 12%|â–ˆâ–        | 131/1070 [00:10<01:05, 14.40it/s] 12%|â–ˆâ–        | 133/1070 [00:10<01:05, 14.40it/s] 13%|â–ˆâ–Ž        | 135/1070 [00:10<01:04, 14.43it/s] 13%|â–ˆâ–Ž        | 137/1070 [00:10<01:04, 14.45it/s] 13%|â–ˆâ–Ž        | 139/1070 [00:10<01:04, 14.44it/s] 13%|â–ˆâ–Ž        | 141/1070 [00:10<01:04, 14.40it/s] 13%|â–ˆâ–Ž        | 143/1070 [00:10<01:04, 14.37it/s] 14%|â–ˆâ–Ž        | 145/1070 [00:11<01:04, 14.38it/s] 14%|â–ˆâ–Ž        | 147/1070 [00:11<01:04, 14.39it/s] 14%|â–ˆâ–        | 149/1070 [00:11<01:04, 14.38it/s] 14%|â–ˆâ–        | 151/1070 [00:11<01:03, 14.38it/s] 14%|â–ˆâ–        | 153/1070 [00:11<01:03, 14.40it/s] 14%|â–ˆâ–        | 155/1070 [00:11<01:03, 14.41it/s] 15%|â–ˆâ–        | 157/1070 [00:11<01:03, 14.39it/s] 15%|â–ˆâ–        | 159/1070 [00:12<01:03, 14.43it/s] 15%|â–ˆâ–Œ        | 161/1070 [00:12<01:03, 14.37it/s] 15%|â–ˆâ–Œ        | 163/1070 [00:12<01:03, 14.40it/s] 15%|â–ˆâ–Œ        | 165/1070 [00:12<01:02, 14.40it/s] 16%|â–ˆâ–Œ        | 167/1070 [00:12<01:02, 14.40it/s] 16%|â–ˆâ–Œ        | 169/1070 [00:12<01:02, 14.39it/s] 16%|â–ˆâ–Œ        | 171/1070 [00:12<01:02, 14.39it/s] 16%|â–ˆâ–Œ        | 173/1070 [00:13<01:02, 14.37it/s] 16%|â–ˆâ–‹        | 175/1070 [00:13<01:02, 14.39it/s] 17%|â–ˆâ–‹        | 177/1070 [00:13<01:01, 14.41it/s] 17%|â–ˆâ–‹        | 179/1070 [00:13<01:01, 14.40it/s] 17%|â–ˆâ–‹        | 181/1070 [00:13<01:01, 14.40it/s] 17%|â–ˆâ–‹        | 183/1070 [00:13<01:01, 14.37it/s] 17%|â–ˆâ–‹        | 185/1070 [00:13<01:01, 14.37it/s] 17%|â–ˆâ–‹        | 187/1070 [00:13<01:01, 14.37it/s] 18%|â–ˆâ–Š        | 189/1070 [00:14<01:01, 14.38it/s] 18%|â–ˆâ–Š        | 191/1070 [00:14<01:01, 14.39it/s] 18%|â–ˆâ–Š        | 193/1070 [00:14<01:00, 14.40it/s] 18%|â–ˆâ–Š        | 195/1070 [00:14<01:00, 14.41it/s] 18%|â–ˆâ–Š        | 197/1070 [00:14<01:00, 14.42it/s] 19%|â–ˆâ–Š        | 199/1070 [00:14<01:00, 14.44it/s] 19%|â–ˆâ–‰        | 201/1070 [00:14<01:00, 14.42it/s] 19%|â–ˆâ–‰        | 203/1070 [00:15<01:00, 14.40it/s] 19%|â–ˆâ–‰        | 205/1070 [00:15<01:00, 14.40it/s] 19%|â–ˆâ–‰        | 207/1070 [00:15<01:00, 14.37it/s] 20%|â–ˆâ–‰        | 209/1070 [00:15<00:59, 14.37it/s] 20%|â–ˆâ–‰        | 211/1070 [00:15<00:59, 14.38it/s] 20%|â–ˆâ–‰        | 213/1070 [00:15<00:59, 14.40it/s]                                                   20%|â–ˆâ–ˆ        | 214/1070 [00:15<00:59, 14.40it/s][INFO|trainer.py:755] 2023-11-15 21:57:06,033 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModelForTokenClassification.forward` and have been ignored: text. If text are not expected by `PeftModelForTokenClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 21:57:06,035 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 21:57:06,035 >>   Num examples = 760
[INFO|trainer.py:3134] 2023-11-15 21:57:06,035 >>   Batch size = 8
{'loss': 0.4216, 'learning_rate': 0.0004, 'epoch': 1.0}

  0%|          | 0/95 [00:00<?, ?it/s][A
 13%|â–ˆâ–Ž        | 12/95 [00:00<00:00, 118.04it/s][A
 25%|â–ˆâ–ˆâ–Œ       | 24/95 [00:00<00:00, 111.91it/s][A
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 36/95 [00:00<00:00, 109.48it/s][A
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 47/95 [00:00<00:00, 108.74it/s][A
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 58/95 [00:00<00:00, 107.93it/s][A
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 69/95 [00:00<00:00, 107.62it/s][A
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 80/95 [00:00<00:00, 107.52it/s][A
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 91/95 [00:00<00:00, 107.71it/s][A                                                  
                                                [A 20%|â–ˆâ–ˆ        | 214/1070 [00:16<00:59, 14.40it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 95/95 [00:00<00:00, 107.71it/s][A
                                                [A 20%|â–ˆâ–ˆ        | 215/1070 [00:16<02:56,  4.85it/s] 20%|â–ˆâ–ˆ        | 217/1070 [00:16<02:20,  6.06it/s] 20%|â–ˆâ–ˆ        | 219/1070 [00:17<01:56,  7.33it/s] 21%|â–ˆâ–ˆ        | 221/1070 [00:17<01:38,  8.61it/s] 21%|â–ˆâ–ˆ        | 223/1070 [00:17<01:26,  9.78it/s] 21%|â–ˆâ–ˆ        | 225/1070 [00:17<01:18, 10.82it/s] 21%|â–ˆâ–ˆ        | 227/1070 [00:17<01:12, 11.67it/s] 21%|â–ˆâ–ˆâ–       | 229/1070 [00:17<01:08, 12.33it/s] 22%|â–ˆâ–ˆâ–       | 231/1070 [00:17<01:05, 12.87it/s] 22%|â–ˆâ–ˆâ–       | 233/1070 [00:18<01:02, 13.29it/s] 22%|â–ˆâ–ˆâ–       | 235/1070 [00:18<01:01, 13.60it/s] 22%|â–ˆâ–ˆâ–       | 237/1070 [00:18<01:00, 13.82it/s] 22%|â–ˆâ–ˆâ–       | 239/1070 [00:18<00:59, 13.96it/s] 23%|â–ˆâ–ˆâ–Ž       | 241/1070 [00:18<00:58, 14.07it/s] 23%|â–ˆâ–ˆâ–Ž       | 243/1070 [00:18<00:58, 14.14it/s] 23%|â–ˆâ–ˆâ–Ž       | 245/1070 [00:18<00:58, 14.22it/s] 23%|â–ˆâ–ˆâ–Ž       | 247/1070 [00:19<00:57, 14.28it/s] 23%|â–ˆâ–ˆâ–Ž       | 249/1070 [00:19<00:57, 14.32it/s] 23%|â–ˆâ–ˆâ–Ž       | 251/1070 [00:19<00:57, 14.31it/s] 24%|â–ˆâ–ˆâ–Ž       | 253/1070 [00:19<00:57, 14.33it/s] 24%|â–ˆâ–ˆâ–       | 255/1070 [00:19<00:56, 14.35it/s] 24%|â–ˆâ–ˆâ–       | 257/1070 [00:19<00:56, 14.34it/s] 24%|â–ˆâ–ˆâ–       | 259/1070 [00:19<00:56, 14.33it/s] 24%|â–ˆâ–ˆâ–       | 261/1070 [00:20<00:56, 14.34it/s] 25%|â–ˆâ–ˆâ–       | 263/1070 [00:20<00:56, 14.31it/s] 25%|â–ˆâ–ˆâ–       | 265/1070 [00:20<00:56, 14.32it/s] 25%|â–ˆâ–ˆâ–       | 267/1070 [00:20<00:56, 14.34it/s] 25%|â–ˆâ–ˆâ–Œ       | 269/1070 [00:20<00:55, 14.34it/s] 25%|â–ˆâ–ˆâ–Œ       | 271/1070 [00:20<00:55, 14.36it/s] 26%|â–ˆâ–ˆâ–Œ       | 273/1070 [00:20<00:55, 14.31it/s] 26%|â–ˆâ–ˆâ–Œ       | 275/1070 [00:21<00:55, 14.33it/s] 26%|â–ˆâ–ˆâ–Œ       | 277/1070 [00:21<00:55, 14.35it/s] 26%|â–ˆâ–ˆâ–Œ       | 279/1070 [00:21<00:55, 14.34it/s] 26%|â–ˆâ–ˆâ–‹       | 281/1070 [00:21<00:54, 14.35it/s] 26%|â–ˆâ–ˆâ–‹       | 283/1070 [00:21<00:54, 14.38it/s] 27%|â–ˆâ–ˆâ–‹       | 285/1070 [00:21<00:54, 14.32it/s] 27%|â–ˆâ–ˆâ–‹       | 287/1070 [00:21<00:54, 14.35it/s] 27%|â–ˆâ–ˆâ–‹       | 289/1070 [00:21<00:54, 14.37it/s] 27%|â–ˆâ–ˆâ–‹       | 291/1070 [00:22<00:54, 14.39it/s] 27%|â–ˆâ–ˆâ–‹       | 293/1070 [00:22<00:54, 14.36it/s] 28%|â–ˆâ–ˆâ–Š       | 295/1070 [00:22<00:54, 14.33it/s] 28%|â–ˆâ–ˆâ–Š       | 297/1070 [00:22<00:53, 14.33it/s] 28%|â–ˆâ–ˆâ–Š       | 299/1070 [00:22<00:53, 14.32it/s] 28%|â–ˆâ–ˆâ–Š       | 301/1070 [00:22<00:53, 14.32it/s] 28%|â–ˆâ–ˆâ–Š       | 303/1070 [00:22<00:53, 14.32it/s] 29%|â–ˆâ–ˆâ–Š       | 305/1070 [00:23<00:53, 14.32it/s] 29%|â–ˆâ–ˆâ–Š       | 307/1070 [00:23<00:53, 14.31it/s] 29%|â–ˆâ–ˆâ–‰       | 309/1070 [00:23<00:53, 14.31it/s] 29%|â–ˆâ–ˆâ–‰       | 311/1070 [00:23<00:53, 14.32it/s] 29%|â–ˆâ–ˆâ–‰       | 313/1070 [00:23<00:52, 14.31it/s] 29%|â–ˆâ–ˆâ–‰       | 315/1070 [00:23<00:52, 14.32it/s] 30%|â–ˆâ–ˆâ–‰       | 317/1070 [00:23<00:52, 14.30it/s] 30%|â–ˆâ–ˆâ–‰       | 319/1070 [00:24<00:52, 14.30it/s] 30%|â–ˆâ–ˆâ–ˆ       | 321/1070 [00:24<00:52, 14.31it/s] 30%|â–ˆâ–ˆâ–ˆ       | 323/1070 [00:24<00:52, 14.29it/s] 30%|â–ˆâ–ˆâ–ˆ       | 325/1070 [00:24<00:52, 14.31it/s] 31%|â–ˆâ–ˆâ–ˆ       | 327/1070 [00:24<00:51, 14.30it/s] 31%|â–ˆâ–ˆâ–ˆ       | 329/1070 [00:24<00:51, 14.29it/s] 31%|â–ˆâ–ˆâ–ˆ       | 331/1070 [00:24<00:51, 14.30it/s] 31%|â–ˆâ–ˆâ–ˆ       | 333/1070 [00:25<00:51, 14.31it/s] 31%|â–ˆâ–ˆâ–ˆâ–      | 335/1070 [00:25<00:51, 14.32it/s] 31%|â–ˆâ–ˆâ–ˆâ–      | 337/1070 [00:25<00:51, 14.33it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 339/1070 [00:25<00:50, 14.33it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 341/1070 [00:25<00:50, 14.34it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 343/1070 [00:25<00:50, 14.35it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 345/1070 [00:25<00:50, 14.36it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 347/1070 [00:26<00:50, 14.41it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 349/1070 [00:26<00:50, 14.38it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 351/1070 [00:26<00:50, 14.33it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 353/1070 [00:26<00:50, 14.33it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 355/1070 [00:26<00:49, 14.33it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 357/1070 [00:26<00:49, 14.29it/s] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 359/1070 [00:26<00:49, 14.29it/s] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 361/1070 [00:27<00:49, 14.31it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 363/1070 [00:27<00:49, 14.31it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 365/1070 [00:27<00:49, 14.30it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 367/1070 [00:27<00:49, 14.30it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 369/1070 [00:27<00:49, 14.30it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 371/1070 [00:27<00:48, 14.30it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 373/1070 [00:27<00:48, 14.27it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 375/1070 [00:27<00:48, 14.28it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 377/1070 [00:28<00:48, 14.30it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 379/1070 [00:28<00:48, 14.29it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 381/1070 [00:28<00:48, 14.30it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 383/1070 [00:28<00:48, 14.27it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 385/1070 [00:28<00:47, 14.29it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 387/1070 [00:28<00:47, 14.28it/s] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 389/1070 [00:28<00:47, 14.29it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 391/1070 [00:29<00:47, 14.30it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 393/1070 [00:29<00:47, 14.30it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 395/1070 [00:29<00:47, 14.30it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 397/1070 [00:29<00:47, 14.30it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 399/1070 [00:29<00:46, 14.28it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 401/1070 [00:29<00:46, 14.29it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 403/1070 [00:29<00:46, 14.29it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 405/1070 [00:30<00:46, 14.29it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 407/1070 [00:30<00:46, 14.29it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 409/1070 [00:30<00:46, 14.30it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 411/1070 [00:30<00:46, 14.29it/s] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 413/1070 [00:30<00:45, 14.29it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 415/1070 [00:30<00:45, 14.26it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 417/1070 [00:30<00:45, 14.28it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 419/1070 [00:31<00:45, 14.30it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 421/1070 [00:31<00:45, 14.30it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 423/1070 [00:31<00:45, 14.29it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 425/1070 [00:31<00:45, 14.27it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 427/1070 [00:31<00:45, 14.28it/s]                                                   40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 428/1070 [00:31<00:44, 14.28it/s][INFO|trainer.py:755] 2023-11-15 21:57:21,884 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModelForTokenClassification.forward` and have been ignored: text. If text are not expected by `PeftModelForTokenClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 21:57:21,886 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 21:57:21,886 >>   Num examples = 760
[INFO|trainer.py:3134] 2023-11-15 21:57:21,886 >>   Batch size = 8
{'eval_loss': 0.3247620463371277, 'eval_accuracy': 0.8736842105263158, 'eval_micro_f1': 0.8736842105263158, 'eval_macro_f1': 0.8715858791493394, 'eval_runtime': 0.9173, 'eval_samples_per_second': 828.542, 'eval_steps_per_second': 103.568, 'epoch': 1.0}
{'loss': 0.2677, 'learning_rate': 0.0003, 'epoch': 2.0}

  0%|          | 0/95 [00:00<?, ?it/s][A
 13%|â–ˆâ–Ž        | 12/95 [00:00<00:00, 117.85it/s][A
 25%|â–ˆâ–ˆâ–Œ       | 24/95 [00:00<00:00, 111.12it/s][A
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 36/95 [00:00<00:00, 109.07it/s][A
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 47/95 [00:00<00:00, 108.36it/s][A
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 58/95 [00:00<00:00, 107.98it/s][A
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 69/95 [00:00<00:00, 107.44it/s][A
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 80/95 [00:00<00:00, 107.26it/s][A
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 91/95 [00:00<00:00, 106.76it/s][A                                                  
                                                [A 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 428/1070 [00:32<00:44, 14.28it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 95/95 [00:00<00:00, 106.76it/s][A
                                                [A 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 429/1070 [00:32<02:12,  4.84it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 431/1070 [00:32<01:45,  6.04it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 433/1070 [00:32<01:27,  7.30it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 435/1070 [00:33<01:14,  8.56it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 437/1070 [00:33<01:04,  9.75it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 439/1070 [00:33<00:58, 10.79it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 441/1070 [00:33<00:54, 11.65it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 443/1070 [00:33<00:50, 12.32it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 445/1070 [00:33<00:48, 12.85it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 447/1070 [00:33<00:46, 13.26it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 449/1070 [00:34<00:45, 13.56it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 451/1070 [00:34<00:44, 13.77it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 453/1070 [00:34<00:44, 13.92it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 455/1070 [00:34<00:43, 14.00it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 457/1070 [00:34<00:43, 14.05it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 459/1070 [00:34<00:43, 14.12it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 461/1070 [00:34<00:42, 14.21it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 463/1070 [00:35<00:42, 14.26it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 465/1070 [00:35<00:42, 14.28it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 467/1070 [00:35<00:42, 14.27it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 469/1070 [00:35<00:42, 14.29it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 471/1070 [00:35<00:41, 14.29it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 473/1070 [00:35<00:41, 14.26it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 475/1070 [00:35<00:41, 14.28it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 477/1070 [00:36<00:41, 14.21it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 479/1070 [00:36<00:41, 14.25it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 481/1070 [00:36<00:41, 14.29it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 483/1070 [00:36<00:41, 14.31it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 485/1070 [00:36<00:40, 14.32it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 487/1070 [00:36<00:40, 14.29it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 489/1070 [00:36<00:40, 14.31it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 491/1070 [00:37<00:40, 14.32it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 493/1070 [00:37<00:40, 14.31it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 495/1070 [00:37<00:40, 14.30it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 497/1070 [00:37<00:40, 14.30it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 499/1070 [00:37<00:39, 14.28it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 501/1070 [00:37<00:39, 14.29it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 503/1070 [00:37<00:39, 14.30it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 505/1070 [00:38<00:39, 14.30it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 507/1070 [00:38<00:39, 14.30it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 509/1070 [00:38<00:39, 14.30it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 511/1070 [00:38<00:39, 14.30it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 513/1070 [00:38<00:38, 14.29it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 515/1070 [00:38<00:38, 14.28it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 517/1070 [00:38<00:38, 14.29it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 519/1070 [00:38<00:38, 14.27it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 521/1070 [00:39<00:38, 14.28it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 523/1070 [00:39<00:38, 14.30it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 525/1070 [00:39<00:38, 14.29it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 527/1070 [00:39<00:38, 14.29it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 529/1070 [00:39<00:37, 14.29it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 531/1070 [00:39<00:37, 14.27it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 533/1070 [00:39<00:37, 14.29it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 535/1070 [00:40<00:37, 14.30it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 537/1070 [00:40<00:37, 14.30it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 539/1070 [00:40<00:37, 14.27it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 541/1070 [00:40<00:37, 14.29it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 543/1070 [00:40<00:36, 14.28it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 545/1070 [00:40<00:36, 14.30it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 547/1070 [00:40<00:36, 14.30it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 549/1070 [00:41<00:36, 14.30it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 551/1070 [00:41<00:36, 14.30it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 553/1070 [00:41<00:36, 14.29it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 555/1070 [00:41<00:36, 14.29it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 557/1070 [00:41<00:35, 14.29it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 559/1070 [00:41<00:35, 14.25it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 561/1070 [00:41<00:35, 14.27it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 563/1070 [00:42<00:35, 14.28it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 565/1070 [00:42<00:35, 14.31it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 567/1070 [00:42<00:35, 14.31it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 569/1070 [00:42<00:35, 14.30it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 571/1070 [00:42<00:34, 14.30it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 573/1070 [00:42<00:34, 14.23it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 575/1070 [00:42<00:34, 14.23it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 577/1070 [00:43<00:34, 14.27it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 579/1070 [00:43<00:34, 14.30it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 581/1070 [00:43<00:34, 14.32it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 583/1070 [00:43<00:34, 14.32it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 585/1070 [00:43<00:33, 14.31it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 587/1070 [00:43<00:33, 14.30it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 589/1070 [00:43<00:33, 14.29it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 591/1070 [00:44<00:33, 14.29it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 593/1070 [00:44<00:33, 14.30it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 595/1070 [00:44<00:33, 14.30it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 597/1070 [00:44<00:33, 14.29it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 599/1070 [00:44<00:32, 14.30it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 601/1070 [00:44<00:32, 14.30it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 603/1070 [00:44<00:32, 14.29it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 605/1070 [00:45<00:32, 14.28it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 607/1070 [00:45<00:32, 14.29it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 609/1070 [00:45<00:32, 14.29it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 611/1070 [00:45<00:32, 14.30it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 613/1070 [00:45<00:31, 14.30it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 615/1070 [00:45<00:31, 14.29it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 617/1070 [00:45<00:31, 14.25it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 619/1070 [00:45<00:31, 14.29it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 621/1070 [00:46<00:31, 14.30it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 623/1070 [00:46<00:31, 14.29it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 625/1070 [00:46<00:31, 14.30it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 627/1070 [00:46<00:31, 14.29it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 629/1070 [00:46<00:30, 14.30it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 631/1070 [00:46<00:30, 14.30it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 633/1070 [00:46<00:30, 14.30it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 635/1070 [00:47<00:30, 14.30it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 637/1070 [00:47<00:30, 14.28it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 639/1070 [00:47<00:30, 14.28it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 641/1070 [00:47<00:29, 14.30it/s]                                                   60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 642/1070 [00:47<00:29, 14.30it/s][INFO|trainer.py:755] 2023-11-15 21:57:37,768 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModelForTokenClassification.forward` and have been ignored: text. If text are not expected by `PeftModelForTokenClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 21:57:37,770 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 21:57:37,770 >>   Num examples = 760
[INFO|trainer.py:3134] 2023-11-15 21:57:37,770 >>   Batch size = 8
{'eval_loss': 0.2721836566925049, 'eval_accuracy': 0.9105263157894737, 'eval_micro_f1': 0.9105263157894739, 'eval_macro_f1': 0.9079359707223443, 'eval_runtime': 0.9205, 'eval_samples_per_second': 825.635, 'eval_steps_per_second': 103.204, 'epoch': 2.0}
{'loss': 0.2317, 'learning_rate': 0.0002, 'epoch': 3.0}

  0%|          | 0/95 [00:00<?, ?it/s][A
 13%|â–ˆâ–Ž        | 12/95 [00:00<00:00, 117.39it/s][A
 25%|â–ˆâ–ˆâ–Œ       | 24/95 [00:00<00:00, 111.13it/s][A
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 36/95 [00:00<00:00, 108.70it/s][A
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 47/95 [00:00<00:00, 107.91it/s][A
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 58/95 [00:00<00:00, 107.74it/s][A
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 69/95 [00:00<00:00, 107.61it/s][A
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 80/95 [00:00<00:00, 107.13it/s][A
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 91/95 [00:00<00:00, 107.21it/s][A                                                  
                                                [A 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 642/1070 [00:48<00:29, 14.30it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 95/95 [00:00<00:00, 107.21it/s][A
                                                [A 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 643/1070 [00:48<01:28,  4.84it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 645/1070 [00:48<01:10,  6.04it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 647/1070 [00:48<00:57,  7.30it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 649/1070 [00:48<00:49,  8.56it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 651/1070 [00:49<00:43,  9.73it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 653/1070 [00:49<00:38, 10.77it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 655/1070 [00:49<00:35, 11.61it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 657/1070 [00:49<00:33, 12.31it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 659/1070 [00:49<00:31, 12.85it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 661/1070 [00:49<00:30, 13.25it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 663/1070 [00:49<00:30, 13.56it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 665/1070 [00:50<00:29, 13.76it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 667/1070 [00:50<00:28, 13.92it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 669/1070 [00:50<00:28, 14.02it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 671/1070 [00:50<00:28, 14.11it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 673/1070 [00:50<00:28, 14.17it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 675/1070 [00:50<00:27, 14.18it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 677/1070 [00:50<00:27, 14.21it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 679/1070 [00:51<00:27, 14.22it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 681/1070 [00:51<00:27, 14.26it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 683/1070 [00:51<00:27, 14.28it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 685/1070 [00:51<00:26, 14.29it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 687/1070 [00:51<00:26, 14.30it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 689/1070 [00:51<00:26, 14.28it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 691/1070 [00:51<00:26, 14.29it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 693/1070 [00:52<00:26, 14.29it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 695/1070 [00:52<00:26, 14.29it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 697/1070 [00:52<00:26, 14.29it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 699/1070 [00:52<00:25, 14.29it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 701/1070 [00:52<00:25, 14.29it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 703/1070 [00:52<00:25, 14.29it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 705/1070 [00:52<00:25, 14.29it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 707/1070 [00:53<00:25, 14.30it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 709/1070 [00:53<00:25, 14.30it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 711/1070 [00:53<00:25, 14.28it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 713/1070 [00:53<00:25, 14.28it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 715/1070 [00:53<00:24, 14.28it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 717/1070 [00:53<00:24, 14.29it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 719/1070 [00:53<00:24, 14.30it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 721/1070 [00:54<00:24, 14.24it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 723/1070 [00:54<00:24, 14.28it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 725/1070 [00:54<00:24, 14.29it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 727/1070 [00:54<00:23, 14.30it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 729/1070 [00:54<00:23, 14.31it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 731/1070 [00:54<00:23, 14.31it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 733/1070 [00:54<00:23, 14.30it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 735/1070 [00:55<00:23, 14.29it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 737/1070 [00:55<00:23, 14.30it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 739/1070 [00:55<00:23, 14.30it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 741/1070 [00:55<00:23, 14.29it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 743/1070 [00:55<00:22, 14.26it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 745/1070 [00:55<00:22, 14.27it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 747/1070 [00:55<00:22, 14.29it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 749/1070 [00:55<00:22, 14.30it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 751/1070 [00:56<00:22, 14.29it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 753/1070 [00:56<00:22, 14.29it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 755/1070 [00:56<00:22, 14.28it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 757/1070 [00:56<00:21, 14.27it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 759/1070 [00:56<00:21, 14.30it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 761/1070 [00:56<00:21, 14.31it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 763/1070 [00:56<00:21, 14.31it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 765/1070 [00:57<00:21, 14.29it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 767/1070 [00:57<00:21, 14.30it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 769/1070 [00:57<00:21, 14.30it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 771/1070 [00:57<00:20, 14.30it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 773/1070 [00:57<00:20, 14.28it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 775/1070 [00:57<00:20, 14.26it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 777/1070 [00:57<00:20, 14.27it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 779/1070 [00:58<00:20, 14.29it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 781/1070 [00:58<00:20, 14.30it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 783/1070 [00:58<00:20, 14.30it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 785/1070 [00:58<00:19, 14.27it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 787/1070 [00:58<00:19, 14.27it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 789/1070 [00:58<00:19, 14.29it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 791/1070 [00:58<00:19, 14.30it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 793/1070 [00:59<00:19, 14.27it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 795/1070 [00:59<00:19, 14.24it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 797/1070 [00:59<00:19, 14.27it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 799/1070 [00:59<00:18, 14.30it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 801/1070 [00:59<00:18, 14.31it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 803/1070 [00:59<00:18, 14.31it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 805/1070 [00:59<00:18, 14.30it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 807/1070 [01:00<00:18, 14.29it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 809/1070 [01:00<00:18, 14.31it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 811/1070 [01:00<00:18, 14.31it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 813/1070 [01:00<00:17, 14.31it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 815/1070 [01:00<00:17, 14.31it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 817/1070 [01:00<00:17, 14.28it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 819/1070 [01:00<00:17, 14.29it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 821/1070 [01:01<00:17, 14.29it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 823/1070 [01:01<00:17, 14.30it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 825/1070 [01:01<00:17, 14.30it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 827/1070 [01:01<00:16, 14.30it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 829/1070 [01:01<00:16, 14.28it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 831/1070 [01:01<00:16, 14.29it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 833/1070 [01:01<00:16, 14.30it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 835/1070 [01:02<00:16, 14.30it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 837/1070 [01:02<00:16, 14.29it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 839/1070 [01:02<00:16, 14.25it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 841/1070 [01:02<00:16, 14.28it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 843/1070 [01:02<00:15, 14.29it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 845/1070 [01:02<00:15, 14.29it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 847/1070 [01:02<00:15, 14.30it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 849/1070 [01:02<00:15, 14.25it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 851/1070 [01:03<00:15, 14.27it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 853/1070 [01:03<00:15, 14.29it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 855/1070 [01:03<00:15, 14.31it/s]                                                   80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 856/1070 [01:03<00:14, 14.31it/s][INFO|trainer.py:755] 2023-11-15 21:57:53,654 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModelForTokenClassification.forward` and have been ignored: text. If text are not expected by `PeftModelForTokenClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 21:57:53,655 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 21:57:53,656 >>   Num examples = 760
[INFO|trainer.py:3134] 2023-11-15 21:57:53,656 >>   Batch size = 8
{'eval_loss': 0.2852332293987274, 'eval_accuracy': 0.9105263157894737, 'eval_micro_f1': 0.9105263157894739, 'eval_macro_f1': 0.9081207490209569, 'eval_runtime': 0.9194, 'eval_samples_per_second': 826.665, 'eval_steps_per_second': 103.333, 'epoch': 3.0}
{'loss': 0.1865, 'learning_rate': 0.0001, 'epoch': 4.0}

  0%|          | 0/95 [00:00<?, ?it/s][A
 13%|â–ˆâ–Ž        | 12/95 [00:00<00:00, 117.43it/s][A
 25%|â–ˆâ–ˆâ–Œ       | 24/95 [00:00<00:00, 110.98it/s][A
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 36/95 [00:00<00:00, 109.03it/s][A
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 47/95 [00:00<00:00, 108.32it/s][A
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 58/95 [00:00<00:00, 107.66it/s][A
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 69/95 [00:00<00:00, 107.43it/s][A
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 80/95 [00:00<00:00, 107.16it/s][A
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 91/95 [00:00<00:00, 106.84it/s][A                                                  
                                                [A 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 856/1070 [01:04<00:14, 14.31it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 95/95 [00:00<00:00, 106.84it/s][A
                                                [A 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 857/1070 [01:04<00:44,  4.84it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 859/1070 [01:04<00:34,  6.03it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 861/1070 [01:04<00:28,  7.30it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 863/1070 [01:04<00:24,  8.56it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 865/1070 [01:05<00:21,  9.74it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 867/1070 [01:05<00:18, 10.78it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 869/1070 [01:05<00:17, 11.62it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 871/1070 [01:05<00:16, 12.31it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 873/1070 [01:05<00:15, 12.86it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 875/1070 [01:05<00:14, 13.27it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 877/1070 [01:05<00:14, 13.57it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 879/1070 [01:05<00:13, 13.78it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 881/1070 [01:06<00:13, 13.95it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 883/1070 [01:06<00:13, 14.05it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 885/1070 [01:06<00:13, 14.12it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 887/1070 [01:06<00:12, 14.17it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 889/1070 [01:06<00:12, 14.19it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 891/1070 [01:06<00:12, 14.21it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 893/1070 [01:06<00:12, 14.25it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 895/1070 [01:07<00:12, 14.26it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 897/1070 [01:07<00:12, 14.28it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 899/1070 [01:07<00:11, 14.29it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 901/1070 [01:07<00:11, 14.28it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 903/1070 [01:07<00:11, 14.30it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 905/1070 [01:07<00:11, 14.30it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 907/1070 [01:07<00:11, 14.30it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 909/1070 [01:08<00:11, 14.30it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 911/1070 [01:08<00:11, 14.29it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 913/1070 [01:08<00:10, 14.28it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 915/1070 [01:08<00:10, 14.29it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 917/1070 [01:08<00:10, 14.30it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 919/1070 [01:08<00:10, 14.30it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 921/1070 [01:08<00:10, 14.29it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 923/1070 [01:09<00:10, 14.29it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 925/1070 [01:09<00:10, 14.26it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 927/1070 [01:09<00:10, 14.25it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 929/1070 [01:09<00:09, 14.27it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 931/1070 [01:09<00:09, 14.26it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 933/1070 [01:09<00:09, 14.24it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 935/1070 [01:09<00:09, 14.13it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 937/1070 [01:10<00:09, 14.03it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 939/1070 [01:10<00:09, 13.96it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 941/1070 [01:10<00:09, 13.98it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 943/1070 [01:10<00:09, 13.98it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 945/1070 [01:10<00:08, 14.06it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 947/1070 [01:10<00:08, 14.13it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 949/1070 [01:10<00:08, 14.18it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 951/1070 [01:11<00:08, 14.22it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 953/1070 [01:11<00:08, 14.26it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 955/1070 [01:11<00:08, 14.28it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 957/1070 [01:11<00:07, 14.30it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 959/1070 [01:11<00:07, 14.29it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 961/1070 [01:11<00:07, 14.27it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 963/1070 [01:11<00:07, 14.28it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 965/1070 [01:12<00:07, 14.29it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 967/1070 [01:12<00:07, 14.30it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 969/1070 [01:12<00:07, 14.32it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 971/1070 [01:12<00:06, 14.30it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 973/1070 [01:12<00:06, 14.28it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 975/1070 [01:12<00:06, 14.30it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 977/1070 [01:12<00:06, 14.28it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 979/1070 [01:13<00:06, 14.28it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 981/1070 [01:13<00:06, 14.29it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 983/1070 [01:13<00:06, 14.30it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 985/1070 [01:13<00:05, 14.31it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 987/1070 [01:13<00:05, 14.31it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 989/1070 [01:13<00:05, 14.30it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 991/1070 [01:13<00:05, 14.30it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 993/1070 [01:13<00:05, 14.29it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 995/1070 [01:14<00:05, 14.29it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 997/1070 [01:14<00:05, 14.30it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 999/1070 [01:14<00:04, 14.30it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1001/1070 [01:14<00:04, 14.30it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1003/1070 [01:14<00:04, 14.30it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1005/1070 [01:14<00:04, 14.24it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1007/1070 [01:14<00:04, 14.23it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1009/1070 [01:15<00:04, 14.24it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1011/1070 [01:15<00:04, 14.24it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1013/1070 [01:15<00:04, 14.25it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1015/1070 [01:15<00:03, 14.25it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1017/1070 [01:15<00:03, 14.24it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1019/1070 [01:15<00:03, 14.24it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1021/1070 [01:15<00:03, 14.24it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1023/1070 [01:16<00:03, 14.22it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1025/1070 [01:16<00:03, 14.21it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1027/1070 [01:16<00:03, 14.21it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1029/1070 [01:16<00:02, 14.20it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1031/1070 [01:16<00:02, 14.21it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1033/1070 [01:16<00:02, 14.21it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1035/1070 [01:16<00:02, 14.22it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1037/1070 [01:17<00:02, 14.23it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1039/1070 [01:17<00:02, 14.24it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1041/1070 [01:17<00:02, 14.24it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1043/1070 [01:17<00:01, 14.24it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1045/1070 [01:17<00:01, 14.22it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1047/1070 [01:17<00:01, 14.21it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1049/1070 [01:17<00:01, 14.21it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1051/1070 [01:18<00:01, 14.20it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1053/1070 [01:18<00:01, 14.18it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1055/1070 [01:18<00:01, 14.17it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1057/1070 [01:18<00:00, 14.18it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1059/1070 [01:18<00:00, 14.19it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1061/1070 [01:18<00:00, 14.18it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1063/1070 [01:18<00:00, 14.17it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1065/1070 [01:19<00:00, 14.17it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1067/1070 [01:19<00:00, 14.17it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1069/1070 [01:19<00:00, 14.17it/s]                                                   100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1070/1070 [01:19<00:00, 14.17it/s][INFO|trainer.py:755] 2023-11-15 21:58:09,590 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModelForTokenClassification.forward` and have been ignored: text. If text are not expected by `PeftModelForTokenClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 21:58:09,592 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 21:58:09,593 >>   Num examples = 760
[INFO|trainer.py:3134] 2023-11-15 21:58:09,593 >>   Batch size = 8
{'eval_loss': 0.28536975383758545, 'eval_accuracy': 0.906578947368421, 'eval_micro_f1': 0.906578947368421, 'eval_macro_f1': 0.9039632878242331, 'eval_runtime': 0.9214, 'eval_samples_per_second': 824.862, 'eval_steps_per_second': 103.108, 'epoch': 4.0}
{'loss': 0.158, 'learning_rate': 0.0, 'epoch': 5.0}

  0%|          | 0/95 [00:00<?, ?it/s][A
 13%|â–ˆâ–Ž        | 12/95 [00:00<00:00, 116.84it/s][A
 25%|â–ˆâ–ˆâ–Œ       | 24/95 [00:00<00:00, 109.69it/s][A
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 35/95 [00:00<00:00, 107.99it/s][A
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 46/95 [00:00<00:00, 107.19it/s][A
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 57/95 [00:00<00:00, 106.84it/s][A
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 68/95 [00:00<00:00, 106.55it/s][A
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 79/95 [00:00<00:00, 106.37it/s][A
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 90/95 [00:00<00:00, 106.17it/s][A                                                   
                                                [A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1070/1070 [01:20<00:00, 14.17it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 95/95 [00:00<00:00, 106.17it/s][A
                                                [A[INFO|trainer.py:1963] 2023-11-15 21:58:10,526 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                   100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1070/1070 [01:20<00:00, 14.17it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1070/1070 [01:20<00:00, 13.32it/s]
[INFO|trainer.py:2855] 2023-11-15 21:58:10,529 >> Saving model checkpoint to ./result/agnews_sup_roberta-base_seed0_lora
[INFO|tokenization_utils_base.py:2235] 2023-11-15 21:58:10,648 >> tokenizer config file saved in ./result/agnews_sup_roberta-base_seed0_lora/tokenizer_config.json
[INFO|tokenization_utils_base.py:2242] 2023-11-15 21:58:10,650 >> Special tokens file saved in ./result/agnews_sup_roberta-base_seed0_lora/special_tokens_map.json
{'eval_loss': 0.29251375794410706, 'eval_accuracy': 0.9078947368421053, 'eval_micro_f1': 0.9078947368421053, 'eval_macro_f1': 0.9053963916163836, 'eval_runtime': 0.9307, 'eval_samples_per_second': 816.588, 'eval_steps_per_second': 102.073, 'epoch': 5.0}
{'train_runtime': 80.336, 'train_samples_per_second': 425.712, 'train_steps_per_second': 13.319, 'train_loss': 0.25310126331364996, 'epoch': 5.0}
***** train metrics *****
  epoch                    =        5.0
  train_loss               =     0.2531
  train_runtime            = 0:01:20.33
  train_samples            =       6840
  train_samples_per_second =    425.712
  train_steps_per_second   =     13.319
11/15/2023 21:58:10 - INFO - __main__ - *** Evaluate ***
[INFO|trainer.py:755] 2023-11-15 21:58:10,777 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModelForTokenClassification.forward` and have been ignored: text. If text are not expected by `PeftModelForTokenClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 21:58:10,779 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 21:58:10,780 >>   Num examples = 760
[INFO|trainer.py:3134] 2023-11-15 21:58:10,780 >>   Batch size = 8
  0%|          | 0/95 [00:00<?, ?it/s] 13%|â–ˆâ–Ž        | 12/95 [00:00<00:00, 116.46it/s] 25%|â–ˆâ–ˆâ–Œ       | 24/95 [00:00<00:00, 110.55it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 36/95 [00:00<00:00, 108.66it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 47/95 [00:00<00:00, 107.71it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 58/95 [00:00<00:00, 106.86it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 69/95 [00:00<00:00, 106.75it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 80/95 [00:00<00:00, 105.78it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 91/95 [00:00<00:00, 106.00it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 95/95 [00:00<00:00, 103.87it/s]
***** eval metrics *****
  epoch                   =        5.0
  eval_accuracy           =     0.9079
  eval_loss               =     0.2925
  eval_macro_f1           =     0.9054
  eval_micro_f1           =     0.9079
  eval_runtime            = 0:00:00.92
  eval_samples            =        760
  eval_samples_per_second =    818.925
  eval_steps_per_second   =    102.366
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                  eval/accuracy â–â–ˆâ–ˆâ–‡â–‡â–‡
wandb:                      eval/loss â–ˆâ–â–ƒâ–ƒâ–„â–„
wandb:                  eval/macro_f1 â–â–ˆâ–ˆâ–‡â–‡â–‡
wandb:                  eval/micro_f1 â–â–ˆâ–ˆâ–‡â–‡â–‡
wandb:                   eval/runtime â–â–ƒâ–‚â–ƒâ–ˆâ–‡
wandb:        eval/samples_per_second â–ˆâ–†â–‡â–†â–â–‚
wandb:          eval/steps_per_second â–ˆâ–†â–‡â–†â–â–‚
wandb:                    train/epoch â–â–â–ƒâ–ƒâ–…â–…â–†â–†â–ˆâ–ˆâ–ˆâ–ˆ
wandb:              train/global_step â–â–â–ƒâ–ƒâ–„â–„â–†â–†â–ˆâ–ˆâ–ˆâ–ˆ
wandb:            train/learning_rate â–ˆâ–†â–…â–ƒâ–
wandb:                     train/loss â–ˆâ–„â–ƒâ–‚â–
wandb:               train/total_flos â–
wandb:               train/train_loss â–
wandb:            train/train_runtime â–
wandb: train/train_samples_per_second â–
wandb:   train/train_steps_per_second â–
wandb: 
wandb: Run summary:
wandb:                  eval/accuracy 0.90789
wandb:                      eval/loss 0.29251
wandb:                  eval/macro_f1 0.9054
wandb:                  eval/micro_f1 0.90789
wandb:                   eval/runtime 0.928
wandb:        eval/samples_per_second 818.925
wandb:          eval/steps_per_second 102.366
wandb:                    train/epoch 5.0
wandb:              train/global_step 1070
wandb:            train/learning_rate 0.0
wandb:                     train/loss 0.158
wandb:               train/total_flos 1140362523648000.0
wandb:               train/train_loss 0.2531
wandb:            train/train_runtime 80.336
wandb: train/train_samples_per_second 425.712
wandb:   train/train_steps_per_second 13.319
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /ceph/home/wangyifei/a3/wandb/offline-run-20231115_215533-gbw3e1cq
wandb: Find logs at: ./wandb/offline-run-20231115_215533-gbw3e1cq/logs
(ModelArguments(model_name_or_path='roberta-base', cache_dir='./cache', use_adapter=False, use_lora=True), DataTrainingArguments(dataset_name='restaurant', max_seq_length=64), TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.0005,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=./result/restaurant_roberta-base_seed1_lora/runs/Nov15_21-58-21_node010,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.EPOCH,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=5.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
output_dir=./result/restaurant_roberta-base_seed1_lora,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
resume_from_checkpoint=None,
run_name=./result/restaurant_roberta-base_seed1_lora,
save_on_each_node=False,
save_safetensors=False,
save_steps=500,
save_strategy=IntervalStrategy.NO,
save_total_limit=None,
seed=222,
sharded_ddp=[],
skip_memory_metrics=True,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
))
11/15/2023 21:58:21 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False
11/15/2023 21:58:21 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.0005,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=./result/restaurant_roberta-base_seed1_lora/runs/Nov15_21-58-20_node010,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.EPOCH,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=5.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
output_dir=./result/restaurant_roberta-base_seed1_lora,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
resume_from_checkpoint=None,
run_name=./result/restaurant_roberta-base_seed1_lora,
save_on_each_node=False,
save_safetensors=False,
save_steps=500,
save_strategy=IntervalStrategy.NO,
save_total_limit=None,
seed=222,
sharded_ddp=[],
skip_memory_metrics=True,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
wandb: Tracking run with wandb version 0.15.12
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
Map:   0%|          | 0/4722 [00:00<?, ? examples/s]Map:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4183/4722 [00:00<00:00, 41568.69 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4722/4722 [00:00<00:00, 40814.55 examples/s]
[INFO|configuration_utils.py:715] 2023-11-15 21:58:36,880 >> loading configuration file config.json from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/config.json
[INFO|configuration_utils.py:775] 2023-11-15 21:58:36,889 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.33.3",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_auto.py:535] 2023-11-15 21:58:46,905 >> Could not locate the tokenizer configuration file, will try to use the model config instead.
[INFO|configuration_utils.py:715] 2023-11-15 21:58:56,921 >> loading configuration file config.json from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/config.json
[INFO|configuration_utils.py:775] 2023-11-15 21:58:56,922 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.33.3",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:1852] 2023-11-15 21:59:16,971 >> loading file vocab.json from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/vocab.json
[INFO|tokenization_utils_base.py:1852] 2023-11-15 21:59:16,972 >> loading file merges.txt from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/merges.txt
[INFO|tokenization_utils_base.py:1852] 2023-11-15 21:59:16,972 >> loading file tokenizer.json from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/tokenizer.json
[INFO|tokenization_utils_base.py:1852] 2023-11-15 21:59:16,972 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1852] 2023-11-15 21:59:16,972 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1852] 2023-11-15 21:59:16,973 >> loading file tokenizer_config.json from cache at None
[INFO|configuration_utils.py:715] 2023-11-15 21:59:16,974 >> loading configuration file config.json from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/config.json
[INFO|configuration_utils.py:775] 2023-11-15 21:59:16,974 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.33.3",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:2862] 2023-11-15 21:59:37,144 >> loading weights file model.safetensors from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/model.safetensors
[INFO|modeling_utils.py:3638] 2023-11-15 21:59:37,841 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.bias']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3650] 2023-11-15 21:59:37,842 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
trainable params: 1,284,867 || all params: 125,830,662 || trainable%: 1.0211080348603745
model_max_length: 512
Running tokenizer on dataset:   0%|          | 0/3777 [00:00<?, ? examples/s]Running tokenizer on dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3777/3777 [00:00<00:00, 25020.55 examples/s]Running tokenizer on dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3777/3777 [00:00<00:00, 24709.55 examples/s]
Running tokenizer on dataset:   0%|          | 0/945 [00:00<?, ? examples/s]Running tokenizer on dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 945/945 [00:00<00:00, 27923.81 examples/s]
11/15/2023 21:59:38 - INFO - __main__ - Sample 3190 of the training set: {'text': 'priced <SEP> The food is great and reasonably priced.', 'label': 0, 'input_ids': [0, 18288, 28696, 3388, 510, 15698, 20, 689, 16, 372, 8, 15646, 7663, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
11/15/2023 21:59:38 - INFO - __main__ - Sample 441 of the training set: {'text': 'dhosas <SEP> I like the somosas, chai, and the chole, but the dhosas and dhal were kinda dissapointing.', 'label': 2, 'input_ids': [0, 16593, 366, 281, 28696, 3388, 510, 15698, 38, 101, 5, 16487, 366, 281, 6, 1855, 1439, 6, 8, 5, 14310, 459, 6, 53, 5, 19480, 366, 281, 8, 385, 11124, 58, 24282, 14863, 1115, 15494, 154, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
11/15/2023 21:59:38 - INFO - __main__ - Sample 963 of the training set: {'text': 'beef carpaachio <SEP> Service was warm and attentive, beef carpaachio was exellent (huge portion) and pasta was fresh and well-prepared.', 'label': 0, 'input_ids': [0, 1610, 4550, 512, 6709, 1488, 1020, 28696, 3388, 510, 15698, 1841, 21, 3279, 8, 36670, 6, 6829, 512, 6709, 1488, 1020, 21, 1931, 1641, 1342, 36, 30214, 4745, 43, 8, 18236, 21, 2310, 8, 157, 12, 5234, 33160, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
11/15/2023 21:59:38 - WARNING - accelerate.utils.other - Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[INFO|trainer.py:755] 2023-11-15 21:59:39,146 >> The following columns in the training set don't have a corresponding argument in `PeftModelForTokenClassification.forward` and have been ignored: text. If text are not expected by `PeftModelForTokenClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:1715] 2023-11-15 21:59:39,156 >> ***** Running training *****
[INFO|trainer.py:1716] 2023-11-15 21:59:39,156 >>   Num examples = 3,777
[INFO|trainer.py:1717] 2023-11-15 21:59:39,157 >>   Num Epochs = 5
[INFO|trainer.py:1718] 2023-11-15 21:59:39,157 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1721] 2023-11-15 21:59:39,157 >>   Total train batch size (w. parallel, distributed & accumulation) = 32
[INFO|trainer.py:1722] 2023-11-15 21:59:39,157 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:1723] 2023-11-15 21:59:39,158 >>   Total optimization steps = 595
[INFO|trainer.py:1724] 2023-11-15 21:59:39,158 >>   Number of trainable parameters = 1,284,867
[INFO|integration_utils.py:716] 2023-11-15 21:59:39,160 >> Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"
  0%|          | 0/595 [00:00<?, ?it/s]  0%|          | 1/595 [00:00<09:49,  1.01it/s]  1%|          | 3/595 [00:01<03:02,  3.24it/s]  1%|          | 5/595 [00:01<01:49,  5.39it/s]  1%|          | 7/595 [00:01<01:20,  7.34it/s]  2%|â–         | 9/595 [00:01<01:05,  9.01it/s]  2%|â–         | 11/595 [00:01<00:56, 10.35it/s]  2%|â–         | 13/595 [00:01<00:50, 11.46it/s]  3%|â–Ž         | 15/595 [00:01<00:47, 12.27it/s]  3%|â–Ž         | 17/595 [00:02<00:45, 12.79it/s]  3%|â–Ž         | 19/595 [00:02<00:43, 13.23it/s]  4%|â–Ž         | 21/595 [00:02<00:42, 13.57it/s]  4%|â–         | 23/595 [00:02<00:41, 13.78it/s]  4%|â–         | 25/595 [00:02<00:40, 13.96it/s]  5%|â–         | 27/595 [00:02<00:40, 14.08it/s]  5%|â–         | 29/595 [00:02<00:40, 14.12it/s]  5%|â–Œ         | 31/595 [00:03<00:39, 14.21it/s]  6%|â–Œ         | 33/595 [00:03<00:39, 14.26it/s]  6%|â–Œ         | 35/595 [00:03<00:39, 14.24it/s]  6%|â–Œ         | 37/595 [00:03<00:39, 14.28it/s]  7%|â–‹         | 39/595 [00:03<00:38, 14.32it/s]  7%|â–‹         | 41/595 [00:03<00:38, 14.36it/s]  7%|â–‹         | 43/595 [00:03<00:38, 14.34it/s]  8%|â–Š         | 45/595 [00:04<00:38, 14.38it/s]  8%|â–Š         | 47/595 [00:04<00:38, 14.39it/s]  8%|â–Š         | 49/595 [00:04<00:37, 14.44it/s]  9%|â–Š         | 51/595 [00:04<00:37, 14.43it/s]  9%|â–‰         | 53/595 [00:04<00:37, 14.42it/s]  9%|â–‰         | 55/595 [00:04<00:37, 14.42it/s] 10%|â–‰         | 57/595 [00:04<00:37, 14.43it/s] 10%|â–‰         | 59/595 [00:05<00:37, 14.44it/s] 10%|â–ˆ         | 61/595 [00:05<00:36, 14.45it/s] 11%|â–ˆ         | 63/595 [00:05<00:36, 14.44it/s] 11%|â–ˆ         | 65/595 [00:05<00:36, 14.43it/s] 11%|â–ˆâ–        | 67/595 [00:05<00:36, 14.41it/s] 12%|â–ˆâ–        | 69/595 [00:05<00:36, 14.41it/s] 12%|â–ˆâ–        | 71/595 [00:05<00:36, 14.42it/s] 12%|â–ˆâ–        | 73/595 [00:05<00:36, 14.43it/s] 13%|â–ˆâ–Ž        | 75/595 [00:06<00:36, 14.43it/s] 13%|â–ˆâ–Ž        | 77/595 [00:06<00:35, 14.46it/s] 13%|â–ˆâ–Ž        | 79/595 [00:06<00:35, 14.44it/s] 14%|â–ˆâ–Ž        | 81/595 [00:06<00:35, 14.41it/s] 14%|â–ˆâ–        | 83/595 [00:06<00:35, 14.41it/s] 14%|â–ˆâ–        | 85/595 [00:06<00:35, 14.40it/s] 15%|â–ˆâ–        | 87/595 [00:06<00:35, 14.40it/s] 15%|â–ˆâ–        | 89/595 [00:07<00:35, 14.42it/s] 15%|â–ˆâ–Œ        | 91/595 [00:07<00:34, 14.43it/s] 16%|â–ˆâ–Œ        | 93/595 [00:07<00:34, 14.44it/s] 16%|â–ˆâ–Œ        | 95/595 [00:07<00:34, 14.43it/s] 16%|â–ˆâ–‹        | 97/595 [00:07<00:34, 14.41it/s] 17%|â–ˆâ–‹        | 99/595 [00:07<00:34, 14.40it/s] 17%|â–ˆâ–‹        | 101/595 [00:07<00:34, 14.39it/s] 17%|â–ˆâ–‹        | 103/595 [00:08<00:34, 14.40it/s] 18%|â–ˆâ–Š        | 105/595 [00:08<00:34, 14.41it/s] 18%|â–ˆâ–Š        | 107/595 [00:08<00:33, 14.43it/s] 18%|â–ˆâ–Š        | 109/595 [00:08<00:33, 14.45it/s] 19%|â–ˆâ–Š        | 111/595 [00:08<00:33, 14.45it/s] 19%|â–ˆâ–‰        | 113/595 [00:08<00:33, 14.42it/s] 19%|â–ˆâ–‰        | 115/595 [00:08<00:33, 14.40it/s] 20%|â–ˆâ–‰        | 117/595 [00:09<00:33, 14.40it/s] 20%|â–ˆâ–ˆ        | 119/595 [00:09<00:30, 15.71it/s]                                                  20%|â–ˆâ–ˆ        | 119/595 [00:09<00:30, 15.71it/s][INFO|trainer.py:755] 2023-11-15 21:59:48,312 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModelForTokenClassification.forward` and have been ignored: text. If text are not expected by `PeftModelForTokenClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 21:59:48,315 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 21:59:48,316 >>   Num examples = 945
[INFO|trainer.py:3134] 2023-11-15 21:59:48,316 >>   Batch size = 8
{'loss': 0.7132, 'learning_rate': 0.0004, 'epoch': 1.0}

  0%|          | 0/119 [00:00<?, ?it/s][A
 10%|â–ˆ         | 12/119 [00:00<00:00, 118.17it/s][A
 20%|â–ˆâ–ˆ        | 24/119 [00:00<00:00, 110.86it/s][A
 30%|â–ˆâ–ˆâ–ˆ       | 36/119 [00:00<00:00, 109.46it/s][A
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 47/119 [00:00<00:00, 109.14it/s][A
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 58/119 [00:00<00:00, 108.48it/s][A
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 69/119 [00:00<00:00, 107.91it/s][A
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 80/119 [00:00<00:00, 106.19it/s][A
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 91/119 [00:00<00:00, 105.06it/s][A
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 102/119 [00:00<00:00, 105.99it/s][A
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 113/119 [00:01<00:00, 106.73it/s][A                                                 
                                                  [A 20%|â–ˆâ–ˆ        | 119/595 [00:10<00:30, 15.71it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 119/119 [00:01<00:00, 106.73it/s][A
                                                  [A 20%|â–ˆâ–ˆ        | 121/595 [00:10<01:53,  4.17it/s] 21%|â–ˆâ–ˆ        | 123/595 [00:10<01:29,  5.30it/s] 21%|â–ˆâ–ˆ        | 125/595 [00:10<01:11,  6.54it/s] 21%|â–ˆâ–ˆâ–       | 127/595 [00:10<00:59,  7.82it/s] 22%|â–ˆâ–ˆâ–       | 129/595 [00:11<00:51,  9.07it/s] 22%|â–ˆâ–ˆâ–       | 131/595 [00:11<00:45, 10.19it/s] 22%|â–ˆâ–ˆâ–       | 133/595 [00:11<00:41, 11.18it/s] 23%|â–ˆâ–ˆâ–Ž       | 135/595 [00:11<00:38, 12.00it/s] 23%|â–ˆâ–ˆâ–Ž       | 137/595 [00:11<00:36, 12.64it/s] 23%|â–ˆâ–ˆâ–Ž       | 139/595 [00:11<00:34, 13.09it/s] 24%|â–ˆâ–ˆâ–Ž       | 141/595 [00:11<00:33, 13.45it/s] 24%|â–ˆâ–ˆâ–       | 143/595 [00:11<00:32, 13.71it/s] 24%|â–ˆâ–ˆâ–       | 145/595 [00:12<00:32, 13.92it/s] 25%|â–ˆâ–ˆâ–       | 147/595 [00:12<00:31, 14.07it/s] 25%|â–ˆâ–ˆâ–Œ       | 149/595 [00:12<00:31, 14.17it/s] 25%|â–ˆâ–ˆâ–Œ       | 151/595 [00:12<00:31, 14.25it/s] 26%|â–ˆâ–ˆâ–Œ       | 153/595 [00:12<00:30, 14.28it/s] 26%|â–ˆâ–ˆâ–Œ       | 155/595 [00:12<00:30, 14.35it/s] 26%|â–ˆâ–ˆâ–‹       | 157/595 [00:12<00:30, 14.38it/s] 27%|â–ˆâ–ˆâ–‹       | 159/595 [00:13<00:30, 14.37it/s] 27%|â–ˆâ–ˆâ–‹       | 161/595 [00:13<00:30, 14.38it/s] 27%|â–ˆâ–ˆâ–‹       | 163/595 [00:13<00:29, 14.40it/s] 28%|â–ˆâ–ˆâ–Š       | 165/595 [00:13<00:29, 14.40it/s] 28%|â–ˆâ–ˆâ–Š       | 167/595 [00:13<00:29, 14.43it/s] 28%|â–ˆâ–ˆâ–Š       | 169/595 [00:13<00:29, 14.44it/s] 29%|â–ˆâ–ˆâ–Š       | 171/595 [00:13<00:29, 14.44it/s] 29%|â–ˆâ–ˆâ–‰       | 173/595 [00:14<00:29, 14.43it/s] 29%|â–ˆâ–ˆâ–‰       | 175/595 [00:14<00:29, 14.43it/s] 30%|â–ˆâ–ˆâ–‰       | 177/595 [00:14<00:29, 14.41it/s] 30%|â–ˆâ–ˆâ–ˆ       | 179/595 [00:14<00:28, 14.38it/s] 30%|â–ˆâ–ˆâ–ˆ       | 181/595 [00:14<00:29, 14.24it/s] 31%|â–ˆâ–ˆâ–ˆ       | 183/595 [00:14<00:28, 14.26it/s] 31%|â–ˆâ–ˆâ–ˆ       | 185/595 [00:14<00:28, 14.30it/s] 31%|â–ˆâ–ˆâ–ˆâ–      | 187/595 [00:15<00:29, 13.80it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 189/595 [00:15<00:29, 13.93it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 191/595 [00:15<00:28, 14.09it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 193/595 [00:15<00:28, 14.20it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 195/595 [00:15<00:28, 14.26it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 197/595 [00:15<00:27, 14.32it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 199/595 [00:15<00:27, 14.35it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 201/595 [00:16<00:27, 14.28it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 203/595 [00:16<00:27, 14.26it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 205/595 [00:16<00:27, 14.32it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 207/595 [00:16<00:27, 14.32it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 209/595 [00:16<00:27, 14.29it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 211/595 [00:16<00:26, 14.32it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 213/595 [00:16<00:26, 14.34it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 215/595 [00:17<00:26, 14.38it/s] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 217/595 [00:17<00:26, 14.40it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 219/595 [00:17<00:26, 14.43it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 221/595 [00:17<00:25, 14.44it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 223/595 [00:17<00:25, 14.43it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 225/595 [00:17<00:25, 14.43it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 227/595 [00:17<00:25, 14.43it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 229/595 [00:17<00:25, 14.42it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 231/595 [00:18<00:25, 14.40it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 233/595 [00:18<00:25, 14.38it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 235/595 [00:18<00:24, 14.41it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 237/595 [00:18<00:24, 14.50it/s]                                                  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 238/595 [00:18<00:24, 14.50it/s][INFO|trainer.py:755] 2023-11-15 21:59:57,719 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModelForTokenClassification.forward` and have been ignored: text. If text are not expected by `PeftModelForTokenClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 21:59:57,721 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 21:59:57,722 >>   Num examples = 945
[INFO|trainer.py:3134] 2023-11-15 21:59:57,722 >>   Batch size = 8
{'eval_loss': 0.501337468624115, 'eval_accuracy': 0.8052910052910053, 'eval_micro_f1': 0.8052910052910053, 'eval_macro_f1': 0.6848386264598297, 'eval_runtime': 1.1512, 'eval_samples_per_second': 820.897, 'eval_steps_per_second': 103.372, 'epoch': 1.0}
{'loss': 0.5122, 'learning_rate': 0.0003, 'epoch': 2.0}

  0%|          | 0/119 [00:00<?, ?it/s][A
 10%|â–ˆ         | 12/119 [00:00<00:00, 117.91it/s][A
 20%|â–ˆâ–ˆ        | 24/119 [00:00<00:00, 112.10it/s][A
 30%|â–ˆâ–ˆâ–ˆ       | 36/119 [00:00<00:00, 110.31it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 48/119 [00:00<00:00, 108.87it/s][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 59/119 [00:00<00:00, 108.73it/s][A
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 70/119 [00:00<00:00, 107.30it/s][A
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 81/119 [00:00<00:00, 107.68it/s][A
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 92/119 [00:00<00:00, 107.83it/s][A
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 103/119 [00:00<00:00, 107.64it/s][A
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 114/119 [00:01<00:00, 107.70it/s][A                                                 
                                                  [A 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 238/595 [00:19<00:24, 14.50it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 119/119 [00:01<00:00, 107.70it/s][A
                                                  [A 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 239/595 [00:19<01:24,  4.23it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 241/595 [00:19<01:05,  5.37it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 243/595 [00:20<00:53,  6.62it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 245/595 [00:20<00:44,  7.88it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 247/595 [00:20<00:38,  9.11it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 249/595 [00:20<00:33, 10.24it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 251/595 [00:20<00:30, 11.19it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 253/595 [00:20<00:28, 11.99it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 255/595 [00:20<00:27, 12.58it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 257/595 [00:21<00:26, 12.93it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 259/595 [00:21<00:25, 13.22it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 261/595 [00:21<00:24, 13.40it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 263/595 [00:21<00:24, 13.67it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 265/595 [00:21<00:23, 13.89it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 267/595 [00:21<00:23, 13.99it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 269/595 [00:21<00:23, 14.10it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 271/595 [00:22<00:22, 14.17it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 273/595 [00:22<00:22, 14.20it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 275/595 [00:22<00:22, 14.23it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 277/595 [00:22<00:22, 14.26it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 279/595 [00:22<00:22, 14.28it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 281/595 [00:22<00:22, 14.25it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 283/595 [00:22<00:21, 14.23it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 285/595 [00:23<00:21, 14.24it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 287/595 [00:23<00:21, 14.24it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 289/595 [00:23<00:21, 14.30it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 291/595 [00:23<00:21, 14.25it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 293/595 [00:23<00:21, 14.25it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 295/595 [00:23<00:21, 14.20it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 297/595 [00:23<00:21, 14.17it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 299/595 [00:23<00:20, 14.21it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 301/595 [00:24<00:20, 14.26it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 303/595 [00:24<00:20, 14.32it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 305/595 [00:24<00:20, 14.37it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 307/595 [00:24<00:20, 14.39it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 309/595 [00:24<00:19, 14.38it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 311/595 [00:24<00:19, 14.38it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 313/595 [00:24<00:19, 14.36it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 315/595 [00:25<00:19, 14.36it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 317/595 [00:25<00:19, 14.38it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 319/595 [00:25<00:19, 14.39it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 321/595 [00:25<00:19, 14.39it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 323/595 [00:25<00:18, 14.41it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 325/595 [00:25<00:18, 14.39it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 327/595 [00:25<00:18, 14.39it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 329/595 [00:26<00:18, 14.38it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 331/595 [00:26<00:18, 14.37it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 333/595 [00:26<00:18, 14.36it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 335/595 [00:26<00:18, 14.36it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 337/595 [00:26<00:17, 14.34it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 339/595 [00:26<00:17, 14.36it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 341/595 [00:26<00:17, 14.35it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 343/595 [00:27<00:17, 14.36it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 345/595 [00:27<00:17, 14.37it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 347/595 [00:27<00:17, 14.37it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 349/595 [00:27<00:17, 14.35it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 351/595 [00:27<00:16, 14.38it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 353/595 [00:27<00:16, 14.40it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 355/595 [00:27<00:16, 14.39it/s]                                                  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 357/595 [00:27<00:16, 14.39it/s][INFO|trainer.py:755] 2023-11-15 22:00:07,145 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModelForTokenClassification.forward` and have been ignored: text. If text are not expected by `PeftModelForTokenClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 22:00:07,147 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 22:00:07,148 >>   Num examples = 945
[INFO|trainer.py:3134] 2023-11-15 22:00:07,148 >>   Batch size = 8
{'eval_loss': 0.45856741070747375, 'eval_accuracy': 0.8095238095238095, 'eval_micro_f1': 0.8095238095238095, 'eval_macro_f1': 0.7203385004594239, 'eval_runtime': 1.1443, 'eval_samples_per_second': 825.861, 'eval_steps_per_second': 103.997, 'epoch': 2.0}
{'loss': 0.4184, 'learning_rate': 0.0002, 'epoch': 3.0}

  0%|          | 0/119 [00:00<?, ?it/s][A
 10%|â–ˆ         | 12/119 [00:00<00:00, 118.79it/s][A
 20%|â–ˆâ–ˆ        | 24/119 [00:00<00:00, 112.09it/s][A
 30%|â–ˆâ–ˆâ–ˆ       | 36/119 [00:00<00:00, 110.26it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 48/119 [00:00<00:00, 108.57it/s][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 59/119 [00:00<00:00, 108.44it/s][A
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 70/119 [00:00<00:00, 108.16it/s][A
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 81/119 [00:00<00:00, 108.12it/s][A
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 92/119 [00:00<00:00, 107.80it/s][A
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 103/119 [00:00<00:00, 107.91it/s][A
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 114/119 [00:01<00:00, 107.97it/s][A                                                 
                                                  [A 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 357/595 [00:29<00:16, 14.39it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 119/119 [00:01<00:00, 107.97it/s][A
                                                  [A 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 358/595 [00:29<00:50,  4.69it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 360/595 [00:29<00:40,  5.74it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 362/595 [00:29<00:33,  6.89it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 364/595 [00:29<00:28,  8.09it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 366/595 [00:29<00:24,  9.26it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 368/595 [00:29<00:21, 10.32it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 370/595 [00:30<00:20, 11.24it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 372/595 [00:30<00:18, 12.01it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 374/595 [00:30<00:17, 12.62it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 376/595 [00:30<00:16, 13.09it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 378/595 [00:30<00:16, 13.44it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 380/595 [00:30<00:15, 13.68it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 382/595 [00:30<00:15, 13.88it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 384/595 [00:31<00:15, 14.02it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 386/595 [00:31<00:14, 14.12it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 388/595 [00:31<00:14, 14.20it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 390/595 [00:31<00:14, 14.20it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 392/595 [00:31<00:14, 14.24it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 394/595 [00:31<00:14, 14.29it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 396/595 [00:31<00:13, 14.31it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 398/595 [00:31<00:13, 14.35it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 400/595 [00:32<00:13, 14.38it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 402/595 [00:32<00:13, 14.31it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 404/595 [00:32<00:13, 14.34it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 406/595 [00:32<00:13, 14.36it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 408/595 [00:32<00:13, 14.32it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 410/595 [00:32<00:12, 14.35it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 412/595 [00:32<00:12, 14.31it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 414/595 [00:33<00:12, 14.33it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 416/595 [00:33<00:12, 14.32it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 418/595 [00:33<00:12, 14.32it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 420/595 [00:33<00:12, 14.32it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 422/595 [00:33<00:12, 14.31it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 424/595 [00:33<00:11, 14.30it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 426/595 [00:33<00:11, 14.30it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 428/595 [00:34<00:11, 14.32it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 430/595 [00:34<00:11, 14.32it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 432/595 [00:34<00:11, 14.27it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 434/595 [00:34<00:11, 14.25it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 436/595 [00:34<00:11, 14.23it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 438/595 [00:34<00:11, 14.26it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 440/595 [00:34<00:10, 14.29it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 442/595 [00:35<00:10, 14.29it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 444/595 [00:35<00:10, 14.28it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 446/595 [00:35<00:10, 14.27it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 448/595 [00:35<00:10, 14.30it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 450/595 [00:35<00:10, 14.28it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 452/595 [00:35<00:10, 14.28it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 454/595 [00:35<00:09, 14.31it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 456/595 [00:36<00:09, 14.30it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 458/595 [00:36<00:09, 14.30it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 460/595 [00:36<00:09, 14.30it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 462/595 [00:36<00:09, 14.29it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 464/595 [00:36<00:09, 14.29it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 466/595 [00:36<00:09, 14.28it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 468/595 [00:36<00:08, 14.29it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 470/595 [00:37<00:08, 14.29it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 472/595 [00:37<00:08, 14.30it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 474/595 [00:37<00:08, 14.30it/s]                                                  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 476/595 [00:37<00:08, 14.30it/s][INFO|trainer.py:755] 2023-11-15 22:00:16,562 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModelForTokenClassification.forward` and have been ignored: text. If text are not expected by `PeftModelForTokenClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 22:00:16,563 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 22:00:16,564 >>   Num examples = 945
[INFO|trainer.py:3134] 2023-11-15 22:00:16,564 >>   Batch size = 8
{'eval_loss': 0.3902476131916046, 'eval_accuracy': 0.8391534391534392, 'eval_micro_f1': 0.8391534391534392, 'eval_macro_f1': 0.7681708746517518, 'eval_runtime': 1.1366, 'eval_samples_per_second': 831.427, 'eval_steps_per_second': 104.698, 'epoch': 3.0}
{'loss': 0.3532, 'learning_rate': 0.0001, 'epoch': 4.0}

  0%|          | 0/119 [00:00<?, ?it/s][A
 10%|â–ˆ         | 12/119 [00:00<00:00, 117.20it/s][A
 20%|â–ˆâ–ˆ        | 24/119 [00:00<00:00, 110.78it/s][A
 30%|â–ˆâ–ˆâ–ˆ       | 36/119 [00:00<00:00, 108.97it/s][A
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 47/119 [00:00<00:00, 108.56it/s][A
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 58/119 [00:00<00:00, 108.32it/s][A
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 69/119 [00:00<00:00, 107.76it/s][A
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 80/119 [00:00<00:00, 107.81it/s][A
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 91/119 [00:00<00:00, 107.52it/s][A
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 102/119 [00:00<00:00, 107.20it/s][A
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 113/119 [00:01<00:00, 107.25it/s][A                                                 
                                                  [A 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 476/595 [00:38<00:08, 14.30it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 119/119 [00:01<00:00, 107.25it/s][A
                                                  [A 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 477/595 [00:38<00:25,  4.67it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 479/595 [00:38<00:20,  5.71it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 481/595 [00:38<00:16,  6.87it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 483/595 [00:39<00:13,  8.07it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 485/595 [00:39<00:11,  9.24it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 487/595 [00:39<00:10, 10.31it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 489/595 [00:39<00:09, 11.23it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 491/595 [00:39<00:08, 12.02it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 493/595 [00:39<00:08, 12.63it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 495/595 [00:39<00:07, 13.10it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 497/595 [00:40<00:07, 13.45it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 499/595 [00:40<00:07, 13.70it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 501/595 [00:40<00:06, 13.88it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 503/595 [00:40<00:06, 14.01it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 505/595 [00:40<00:06, 14.09it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 507/595 [00:40<00:06, 14.11it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 509/595 [00:40<00:06, 14.18it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 511/595 [00:40<00:05, 14.23it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 513/595 [00:41<00:05, 14.26it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 515/595 [00:41<00:05, 14.27it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 517/595 [00:41<00:05, 14.27it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 519/595 [00:41<00:05, 14.27it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 521/595 [00:41<00:05, 14.27it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 523/595 [00:41<00:05, 14.28it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 525/595 [00:41<00:04, 14.29it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 527/595 [00:42<00:04, 14.29it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 529/595 [00:42<00:04, 14.29it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 531/595 [00:42<00:04, 14.29it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 533/595 [00:42<00:04, 14.29it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 535/595 [00:42<00:04, 14.29it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 537/595 [00:42<00:04, 14.30it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 539/595 [00:42<00:03, 14.28it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 541/595 [00:43<00:03, 14.27it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 543/595 [00:43<00:03, 14.26it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 545/595 [00:43<00:03, 14.29it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 547/595 [00:43<00:03, 14.35it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 549/595 [00:43<00:03, 14.32it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 551/595 [00:43<00:03, 14.28it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 553/595 [00:43<00:02, 14.27it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 555/595 [00:44<00:02, 14.29it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 557/595 [00:44<00:02, 14.30it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 559/595 [00:44<00:02, 14.28it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 561/595 [00:44<00:02, 14.27it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 563/595 [00:44<00:02, 14.29it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 565/595 [00:44<00:02, 14.30it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 567/595 [00:44<00:01, 14.30it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 569/595 [00:45<00:01, 14.30it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 571/595 [00:45<00:01, 14.29it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 573/595 [00:45<00:01, 14.29it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 575/595 [00:45<00:01, 14.29it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 577/595 [00:45<00:01, 14.29it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 579/595 [00:45<00:01, 14.29it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 581/595 [00:45<00:00, 14.28it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 583/595 [00:46<00:00, 14.29it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 585/595 [00:46<00:00, 14.29it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 587/595 [00:46<00:00, 14.29it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 589/595 [00:46<00:00, 14.29it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 591/595 [00:46<00:00, 14.29it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 593/595 [00:46<00:00, 14.29it/s]                                                 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 595/595 [00:46<00:00, 14.29it/s][INFO|trainer.py:755] 2023-11-15 22:00:25,988 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModelForTokenClassification.forward` and have been ignored: text. If text are not expected by `PeftModelForTokenClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 22:00:25,990 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 22:00:25,990 >>   Num examples = 945
[INFO|trainer.py:3134] 2023-11-15 22:00:25,990 >>   Batch size = 8
{'eval_loss': 0.37067753076553345, 'eval_accuracy': 0.8560846560846561, 'eval_micro_f1': 0.8560846560846561, 'eval_macro_f1': 0.7808155331815287, 'eval_runtime': 1.1423, 'eval_samples_per_second': 827.306, 'eval_steps_per_second': 104.179, 'epoch': 4.0}
{'loss': 0.321, 'learning_rate': 0.0, 'epoch': 5.0}

  0%|          | 0/119 [00:00<?, ?it/s][A
 10%|â–ˆ         | 12/119 [00:00<00:00, 117.41it/s][A
 20%|â–ˆâ–ˆ        | 24/119 [00:00<00:00, 110.81it/s][A
 30%|â–ˆâ–ˆâ–ˆ       | 36/119 [00:00<00:00, 109.31it/s][A
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 47/119 [00:00<00:00, 107.96it/s][A
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 58/119 [00:00<00:00, 107.19it/s][A
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 69/119 [00:00<00:00, 107.09it/s][A
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 80/119 [00:00<00:00, 107.09it/s][A
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 91/119 [00:00<00:00, 106.93it/s][A
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 102/119 [00:00<00:00, 107.18it/s][A
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 113/119 [00:01<00:00, 106.57it/s][A                                                 
                                                  [A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 595/595 [00:47<00:00, 14.29it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 119/119 [00:01<00:00, 106.57it/s][A
                                                  [A[INFO|trainer.py:1963] 2023-11-15 22:00:27,138 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 595/595 [00:47<00:00, 14.29it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 595/595 [00:47<00:00, 12.40it/s]
[INFO|trainer.py:2855] 2023-11-15 22:00:27,141 >> Saving model checkpoint to ./result/restaurant_roberta-base_seed1_lora
[INFO|tokenization_utils_base.py:2235] 2023-11-15 22:00:27,246 >> tokenizer config file saved in ./result/restaurant_roberta-base_seed1_lora/tokenizer_config.json
[INFO|tokenization_utils_base.py:2242] 2023-11-15 22:00:27,248 >> Special tokens file saved in ./result/restaurant_roberta-base_seed1_lora/special_tokens_map.json
{'eval_loss': 0.36700791120529175, 'eval_accuracy': 0.852910052910053, 'eval_micro_f1': 0.852910052910053, 'eval_macro_f1': 0.781767586676942, 'eval_runtime': 1.1451, 'eval_samples_per_second': 825.249, 'eval_steps_per_second': 103.92, 'epoch': 5.0}
{'train_runtime': 47.9797, 'train_samples_per_second': 393.604, 'train_steps_per_second': 12.401, 'train_loss': 0.46359294602850903, 'epoch': 5.0}
***** train metrics *****
  epoch                    =        5.0
  train_loss               =     0.4636
  train_runtime            = 0:00:47.97
  train_samples            =       3777
  train_samples_per_second =    393.604
  train_steps_per_second   =     12.401
11/15/2023 22:00:27 - INFO - __main__ - *** Evaluate ***
[INFO|trainer.py:755] 2023-11-15 22:00:27,342 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModelForTokenClassification.forward` and have been ignored: text. If text are not expected by `PeftModelForTokenClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 22:00:27,343 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 22:00:27,343 >>   Num examples = 945
[INFO|trainer.py:3134] 2023-11-15 22:00:27,344 >>   Batch size = 8
  0%|          | 0/119 [00:00<?, ?it/s] 10%|â–ˆ         | 12/119 [00:00<00:00, 117.83it/s] 20%|â–ˆâ–ˆ        | 24/119 [00:00<00:00, 111.32it/s] 30%|â–ˆâ–ˆâ–ˆ       | 36/119 [00:00<00:00, 109.42it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 47/119 [00:00<00:00, 108.71it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 58/119 [00:00<00:00, 107.81it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 69/119 [00:00<00:00, 107.33it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 80/119 [00:00<00:00, 107.08it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 91/119 [00:00<00:00, 107.39it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 102/119 [00:00<00:00, 107.61it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 113/119 [00:01<00:00, 107.67it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 119/119 [00:01<00:00, 105.63it/s]
***** eval metrics *****
  epoch                   =        5.0
  eval_accuracy           =     0.8529
  eval_loss               =      0.367
  eval_macro_f1           =     0.7818
  eval_micro_f1           =     0.8529
  eval_runtime            = 0:00:01.14
  eval_samples            =        945
  eval_samples_per_second =     828.81
  eval_steps_per_second   =    104.369
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                  eval/accuracy â–â–‚â–†â–ˆâ–ˆâ–ˆ
wandb:                      eval/loss â–ˆâ–†â–‚â–â–â–
wandb:                  eval/macro_f1 â–â–„â–‡â–ˆâ–ˆâ–ˆ
wandb:                  eval/micro_f1 â–â–‚â–†â–ˆâ–ˆâ–ˆ
wandb:                   eval/runtime â–ˆâ–…â–â–„â–…â–ƒ
wandb:        eval/samples_per_second â–â–„â–ˆâ–…â–„â–†
wandb:          eval/steps_per_second â–â–„â–ˆâ–…â–„â–†
wandb:                    train/epoch â–â–â–ƒâ–ƒâ–…â–…â–†â–†â–ˆâ–ˆâ–ˆâ–ˆ
wandb:              train/global_step â–â–â–ƒâ–ƒâ–…â–…â–†â–†â–ˆâ–ˆâ–ˆâ–ˆ
wandb:            train/learning_rate â–ˆâ–†â–…â–ƒâ–
wandb:                     train/loss â–ˆâ–„â–ƒâ–‚â–
wandb:               train/total_flos â–
wandb:               train/train_loss â–
wandb:            train/train_runtime â–
wandb: train/train_samples_per_second â–
wandb:   train/train_steps_per_second â–
wandb: 
wandb: Run summary:
wandb:                  eval/accuracy 0.85291
wandb:                      eval/loss 0.36701
wandb:                  eval/macro_f1 0.78177
wandb:                  eval/micro_f1 0.85291
wandb:                   eval/runtime 1.1402
wandb:        eval/samples_per_second 828.81
wandb:          eval/steps_per_second 104.369
wandb:                    train/epoch 5.0
wandb:              train/global_step 595
wandb:            train/learning_rate 0.0
wandb:                     train/loss 0.321
wandb:               train/total_flos 629689029684480.0
wandb:               train/train_loss 0.46359
wandb:            train/train_runtime 47.9797
wandb: train/train_samples_per_second 393.604
wandb:   train/train_steps_per_second 12.401
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /ceph/home/wangyifei/a3/wandb/offline-run-20231115_215822-kkvzoe9u
wandb: Find logs at: ./wandb/offline-run-20231115_215822-kkvzoe9u/logs
(ModelArguments(model_name_or_path='roberta-base', cache_dir='./cache', use_adapter=False, use_lora=True), DataTrainingArguments(dataset_name='acl', max_seq_length=64), TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.0005,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=./result/acl_roberta-base_seed1_lora/runs/Nov15_22-00-37_node010,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.EPOCH,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=5.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
output_dir=./result/acl_roberta-base_seed1_lora,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
resume_from_checkpoint=None,
run_name=./result/acl_roberta-base_seed1_lora,
save_on_each_node=False,
save_safetensors=False,
save_steps=500,
save_strategy=IntervalStrategy.NO,
save_total_limit=None,
seed=222,
sharded_ddp=[],
skip_memory_metrics=True,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
))
11/15/2023 22:00:37 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False
11/15/2023 22:00:37 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.0005,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=./result/acl_roberta-base_seed1_lora/runs/Nov15_22-00-37_node010,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.EPOCH,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=5.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
output_dir=./result/acl_roberta-base_seed1_lora,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
resume_from_checkpoint=None,
run_name=./result/acl_roberta-base_seed1_lora,
save_on_each_node=False,
save_safetensors=False,
save_steps=500,
save_strategy=IntervalStrategy.NO,
save_total_limit=None,
seed=222,
sharded_ddp=[],
skip_memory_metrics=True,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
wandb: Tracking run with wandb version 0.15.12
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
Map:   0%|          | 0/11020 [00:00<?, ? examples/s]Map:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 4018/11020 [00:00<00:00, 38953.20 examples/s]Map:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 8244/11020 [00:00<00:00, 40872.88 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11020/11020 [00:00<00:00, 40435.73 examples/s]
[INFO|configuration_utils.py:715] 2023-11-15 22:00:53,694 >> loading configuration file config.json from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/config.json
[INFO|configuration_utils.py:775] 2023-11-15 22:00:53,703 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.33.3",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_auto.py:535] 2023-11-15 22:01:03,719 >> Could not locate the tokenizer configuration file, will try to use the model config instead.
[INFO|configuration_utils.py:715] 2023-11-15 22:01:13,736 >> loading configuration file config.json from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/config.json
[INFO|configuration_utils.py:775] 2023-11-15 22:01:13,737 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.33.3",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:1852] 2023-11-15 22:01:33,821 >> loading file vocab.json from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/vocab.json
[INFO|tokenization_utils_base.py:1852] 2023-11-15 22:01:33,822 >> loading file merges.txt from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/merges.txt
[INFO|tokenization_utils_base.py:1852] 2023-11-15 22:01:33,822 >> loading file tokenizer.json from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/tokenizer.json
[INFO|tokenization_utils_base.py:1852] 2023-11-15 22:01:33,822 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1852] 2023-11-15 22:01:33,823 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1852] 2023-11-15 22:01:33,823 >> loading file tokenizer_config.json from cache at None
[INFO|configuration_utils.py:715] 2023-11-15 22:01:33,824 >> loading configuration file config.json from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/config.json
[INFO|configuration_utils.py:775] 2023-11-15 22:01:33,825 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.33.3",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:2862] 2023-11-15 22:01:53,990 >> loading weights file model.safetensors from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/model.safetensors
[INFO|modeling_utils.py:3638] 2023-11-15 22:01:54,698 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3650] 2023-11-15 22:01:54,699 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
trainable params: 1,284,867 || all params: 125,830,662 || trainable%: 1.0211080348603745
model_max_length: 512
Running tokenizer on dataset:   0%|          | 0/8816 [00:00<?, ? examples/s]Running tokenizer on dataset:  23%|â–ˆâ–ˆâ–Ž       | 2000/8816 [00:00<00:00, 17209.55 examples/s]Running tokenizer on dataset:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 6000/8816 [00:00<00:00, 19499.84 examples/s]Running tokenizer on dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8816/8816 [00:00<00:00, 19770.42 examples/s]Running tokenizer on dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8816/8816 [00:00<00:00, 19431.43 examples/s]
Running tokenizer on dataset:   0%|          | 0/2204 [00:00<?, ? examples/s]Running tokenizer on dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2204/2204 [00:00<00:00, 21139.18 examples/s]Running tokenizer on dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2204/2204 [00:00<00:00, 20877.74 examples/s]
11/15/2023 22:01:55 - INFO - __main__ - Sample 1767 of the training set: {'text': 'Second, HASM cells in culture, when observed between 3 and 6 h after plating, were not spindle shaped or aligned in parallel, as they are at the tissue level (5); instead, they were irregularly shaped (Fig.', 'label': 0, 'input_ids': [0, 32703, 6, 31963, 448, 4590, 11, 2040, 6, 77, 6373, 227, 155, 8, 231, 1368, 71, 2968, 1295, 6, 58, 45, 2292, 38969, 14216, 50, 14485, 11, 12980, 6, 25, 51, 32, 23, 5, 11576, 672, 36, 245, 4397, 1386, 6, 51, 58, 22937, 352, 14216, 36, 44105, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
11/15/2023 22:01:55 - INFO - __main__ - Sample 3854 of the training set: {'text': 'Yet, allowing for interruptions might decrease classification accuracy [24] as well as making results vulnerable to variation in wear time if analyzed with different epoch lengths [37].', 'label': 0, 'input_ids': [0, 34995, 6, 2455, 13, 22749, 2485, 429, 7280, 20257, 8611, 646, 1978, 742, 25, 157, 25, 442, 775, 4478, 7, 21875, 11, 3568, 86, 114, 13773, 19, 430, 43660, 18915, 646, 3272, 8174, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
11/15/2023 22:01:55 - INFO - __main__ - Sample 4652 of the training set: {'text': 'Examination of plaque formation and growth curves were performed by standard methods as previously described (10, 35).', 'label': 1, 'input_ids': [0, 9089, 41121, 9, 22054, 9285, 8, 434, 23739, 58, 3744, 30, 2526, 6448, 25, 1433, 1602, 36, 698, 6, 1718, 322, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
11/15/2023 22:01:55 - WARNING - accelerate.utils.other - Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[INFO|trainer.py:755] 2023-11-15 22:01:56,414 >> The following columns in the training set don't have a corresponding argument in `PeftModelForTokenClassification.forward` and have been ignored: text. If text are not expected by `PeftModelForTokenClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:1715] 2023-11-15 22:01:56,425 >> ***** Running training *****
[INFO|trainer.py:1716] 2023-11-15 22:01:56,425 >>   Num examples = 8,816
[INFO|trainer.py:1717] 2023-11-15 22:01:56,425 >>   Num Epochs = 5
[INFO|trainer.py:1718] 2023-11-15 22:01:56,425 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1721] 2023-11-15 22:01:56,426 >>   Total train batch size (w. parallel, distributed & accumulation) = 32
[INFO|trainer.py:1722] 2023-11-15 22:01:56,426 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:1723] 2023-11-15 22:01:56,426 >>   Total optimization steps = 1,380
[INFO|trainer.py:1724] 2023-11-15 22:01:56,427 >>   Number of trainable parameters = 1,284,867
[INFO|integration_utils.py:716] 2023-11-15 22:01:56,428 >> Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"
  0%|          | 0/1380 [00:00<?, ?it/s]  0%|          | 1/1380 [00:01<23:21,  1.02s/it]  0%|          | 3/1380 [00:01<07:14,  3.17it/s]  0%|          | 5/1380 [00:01<04:19,  5.30it/s]  1%|          | 7/1380 [00:01<03:09,  7.25it/s]  1%|          | 9/1380 [00:01<02:33,  8.93it/s]  1%|          | 11/1380 [00:01<02:12, 10.30it/s]  1%|          | 13/1380 [00:01<02:00, 11.38it/s]  1%|          | 15/1380 [00:01<01:51, 12.20it/s]  1%|          | 17/1380 [00:02<01:46, 12.80it/s]  1%|â–         | 19/1380 [00:02<01:42, 13.26it/s]  2%|â–         | 21/1380 [00:02<01:39, 13.59it/s]  2%|â–         | 23/1380 [00:02<01:37, 13.89it/s]  2%|â–         | 25/1380 [00:02<01:36, 14.04it/s]  2%|â–         | 27/1380 [00:02<01:35, 14.16it/s]  2%|â–         | 29/1380 [00:02<01:35, 14.20it/s]  2%|â–         | 31/1380 [00:03<01:34, 14.26it/s]  2%|â–         | 33/1380 [00:03<01:34, 14.29it/s]  3%|â–Ž         | 35/1380 [00:03<01:34, 14.27it/s]  3%|â–Ž         | 37/1380 [00:03<01:33, 14.30it/s]  3%|â–Ž         | 39/1380 [00:03<01:35, 14.11it/s]  3%|â–Ž         | 41/1380 [00:03<01:35, 14.03it/s]  3%|â–Ž         | 43/1380 [00:03<01:35, 14.02it/s]  3%|â–Ž         | 45/1380 [00:04<01:35, 14.00it/s]  3%|â–Ž         | 47/1380 [00:04<01:34, 14.07it/s]  4%|â–Ž         | 49/1380 [00:04<01:34, 14.15it/s]  4%|â–Ž         | 51/1380 [00:04<01:33, 14.22it/s]  4%|â–         | 53/1380 [00:04<01:33, 14.25it/s]  4%|â–         | 55/1380 [00:04<01:32, 14.25it/s]  4%|â–         | 57/1380 [00:04<01:32, 14.27it/s]  4%|â–         | 59/1380 [00:05<01:32, 14.27it/s]  4%|â–         | 61/1380 [00:05<01:32, 14.29it/s]  5%|â–         | 63/1380 [00:05<01:31, 14.33it/s]  5%|â–         | 65/1380 [00:05<01:31, 14.36it/s]  5%|â–         | 67/1380 [00:05<01:31, 14.37it/s]  5%|â–Œ         | 69/1380 [00:05<01:31, 14.34it/s]  5%|â–Œ         | 71/1380 [00:05<01:31, 14.31it/s]  5%|â–Œ         | 73/1380 [00:06<01:31, 14.33it/s]  5%|â–Œ         | 75/1380 [00:06<01:31, 14.33it/s]  6%|â–Œ         | 77/1380 [00:06<01:30, 14.32it/s]  6%|â–Œ         | 79/1380 [00:06<01:30, 14.33it/s]  6%|â–Œ         | 81/1380 [00:06<01:30, 14.28it/s]  6%|â–Œ         | 83/1380 [00:06<01:31, 14.23it/s]  6%|â–Œ         | 85/1380 [00:06<01:30, 14.26it/s]  6%|â–‹         | 87/1380 [00:07<01:30, 14.28it/s]  6%|â–‹         | 89/1380 [00:07<01:30, 14.29it/s]  7%|â–‹         | 91/1380 [00:07<01:30, 14.28it/s]  7%|â–‹         | 93/1380 [00:07<01:30, 14.19it/s]  7%|â–‹         | 95/1380 [00:07<01:30, 14.16it/s]  7%|â–‹         | 97/1380 [00:07<01:30, 14.20it/s]  7%|â–‹         | 99/1380 [00:07<01:29, 14.25it/s]  7%|â–‹         | 101/1380 [00:08<01:29, 14.25it/s]  7%|â–‹         | 103/1380 [00:08<01:29, 14.27it/s]  8%|â–Š         | 105/1380 [00:08<01:29, 14.26it/s]  8%|â–Š         | 107/1380 [00:08<01:29, 14.28it/s]  8%|â–Š         | 109/1380 [00:08<01:29, 14.27it/s]  8%|â–Š         | 111/1380 [00:08<01:28, 14.30it/s]  8%|â–Š         | 113/1380 [00:08<01:28, 14.29it/s]  8%|â–Š         | 115/1380 [00:08<01:28, 14.29it/s]  8%|â–Š         | 117/1380 [00:09<01:28, 14.31it/s]  9%|â–Š         | 119/1380 [00:09<01:27, 14.33it/s]  9%|â–‰         | 121/1380 [00:09<01:27, 14.35it/s]  9%|â–‰         | 123/1380 [00:09<01:27, 14.37it/s]  9%|â–‰         | 125/1380 [00:09<01:27, 14.32it/s]  9%|â–‰         | 127/1380 [00:09<01:27, 14.35it/s]  9%|â–‰         | 129/1380 [00:09<01:27, 14.33it/s]  9%|â–‰         | 131/1380 [00:10<01:27, 14.35it/s] 10%|â–‰         | 133/1380 [00:10<01:26, 14.35it/s] 10%|â–‰         | 135/1380 [00:10<01:26, 14.34it/s] 10%|â–‰         | 137/1380 [00:10<01:26, 14.34it/s] 10%|â–ˆ         | 139/1380 [00:10<01:26, 14.30it/s] 10%|â–ˆ         | 141/1380 [00:10<01:26, 14.30it/s] 10%|â–ˆ         | 143/1380 [00:10<01:26, 14.30it/s] 11%|â–ˆ         | 145/1380 [00:11<01:26, 14.29it/s] 11%|â–ˆ         | 147/1380 [00:11<01:26, 14.29it/s] 11%|â–ˆ         | 149/1380 [00:11<01:26, 14.31it/s] 11%|â–ˆ         | 151/1380 [00:11<01:26, 14.29it/s] 11%|â–ˆ         | 153/1380 [00:11<01:25, 14.29it/s] 11%|â–ˆ         | 155/1380 [00:11<01:25, 14.30it/s] 11%|â–ˆâ–        | 157/1380 [00:11<01:25, 14.28it/s] 12%|â–ˆâ–        | 159/1380 [00:12<01:25, 14.28it/s] 12%|â–ˆâ–        | 161/1380 [00:12<01:25, 14.29it/s] 12%|â–ˆâ–        | 163/1380 [00:12<01:25, 14.29it/s] 12%|â–ˆâ–        | 165/1380 [00:12<01:24, 14.30it/s] 12%|â–ˆâ–        | 167/1380 [00:12<01:24, 14.33it/s] 12%|â–ˆâ–        | 169/1380 [00:12<01:24, 14.35it/s] 12%|â–ˆâ–        | 171/1380 [00:12<01:24, 14.35it/s] 13%|â–ˆâ–Ž        | 173/1380 [00:13<01:24, 14.37it/s] 13%|â–ˆâ–Ž        | 175/1380 [00:13<01:23, 14.38it/s] 13%|â–ˆâ–Ž        | 177/1380 [00:13<01:23, 14.40it/s] 13%|â–ˆâ–Ž        | 179/1380 [00:13<01:23, 14.38it/s] 13%|â–ˆâ–Ž        | 181/1380 [00:13<01:23, 14.42it/s] 13%|â–ˆâ–Ž        | 183/1380 [00:13<01:22, 14.43it/s] 13%|â–ˆâ–Ž        | 185/1380 [00:13<01:22, 14.42it/s] 14%|â–ˆâ–Ž        | 187/1380 [00:14<01:22, 14.42it/s] 14%|â–ˆâ–Ž        | 189/1380 [00:14<01:22, 14.41it/s] 14%|â–ˆâ–        | 191/1380 [00:14<01:22, 14.41it/s] 14%|â–ˆâ–        | 193/1380 [00:14<01:22, 14.42it/s] 14%|â–ˆâ–        | 195/1380 [00:14<01:22, 14.45it/s] 14%|â–ˆâ–        | 197/1380 [00:14<01:21, 14.45it/s] 14%|â–ˆâ–        | 199/1380 [00:14<01:21, 14.43it/s] 15%|â–ˆâ–        | 201/1380 [00:14<01:21, 14.42it/s] 15%|â–ˆâ–        | 203/1380 [00:15<01:22, 14.34it/s] 15%|â–ˆâ–        | 205/1380 [00:15<01:21, 14.35it/s] 15%|â–ˆâ–Œ        | 207/1380 [00:15<01:21, 14.36it/s] 15%|â–ˆâ–Œ        | 209/1380 [00:15<01:21, 14.36it/s] 15%|â–ˆâ–Œ        | 211/1380 [00:15<01:21, 14.38it/s] 15%|â–ˆâ–Œ        | 213/1380 [00:15<01:21, 14.41it/s] 16%|â–ˆâ–Œ        | 215/1380 [00:15<01:20, 14.43it/s] 16%|â–ˆâ–Œ        | 217/1380 [00:16<01:20, 14.46it/s] 16%|â–ˆâ–Œ        | 219/1380 [00:16<01:20, 14.45it/s] 16%|â–ˆâ–Œ        | 221/1380 [00:16<01:20, 14.45it/s] 16%|â–ˆâ–Œ        | 223/1380 [00:16<01:20, 14.40it/s] 16%|â–ˆâ–‹        | 225/1380 [00:16<01:20, 14.41it/s] 16%|â–ˆâ–‹        | 227/1380 [00:16<01:19, 14.43it/s] 17%|â–ˆâ–‹        | 229/1380 [00:16<01:19, 14.44it/s] 17%|â–ˆâ–‹        | 231/1380 [00:17<01:19, 14.44it/s] 17%|â–ˆâ–‹        | 233/1380 [00:17<01:19, 14.44it/s] 17%|â–ˆâ–‹        | 235/1380 [00:17<01:19, 14.42it/s] 17%|â–ˆâ–‹        | 237/1380 [00:17<01:19, 14.43it/s] 17%|â–ˆâ–‹        | 239/1380 [00:17<01:19, 14.43it/s] 17%|â–ˆâ–‹        | 241/1380 [00:17<01:18, 14.44it/s] 18%|â–ˆâ–Š        | 243/1380 [00:17<01:18, 14.46it/s] 18%|â–ˆâ–Š        | 245/1380 [00:18<01:18, 14.47it/s] 18%|â–ˆâ–Š        | 247/1380 [00:18<01:18, 14.45it/s] 18%|â–ˆâ–Š        | 249/1380 [00:18<01:18, 14.44it/s] 18%|â–ˆâ–Š        | 251/1380 [00:18<01:18, 14.39it/s] 18%|â–ˆâ–Š        | 253/1380 [00:18<01:18, 14.39it/s] 18%|â–ˆâ–Š        | 255/1380 [00:18<01:18, 14.39it/s] 19%|â–ˆâ–Š        | 257/1380 [00:18<01:18, 14.39it/s] 19%|â–ˆâ–‰        | 259/1380 [00:19<01:17, 14.38it/s] 19%|â–ˆâ–‰        | 261/1380 [00:19<01:17, 14.36it/s] 19%|â–ˆâ–‰        | 263/1380 [00:19<01:17, 14.35it/s] 19%|â–ˆâ–‰        | 265/1380 [00:19<01:17, 14.39it/s] 19%|â–ˆâ–‰        | 267/1380 [00:19<01:17, 14.39it/s] 19%|â–ˆâ–‰        | 269/1380 [00:19<01:17, 14.41it/s] 20%|â–ˆâ–‰        | 271/1380 [00:19<01:17, 14.40it/s] 20%|â–ˆâ–‰        | 273/1380 [00:19<01:16, 14.39it/s] 20%|â–ˆâ–‰        | 275/1380 [00:20<01:16, 14.42it/s]                                                   20%|â–ˆâ–ˆ        | 276/1380 [00:20<01:16, 14.42it/s][INFO|trainer.py:755] 2023-11-15 22:02:16,594 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModelForTokenClassification.forward` and have been ignored: text. If text are not expected by `PeftModelForTokenClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 22:02:16,595 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 22:02:16,596 >>   Num examples = 2204
[INFO|trainer.py:3134] 2023-11-15 22:02:16,596 >>   Batch size = 8
{'loss': 0.5066, 'learning_rate': 0.0004, 'epoch': 1.0}

  0%|          | 0/276 [00:00<?, ?it/s][A
  4%|â–         | 12/276 [00:00<00:02, 118.69it/s][A
  9%|â–Š         | 24/276 [00:00<00:02, 112.15it/s][A
 13%|â–ˆâ–Ž        | 36/276 [00:00<00:02, 110.10it/s][A
 17%|â–ˆâ–‹        | 48/276 [00:00<00:02, 108.94it/s][A
 21%|â–ˆâ–ˆâ–       | 59/276 [00:00<00:01, 108.60it/s][A
 25%|â–ˆâ–ˆâ–Œ       | 70/276 [00:00<00:01, 108.38it/s][A
 29%|â–ˆâ–ˆâ–‰       | 81/276 [00:00<00:01, 108.35it/s][A
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 92/276 [00:00<00:01, 108.34it/s][A
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 103/276 [00:00<00:01, 108.08it/s][A
 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 114/276 [00:01<00:01, 107.56it/s][A
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 125/276 [00:01<00:01, 107.60it/s][A
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 136/276 [00:01<00:01, 107.40it/s][A
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 147/276 [00:01<00:01, 107.39it/s][A
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 158/276 [00:01<00:01, 107.55it/s][A
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 169/276 [00:01<00:00, 107.63it/s][A
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 180/276 [00:01<00:00, 107.56it/s][A
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 191/276 [00:01<00:00, 107.63it/s][A
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 202/276 [00:01<00:00, 107.70it/s][A
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 213/276 [00:01<00:00, 107.27it/s][A
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 224/276 [00:02<00:00, 106.00it/s][A
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 235/276 [00:02<00:00, 106.15it/s][A
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 246/276 [00:02<00:00, 106.69it/s][A
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 257/276 [00:02<00:00, 107.12it/s][A
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 268/276 [00:02<00:00, 107.02it/s][A                                                  
                                                  [A 20%|â–ˆâ–ˆ        | 276/1380 [00:22<01:16, 14.42it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 276/276 [00:02<00:00, 107.02it/s][A
                                                  [A 20%|â–ˆâ–ˆ        | 277/1380 [00:22<08:27,  2.17it/s] 20%|â–ˆâ–ˆ        | 279/1380 [00:23<06:17,  2.92it/s] 20%|â–ˆâ–ˆ        | 281/1380 [00:23<04:46,  3.83it/s] 21%|â–ˆâ–ˆ        | 283/1380 [00:23<03:43,  4.91it/s] 21%|â–ˆâ–ˆ        | 285/1380 [00:23<02:58,  6.12it/s] 21%|â–ˆâ–ˆ        | 287/1380 [00:23<02:27,  7.40it/s] 21%|â–ˆâ–ˆ        | 289/1380 [00:23<02:06,  8.66it/s] 21%|â–ˆâ–ˆ        | 291/1380 [00:23<01:50,  9.84it/s] 21%|â–ˆâ–ˆ        | 293/1380 [00:23<01:39, 10.88it/s] 21%|â–ˆâ–ˆâ–       | 295/1380 [00:24<01:32, 11.75it/s] 22%|â–ˆâ–ˆâ–       | 297/1380 [00:24<01:27, 12.42it/s] 22%|â–ˆâ–ˆâ–       | 299/1380 [00:24<01:23, 12.92it/s] 22%|â–ˆâ–ˆâ–       | 301/1380 [00:24<01:21, 13.32it/s] 22%|â–ˆâ–ˆâ–       | 303/1380 [00:24<01:19, 13.60it/s] 22%|â–ˆâ–ˆâ–       | 305/1380 [00:24<01:17, 13.80it/s] 22%|â–ˆâ–ˆâ–       | 307/1380 [00:24<01:16, 13.95it/s] 22%|â–ˆâ–ˆâ–       | 309/1380 [00:25<01:16, 14.06it/s] 23%|â–ˆâ–ˆâ–Ž       | 311/1380 [00:25<01:15, 14.12it/s] 23%|â–ˆâ–ˆâ–Ž       | 313/1380 [00:25<01:15, 14.16it/s] 23%|â–ˆâ–ˆâ–Ž       | 315/1380 [00:25<01:14, 14.21it/s] 23%|â–ˆâ–ˆâ–Ž       | 317/1380 [00:25<01:14, 14.24it/s] 23%|â–ˆâ–ˆâ–Ž       | 319/1380 [00:25<01:14, 14.27it/s] 23%|â–ˆâ–ˆâ–Ž       | 321/1380 [00:25<01:14, 14.29it/s] 23%|â–ˆâ–ˆâ–Ž       | 323/1380 [00:26<01:13, 14.31it/s] 24%|â–ˆâ–ˆâ–Ž       | 325/1380 [00:26<01:13, 14.32it/s] 24%|â–ˆâ–ˆâ–Ž       | 327/1380 [00:26<01:13, 14.33it/s] 24%|â–ˆâ–ˆâ–       | 329/1380 [00:26<01:13, 14.33it/s] 24%|â–ˆâ–ˆâ–       | 331/1380 [00:26<01:13, 14.33it/s] 24%|â–ˆâ–ˆâ–       | 333/1380 [00:26<01:13, 14.31it/s] 24%|â–ˆâ–ˆâ–       | 335/1380 [00:26<01:13, 14.31it/s] 24%|â–ˆâ–ˆâ–       | 337/1380 [00:27<01:12, 14.32it/s] 25%|â–ˆâ–ˆâ–       | 339/1380 [00:27<01:12, 14.33it/s] 25%|â–ˆâ–ˆâ–       | 341/1380 [00:27<01:12, 14.35it/s] 25%|â–ˆâ–ˆâ–       | 343/1380 [00:27<01:12, 14.33it/s] 25%|â–ˆâ–ˆâ–Œ       | 345/1380 [00:27<01:12, 14.34it/s] 25%|â–ˆâ–ˆâ–Œ       | 347/1380 [00:27<01:11, 14.35it/s] 25%|â–ˆâ–ˆâ–Œ       | 349/1380 [00:27<01:11, 14.36it/s] 25%|â–ˆâ–ˆâ–Œ       | 351/1380 [00:28<01:11, 14.35it/s] 26%|â–ˆâ–ˆâ–Œ       | 353/1380 [00:28<01:11, 14.34it/s] 26%|â–ˆâ–ˆâ–Œ       | 355/1380 [00:28<01:11, 14.29it/s] 26%|â–ˆâ–ˆâ–Œ       | 357/1380 [00:28<01:11, 14.29it/s] 26%|â–ˆâ–ˆâ–Œ       | 359/1380 [00:28<01:11, 14.31it/s] 26%|â–ˆâ–ˆâ–Œ       | 361/1380 [00:28<01:11, 14.31it/s] 26%|â–ˆâ–ˆâ–‹       | 363/1380 [00:28<01:11, 14.31it/s] 26%|â–ˆâ–ˆâ–‹       | 365/1380 [00:29<01:10, 14.31it/s] 27%|â–ˆâ–ˆâ–‹       | 367/1380 [00:29<01:10, 14.30it/s] 27%|â–ˆâ–ˆâ–‹       | 369/1380 [00:29<01:10, 14.29it/s] 27%|â–ˆâ–ˆâ–‹       | 371/1380 [00:29<01:10, 14.29it/s] 27%|â–ˆâ–ˆâ–‹       | 373/1380 [00:29<01:10, 14.29it/s] 27%|â–ˆâ–ˆâ–‹       | 375/1380 [00:29<01:10, 14.29it/s] 27%|â–ˆâ–ˆâ–‹       | 377/1380 [00:29<01:10, 14.29it/s] 27%|â–ˆâ–ˆâ–‹       | 379/1380 [00:29<01:10, 14.29it/s] 28%|â–ˆâ–ˆâ–Š       | 381/1380 [00:30<01:09, 14.30it/s] 28%|â–ˆâ–ˆâ–Š       | 383/1380 [00:30<01:09, 14.29it/s] 28%|â–ˆâ–ˆâ–Š       | 385/1380 [00:30<01:09, 14.29it/s] 28%|â–ˆâ–ˆâ–Š       | 387/1380 [00:30<01:09, 14.28it/s] 28%|â–ˆâ–ˆâ–Š       | 389/1380 [00:30<01:09, 14.29it/s] 28%|â–ˆâ–ˆâ–Š       | 391/1380 [00:30<01:09, 14.29it/s] 28%|â–ˆâ–ˆâ–Š       | 393/1380 [00:30<01:09, 14.30it/s] 29%|â–ˆâ–ˆâ–Š       | 395/1380 [00:31<01:08, 14.29it/s] 29%|â–ˆâ–ˆâ–‰       | 397/1380 [00:31<01:08, 14.29it/s] 29%|â–ˆâ–ˆâ–‰       | 399/1380 [00:31<01:08, 14.28it/s] 29%|â–ˆâ–ˆâ–‰       | 401/1380 [00:31<01:08, 14.29it/s] 29%|â–ˆâ–ˆâ–‰       | 403/1380 [00:31<01:08, 14.29it/s] 29%|â–ˆâ–ˆâ–‰       | 405/1380 [00:31<01:08, 14.30it/s] 29%|â–ˆâ–ˆâ–‰       | 407/1380 [00:31<01:08, 14.29it/s] 30%|â–ˆâ–ˆâ–‰       | 409/1380 [00:32<01:07, 14.30it/s] 30%|â–ˆâ–ˆâ–‰       | 411/1380 [00:32<01:07, 14.29it/s] 30%|â–ˆâ–ˆâ–‰       | 413/1380 [00:32<01:07, 14.29it/s] 30%|â–ˆâ–ˆâ–ˆ       | 415/1380 [00:32<01:07, 14.30it/s] 30%|â–ˆâ–ˆâ–ˆ       | 417/1380 [00:32<01:07, 14.28it/s] 30%|â–ˆâ–ˆâ–ˆ       | 419/1380 [00:32<01:07, 14.29it/s] 31%|â–ˆâ–ˆâ–ˆ       | 421/1380 [00:32<01:07, 14.29it/s] 31%|â–ˆâ–ˆâ–ˆ       | 423/1380 [00:33<01:06, 14.29it/s] 31%|â–ˆâ–ˆâ–ˆ       | 425/1380 [00:33<01:06, 14.29it/s] 31%|â–ˆâ–ˆâ–ˆ       | 427/1380 [00:33<01:06, 14.29it/s] 31%|â–ˆâ–ˆâ–ˆ       | 429/1380 [00:33<01:06, 14.29it/s] 31%|â–ˆâ–ˆâ–ˆ       | 431/1380 [00:33<01:06, 14.27it/s] 31%|â–ˆâ–ˆâ–ˆâ–      | 433/1380 [00:33<01:06, 14.29it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 435/1380 [00:33<01:06, 14.30it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 437/1380 [00:34<01:05, 14.29it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 439/1380 [00:34<01:05, 14.29it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 441/1380 [00:34<01:05, 14.29it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 443/1380 [00:34<01:05, 14.29it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 445/1380 [00:34<01:05, 14.28it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 447/1380 [00:34<01:05, 14.29it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 449/1380 [00:34<01:05, 14.30it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 451/1380 [00:35<01:04, 14.29it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 453/1380 [00:35<01:04, 14.29it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 455/1380 [00:35<01:04, 14.29it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 457/1380 [00:35<01:04, 14.29it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 459/1380 [00:35<01:04, 14.30it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 461/1380 [00:35<01:04, 14.30it/s] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 463/1380 [00:35<01:04, 14.28it/s] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 465/1380 [00:35<01:04, 14.29it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 467/1380 [00:36<01:03, 14.29it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 469/1380 [00:36<01:03, 14.28it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 471/1380 [00:36<01:03, 14.29it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 473/1380 [00:36<01:03, 14.26it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 475/1380 [00:36<01:03, 14.27it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 477/1380 [00:36<01:03, 14.25it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 479/1380 [00:36<01:03, 14.27it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 481/1380 [00:37<01:03, 14.26it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 483/1380 [00:37<01:02, 14.28it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 485/1380 [00:37<01:02, 14.30it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 487/1380 [00:37<01:02, 14.29it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 489/1380 [00:37<01:02, 14.23it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 491/1380 [00:37<01:02, 14.15it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 493/1380 [00:37<01:02, 14.16it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 495/1380 [00:38<01:02, 14.20it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 497/1380 [00:38<01:02, 14.23it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 499/1380 [00:38<01:01, 14.22it/s] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 501/1380 [00:38<01:01, 14.23it/s] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 503/1380 [00:38<01:01, 14.24it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 505/1380 [00:38<01:01, 14.21it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 507/1380 [00:38<01:01, 14.24it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 509/1380 [00:39<01:00, 14.28it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 511/1380 [00:39<01:00, 14.28it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 513/1380 [00:39<01:00, 14.28it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 515/1380 [00:39<01:00, 14.23it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 517/1380 [00:39<01:00, 14.28it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 519/1380 [00:39<01:00, 14.30it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 521/1380 [00:39<01:00, 14.30it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 523/1380 [00:40<00:59, 14.29it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 525/1380 [00:40<00:59, 14.29it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 527/1380 [00:40<00:59, 14.29it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 529/1380 [00:40<00:59, 14.30it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 531/1380 [00:40<00:59, 14.29it/s] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 533/1380 [00:40<00:59, 14.29it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 535/1380 [00:40<00:59, 14.29it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 537/1380 [00:41<00:58, 14.29it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 539/1380 [00:41<00:58, 14.29it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 541/1380 [00:41<00:58, 14.29it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 543/1380 [00:41<00:58, 14.29it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 545/1380 [00:41<00:58, 14.28it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 547/1380 [00:41<00:58, 14.23it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 549/1380 [00:41<00:58, 14.28it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 551/1380 [00:42<00:57, 14.32it/s]                                                   40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 552/1380 [00:42<00:57, 14.32it/s][INFO|trainer.py:755] 2023-11-15 22:02:38,497 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModelForTokenClassification.forward` and have been ignored: text. If text are not expected by `PeftModelForTokenClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 22:02:38,499 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 22:02:38,500 >>   Num examples = 2204
[INFO|trainer.py:3134] 2023-11-15 22:02:38,500 >>   Batch size = 8
{'eval_loss': 0.37880900502204895, 'eval_accuracy': 0.8539019963702359, 'eval_micro_f1': 0.8539019963702359, 'eval_macro_f1': 0.8379182738032066, 'eval_runtime': 2.6192, 'eval_samples_per_second': 841.473, 'eval_steps_per_second': 105.375, 'epoch': 1.0}
{'loss': 0.3772, 'learning_rate': 0.0003, 'epoch': 2.0}

  0%|          | 0/276 [00:00<?, ?it/s][A
  4%|â–         | 12/276 [00:00<00:02, 117.81it/s][A
  9%|â–Š         | 24/276 [00:00<00:02, 110.87it/s][A
 13%|â–ˆâ–Ž        | 36/276 [00:00<00:02, 109.06it/s][A
 17%|â–ˆâ–‹        | 47/276 [00:00<00:02, 108.03it/s][A
 21%|â–ˆâ–ˆ        | 58/276 [00:00<00:02, 107.25it/s][A
 25%|â–ˆâ–ˆâ–Œ       | 69/276 [00:00<00:01, 106.91it/s][A
 29%|â–ˆâ–ˆâ–‰       | 80/276 [00:00<00:01, 106.90it/s][A
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 91/276 [00:00<00:01, 106.71it/s][A
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 102/276 [00:00<00:01, 106.71it/s][A
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 113/276 [00:01<00:01, 106.59it/s][A
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 124/276 [00:01<00:01, 106.76it/s][A
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 135/276 [00:01<00:01, 106.18it/s][A
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 146/276 [00:01<00:01, 106.52it/s][A
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 157/276 [00:01<00:01, 106.68it/s][A
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 168/276 [00:01<00:01, 106.80it/s][A
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 179/276 [00:01<00:00, 106.66it/s][A
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 190/276 [00:01<00:00, 106.85it/s][A
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 201/276 [00:01<00:00, 106.84it/s][A
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 212/276 [00:01<00:00, 106.94it/s][A
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 223/276 [00:02<00:00, 107.03it/s][A
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 234/276 [00:02<00:00, 106.93it/s][A
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 245/276 [00:02<00:00, 106.96it/s][A
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 256/276 [00:02<00:00, 107.03it/s][A
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 267/276 [00:02<00:00, 106.95it/s][A                                                  
                                                  [A 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 552/1380 [00:44<00:57, 14.32it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 276/276 [00:02<00:00, 106.95it/s][A
                                                  [A 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 553/1380 [00:44<06:21,  2.17it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 555/1380 [00:44<04:43,  2.91it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 557/1380 [00:45<03:35,  3.82it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 559/1380 [00:45<02:47,  4.90it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 561/1380 [00:45<02:14,  6.10it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 563/1380 [00:45<01:50,  7.37it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 565/1380 [00:45<01:34,  8.62it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 567/1380 [00:45<01:23,  9.79it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 569/1380 [00:45<01:14, 10.81it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 571/1380 [00:46<01:09, 11.66it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 573/1380 [00:46<01:05, 12.35it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 575/1380 [00:46<01:02, 12.85it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 577/1380 [00:46<01:00, 13.26it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 579/1380 [00:46<00:59, 13.56it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 581/1380 [00:46<00:58, 13.72it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 583/1380 [00:46<00:57, 13.90it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 585/1380 [00:47<00:56, 14.04it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 587/1380 [00:47<00:56, 14.13it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 589/1380 [00:47<00:55, 14.18it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 591/1380 [00:47<00:55, 14.19it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 593/1380 [00:47<00:55, 14.22it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 595/1380 [00:47<00:55, 14.25it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 597/1380 [00:47<00:54, 14.27it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 599/1380 [00:47<00:54, 14.29it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 601/1380 [00:48<00:54, 14.29it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 603/1380 [00:48<00:54, 14.27it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 605/1380 [00:48<00:54, 14.29it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 607/1380 [00:48<00:54, 14.28it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 609/1380 [00:48<00:53, 14.29it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 611/1380 [00:48<00:53, 14.29it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 613/1380 [00:48<00:53, 14.28it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 615/1380 [00:49<00:53, 14.29it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 617/1380 [00:49<00:53, 14.30it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 619/1380 [00:49<00:53, 14.30it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 621/1380 [00:49<00:53, 14.29it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 623/1380 [00:49<00:52, 14.28it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 625/1380 [00:49<00:52, 14.28it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 627/1380 [00:49<00:52, 14.30it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 629/1380 [00:50<00:52, 14.30it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 631/1380 [00:50<00:52, 14.29it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 633/1380 [00:50<00:52, 14.29it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 635/1380 [00:50<00:52, 14.27it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 637/1380 [00:50<00:52, 14.28it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 639/1380 [00:50<00:51, 14.28it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 641/1380 [00:50<00:51, 14.27it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 643/1380 [00:51<00:51, 14.27it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 645/1380 [00:51<00:51, 14.29it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 647/1380 [00:51<00:51, 14.25it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 649/1380 [00:51<00:51, 14.27it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 651/1380 [00:51<00:51, 14.28it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 653/1380 [00:51<00:50, 14.30it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 655/1380 [00:51<00:50, 14.28it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 657/1380 [00:52<00:50, 14.28it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 659/1380 [00:52<00:50, 14.30it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 661/1380 [00:52<00:50, 14.30it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 663/1380 [00:52<00:50, 14.30it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 665/1380 [00:52<00:50, 14.30it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 667/1380 [00:52<00:49, 14.30it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 669/1380 [00:52<00:49, 14.31it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 671/1380 [00:53<00:49, 14.30it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 673/1380 [00:53<00:49, 14.30it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 675/1380 [00:53<00:49, 14.30it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 677/1380 [00:53<00:49, 14.30it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 679/1380 [00:53<00:49, 14.30it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 681/1380 [00:53<00:48, 14.30it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 683/1380 [00:53<00:48, 14.30it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 685/1380 [00:54<00:48, 14.29it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 687/1380 [00:54<00:48, 14.28it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 689/1380 [00:54<00:48, 14.29it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 691/1380 [00:54<00:48, 14.30it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 693/1380 [00:54<00:48, 14.25it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 695/1380 [00:54<00:48, 14.27it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 697/1380 [00:54<00:47, 14.29it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 699/1380 [00:54<00:47, 14.25it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 701/1380 [00:55<00:47, 14.22it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 703/1380 [00:55<00:47, 14.26it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 705/1380 [00:55<00:47, 14.28it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 707/1380 [00:55<00:47, 14.30it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 709/1380 [00:55<00:46, 14.31it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 711/1380 [00:55<00:46, 14.32it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 713/1380 [00:55<00:46, 14.32it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 715/1380 [00:56<00:46, 14.32it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 717/1380 [00:56<00:46, 14.32it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 719/1380 [00:56<00:46, 14.32it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 721/1380 [00:56<00:46, 14.31it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 723/1380 [00:56<00:45, 14.30it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 725/1380 [00:56<00:45, 14.30it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 727/1380 [00:56<00:45, 14.30it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 729/1380 [00:57<00:45, 14.30it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 731/1380 [00:57<00:45, 14.29it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 733/1380 [00:57<00:45, 14.27it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 735/1380 [00:57<00:45, 14.26it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 737/1380 [00:57<00:45, 14.26it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 739/1380 [00:57<00:44, 14.26it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 741/1380 [00:57<00:44, 14.21it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 743/1380 [00:58<00:44, 14.22it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 745/1380 [00:58<00:44, 14.24it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 747/1380 [00:58<00:44, 14.25it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 749/1380 [00:58<00:44, 14.26it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 751/1380 [00:58<00:44, 14.27it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 753/1380 [00:58<00:43, 14.27it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 755/1380 [00:58<00:43, 14.27it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 757/1380 [00:59<00:43, 14.27it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 759/1380 [00:59<00:43, 14.27it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 761/1380 [00:59<00:43, 14.28it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 763/1380 [00:59<00:43, 14.29it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 765/1380 [00:59<00:43, 14.29it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 767/1380 [00:59<00:42, 14.29it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 769/1380 [00:59<00:42, 14.29it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 771/1380 [01:00<00:42, 14.29it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 773/1380 [01:00<00:42, 14.23it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 775/1380 [01:00<00:42, 14.24it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 777/1380 [01:00<00:42, 14.22it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 779/1380 [01:00<00:42, 14.21it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 781/1380 [01:00<00:42, 14.22it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 783/1380 [01:00<00:41, 14.25it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 785/1380 [01:01<00:41, 14.26it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 787/1380 [01:01<00:41, 14.26it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 789/1380 [01:01<00:41, 14.27it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 791/1380 [01:01<00:41, 14.28it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 793/1380 [01:01<00:41, 14.26it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 795/1380 [01:01<00:40, 14.28it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 797/1380 [01:01<00:40, 14.28it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 799/1380 [01:02<00:40, 14.29it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 801/1380 [01:02<00:40, 14.29it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 803/1380 [01:02<00:40, 14.30it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 805/1380 [01:02<00:40, 14.29it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 807/1380 [01:02<00:40, 14.29it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 809/1380 [01:02<00:40, 14.27it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 811/1380 [01:02<00:39, 14.28it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 813/1380 [01:02<00:39, 14.29it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 815/1380 [01:03<00:39, 14.23it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 817/1380 [01:03<00:39, 14.25it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 819/1380 [01:03<00:39, 14.27it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 821/1380 [01:03<00:39, 14.26it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 823/1380 [01:03<00:39, 14.27it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 825/1380 [01:03<00:38, 14.29it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 827/1380 [01:03<00:38, 14.32it/s]                                                   60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 828/1380 [01:04<00:38, 14.32it/s][INFO|trainer.py:755] 2023-11-15 22:03:00,432 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModelForTokenClassification.forward` and have been ignored: text. If text are not expected by `PeftModelForTokenClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 22:03:00,434 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 22:03:00,434 >>   Num examples = 2204
[INFO|trainer.py:3134] 2023-11-15 22:03:00,434 >>   Batch size = 8
{'eval_loss': 0.41789713501930237, 'eval_accuracy': 0.8502722323049002, 'eval_micro_f1': 0.8502722323049003, 'eval_macro_f1': 0.8344224738555854, 'eval_runtime': 2.6313, 'eval_samples_per_second': 837.619, 'eval_steps_per_second': 104.892, 'epoch': 2.0}
{'loss': 0.3382, 'learning_rate': 0.0002, 'epoch': 3.0}

  0%|          | 0/276 [00:00<?, ?it/s][A
  4%|â–         | 12/276 [00:00<00:02, 117.45it/s][A
  9%|â–Š         | 24/276 [00:00<00:02, 110.44it/s][A
 13%|â–ˆâ–Ž        | 36/276 [00:00<00:02, 108.93it/s][A
 17%|â–ˆâ–‹        | 47/276 [00:00<00:02, 107.85it/s][A
 21%|â–ˆâ–ˆ        | 58/276 [00:00<00:02, 107.08it/s][A
 25%|â–ˆâ–ˆâ–Œ       | 69/276 [00:00<00:01, 107.00it/s][A
 29%|â–ˆâ–ˆâ–‰       | 80/276 [00:00<00:01, 106.96it/s][A
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 91/276 [00:00<00:01, 106.76it/s][A
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 102/276 [00:00<00:01, 106.44it/s][A
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 113/276 [00:01<00:01, 106.58it/s][A
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 124/276 [00:01<00:01, 106.39it/s][A
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 135/276 [00:01<00:01, 106.22it/s][A
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 146/276 [00:01<00:01, 106.00it/s][A
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 157/276 [00:01<00:01, 106.20it/s][A
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 168/276 [00:01<00:01, 106.34it/s][A
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 179/276 [00:01<00:00, 106.04it/s][A
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 190/276 [00:01<00:00, 106.30it/s][A
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 201/276 [00:01<00:00, 106.28it/s][A
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 212/276 [00:01<00:00, 106.28it/s][A
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 223/276 [00:02<00:00, 106.39it/s][A
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 234/276 [00:02<00:00, 106.32it/s][A
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 245/276 [00:02<00:00, 106.10it/s][A
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 256/276 [00:02<00:00, 106.16it/s][A
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 267/276 [00:02<00:00, 106.27it/s][A                                                  
                                                  [A 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 828/1380 [01:06<00:38, 14.32it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 276/276 [00:02<00:00, 106.27it/s][A
                                                  [A 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 829/1380 [01:06<04:14,  2.16it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 831/1380 [01:06<03:09,  2.90it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 833/1380 [01:07<02:23,  3.81it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 835/1380 [01:07<01:51,  4.88it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 837/1380 [01:07<01:29,  6.06it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 839/1380 [01:07<01:13,  7.33it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 841/1380 [01:07<01:02,  8.58it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 843/1380 [01:07<00:55,  9.74it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 845/1380 [01:07<00:49, 10.71it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 847/1380 [01:07<00:46, 11.52it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 849/1380 [01:08<00:43, 12.20it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 851/1380 [01:08<00:41, 12.75it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 853/1380 [01:08<00:40, 13.17it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 855/1380 [01:08<00:38, 13.49it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 857/1380 [01:08<00:38, 13.69it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 859/1380 [01:08<00:37, 13.86it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 861/1380 [01:08<00:37, 13.98it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 863/1380 [01:09<00:36, 14.05it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 865/1380 [01:09<00:36, 14.08it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 867/1380 [01:09<00:36, 14.10it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 869/1380 [01:09<00:36, 14.16it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 871/1380 [01:09<00:35, 14.21it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 873/1380 [01:09<00:35, 14.23it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 875/1380 [01:09<00:35, 14.25it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 877/1380 [01:10<00:35, 14.25it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 879/1380 [01:10<00:35, 14.27it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 881/1380 [01:10<00:35, 14.24it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 883/1380 [01:10<00:34, 14.27it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 885/1380 [01:10<00:34, 14.28it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 887/1380 [01:10<00:34, 14.29it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 889/1380 [01:10<00:34, 14.29it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 891/1380 [01:11<00:34, 14.30it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 893/1380 [01:11<00:34, 14.29it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 895/1380 [01:11<00:33, 14.30it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 897/1380 [01:11<00:33, 14.30it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 899/1380 [01:11<00:33, 14.30it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 901/1380 [01:11<00:33, 14.29it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 903/1380 [01:11<00:33, 14.28it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 905/1380 [01:12<00:33, 14.28it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 907/1380 [01:12<00:33, 14.29it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 909/1380 [01:12<00:33, 14.26it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 911/1380 [01:12<00:32, 14.27it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 913/1380 [01:12<00:32, 14.26it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 915/1380 [01:12<00:32, 14.26it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 917/1380 [01:12<00:32, 14.27it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 919/1380 [01:13<00:32, 14.28it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 921/1380 [01:13<00:32, 14.29it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 923/1380 [01:13<00:31, 14.29it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 925/1380 [01:13<00:31, 14.29it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 927/1380 [01:13<00:31, 14.28it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 929/1380 [01:13<00:31, 14.28it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 931/1380 [01:13<00:31, 14.25it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 933/1380 [01:14<00:31, 14.27it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 935/1380 [01:14<00:31, 14.29it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 937/1380 [01:14<00:30, 14.30it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 939/1380 [01:14<00:30, 14.29it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 941/1380 [01:14<00:30, 14.23it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 943/1380 [01:14<00:30, 14.25it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 945/1380 [01:14<00:30, 14.26it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 947/1380 [01:15<00:30, 14.26it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 949/1380 [01:15<00:30, 14.27it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 951/1380 [01:15<00:30, 14.27it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 953/1380 [01:15<00:29, 14.27it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 955/1380 [01:15<00:29, 14.23it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 957/1380 [01:15<00:29, 14.24it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 959/1380 [01:15<00:29, 14.24it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 961/1380 [01:15<00:29, 14.24it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 963/1380 [01:16<00:29, 14.25it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 965/1380 [01:16<00:29, 14.24it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 967/1380 [01:16<00:29, 14.24it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 969/1380 [01:16<00:28, 14.24it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 971/1380 [01:16<00:28, 14.24it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 973/1380 [01:16<00:28, 14.25it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 975/1380 [01:16<00:28, 14.26it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 977/1380 [01:17<00:28, 14.25it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 979/1380 [01:17<00:28, 14.25it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 981/1380 [01:17<00:27, 14.27it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 983/1380 [01:17<00:27, 14.25it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 985/1380 [01:17<00:27, 14.27it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 987/1380 [01:17<00:27, 14.27it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 989/1380 [01:17<00:27, 14.28it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 991/1380 [01:18<00:27, 14.27it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 993/1380 [01:18<00:27, 14.25it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 995/1380 [01:18<00:27, 14.26it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 997/1380 [01:18<00:26, 14.25it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 999/1380 [01:18<00:26, 14.21it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1001/1380 [01:18<00:26, 14.22it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1003/1380 [01:18<00:26, 14.19it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1005/1380 [01:19<00:26, 14.20it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1007/1380 [01:19<00:26, 14.20it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1009/1380 [01:19<00:26, 14.21it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1011/1380 [01:19<00:25, 14.23it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1013/1380 [01:19<00:25, 14.20it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1015/1380 [01:19<00:25, 14.23it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1017/1380 [01:19<00:25, 14.24it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1019/1380 [01:20<00:25, 14.26it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1021/1380 [01:20<00:25, 14.27it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1023/1380 [01:20<00:25, 14.28it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1025/1380 [01:20<00:24, 14.28it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1027/1380 [01:20<00:24, 14.30it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1029/1380 [01:20<00:24, 14.30it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1031/1380 [01:20<00:24, 14.31it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1033/1380 [01:21<00:24, 14.30it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1035/1380 [01:21<00:24, 14.30it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1037/1380 [01:21<00:24, 14.29it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1039/1380 [01:21<00:23, 14.29it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1041/1380 [01:21<00:23, 14.28it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1043/1380 [01:21<00:23, 14.24it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1045/1380 [01:21<00:23, 14.24it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1047/1380 [01:22<00:23, 14.26it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1049/1380 [01:22<00:23, 14.27it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1051/1380 [01:22<00:23, 14.28it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1053/1380 [01:22<00:22, 14.27it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1055/1380 [01:22<00:22, 14.22it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1057/1380 [01:22<00:22, 14.23it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1059/1380 [01:22<00:22, 14.24it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1061/1380 [01:23<00:23, 13.85it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1063/1380 [01:23<00:22, 13.97it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1065/1380 [01:23<00:22, 14.06it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1067/1380 [01:23<00:22, 14.13it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1069/1380 [01:23<00:21, 14.17it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1071/1380 [01:23<00:21, 14.20it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1073/1380 [01:23<00:21, 14.23it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1075/1380 [01:23<00:21, 14.25it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1077/1380 [01:24<00:21, 14.26it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1079/1380 [01:24<00:21, 14.27it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1081/1380 [01:24<00:20, 14.26it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1083/1380 [01:24<00:20, 14.26it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1085/1380 [01:24<00:20, 14.24it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1087/1380 [01:24<00:20, 14.25it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1089/1380 [01:24<00:20, 14.27it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1091/1380 [01:25<00:20, 14.27it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1093/1380 [01:25<00:20, 14.26it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1095/1380 [01:25<00:19, 14.27it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1097/1380 [01:25<00:19, 14.27it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1099/1380 [01:25<00:19, 14.28it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1101/1380 [01:25<00:19, 14.29it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1103/1380 [01:25<00:19, 14.32it/s]                                                    80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1104/1380 [01:25<00:19, 14.32it/s][INFO|trainer.py:755] 2023-11-15 22:03:22,426 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModelForTokenClassification.forward` and have been ignored: text. If text are not expected by `PeftModelForTokenClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 22:03:22,427 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 22:03:22,427 >>   Num examples = 2204
[INFO|trainer.py:3134] 2023-11-15 22:03:22,427 >>   Batch size = 8
{'eval_loss': 0.37969765067100525, 'eval_accuracy': 0.852994555353902, 'eval_micro_f1': 0.852994555353902, 'eval_macro_f1': 0.8374838043860938, 'eval_runtime': 2.6404, 'eval_samples_per_second': 834.714, 'eval_steps_per_second': 104.529, 'epoch': 3.0}
{'loss': 0.3117, 'learning_rate': 0.0001, 'epoch': 4.0}

  0%|          | 0/276 [00:00<?, ?it/s][A
  4%|â–         | 12/276 [00:00<00:02, 115.92it/s][A
  9%|â–Š         | 24/276 [00:00<00:02, 110.25it/s][A
 13%|â–ˆâ–Ž        | 36/276 [00:00<00:02, 108.48it/s][A
 17%|â–ˆâ–‹        | 47/276 [00:00<00:02, 107.15it/s][A
 21%|â–ˆâ–ˆ        | 58/276 [00:00<00:02, 106.95it/s][A
 25%|â–ˆâ–ˆâ–Œ       | 69/276 [00:00<00:01, 106.76it/s][A
 29%|â–ˆâ–ˆâ–‰       | 80/276 [00:00<00:01, 106.22it/s][A
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 91/276 [00:00<00:01, 106.40it/s][A
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 102/276 [00:00<00:01, 106.41it/s][A
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 113/276 [00:01<00:01, 105.89it/s][A
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 124/276 [00:01<00:01, 106.09it/s][A
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 135/276 [00:01<00:01, 106.03it/s][A
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 146/276 [00:01<00:01, 106.16it/s][A
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 157/276 [00:01<00:01, 106.14it/s][A
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 168/276 [00:01<00:01, 106.02it/s][A
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 179/276 [00:01<00:00, 105.64it/s][A
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 190/276 [00:01<00:00, 105.53it/s][A
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 201/276 [00:01<00:00, 105.80it/s][A
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 212/276 [00:01<00:00, 106.18it/s][A
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 223/276 [00:02<00:00, 106.25it/s][A
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 234/276 [00:02<00:00, 106.33it/s][A
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 245/276 [00:02<00:00, 106.40it/s][A
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 256/276 [00:02<00:00, 106.22it/s][A
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 267/276 [00:02<00:00, 106.21it/s][A                                                   
                                                  [A 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1104/1380 [01:28<00:19, 14.32it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 276/276 [00:02<00:00, 106.21it/s][A
                                                  [A 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1105/1380 [01:28<02:07,  2.16it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1107/1380 [01:28<01:34,  2.89it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1109/1380 [01:28<01:11,  3.80it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1111/1380 [01:29<00:55,  4.87it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1113/1380 [01:29<00:43,  6.07it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1115/1380 [01:29<00:36,  7.33it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1117/1380 [01:29<00:30,  8.58it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1119/1380 [01:29<00:26,  9.74it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1121/1380 [01:29<00:24, 10.76it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1123/1380 [01:29<00:22, 11.62it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1125/1380 [01:30<00:20, 12.31it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1127/1380 [01:30<00:19, 12.83it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1129/1380 [01:30<00:18, 13.23it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1131/1380 [01:30<00:18, 13.52it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1133/1380 [01:30<00:17, 13.73it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1135/1380 [01:30<00:17, 13.88it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1137/1380 [01:30<00:17, 13.96it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1139/1380 [01:31<00:17, 14.04it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1141/1380 [01:31<00:16, 14.10it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1143/1380 [01:31<00:16, 14.14it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1145/1380 [01:31<00:16, 14.17it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1147/1380 [01:31<00:16, 14.18it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1149/1380 [01:31<00:16, 14.19it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1151/1380 [01:31<00:16, 14.19it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1153/1380 [01:32<00:15, 14.20it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1155/1380 [01:32<00:15, 14.22it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1157/1380 [01:32<00:15, 14.23it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1159/1380 [01:32<00:15, 14.24it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1161/1380 [01:32<00:15, 14.26it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1163/1380 [01:32<00:15, 14.23it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1165/1380 [01:32<00:15, 14.24it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1167/1380 [01:33<00:14, 14.26it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1169/1380 [01:33<00:14, 14.27it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1171/1380 [01:33<00:14, 14.27it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1173/1380 [01:33<00:14, 14.27it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1175/1380 [01:33<00:14, 14.28it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1177/1380 [01:33<00:14, 14.26it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1179/1380 [01:33<00:14, 14.24it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1181/1380 [01:34<00:13, 14.25it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1183/1380 [01:34<00:13, 14.25it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1185/1380 [01:34<00:13, 14.25it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1187/1380 [01:34<00:13, 14.25it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1189/1380 [01:34<00:13, 14.25it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1191/1380 [01:34<00:13, 14.24it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1193/1380 [01:34<00:13, 14.24it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1195/1380 [01:35<00:12, 14.24it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1197/1380 [01:35<00:12, 14.21it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1199/1380 [01:35<00:12, 14.20it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1201/1380 [01:35<00:12, 14.21it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1203/1380 [01:35<00:12, 14.23it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1205/1380 [01:35<00:12, 14.25it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1207/1380 [01:35<00:12, 14.25it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1209/1380 [01:36<00:12, 14.22it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1211/1380 [01:36<00:11, 14.23it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1213/1380 [01:36<00:11, 14.25it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1215/1380 [01:36<00:11, 14.26it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1217/1380 [01:36<00:11, 14.26it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1219/1380 [01:36<00:11, 14.25it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1221/1380 [01:36<00:11, 14.25it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1223/1380 [01:37<00:11, 14.22it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1225/1380 [01:37<00:10, 14.23it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1227/1380 [01:37<00:10, 14.23it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1229/1380 [01:37<00:10, 14.14it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1231/1380 [01:37<00:10, 14.16it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1233/1380 [01:37<00:10, 14.15it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1235/1380 [01:37<00:10, 14.18it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1237/1380 [01:37<00:10, 14.18it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1239/1380 [01:38<00:09, 14.18it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1241/1380 [01:38<00:09, 14.15it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1243/1380 [01:38<00:09, 14.10it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1245/1380 [01:38<00:09, 14.12it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1247/1380 [01:38<00:09, 14.12it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1249/1380 [01:38<00:09, 14.11it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1251/1380 [01:38<00:09, 14.12it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1253/1380 [01:39<00:08, 14.13it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1255/1380 [01:39<00:08, 14.13it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1257/1380 [01:39<00:08, 14.14it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1259/1380 [01:39<00:08, 14.16it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1261/1380 [01:39<00:08, 14.17it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1263/1380 [01:39<00:08, 14.18it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1265/1380 [01:39<00:08, 14.19it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1267/1380 [01:40<00:07, 14.19it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1269/1380 [01:40<00:07, 14.19it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1271/1380 [01:40<00:07, 14.19it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1273/1380 [01:40<00:07, 14.18it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1275/1380 [01:40<00:07, 14.17it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1277/1380 [01:40<00:07, 14.17it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1279/1380 [01:40<00:07, 14.18it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1281/1380 [01:41<00:07, 14.06it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1283/1380 [01:41<00:06, 14.02it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1285/1380 [01:41<00:06, 13.97it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1287/1380 [01:41<00:06, 14.01it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1289/1380 [01:41<00:06, 14.06it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1291/1380 [01:41<00:06, 13.97it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1293/1380 [01:41<00:06, 13.96it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1295/1380 [01:42<00:06, 14.01it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1297/1380 [01:42<00:05, 14.06it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1299/1380 [01:42<00:05, 14.09it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1301/1380 [01:42<00:05, 14.15it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1303/1380 [01:42<00:05, 14.18it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1305/1380 [01:42<00:05, 14.19it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1307/1380 [01:42<00:05, 14.22it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1309/1380 [01:43<00:04, 14.23it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1311/1380 [01:43<00:04, 14.25it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1313/1380 [01:43<00:04, 14.25it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1315/1380 [01:43<00:04, 14.18it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1317/1380 [01:43<00:04, 14.18it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1319/1380 [01:43<00:04, 14.17it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1321/1380 [01:43<00:04, 14.13it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1323/1380 [01:44<00:04, 14.14it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1325/1380 [01:44<00:03, 14.16it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1327/1380 [01:44<00:03, 14.16it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1329/1380 [01:44<00:03, 14.16it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1331/1380 [01:44<00:03, 14.15it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1333/1380 [01:44<00:03, 14.15it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1335/1380 [01:44<00:03, 14.14it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1337/1380 [01:45<00:03, 14.16it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1339/1380 [01:45<00:02, 14.11it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1341/1380 [01:45<00:02, 14.11it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1343/1380 [01:45<00:02, 14.12it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1345/1380 [01:45<00:02, 14.12it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1347/1380 [01:45<00:02, 14.04it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1349/1380 [01:45<00:02, 14.04it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1351/1380 [01:46<00:02, 14.06it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1353/1380 [01:46<00:01, 14.08it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1355/1380 [01:46<00:01, 14.05it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1357/1380 [01:46<00:01, 14.10it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1359/1380 [01:46<00:01, 14.12it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1361/1380 [01:46<00:01, 14.11it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1363/1380 [01:46<00:01, 14.12it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1365/1380 [01:47<00:01, 14.12it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1367/1380 [01:47<00:00, 14.15it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1369/1380 [01:47<00:00, 14.16it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1371/1380 [01:47<00:00, 14.18it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1373/1380 [01:47<00:00, 14.18it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1375/1380 [01:47<00:00, 14.19it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1377/1380 [01:47<00:00, 14.18it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1379/1380 [01:48<00:00, 14.13it/s]                                                   100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1380/1380 [01:48<00:00, 14.13it/s][INFO|trainer.py:755] 2023-11-15 22:03:44,515 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModelForTokenClassification.forward` and have been ignored: text. If text are not expected by `PeftModelForTokenClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 22:03:44,517 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 22:03:44,517 >>   Num examples = 2204
[INFO|trainer.py:3134] 2023-11-15 22:03:44,518 >>   Batch size = 8
{'eval_loss': 0.3653908967971802, 'eval_accuracy': 0.8656987295825771, 'eval_micro_f1': 0.8656987295825771, 'eval_macro_f1': 0.8539586496515055, 'eval_runtime': 2.6436, 'eval_samples_per_second': 833.708, 'eval_steps_per_second': 104.403, 'epoch': 4.0}
{'loss': 0.287, 'learning_rate': 0.0, 'epoch': 5.0}

  0%|          | 0/276 [00:00<?, ?it/s][A
  4%|â–         | 12/276 [00:00<00:02, 115.36it/s][A
  9%|â–Š         | 24/276 [00:00<00:02, 109.31it/s][A
 13%|â–ˆâ–Ž        | 35/276 [00:00<00:02, 107.88it/s][A
 17%|â–ˆâ–‹        | 46/276 [00:00<00:02, 107.04it/s][A
 21%|â–ˆâ–ˆ        | 57/276 [00:00<00:02, 106.19it/s][A
 25%|â–ˆâ–ˆâ–       | 68/276 [00:00<00:01, 106.15it/s][A
 29%|â–ˆâ–ˆâ–Š       | 79/276 [00:00<00:01, 106.00it/s][A
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 90/276 [00:00<00:01, 105.99it/s][A
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 101/276 [00:00<00:01, 105.42it/s][A
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 112/276 [00:01<00:01, 105.60it/s][A
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 123/276 [00:01<00:01, 105.65it/s][A
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 134/276 [00:01<00:01, 105.65it/s][A
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 145/276 [00:01<00:01, 105.71it/s][A
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 156/276 [00:01<00:01, 105.69it/s][A
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 167/276 [00:01<00:01, 105.74it/s][A
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 178/276 [00:01<00:00, 105.33it/s][A
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 189/276 [00:01<00:00, 105.55it/s][A
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 200/276 [00:01<00:00, 105.60it/s][A
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 211/276 [00:01<00:00, 105.65it/s][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 222/276 [00:02<00:00, 105.81it/s][A
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 233/276 [00:02<00:00, 105.79it/s][A
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 244/276 [00:02<00:00, 105.81it/s][A
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 255/276 [00:02<00:00, 105.79it/s][A
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 266/276 [00:02<00:00, 105.77it/s][A                                                   
                                                  [A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1380/1380 [01:50<00:00, 14.13it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 276/276 [00:02<00:00, 105.77it/s][A
                                                  [A[INFO|trainer.py:1963] 2023-11-15 22:03:47,180 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                   100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1380/1380 [01:50<00:00, 14.13it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1380/1380 [01:50<00:00, 12.46it/s]
[INFO|trainer.py:2855] 2023-11-15 22:03:47,183 >> Saving model checkpoint to ./result/acl_roberta-base_seed1_lora
[INFO|tokenization_utils_base.py:2235] 2023-11-15 22:03:47,291 >> tokenizer config file saved in ./result/acl_roberta-base_seed1_lora/tokenizer_config.json
[INFO|tokenization_utils_base.py:2242] 2023-11-15 22:03:47,293 >> Special tokens file saved in ./result/acl_roberta-base_seed1_lora/special_tokens_map.json
{'eval_loss': 0.3897862434387207, 'eval_accuracy': 0.8579854809437386, 'eval_micro_f1': 0.8579854809437386, 'eval_macro_f1': 0.8453552743840755, 'eval_runtime': 2.6585, 'eval_samples_per_second': 829.046, 'eval_steps_per_second': 103.819, 'epoch': 5.0}
{'train_runtime': 110.7527, 'train_samples_per_second': 398.004, 'train_steps_per_second': 12.46, 'train_loss': 0.3641672604325889, 'epoch': 5.0}
***** train metrics *****
  epoch                    =        5.0
  train_loss               =     0.3642
  train_runtime            = 0:01:50.75
  train_samples            =       8816
  train_samples_per_second =    398.004
  train_steps_per_second   =      12.46
11/15/2023 22:03:47 - INFO - __main__ - *** Evaluate ***
[INFO|trainer.py:755] 2023-11-15 22:03:47,389 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModelForTokenClassification.forward` and have been ignored: text. If text are not expected by `PeftModelForTokenClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 22:03:47,391 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 22:03:47,391 >>   Num examples = 2204
[INFO|trainer.py:3134] 2023-11-15 22:03:47,391 >>   Batch size = 8
  0%|          | 0/276 [00:00<?, ?it/s]  4%|â–         | 12/276 [00:00<00:02, 116.83it/s]  9%|â–Š         | 24/276 [00:00<00:02, 110.49it/s] 13%|â–ˆâ–Ž        | 36/276 [00:00<00:02, 108.50it/s] 17%|â–ˆâ–‹        | 47/276 [00:00<00:02, 107.68it/s] 21%|â–ˆâ–ˆ        | 58/276 [00:00<00:02, 107.24it/s] 25%|â–ˆâ–ˆâ–Œ       | 69/276 [00:00<00:01, 106.96it/s] 29%|â–ˆâ–ˆâ–‰       | 80/276 [00:00<00:01, 106.82it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 91/276 [00:00<00:01, 106.79it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 102/276 [00:00<00:01, 106.53it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 113/276 [00:01<00:01, 106.30it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 124/276 [00:01<00:01, 106.34it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 135/276 [00:01<00:01, 106.33it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 146/276 [00:01<00:01, 106.29it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 157/276 [00:01<00:01, 106.35it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 168/276 [00:01<00:01, 106.39it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 179/276 [00:01<00:00, 106.42it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 190/276 [00:01<00:00, 106.45it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 201/276 [00:01<00:00, 105.36it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 212/276 [00:01<00:00, 105.67it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 223/276 [00:02<00:00, 105.86it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 234/276 [00:02<00:00, 105.95it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 245/276 [00:02<00:00, 105.57it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 256/276 [00:02<00:00, 105.76it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 267/276 [00:02<00:00, 105.98it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 276/276 [00:02<00:00, 104.69it/s]
***** eval metrics *****
  epoch                   =        5.0
  eval_accuracy           =      0.858
  eval_loss               =     0.3898
  eval_macro_f1           =     0.8454
  eval_micro_f1           =      0.858
  eval_runtime            = 0:00:02.64
  eval_samples            =       2204
  eval_samples_per_second =     832.02
  eval_steps_per_second   =    104.191
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                  eval/accuracy â–ƒâ–â–‚â–ˆâ–…â–…
wandb:                      eval/loss â–ƒâ–ˆâ–ƒâ–â–„â–„
wandb:                  eval/macro_f1 â–‚â–â–‚â–ˆâ–…â–…
wandb:                  eval/micro_f1 â–ƒâ–â–‚â–ˆâ–…â–…
wandb:                   eval/runtime â–â–ƒâ–…â–…â–ˆâ–†
wandb:        eval/samples_per_second â–ˆâ–†â–„â–„â–â–ƒ
wandb:          eval/steps_per_second â–ˆâ–†â–„â–„â–â–ƒ
wandb:                    train/epoch â–â–â–ƒâ–ƒâ–…â–…â–†â–†â–ˆâ–ˆâ–ˆâ–ˆ
wandb:              train/global_step â–â–â–ƒâ–ƒâ–…â–…â–†â–†â–ˆâ–ˆâ–ˆâ–ˆ
wandb:            train/learning_rate â–ˆâ–†â–…â–ƒâ–
wandb:                     train/loss â–ˆâ–„â–ƒâ–‚â–
wandb:               train/total_flos â–
wandb:               train/train_loss â–
wandb:            train/train_runtime â–
wandb: train/train_samples_per_second â–
wandb:   train/train_steps_per_second â–
wandb: 
wandb: Run summary:
wandb:                  eval/accuracy 0.85799
wandb:                      eval/loss 0.38979
wandb:                  eval/macro_f1 0.84536
wandb:                  eval/micro_f1 0.85799
wandb:                   eval/runtime 2.649
wandb:        eval/samples_per_second 832.02
wandb:          eval/steps_per_second 104.191
wandb:                    train/epoch 5.0
wandb:              train/global_step 1380
wandb:            train/learning_rate 0.0
wandb:                     train/loss 0.287
wandb:               train/total_flos 1469774552739840.0
wandb:               train/train_loss 0.36417
wandb:            train/train_runtime 110.7527
wandb: train/train_samples_per_second 398.004
wandb:   train/train_steps_per_second 12.46
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /ceph/home/wangyifei/a3/wandb/offline-run-20231115_220039-gald3znr
wandb: Find logs at: ./wandb/offline-run-20231115_220039-gald3znr/logs
(ModelArguments(model_name_or_path='roberta-base', cache_dir='./cache', use_adapter=False, use_lora=True), DataTrainingArguments(dataset_name='agnews_sup', max_seq_length=64), TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.0005,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=./result/agnews_sup_roberta-base_seed1_lora/runs/Nov15_22-04-00_node010,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.EPOCH,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=5.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
output_dir=./result/agnews_sup_roberta-base_seed1_lora,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
resume_from_checkpoint=None,
run_name=./result/agnews_sup_roberta-base_seed1_lora,
save_on_each_node=False,
save_safetensors=False,
save_steps=500,
save_strategy=IntervalStrategy.NO,
save_total_limit=None,
seed=222,
sharded_ddp=[],
skip_memory_metrics=True,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
))
11/15/2023 22:04:00 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False
11/15/2023 22:04:00 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.0005,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=./result/agnews_sup_roberta-base_seed1_lora/runs/Nov15_22-03-59_node010,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.EPOCH,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=5.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
output_dir=./result/agnews_sup_roberta-base_seed1_lora,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
resume_from_checkpoint=None,
run_name=./result/agnews_sup_roberta-base_seed1_lora,
save_on_each_node=False,
save_safetensors=False,
save_steps=500,
save_strategy=IntervalStrategy.NO,
save_total_limit=None,
seed=222,
sharded_ddp=[],
skip_memory_metrics=True,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
wandb: Tracking run with wandb version 0.15.12
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[INFO|configuration_utils.py:715] 2023-11-15 22:04:15,648 >> loading configuration file config.json from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/config.json
[INFO|configuration_utils.py:775] 2023-11-15 22:04:15,658 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.33.3",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_auto.py:535] 2023-11-15 22:04:25,673 >> Could not locate the tokenizer configuration file, will try to use the model config instead.
[INFO|configuration_utils.py:715] 2023-11-15 22:04:35,690 >> loading configuration file config.json from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/config.json
[INFO|configuration_utils.py:775] 2023-11-15 22:04:35,691 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.33.3",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:1852] 2023-11-15 22:04:55,738 >> loading file vocab.json from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/vocab.json
[INFO|tokenization_utils_base.py:1852] 2023-11-15 22:04:55,739 >> loading file merges.txt from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/merges.txt
[INFO|tokenization_utils_base.py:1852] 2023-11-15 22:04:55,739 >> loading file tokenizer.json from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/tokenizer.json
[INFO|tokenization_utils_base.py:1852] 2023-11-15 22:04:55,739 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1852] 2023-11-15 22:04:55,739 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1852] 2023-11-15 22:04:55,740 >> loading file tokenizer_config.json from cache at None
[INFO|configuration_utils.py:715] 2023-11-15 22:04:55,741 >> loading configuration file config.json from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/config.json
[INFO|configuration_utils.py:775] 2023-11-15 22:04:55,741 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.33.3",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:2862] 2023-11-15 22:05:15,907 >> loading weights file model.safetensors from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/model.safetensors
[INFO|modeling_utils.py:3638] 2023-11-15 22:05:16,604 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.bias']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3650] 2023-11-15 22:05:16,605 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
trainable params: 1,285,636 || all params: 125,832,200 || trainable%: 1.0217066855701482
model_max_length: 512
Running tokenizer on dataset:   0%|          | 0/6840 [00:00<?, ? examples/s]Running tokenizer on dataset:  29%|â–ˆâ–ˆâ–‰       | 2000/6840 [00:00<00:00, 16133.12 examples/s]Running tokenizer on dataset:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 4000/6840 [00:00<00:00, 17244.37 examples/s]Running tokenizer on dataset:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 6000/6840 [00:00<00:00, 17686.28 examples/s]Running tokenizer on dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6840/6840 [00:00<00:00, 17475.52 examples/s]
Running tokenizer on dataset:   0%|          | 0/760 [00:00<?, ? examples/s]Running tokenizer on dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 760/760 [00:00<00:00, 20143.52 examples/s]
11/15/2023 22:05:17 - INFO - __main__ - Sample 6380 of the training set: {'text': 'Mich. Elephant Gets Therapy for Arthritis ROYAL OAK, Mich. - Like any patient, Wanda needs positive reinforcement to wrestle through her physical therapy...', 'label': 3, 'input_ids': [0, 40648, 4, 36516, 32810, 25889, 13, 1586, 44491, 10033, 975, 2118, 384, 7140, 6, 9605, 4, 111, 2011, 143, 3186, 6, 305, 5219, 782, 1313, 37700, 7, 27881, 149, 69, 2166, 5804, 734, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
11/15/2023 22:05:17 - INFO - __main__ - Sample 883 of the training set: {'text': "Chicago to Hold EBay Auction to Raise Money for Cultural Programs City officials hope there are people willing to pay plenty of money to own a vintage Playboy Bunny costume, toss green dye into the Chicago River or throw a dinner party prepared by Oprah Winfrey's chef.", 'label': 2, 'input_ids': [0, 21897, 7, 10357, 381, 20861, 26342, 7, 39208, 8028, 13, 15309, 25740, 412, 503, 1034, 89, 32, 82, 2882, 7, 582, 2710, 9, 418, 7, 308, 10, 12669, 24526, 33470, 12111, 6, 13027, 2272, 31800, 88, 5, 1568, 1995, 50, 3211, 10, 3630, 537, 2460, 30, 20015, 5711, 16127, 18, 8172, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
11/15/2023 22:05:17 - INFO - __main__ - Sample 1927 of the training set: {'text': 'Astros 10, Pirates 5 HOUSTON Mike Lamb went four-for-five with a homer and four RB-Is to lead the Houston Astros to their ninth straight win with a 10-to-five victory over the Pittsburgh Pirates today.', 'label': 0, 'input_ids': [0, 39021, 3985, 158, 6, 11114, 195, 30392, 12917, 1483, 13132, 439, 237, 12, 1990, 12, 9579, 19, 10, 8646, 8, 237, 11191, 12, 6209, 7, 483, 5, 2499, 10938, 7, 49, 5127, 1359, 339, 19, 10, 158, 12, 560, 12, 9579, 1124, 81, 5, 4386, 11114, 452, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
11/15/2023 22:05:17 - WARNING - accelerate.utils.other - Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[INFO|trainer.py:755] 2023-11-15 22:05:18,172 >> The following columns in the training set don't have a corresponding argument in `PeftModelForTokenClassification.forward` and have been ignored: text. If text are not expected by `PeftModelForTokenClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:1715] 2023-11-15 22:05:18,183 >> ***** Running training *****
[INFO|trainer.py:1716] 2023-11-15 22:05:18,183 >>   Num examples = 6,840
[INFO|trainer.py:1717] 2023-11-15 22:05:18,183 >>   Num Epochs = 5
[INFO|trainer.py:1718] 2023-11-15 22:05:18,183 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1721] 2023-11-15 22:05:18,184 >>   Total train batch size (w. parallel, distributed & accumulation) = 32
[INFO|trainer.py:1722] 2023-11-15 22:05:18,184 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:1723] 2023-11-15 22:05:18,184 >>   Total optimization steps = 1,070
[INFO|trainer.py:1724] 2023-11-15 22:05:18,185 >>   Number of trainable parameters = 1,285,636
[INFO|integration_utils.py:716] 2023-11-15 22:05:18,186 >> Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"
  0%|          | 0/1070 [00:00<?, ?it/s]  0%|          | 1/1070 [00:00<17:39,  1.01it/s]  0%|          | 3/1070 [00:01<05:29,  3.24it/s]  0%|          | 5/1070 [00:01<03:17,  5.39it/s]  1%|          | 7/1070 [00:01<02:24,  7.35it/s]  1%|          | 9/1070 [00:01<01:57,  9.02it/s]  1%|          | 11/1070 [00:01<01:41, 10.38it/s]  1%|          | 13/1070 [00:01<01:32, 11.45it/s]  1%|â–         | 15/1070 [00:01<01:26, 12.27it/s]  2%|â–         | 17/1070 [00:02<01:21, 12.84it/s]  2%|â–         | 19/1070 [00:02<01:19, 13.29it/s]  2%|â–         | 21/1070 [00:02<01:17, 13.57it/s]  2%|â–         | 23/1070 [00:02<01:15, 13.81it/s]  2%|â–         | 25/1070 [00:02<01:14, 13.97it/s]  3%|â–Ž         | 27/1070 [00:02<01:14, 14.08it/s]  3%|â–Ž         | 29/1070 [00:02<01:13, 14.17it/s]  3%|â–Ž         | 31/1070 [00:03<01:12, 14.26it/s]  3%|â–Ž         | 33/1070 [00:03<01:12, 14.30it/s]  3%|â–Ž         | 35/1070 [00:03<01:12, 14.22it/s]  3%|â–Ž         | 37/1070 [00:03<01:12, 14.28it/s]  4%|â–Ž         | 39/1070 [00:03<01:12, 14.31it/s]  4%|â–         | 41/1070 [00:03<01:11, 14.31it/s]  4%|â–         | 43/1070 [00:03<01:11, 14.33it/s]  4%|â–         | 45/1070 [00:04<01:11, 14.33it/s]  4%|â–         | 47/1070 [00:04<01:11, 14.35it/s]  5%|â–         | 49/1070 [00:04<01:11, 14.34it/s]  5%|â–         | 51/1070 [00:04<01:11, 14.31it/s]  5%|â–         | 53/1070 [00:04<01:10, 14.34it/s]  5%|â–Œ         | 55/1070 [00:04<01:10, 14.36it/s]  5%|â–Œ         | 57/1070 [00:04<01:10, 14.36it/s]  6%|â–Œ         | 59/1070 [00:05<01:10, 14.38it/s]  6%|â–Œ         | 61/1070 [00:05<01:10, 14.35it/s]  6%|â–Œ         | 63/1070 [00:05<01:10, 14.34it/s]  6%|â–Œ         | 65/1070 [00:05<01:09, 14.37it/s]  6%|â–‹         | 67/1070 [00:05<01:09, 14.37it/s]  6%|â–‹         | 69/1070 [00:05<01:09, 14.36it/s]  7%|â–‹         | 71/1070 [00:05<01:09, 14.36it/s]  7%|â–‹         | 73/1070 [00:06<01:09, 14.33it/s]  7%|â–‹         | 75/1070 [00:06<01:09, 14.33it/s]  7%|â–‹         | 77/1070 [00:06<01:09, 14.33it/s]  7%|â–‹         | 79/1070 [00:06<01:09, 14.34it/s]  8%|â–Š         | 81/1070 [00:06<01:08, 14.35it/s]  8%|â–Š         | 83/1070 [00:06<01:08, 14.33it/s]  8%|â–Š         | 85/1070 [00:06<01:08, 14.33it/s]  8%|â–Š         | 87/1070 [00:06<01:08, 14.35it/s]  8%|â–Š         | 89/1070 [00:07<01:08, 14.36it/s]  9%|â–Š         | 91/1070 [00:07<01:08, 14.33it/s]  9%|â–Š         | 93/1070 [00:07<01:08, 14.33it/s]  9%|â–‰         | 95/1070 [00:07<01:07, 14.35it/s]  9%|â–‰         | 97/1070 [00:07<01:07, 14.37it/s]  9%|â–‰         | 99/1070 [00:07<01:07, 14.39it/s]  9%|â–‰         | 101/1070 [00:07<01:07, 14.38it/s] 10%|â–‰         | 103/1070 [00:08<01:07, 14.33it/s] 10%|â–‰         | 105/1070 [00:08<01:07, 14.36it/s] 10%|â–ˆ         | 107/1070 [00:08<01:07, 14.36it/s] 10%|â–ˆ         | 109/1070 [00:08<01:07, 14.29it/s] 10%|â–ˆ         | 111/1070 [00:08<01:06, 14.33it/s] 11%|â–ˆ         | 113/1070 [00:08<01:06, 14.29it/s] 11%|â–ˆ         | 115/1070 [00:08<01:06, 14.31it/s] 11%|â–ˆ         | 117/1070 [00:09<01:06, 14.29it/s] 11%|â–ˆ         | 119/1070 [00:09<01:06, 14.31it/s] 11%|â–ˆâ–        | 121/1070 [00:09<01:06, 14.32it/s] 11%|â–ˆâ–        | 123/1070 [00:09<01:06, 14.31it/s] 12%|â–ˆâ–        | 125/1070 [00:09<01:06, 14.30it/s] 12%|â–ˆâ–        | 127/1070 [00:09<01:05, 14.30it/s] 12%|â–ˆâ–        | 129/1070 [00:09<01:05, 14.29it/s] 12%|â–ˆâ–        | 131/1070 [00:10<01:05, 14.30it/s] 12%|â–ˆâ–        | 133/1070 [00:10<01:05, 14.31it/s] 13%|â–ˆâ–Ž        | 135/1070 [00:10<01:05, 14.32it/s] 13%|â–ˆâ–Ž        | 137/1070 [00:10<01:05, 14.33it/s] 13%|â–ˆâ–Ž        | 139/1070 [00:10<01:04, 14.33it/s] 13%|â–ˆâ–Ž        | 141/1070 [00:10<01:04, 14.33it/s] 13%|â–ˆâ–Ž        | 143/1070 [00:10<01:04, 14.32it/s] 14%|â–ˆâ–Ž        | 145/1070 [00:11<01:04, 14.33it/s] 14%|â–ˆâ–Ž        | 147/1070 [00:11<01:04, 14.36it/s] 14%|â–ˆâ–        | 149/1070 [00:11<01:04, 14.38it/s] 14%|â–ˆâ–        | 151/1070 [00:11<01:03, 14.40it/s] 14%|â–ˆâ–        | 153/1070 [00:11<01:03, 14.42it/s] 14%|â–ˆâ–        | 155/1070 [00:11<01:03, 14.42it/s] 15%|â–ˆâ–        | 157/1070 [00:11<01:03, 14.42it/s] 15%|â–ˆâ–        | 159/1070 [00:11<01:03, 14.41it/s] 15%|â–ˆâ–Œ        | 161/1070 [00:12<01:03, 14.42it/s] 15%|â–ˆâ–Œ        | 163/1070 [00:12<01:02, 14.42it/s] 15%|â–ˆâ–Œ        | 165/1070 [00:12<01:02, 14.44it/s] 16%|â–ˆâ–Œ        | 167/1070 [00:12<01:02, 14.46it/s] 16%|â–ˆâ–Œ        | 169/1070 [00:12<01:02, 14.45it/s] 16%|â–ˆâ–Œ        | 171/1070 [00:12<01:02, 14.44it/s] 16%|â–ˆâ–Œ        | 173/1070 [00:12<01:02, 14.44it/s] 16%|â–ˆâ–‹        | 175/1070 [00:13<01:02, 14.40it/s] 17%|â–ˆâ–‹        | 177/1070 [00:13<01:01, 14.42it/s] 17%|â–ˆâ–‹        | 179/1070 [00:13<01:01, 14.43it/s] 17%|â–ˆâ–‹        | 181/1070 [00:13<01:01, 14.45it/s] 17%|â–ˆâ–‹        | 183/1070 [00:13<01:01, 14.44it/s] 17%|â–ˆâ–‹        | 185/1070 [00:13<01:01, 14.43it/s] 17%|â–ˆâ–‹        | 187/1070 [00:13<01:01, 14.39it/s] 18%|â–ˆâ–Š        | 189/1070 [00:14<01:01, 14.41it/s] 18%|â–ˆâ–Š        | 191/1070 [00:14<01:00, 14.43it/s] 18%|â–ˆâ–Š        | 193/1070 [00:14<01:00, 14.45it/s] 18%|â–ˆâ–Š        | 195/1070 [00:14<01:00, 14.46it/s] 18%|â–ˆâ–Š        | 197/1070 [00:14<01:00, 14.45it/s] 19%|â–ˆâ–Š        | 199/1070 [00:14<01:00, 14.43it/s] 19%|â–ˆâ–‰        | 201/1070 [00:14<01:00, 14.43it/s] 19%|â–ˆâ–‰        | 203/1070 [00:15<01:00, 14.41it/s] 19%|â–ˆâ–‰        | 205/1070 [00:15<01:00, 14.41it/s] 19%|â–ˆâ–‰        | 207/1070 [00:15<00:59, 14.42it/s] 20%|â–ˆâ–‰        | 209/1070 [00:15<00:59, 14.43it/s] 20%|â–ˆâ–‰        | 211/1070 [00:15<00:59, 14.44it/s] 20%|â–ˆâ–‰        | 213/1070 [00:15<00:59, 14.43it/s]                                                   20%|â–ˆâ–ˆ        | 214/1070 [00:15<00:59, 14.43it/s][INFO|trainer.py:755] 2023-11-15 22:05:33,984 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModelForTokenClassification.forward` and have been ignored: text. If text are not expected by `PeftModelForTokenClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 22:05:33,986 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 22:05:33,986 >>   Num examples = 760
[INFO|trainer.py:3134] 2023-11-15 22:05:33,986 >>   Batch size = 8
{'loss': 0.4208, 'learning_rate': 0.0004, 'epoch': 1.0}

  0%|          | 0/95 [00:00<?, ?it/s][A
 13%|â–ˆâ–Ž        | 12/95 [00:00<00:00, 117.11it/s][A
 25%|â–ˆâ–ˆâ–Œ       | 24/95 [00:00<00:00, 111.26it/s][A
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 36/95 [00:00<00:00, 109.08it/s][A
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 47/95 [00:00<00:00, 108.35it/s][A
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 58/95 [00:00<00:00, 108.29it/s][A
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 69/95 [00:00<00:00, 107.95it/s][A
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 80/95 [00:00<00:00, 107.99it/s][A
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 91/95 [00:00<00:00, 107.96it/s][A                                                  
                                                [A 20%|â–ˆâ–ˆ        | 214/1070 [00:16<00:59, 14.43it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 95/95 [00:00<00:00, 107.96it/s][A
                                                [A 20%|â–ˆâ–ˆ        | 215/1070 [00:16<02:55,  4.86it/s] 20%|â–ˆâ–ˆ        | 217/1070 [00:16<02:20,  6.06it/s] 20%|â–ˆâ–ˆ        | 219/1070 [00:17<01:56,  7.32it/s] 21%|â–ˆâ–ˆ        | 221/1070 [00:17<01:39,  8.57it/s] 21%|â–ˆâ–ˆ        | 223/1070 [00:17<01:26,  9.74it/s] 21%|â–ˆâ–ˆ        | 225/1070 [00:17<01:18, 10.78it/s] 21%|â–ˆâ–ˆ        | 227/1070 [00:17<01:12, 11.64it/s] 21%|â–ˆâ–ˆâ–       | 229/1070 [00:17<01:08, 12.33it/s] 22%|â–ˆâ–ˆâ–       | 231/1070 [00:17<01:05, 12.87it/s] 22%|â–ˆâ–ˆâ–       | 233/1070 [00:18<01:03, 13.27it/s] 22%|â–ˆâ–ˆâ–       | 235/1070 [00:18<01:01, 13.55it/s] 22%|â–ˆâ–ˆâ–       | 237/1070 [00:18<01:00, 13.76it/s] 22%|â–ˆâ–ˆâ–       | 239/1070 [00:18<00:59, 13.93it/s] 23%|â–ˆâ–ˆâ–Ž       | 241/1070 [00:18<00:58, 14.05it/s] 23%|â–ˆâ–ˆâ–Ž       | 243/1070 [00:18<00:58, 14.15it/s] 23%|â–ˆâ–ˆâ–Ž       | 245/1070 [00:18<00:58, 14.21it/s] 23%|â–ˆâ–ˆâ–Ž       | 247/1070 [00:19<00:57, 14.26it/s] 23%|â–ˆâ–ˆâ–Ž       | 249/1070 [00:19<00:57, 14.30it/s] 23%|â–ˆâ–ˆâ–Ž       | 251/1070 [00:19<00:57, 14.33it/s] 24%|â–ˆâ–ˆâ–Ž       | 253/1070 [00:19<00:56, 14.34it/s] 24%|â–ˆâ–ˆâ–       | 255/1070 [00:19<00:56, 14.36it/s] 24%|â–ˆâ–ˆâ–       | 257/1070 [00:19<00:56, 14.36it/s] 24%|â–ˆâ–ˆâ–       | 259/1070 [00:19<00:56, 14.34it/s] 24%|â–ˆâ–ˆâ–       | 261/1070 [00:19<00:56, 14.33it/s] 25%|â–ˆâ–ˆâ–       | 263/1070 [00:20<00:56, 14.30it/s] 25%|â–ˆâ–ˆâ–       | 265/1070 [00:20<00:56, 14.29it/s] 25%|â–ˆâ–ˆâ–       | 267/1070 [00:20<00:56, 14.30it/s] 25%|â–ˆâ–ˆâ–Œ       | 269/1070 [00:20<00:56, 14.28it/s] 25%|â–ˆâ–ˆâ–Œ       | 271/1070 [00:20<00:55, 14.30it/s] 26%|â–ˆâ–ˆâ–Œ       | 273/1070 [00:20<00:55, 14.31it/s] 26%|â–ˆâ–ˆâ–Œ       | 275/1070 [00:20<00:55, 14.30it/s] 26%|â–ˆâ–ˆâ–Œ       | 277/1070 [00:21<00:55, 14.30it/s] 26%|â–ˆâ–ˆâ–Œ       | 279/1070 [00:21<00:55, 14.30it/s] 26%|â–ˆâ–ˆâ–‹       | 281/1070 [00:21<00:55, 14.30it/s] 26%|â–ˆâ–ˆâ–‹       | 283/1070 [00:21<00:55, 14.30it/s] 27%|â–ˆâ–ˆâ–‹       | 285/1070 [00:21<00:54, 14.30it/s] 27%|â–ˆâ–ˆâ–‹       | 287/1070 [00:21<00:54, 14.29it/s] 27%|â–ˆâ–ˆâ–‹       | 289/1070 [00:21<00:54, 14.30it/s] 27%|â–ˆâ–ˆâ–‹       | 291/1070 [00:22<00:54, 14.30it/s] 27%|â–ˆâ–ˆâ–‹       | 293/1070 [00:22<00:54, 14.30it/s] 28%|â–ˆâ–ˆâ–Š       | 295/1070 [00:22<00:54, 14.31it/s] 28%|â–ˆâ–ˆâ–Š       | 297/1070 [00:22<00:54, 14.30it/s] 28%|â–ˆâ–ˆâ–Š       | 299/1070 [00:22<00:53, 14.30it/s] 28%|â–ˆâ–ˆâ–Š       | 301/1070 [00:22<00:53, 14.28it/s] 28%|â–ˆâ–ˆâ–Š       | 303/1070 [00:22<00:53, 14.29it/s] 29%|â–ˆâ–ˆâ–Š       | 305/1070 [00:23<00:53, 14.30it/s] 29%|â–ˆâ–ˆâ–Š       | 307/1070 [00:23<00:53, 14.24it/s] 29%|â–ˆâ–ˆâ–‰       | 309/1070 [00:23<00:53, 14.27it/s] 29%|â–ˆâ–ˆâ–‰       | 311/1070 [00:23<00:53, 14.26it/s] 29%|â–ˆâ–ˆâ–‰       | 313/1070 [00:23<00:53, 14.27it/s] 29%|â–ˆâ–ˆâ–‰       | 315/1070 [00:23<00:52, 14.29it/s] 30%|â–ˆâ–ˆâ–‰       | 317/1070 [00:23<00:52, 14.30it/s] 30%|â–ˆâ–ˆâ–‰       | 319/1070 [00:24<00:52, 14.30it/s] 30%|â–ˆâ–ˆâ–ˆ       | 321/1070 [00:24<00:52, 14.30it/s] 30%|â–ˆâ–ˆâ–ˆ       | 323/1070 [00:24<00:52, 14.29it/s] 30%|â–ˆâ–ˆâ–ˆ       | 325/1070 [00:24<00:52, 14.29it/s] 31%|â–ˆâ–ˆâ–ˆ       | 327/1070 [00:24<00:51, 14.29it/s] 31%|â–ˆâ–ˆâ–ˆ       | 329/1070 [00:24<00:51, 14.28it/s] 31%|â–ˆâ–ˆâ–ˆ       | 331/1070 [00:24<00:51, 14.29it/s] 31%|â–ˆâ–ˆâ–ˆ       | 333/1070 [00:25<00:51, 14.30it/s] 31%|â–ˆâ–ˆâ–ˆâ–      | 335/1070 [00:25<00:51, 14.30it/s] 31%|â–ˆâ–ˆâ–ˆâ–      | 337/1070 [00:25<00:51, 14.29it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 339/1070 [00:25<00:51, 14.28it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 341/1070 [00:25<00:51, 14.29it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 343/1070 [00:25<00:50, 14.30it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 345/1070 [00:25<00:50, 14.29it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 347/1070 [00:26<00:50, 14.28it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 349/1070 [00:26<00:50, 14.29it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 351/1070 [00:26<00:50, 14.29it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 353/1070 [00:26<00:50, 14.30it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 355/1070 [00:26<00:50, 14.29it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 357/1070 [00:26<00:49, 14.29it/s] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 359/1070 [00:26<00:49, 14.30it/s] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 361/1070 [00:26<00:49, 14.29it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 363/1070 [00:27<00:49, 14.29it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 365/1070 [00:27<00:49, 14.29it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 367/1070 [00:27<00:49, 14.29it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 369/1070 [00:27<00:49, 14.29it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 371/1070 [00:27<00:49, 14.26it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 373/1070 [00:27<00:48, 14.28it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 375/1070 [00:27<00:48, 14.29it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 377/1070 [00:28<00:48, 14.28it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 379/1070 [00:28<00:48, 14.29it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 381/1070 [00:28<00:48, 14.25it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 383/1070 [00:28<00:48, 14.27it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 385/1070 [00:28<00:47, 14.29it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 387/1070 [00:28<00:47, 14.30it/s] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 389/1070 [00:28<00:47, 14.30it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 391/1070 [00:29<00:47, 14.30it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 393/1070 [00:29<00:47, 14.30it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 395/1070 [00:29<00:47, 14.30it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 397/1070 [00:29<00:47, 14.30it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 399/1070 [00:29<00:46, 14.30it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 401/1070 [00:29<00:46, 14.29it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 403/1070 [00:29<00:46, 14.30it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 405/1070 [00:30<00:46, 14.29it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 407/1070 [00:30<00:46, 14.28it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 409/1070 [00:30<00:46, 14.28it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 411/1070 [00:30<00:46, 14.29it/s] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 413/1070 [00:30<00:45, 14.30it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 415/1070 [00:30<00:45, 14.28it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 417/1070 [00:30<00:45, 14.29it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 419/1070 [00:31<00:45, 14.29it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 421/1070 [00:31<00:45, 14.27it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 423/1070 [00:31<00:45, 14.28it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 425/1070 [00:31<00:45, 14.24it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 427/1070 [00:31<00:45, 14.28it/s]                                                   40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 428/1070 [00:31<00:44, 14.28it/s][INFO|trainer.py:755] 2023-11-15 22:05:49,861 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModelForTokenClassification.forward` and have been ignored: text. If text are not expected by `PeftModelForTokenClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 22:05:49,862 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 22:05:49,862 >>   Num examples = 760
[INFO|trainer.py:3134] 2023-11-15 22:05:49,863 >>   Batch size = 8
{'eval_loss': 0.31045302748680115, 'eval_accuracy': 0.9, 'eval_micro_f1': 0.9, 'eval_macro_f1': 0.8961714312602972, 'eval_runtime': 0.9177, 'eval_samples_per_second': 828.162, 'eval_steps_per_second': 103.52, 'epoch': 1.0}
{'loss': 0.2599, 'learning_rate': 0.0003, 'epoch': 2.0}

  0%|          | 0/95 [00:00<?, ?it/s][A
 13%|â–ˆâ–Ž        | 12/95 [00:00<00:00, 117.24it/s][A
 25%|â–ˆâ–ˆâ–Œ       | 24/95 [00:00<00:00, 110.75it/s][A
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 36/95 [00:00<00:00, 109.07it/s][A
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 47/95 [00:00<00:00, 108.13it/s][A
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 58/95 [00:00<00:00, 107.62it/s][A
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 69/95 [00:00<00:00, 107.44it/s][A
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 80/95 [00:00<00:00, 107.00it/s][A
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 91/95 [00:00<00:00, 106.78it/s][A                                                  
                                                [A 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 428/1070 [00:32<00:44, 14.28it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 95/95 [00:00<00:00, 106.78it/s][A
                                                [A 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 429/1070 [00:32<02:12,  4.84it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 431/1070 [00:32<01:45,  6.03it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 433/1070 [00:32<01:27,  7.30it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 435/1070 [00:33<01:14,  8.56it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 437/1070 [00:33<01:05,  9.73it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 439/1070 [00:33<00:58, 10.76it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 441/1070 [00:33<00:54, 11.63it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 443/1070 [00:33<00:50, 12.31it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 445/1070 [00:33<00:48, 12.86it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 447/1070 [00:33<00:46, 13.27it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 449/1070 [00:34<00:45, 13.58it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 451/1070 [00:34<00:44, 13.80it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 453/1070 [00:34<00:44, 13.95it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 455/1070 [00:34<00:44, 13.97it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 457/1070 [00:34<00:43, 14.08it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 459/1070 [00:34<00:43, 14.16it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 461/1070 [00:34<00:42, 14.23it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 463/1070 [00:35<00:42, 14.31it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 465/1070 [00:35<00:42, 14.29it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 467/1070 [00:35<00:42, 14.27it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 469/1070 [00:35<00:42, 14.27it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 471/1070 [00:35<00:41, 14.27it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 473/1070 [00:35<00:41, 14.28it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 475/1070 [00:35<00:41, 14.26it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 477/1070 [00:36<00:41, 14.25it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 479/1070 [00:36<00:41, 14.27it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 481/1070 [00:36<00:41, 14.27it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 483/1070 [00:36<00:41, 14.28it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 485/1070 [00:36<00:40, 14.29it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 487/1070 [00:36<00:40, 14.29it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 489/1070 [00:36<00:40, 14.29it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 491/1070 [00:37<00:40, 14.30it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 493/1070 [00:37<00:40, 14.31it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 495/1070 [00:37<00:40, 14.30it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 497/1070 [00:37<00:40, 14.28it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 499/1070 [00:37<00:40, 14.27it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 501/1070 [00:37<00:39, 14.28it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 503/1070 [00:37<00:39, 14.30it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 505/1070 [00:37<00:39, 14.30it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 507/1070 [00:38<00:39, 14.30it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 509/1070 [00:38<00:39, 14.30it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 511/1070 [00:38<00:39, 14.31it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 513/1070 [00:38<00:38, 14.29it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 515/1070 [00:38<00:38, 14.29it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 517/1070 [00:38<00:38, 14.25it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 519/1070 [00:38<00:38, 14.24it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 521/1070 [00:39<00:38, 14.26it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 523/1070 [00:39<00:38, 14.23it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 525/1070 [00:39<00:38, 14.25it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 527/1070 [00:39<00:38, 14.27it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 529/1070 [00:39<00:38, 14.21it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 531/1070 [00:39<00:37, 14.23it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 533/1070 [00:39<00:37, 14.22it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 535/1070 [00:40<00:37, 14.23it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 537/1070 [00:40<00:37, 14.23it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 539/1070 [00:40<00:37, 14.24it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 541/1070 [00:40<00:37, 14.24it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 543/1070 [00:40<00:36, 14.24it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 545/1070 [00:40<00:36, 14.24it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 547/1070 [00:40<00:36, 14.23it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 549/1070 [00:41<00:36, 14.23it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 551/1070 [00:41<00:36, 14.24it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 553/1070 [00:41<00:36, 14.25it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 555/1070 [00:41<00:36, 14.26it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 557/1070 [00:41<00:35, 14.27it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 559/1070 [00:41<00:35, 14.26it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 561/1070 [00:41<00:36, 14.10it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 563/1070 [00:42<00:36, 14.00it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 565/1070 [00:42<00:36, 13.98it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 567/1070 [00:42<00:36, 13.97it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 569/1070 [00:42<00:35, 14.03it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 571/1070 [00:42<00:35, 14.11it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 573/1070 [00:42<00:35, 14.16it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 575/1070 [00:42<00:34, 14.19it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 577/1070 [00:43<00:34, 14.21it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 579/1070 [00:43<00:34, 14.24it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 581/1070 [00:43<00:34, 14.26it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 583/1070 [00:43<00:34, 14.26it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 585/1070 [00:43<00:33, 14.27it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 587/1070 [00:43<00:33, 14.28it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 589/1070 [00:43<00:33, 14.27it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 591/1070 [00:44<00:33, 14.27it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 593/1070 [00:44<00:33, 14.28it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 595/1070 [00:44<00:33, 14.29it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 597/1070 [00:44<00:33, 14.29it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 599/1070 [00:44<00:32, 14.29it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 601/1070 [00:44<00:32, 14.26it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 603/1070 [00:44<00:32, 14.27it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 605/1070 [00:45<00:32, 14.28it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 607/1070 [00:45<00:32, 14.28it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 609/1070 [00:45<00:32, 14.28it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 611/1070 [00:45<00:32, 14.28it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 613/1070 [00:45<00:31, 14.29it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 615/1070 [00:45<00:31, 14.29it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 617/1070 [00:45<00:31, 14.28it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 619/1070 [00:45<00:31, 14.29it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 621/1070 [00:46<00:31, 14.28it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 623/1070 [00:46<00:31, 14.28it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 625/1070 [00:46<00:31, 14.28it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 627/1070 [00:46<00:31, 14.28it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 629/1070 [00:46<00:30, 14.24it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 631/1070 [00:46<00:30, 14.19it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 633/1070 [00:46<00:30, 14.21it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 635/1070 [00:47<00:30, 14.21it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 637/1070 [00:47<00:30, 14.21it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 639/1070 [00:47<00:30, 14.20it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 641/1070 [00:47<00:30, 14.21it/s]                                                   60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 642/1070 [00:47<00:30, 14.21it/s][INFO|trainer.py:755] 2023-11-15 22:06:05,785 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModelForTokenClassification.forward` and have been ignored: text. If text are not expected by `PeftModelForTokenClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 22:06:05,786 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 22:06:05,786 >>   Num examples = 760
[INFO|trainer.py:3134] 2023-11-15 22:06:05,787 >>   Batch size = 8
{'eval_loss': 0.30336320400238037, 'eval_accuracy': 0.9078947368421053, 'eval_micro_f1': 0.9078947368421053, 'eval_macro_f1': 0.9049796980411022, 'eval_runtime': 0.9209, 'eval_samples_per_second': 825.281, 'eval_steps_per_second': 103.16, 'epoch': 2.0}
{'loss': 0.2202, 'learning_rate': 0.0002, 'epoch': 3.0}

  0%|          | 0/95 [00:00<?, ?it/s][A
 13%|â–ˆâ–Ž        | 12/95 [00:00<00:00, 116.99it/s][A
 25%|â–ˆâ–ˆâ–Œ       | 24/95 [00:00<00:00, 110.58it/s][A
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 36/95 [00:00<00:00, 108.64it/s][A
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 47/95 [00:00<00:00, 107.63it/s][A
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 58/95 [00:00<00:00, 107.19it/s][A
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 69/95 [00:00<00:00, 106.99it/s][A
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 80/95 [00:00<00:00, 106.96it/s][A
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 91/95 [00:00<00:00, 106.94it/s][A                                                  
                                                [A 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 642/1070 [00:48<00:30, 14.21it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 95/95 [00:00<00:00, 106.94it/s][A
                                                [A 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 643/1070 [00:48<01:28,  4.82it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 645/1070 [00:48<01:10,  6.02it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 647/1070 [00:48<00:58,  7.28it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 649/1070 [00:49<00:49,  8.53it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 651/1070 [00:49<00:43,  9.70it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 653/1070 [00:49<00:38, 10.73it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 655/1070 [00:49<00:35, 11.59it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 657/1070 [00:49<00:33, 12.28it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 659/1070 [00:49<00:32, 12.80it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 661/1070 [00:49<00:30, 13.20it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 663/1070 [00:49<00:30, 13.50it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 665/1070 [00:50<00:29, 13.71it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 667/1070 [00:50<00:29, 13.85it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 669/1070 [00:50<00:28, 13.97it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 671/1070 [00:50<00:28, 14.06it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 673/1070 [00:50<00:28, 14.13it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 675/1070 [00:50<00:27, 14.17it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 677/1070 [00:50<00:27, 14.21it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 679/1070 [00:51<00:27, 14.20it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 681/1070 [00:51<00:27, 14.23it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 683/1070 [00:51<00:27, 14.25it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 685/1070 [00:51<00:26, 14.27it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 687/1070 [00:51<00:26, 14.26it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 689/1070 [00:51<00:26, 14.26it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 691/1070 [00:51<00:26, 14.24it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 693/1070 [00:52<00:26, 14.25it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 695/1070 [00:52<00:26, 14.26it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 697/1070 [00:52<00:26, 14.25it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 699/1070 [00:52<00:26, 14.26it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 701/1070 [00:52<00:25, 14.26it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 703/1070 [00:52<00:25, 14.24it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 705/1070 [00:52<00:25, 14.26it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 707/1070 [00:53<00:25, 14.26it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 709/1070 [00:53<00:25, 14.25it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 711/1070 [00:53<00:25, 14.23it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 713/1070 [00:53<00:25, 14.23it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 715/1070 [00:53<00:24, 14.23it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 717/1070 [00:53<00:24, 14.23it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 719/1070 [00:53<00:24, 14.22it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 721/1070 [00:54<00:24, 14.20it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 723/1070 [00:54<00:24, 14.21it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 725/1070 [00:54<00:24, 14.22it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 727/1070 [00:54<00:24, 14.24it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 729/1070 [00:54<00:23, 14.24it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 731/1070 [00:54<00:23, 14.25it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 733/1070 [00:54<00:23, 14.26it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 735/1070 [00:55<00:23, 14.27it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 737/1070 [00:55<00:23, 14.27it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 739/1070 [00:55<00:23, 14.28it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 741/1070 [00:55<00:23, 14.28it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 743/1070 [00:55<00:22, 14.28it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 745/1070 [00:55<00:22, 14.29it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 747/1070 [00:55<00:22, 14.28it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 749/1070 [00:56<00:22, 14.29it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 751/1070 [00:56<00:22, 14.27it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 753/1070 [00:56<00:22, 14.25it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 755/1070 [00:56<00:22, 14.26it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 757/1070 [00:56<00:21, 14.27it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 759/1070 [00:56<00:21, 14.27it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 761/1070 [00:56<00:21, 14.28it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 763/1070 [00:57<00:21, 14.23it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 765/1070 [00:57<00:21, 14.24it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 767/1070 [00:57<00:21, 14.24it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 769/1070 [00:57<00:21, 14.22it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 771/1070 [00:57<00:21, 14.22it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 773/1070 [00:57<00:20, 14.22it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 775/1070 [00:57<00:20, 14.22it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 777/1070 [00:57<00:20, 14.23it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 779/1070 [00:58<00:20, 14.25it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 781/1070 [00:58<00:20, 14.25it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 783/1070 [00:58<00:20, 14.23it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 785/1070 [00:58<00:19, 14.25it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 787/1070 [00:58<00:19, 14.27it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 789/1070 [00:58<00:19, 14.28it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 791/1070 [00:58<00:19, 14.27it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 793/1070 [00:59<00:19, 14.26it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 795/1070 [00:59<00:19, 14.27it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 797/1070 [00:59<00:19, 14.28it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 799/1070 [00:59<00:18, 14.29it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 801/1070 [00:59<00:18, 14.27it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 803/1070 [00:59<00:18, 14.27it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 805/1070 [00:59<00:18, 14.28it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 807/1070 [01:00<00:18, 14.28it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 809/1070 [01:00<00:18, 14.27it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 811/1070 [01:00<00:18, 14.27it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 813/1070 [01:00<00:18, 14.27it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 815/1070 [01:00<00:17, 14.28it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 817/1070 [01:00<00:17, 14.29it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 819/1070 [01:00<00:17, 14.29it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 821/1070 [01:01<00:17, 14.29it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 823/1070 [01:01<00:17, 14.29it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 825/1070 [01:01<00:17, 14.28it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 827/1070 [01:01<00:17, 14.26it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 829/1070 [01:01<00:16, 14.25it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 831/1070 [01:01<00:16, 14.25it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 833/1070 [01:01<00:16, 14.25it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 835/1070 [01:02<00:16, 14.23it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 837/1070 [01:02<00:16, 14.22it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 839/1070 [01:02<00:16, 14.22it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 841/1070 [01:02<00:16, 14.22it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 843/1070 [01:02<00:16, 14.19it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 845/1070 [01:02<00:15, 14.20it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 847/1070 [01:02<00:15, 14.22it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 849/1070 [01:03<00:15, 14.24it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 851/1070 [01:03<00:15, 14.25it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 853/1070 [01:03<00:15, 14.26it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 855/1070 [01:03<00:15, 14.27it/s]                                                   80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 856/1070 [01:03<00:14, 14.27it/s][INFO|trainer.py:755] 2023-11-15 22:06:21,711 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModelForTokenClassification.forward` and have been ignored: text. If text are not expected by `PeftModelForTokenClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 22:06:21,712 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 22:06:21,713 >>   Num examples = 760
[INFO|trainer.py:3134] 2023-11-15 22:06:21,713 >>   Batch size = 8
{'eval_loss': 0.27088862657546997, 'eval_accuracy': 0.9118421052631579, 'eval_micro_f1': 0.9118421052631579, 'eval_macro_f1': 0.9089923364053729, 'eval_runtime': 0.9234, 'eval_samples_per_second': 823.031, 'eval_steps_per_second': 102.879, 'epoch': 3.0}
{'loss': 0.1783, 'learning_rate': 0.0001, 'epoch': 4.0}

  0%|          | 0/95 [00:00<?, ?it/s][A
 13%|â–ˆâ–Ž        | 12/95 [00:00<00:00, 117.24it/s][A
 25%|â–ˆâ–ˆâ–Œ       | 24/95 [00:00<00:00, 110.87it/s][A
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 36/95 [00:00<00:00, 108.15it/s][A
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 47/95 [00:00<00:00, 107.39it/s][A
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 58/95 [00:00<00:00, 107.22it/s][A
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 69/95 [00:00<00:00, 106.53it/s][A
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 80/95 [00:00<00:00, 106.51it/s][A
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 91/95 [00:00<00:00, 106.40it/s][A                                                  
                                                [A 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 856/1070 [01:04<00:14, 14.27it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 95/95 [00:00<00:00, 106.40it/s][A
                                                [A 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 857/1070 [01:04<00:44,  4.81it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 859/1070 [01:04<00:35,  6.00it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 861/1070 [01:04<00:28,  7.27it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 863/1070 [01:04<00:24,  8.52it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 865/1070 [01:05<00:21,  9.69it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 867/1070 [01:05<00:18, 10.73it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 869/1070 [01:05<00:17, 11.59it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 871/1070 [01:05<00:16, 12.28it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 873/1070 [01:05<00:15, 12.82it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 875/1070 [01:05<00:14, 13.22it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 877/1070 [01:05<00:14, 13.52it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 879/1070 [01:06<00:13, 13.71it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 881/1070 [01:06<00:13, 13.87it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 883/1070 [01:06<00:13, 13.96it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 885/1070 [01:06<00:13, 14.03it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 887/1070 [01:06<00:12, 14.09it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 889/1070 [01:06<00:12, 14.13it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 891/1070 [01:06<00:12, 14.15it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 893/1070 [01:07<00:12, 14.17it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 895/1070 [01:07<00:12, 14.18it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 897/1070 [01:07<00:12, 14.20it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 899/1070 [01:07<00:12, 14.22it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 901/1070 [01:07<00:11, 14.23it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 903/1070 [01:07<00:11, 14.24it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 905/1070 [01:07<00:11, 14.27it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 907/1070 [01:08<00:11, 14.26it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 909/1070 [01:08<00:11, 14.27it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 911/1070 [01:08<00:11, 14.26it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 913/1070 [01:08<00:11, 14.26it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 915/1070 [01:08<00:10, 14.25it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 917/1070 [01:08<00:10, 14.25it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 919/1070 [01:08<00:10, 14.26it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 921/1070 [01:09<00:10, 14.26it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 923/1070 [01:09<00:10, 14.27it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 925/1070 [01:09<00:10, 14.21it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 927/1070 [01:09<00:10, 14.21it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 929/1070 [01:09<00:09, 14.22it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 931/1070 [01:09<00:09, 14.22it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 933/1070 [01:09<00:09, 14.21it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 935/1070 [01:10<00:09, 14.19it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 937/1070 [01:10<00:09, 14.20it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 939/1070 [01:10<00:09, 14.21it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 941/1070 [01:10<00:09, 14.23it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 943/1070 [01:10<00:08, 14.24it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 945/1070 [01:10<00:08, 14.22it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 947/1070 [01:10<00:08, 14.23it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 949/1070 [01:10<00:08, 14.24it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 951/1070 [01:11<00:08, 14.25it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 953/1070 [01:11<00:08, 14.27it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 955/1070 [01:11<00:08, 14.23it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 957/1070 [01:11<00:07, 14.24it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 959/1070 [01:11<00:07, 14.24it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 961/1070 [01:11<00:07, 14.23it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 963/1070 [01:11<00:07, 14.20it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 965/1070 [01:12<00:07, 14.19it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 967/1070 [01:12<00:07, 14.18it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 969/1070 [01:12<00:07, 14.20it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 971/1070 [01:12<00:06, 14.23it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 973/1070 [01:12<00:06, 14.24it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 975/1070 [01:12<00:06, 14.22it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 977/1070 [01:12<00:06, 14.24it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 979/1070 [01:13<00:06, 14.25it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 981/1070 [01:13<00:06, 14.26it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 983/1070 [01:13<00:06, 14.25it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 985/1070 [01:13<00:05, 14.25it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 987/1070 [01:13<00:05, 14.24it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 989/1070 [01:13<00:05, 14.24it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 991/1070 [01:13<00:05, 14.23it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 993/1070 [01:14<00:05, 14.23it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 995/1070 [01:14<00:05, 14.22it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 997/1070 [01:14<00:05, 14.23it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 999/1070 [01:14<00:04, 14.22it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1001/1070 [01:14<00:04, 14.21it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1003/1070 [01:14<00:04, 14.20it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1005/1070 [01:14<00:04, 14.20it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1007/1070 [01:15<00:04, 14.22it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1009/1070 [01:15<00:04, 14.23it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1011/1070 [01:15<00:04, 14.22it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1013/1070 [01:15<00:04, 14.23it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1015/1070 [01:15<00:03, 14.24it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1017/1070 [01:15<00:03, 14.22it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1019/1070 [01:15<00:03, 14.21it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1021/1070 [01:16<00:03, 14.21it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1023/1070 [01:16<00:03, 14.21it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1025/1070 [01:16<00:03, 14.19it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1027/1070 [01:16<00:03, 14.12it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1029/1070 [01:16<00:02, 14.17it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1031/1070 [01:16<00:02, 14.20it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1033/1070 [01:16<00:02, 14.22it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1035/1070 [01:17<00:02, 14.24it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1037/1070 [01:17<00:02, 14.24it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1039/1070 [01:17<00:02, 14.24it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1041/1070 [01:17<00:02, 14.25it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1043/1070 [01:17<00:01, 14.25it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1045/1070 [01:17<00:01, 14.25it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1047/1070 [01:17<00:01, 14.20it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1049/1070 [01:18<00:01, 14.20it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1051/1070 [01:18<00:01, 14.21it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1053/1070 [01:18<00:01, 14.20it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1055/1070 [01:18<00:01, 14.19it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1057/1070 [01:18<00:00, 14.16it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1059/1070 [01:18<00:00, 14.19it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1061/1070 [01:18<00:00, 14.21it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1063/1070 [01:19<00:00, 14.22it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1065/1070 [01:19<00:00, 14.23it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1067/1070 [01:19<00:00, 14.22it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1069/1070 [01:19<00:00, 14.22it/s]                                                   100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1070/1070 [01:19<00:00, 14.22it/s][INFO|trainer.py:755] 2023-11-15 22:06:37,669 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModelForTokenClassification.forward` and have been ignored: text. If text are not expected by `PeftModelForTokenClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 22:06:37,671 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 22:06:37,671 >>   Num examples = 760
[INFO|trainer.py:3134] 2023-11-15 22:06:37,671 >>   Batch size = 8
{'eval_loss': 0.27993449568748474, 'eval_accuracy': 0.9052631578947369, 'eval_micro_f1': 0.9052631578947369, 'eval_macro_f1': 0.9023783182432938, 'eval_runtime': 0.9252, 'eval_samples_per_second': 821.421, 'eval_steps_per_second': 102.678, 'epoch': 4.0}
{'loss': 0.1566, 'learning_rate': 0.0, 'epoch': 5.0}

  0%|          | 0/95 [00:00<?, ?it/s][A
 13%|â–ˆâ–Ž        | 12/95 [00:00<00:00, 116.84it/s][A
 25%|â–ˆâ–ˆâ–Œ       | 24/95 [00:00<00:00, 110.61it/s][A
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 36/95 [00:00<00:00, 108.58it/s][A
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 47/95 [00:00<00:00, 107.62it/s][A
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 58/95 [00:00<00:00, 107.37it/s][A
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 69/95 [00:00<00:00, 106.97it/s][A
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 80/95 [00:00<00:00, 106.88it/s][A
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 91/95 [00:00<00:00, 106.78it/s][A                                                   
                                                [A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1070/1070 [01:20<00:00, 14.22it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 95/95 [00:00<00:00, 106.78it/s][A
                                                [A[INFO|trainer.py:1963] 2023-11-15 22:06:38,598 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                   100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1070/1070 [01:20<00:00, 14.22it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1070/1070 [01:20<00:00, 13.31it/s]
[INFO|trainer.py:2855] 2023-11-15 22:06:38,601 >> Saving model checkpoint to ./result/agnews_sup_roberta-base_seed1_lora
[INFO|tokenization_utils_base.py:2235] 2023-11-15 22:06:38,709 >> tokenizer config file saved in ./result/agnews_sup_roberta-base_seed1_lora/tokenizer_config.json
[INFO|tokenization_utils_base.py:2242] 2023-11-15 22:06:38,711 >> Special tokens file saved in ./result/agnews_sup_roberta-base_seed1_lora/special_tokens_map.json
{'eval_loss': 0.2798158824443817, 'eval_accuracy': 0.9144736842105263, 'eval_micro_f1': 0.9144736842105263, 'eval_macro_f1': 0.9120826687764378, 'eval_runtime': 0.9232, 'eval_samples_per_second': 823.232, 'eval_steps_per_second': 102.904, 'epoch': 5.0}
{'train_runtime': 80.4127, 'train_samples_per_second': 425.306, 'train_steps_per_second': 13.306, 'train_loss': 0.2471700472252391, 'epoch': 5.0}
***** train metrics *****
  epoch                    =        5.0
  train_loss               =     0.2472
  train_runtime            = 0:01:20.41
  train_samples            =       6840
  train_samples_per_second =    425.306
  train_steps_per_second   =     13.306
11/15/2023 22:06:38 - INFO - __main__ - *** Evaluate ***
[INFO|trainer.py:755] 2023-11-15 22:06:38,819 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModelForTokenClassification.forward` and have been ignored: text. If text are not expected by `PeftModelForTokenClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 22:06:38,820 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 22:06:38,820 >>   Num examples = 760
[INFO|trainer.py:3134] 2023-11-15 22:06:38,821 >>   Batch size = 8
  0%|          | 0/95 [00:00<?, ?it/s] 13%|â–ˆâ–Ž        | 12/95 [00:00<00:00, 118.03it/s] 25%|â–ˆâ–ˆâ–Œ       | 24/95 [00:00<00:00, 111.10it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 36/95 [00:00<00:00, 108.65it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 47/95 [00:00<00:00, 108.03it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 58/95 [00:00<00:00, 107.51it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 69/95 [00:00<00:00, 107.33it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 80/95 [00:00<00:00, 107.18it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 91/95 [00:00<00:00, 107.08it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 95/95 [00:00<00:00, 104.86it/s]
***** eval metrics *****
  epoch                   =        5.0
  eval_accuracy           =     0.9145
  eval_loss               =     0.2798
  eval_macro_f1           =     0.9121
  eval_micro_f1           =     0.9145
  eval_runtime            = 0:00:00.91
  eval_samples            =        760
  eval_samples_per_second =    827.241
  eval_steps_per_second   =    103.405
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                  eval/accuracy â–â–…â–‡â–„â–ˆâ–ˆ
wandb:                      eval/loss â–ˆâ–‡â–â–ƒâ–ƒâ–ƒ
wandb:                  eval/macro_f1 â–â–…â–‡â–„â–ˆâ–ˆ
wandb:                  eval/micro_f1 â–â–…â–‡â–„â–ˆâ–ˆ
wandb:                   eval/runtime â–â–„â–†â–ˆâ–†â–‚
wandb:        eval/samples_per_second â–ˆâ–…â–ƒâ–â–ƒâ–‡
wandb:          eval/steps_per_second â–ˆâ–…â–ƒâ–â–ƒâ–‡
wandb:                    train/epoch â–â–â–ƒâ–ƒâ–…â–…â–†â–†â–ˆâ–ˆâ–ˆâ–ˆ
wandb:              train/global_step â–â–â–ƒâ–ƒâ–„â–„â–†â–†â–ˆâ–ˆâ–ˆâ–ˆ
wandb:            train/learning_rate â–ˆâ–†â–…â–ƒâ–
wandb:                     train/loss â–ˆâ–„â–ƒâ–‚â–
wandb:               train/total_flos â–
wandb:               train/train_loss â–
wandb:            train/train_runtime â–
wandb: train/train_samples_per_second â–
wandb:   train/train_steps_per_second â–
wandb: 
wandb: Run summary:
wandb:                  eval/accuracy 0.91447
wandb:                      eval/loss 0.27982
wandb:                  eval/macro_f1 0.91208
wandb:                  eval/micro_f1 0.91447
wandb:                   eval/runtime 0.9187
wandb:        eval/samples_per_second 827.241
wandb:          eval/steps_per_second 103.405
wandb:                    train/epoch 5.0
wandb:              train/global_step 1070
wandb:            train/learning_rate 0.0
wandb:                     train/loss 0.1566
wandb:               train/total_flos 1140362523648000.0
wandb:               train/train_loss 0.24717
wandb:            train/train_runtime 80.4127
wandb: train/train_samples_per_second 425.306
wandb:   train/train_steps_per_second 13.306
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /ceph/home/wangyifei/a3/wandb/offline-run-20231115_220401-dwcohvcz
wandb: Find logs at: ./wandb/offline-run-20231115_220401-dwcohvcz/logs
(ModelArguments(model_name_or_path='roberta-base', cache_dir='./cache', use_adapter=False, use_lora=True), DataTrainingArguments(dataset_name='restaurant', max_seq_length=64), TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.0005,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=./result/restaurant_roberta-base_seed2_lora/runs/Nov15_22-06-49_node010,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.EPOCH,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=5.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
output_dir=./result/restaurant_roberta-base_seed2_lora,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
resume_from_checkpoint=None,
run_name=./result/restaurant_roberta-base_seed2_lora,
save_on_each_node=False,
save_safetensors=False,
save_steps=500,
save_strategy=IntervalStrategy.NO,
save_total_limit=None,
seed=333,
sharded_ddp=[],
skip_memory_metrics=True,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
))
11/15/2023 22:06:49 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False
11/15/2023 22:06:49 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.0005,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=./result/restaurant_roberta-base_seed2_lora/runs/Nov15_22-06-49_node010,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.EPOCH,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=5.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
output_dir=./result/restaurant_roberta-base_seed2_lora,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
resume_from_checkpoint=None,
run_name=./result/restaurant_roberta-base_seed2_lora,
save_on_each_node=False,
save_safetensors=False,
save_steps=500,
save_strategy=IntervalStrategy.NO,
save_total_limit=None,
seed=333,
sharded_ddp=[],
skip_memory_metrics=True,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
wandb: Tracking run with wandb version 0.15.12
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
Map:   0%|          | 0/4722 [00:00<?, ? examples/s]Map:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3992/4722 [00:00<00:00, 39710.05 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4722/4722 [00:00<00:00, 38398.24 examples/s]
[INFO|configuration_utils.py:715] 2023-11-15 22:07:05,442 >> loading configuration file config.json from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/config.json
[INFO|configuration_utils.py:775] 2023-11-15 22:07:05,451 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.33.3",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_auto.py:535] 2023-11-15 22:07:15,467 >> Could not locate the tokenizer configuration file, will try to use the model config instead.
[INFO|configuration_utils.py:715] 2023-11-15 22:07:25,483 >> loading configuration file config.json from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/config.json
[INFO|configuration_utils.py:775] 2023-11-15 22:07:25,484 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.33.3",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:1852] 2023-11-15 22:07:45,572 >> loading file vocab.json from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/vocab.json
[INFO|tokenization_utils_base.py:1852] 2023-11-15 22:07:45,573 >> loading file merges.txt from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/merges.txt
[INFO|tokenization_utils_base.py:1852] 2023-11-15 22:07:45,573 >> loading file tokenizer.json from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/tokenizer.json
[INFO|tokenization_utils_base.py:1852] 2023-11-15 22:07:45,573 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1852] 2023-11-15 22:07:45,574 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1852] 2023-11-15 22:07:45,574 >> loading file tokenizer_config.json from cache at None
[INFO|configuration_utils.py:715] 2023-11-15 22:07:45,575 >> loading configuration file config.json from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/config.json
[INFO|configuration_utils.py:775] 2023-11-15 22:07:45,576 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.33.3",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:2862] 2023-11-15 22:08:05,737 >> loading weights file model.safetensors from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/model.safetensors
[INFO|modeling_utils.py:3638] 2023-11-15 22:08:06,429 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3650] 2023-11-15 22:08:06,430 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
trainable params: 1,284,867 || all params: 125,830,662 || trainable%: 1.0211080348603745
model_max_length: 512
Running tokenizer on dataset:   0%|          | 0/3777 [00:00<?, ? examples/s]Running tokenizer on dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3777/3777 [00:00<00:00, 25525.82 examples/s]Running tokenizer on dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3777/3777 [00:00<00:00, 25152.68 examples/s]
Running tokenizer on dataset:   0%|          | 0/945 [00:00<?, ? examples/s]Running tokenizer on dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 945/945 [00:00<00:00, 31682.83 examples/s]
11/15/2023 22:08:06 - INFO - __main__ - Sample 2272 of the training set: {'text': 'Carinthia cheese ravioli with wild mushrooms <SEP> Innovations are just as assured, from the simple Carinthia cheese ravioli with wild mushrooms to the caviar-topped sturgeon, beautifully matched with a bright green spinach-vodka sauce.', 'label': 0, 'input_ids': [0, 9518, 35744, 493, 7134, 25283, 14215, 118, 19, 3418, 25038, 28696, 3388, 510, 15698, 20067, 1635, 32, 95, 25, 7189, 6, 31, 5, 2007, 1653, 35744, 493, 7134, 25283, 14215, 118, 19, 3418, 25038, 7, 5, 32426, 12202, 12, 560, 5686, 1690, 710, 20989, 6, 16467, 9184, 19, 10, 4520, 2272, 31225, 12, 705, 40677, 8929, 4, 2, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0]}.
11/15/2023 22:08:06 - INFO - __main__ - Sample 1436 of the training set: {'text': 'jazz singer <SEP> jazz singer had a nice voice + she made us all get up to dance to shake some cals to eat some more.', 'label': 0, 'input_ids': [0, 267, 7706, 3250, 28696, 3388, 510, 15698, 11057, 3250, 56, 10, 2579, 2236, 2055, 79, 156, 201, 70, 120, 62, 7, 3836, 7, 8559, 103, 740, 1536, 7, 3529, 103, 55, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
11/15/2023 22:08:06 - INFO - __main__ - Sample 1446 of the training set: {'text': 'iceberg <SEP> The bruscetta is a bit soggy, but the salads were fresh, included a nice mix of greens (not iceberg) all dishes are served piping hot from the kitchen.', 'label': 1, 'input_ids': [0, 2463, 2865, 28696, 3388, 510, 15698, 20, 5378, 20214, 10464, 16, 10, 828, 579, 2154, 4740, 6, 53, 5, 26924, 58, 2310, 6, 1165, 10, 2579, 3344, 9, 16543, 36, 3654, 26937, 43, 70, 10230, 32, 1665, 37273, 2131, 31, 5, 4647, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
11/15/2023 22:08:06 - WARNING - accelerate.utils.other - Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[INFO|trainer.py:755] 2023-11-15 22:08:07,726 >> The following columns in the training set don't have a corresponding argument in `PeftModelForTokenClassification.forward` and have been ignored: text. If text are not expected by `PeftModelForTokenClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:1715] 2023-11-15 22:08:07,737 >> ***** Running training *****
[INFO|trainer.py:1716] 2023-11-15 22:08:07,737 >>   Num examples = 3,777
[INFO|trainer.py:1717] 2023-11-15 22:08:07,737 >>   Num Epochs = 5
[INFO|trainer.py:1718] 2023-11-15 22:08:07,737 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1721] 2023-11-15 22:08:07,738 >>   Total train batch size (w. parallel, distributed & accumulation) = 32
[INFO|trainer.py:1722] 2023-11-15 22:08:07,738 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:1723] 2023-11-15 22:08:07,738 >>   Total optimization steps = 595
[INFO|trainer.py:1724] 2023-11-15 22:08:07,739 >>   Number of trainable parameters = 1,284,867
[INFO|integration_utils.py:716] 2023-11-15 22:08:07,740 >> Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"
  0%|          | 0/595 [00:00<?, ?it/s]  0%|          | 1/595 [00:01<10:05,  1.02s/it]  1%|          | 3/595 [00:01<03:07,  3.16it/s]  1%|          | 5/595 [00:01<01:51,  5.28it/s]  1%|          | 7/595 [00:01<01:21,  7.23it/s]  2%|â–         | 9/595 [00:01<01:05,  8.91it/s]  2%|â–         | 11/595 [00:01<00:56, 10.28it/s]  2%|â–         | 13/595 [00:01<00:51, 11.31it/s]  3%|â–Ž         | 15/595 [00:01<00:47, 12.15it/s]  3%|â–Ž         | 17/595 [00:02<00:45, 12.78it/s]  3%|â–Ž         | 19/595 [00:02<00:43, 13.20it/s]  4%|â–Ž         | 21/595 [00:02<00:42, 13.53it/s]  4%|â–         | 23/595 [00:02<00:41, 13.78it/s]  4%|â–         | 25/595 [00:02<00:40, 13.94it/s]  5%|â–         | 27/595 [00:02<00:40, 14.08it/s]  5%|â–         | 29/595 [00:02<00:39, 14.15it/s]  5%|â–Œ         | 31/595 [00:03<00:39, 14.22it/s]  6%|â–Œ         | 33/595 [00:03<00:39, 14.30it/s]  6%|â–Œ         | 35/595 [00:03<00:39, 14.34it/s]  6%|â–Œ         | 37/595 [00:03<00:38, 14.36it/s]  7%|â–‹         | 39/595 [00:03<00:38, 14.36it/s]  7%|â–‹         | 41/595 [00:03<00:38, 14.37it/s]  7%|â–‹         | 43/595 [00:03<00:38, 14.36it/s]  8%|â–Š         | 45/595 [00:04<00:38, 14.38it/s]  8%|â–Š         | 47/595 [00:04<00:38, 14.37it/s]  8%|â–Š         | 49/595 [00:04<00:37, 14.39it/s]  9%|â–Š         | 51/595 [00:04<00:37, 14.41it/s]  9%|â–‰         | 53/595 [00:04<00:37, 14.30it/s]  9%|â–‰         | 55/595 [00:04<00:37, 14.33it/s] 10%|â–‰         | 57/595 [00:04<00:37, 14.38it/s] 10%|â–‰         | 59/595 [00:05<00:37, 14.39it/s] 10%|â–ˆ         | 61/595 [00:05<00:37, 14.38it/s] 11%|â–ˆ         | 63/595 [00:05<00:37, 14.35it/s] 11%|â–ˆ         | 65/595 [00:05<00:36, 14.36it/s] 11%|â–ˆâ–        | 67/595 [00:05<00:36, 14.35it/s] 12%|â–ˆâ–        | 69/595 [00:05<00:36, 14.35it/s] 12%|â–ˆâ–        | 71/595 [00:05<00:36, 14.37it/s] 12%|â–ˆâ–        | 73/595 [00:06<00:36, 14.38it/s] 13%|â–ˆâ–Ž        | 75/595 [00:06<00:36, 14.38it/s] 13%|â–ˆâ–Ž        | 77/595 [00:06<00:35, 14.40it/s] 13%|â–ˆâ–Ž        | 79/595 [00:06<00:35, 14.37it/s] 14%|â–ˆâ–Ž        | 81/595 [00:06<00:35, 14.39it/s] 14%|â–ˆâ–        | 83/595 [00:06<00:35, 14.41it/s] 14%|â–ˆâ–        | 85/595 [00:06<00:35, 14.41it/s] 15%|â–ˆâ–        | 87/595 [00:07<00:35, 14.40it/s] 15%|â–ˆâ–        | 89/595 [00:07<00:35, 14.38it/s] 15%|â–ˆâ–Œ        | 91/595 [00:07<00:35, 14.38it/s] 16%|â–ˆâ–Œ        | 93/595 [00:07<00:34, 14.38it/s] 16%|â–ˆâ–Œ        | 95/595 [00:07<00:34, 14.39it/s] 16%|â–ˆâ–‹        | 97/595 [00:07<00:34, 14.40it/s] 17%|â–ˆâ–‹        | 99/595 [00:07<00:34, 14.41it/s] 17%|â–ˆâ–‹        | 101/595 [00:07<00:34, 14.47it/s] 17%|â–ˆâ–‹        | 103/595 [00:08<00:34, 14.47it/s] 18%|â–ˆâ–Š        | 105/595 [00:08<00:33, 14.47it/s] 18%|â–ˆâ–Š        | 107/595 [00:08<00:33, 14.49it/s] 18%|â–ˆâ–Š        | 109/595 [00:08<00:33, 14.51it/s] 19%|â–ˆâ–Š        | 111/595 [00:08<00:33, 14.52it/s] 19%|â–ˆâ–‰        | 113/595 [00:08<00:33, 14.45it/s] 19%|â–ˆâ–‰        | 115/595 [00:08<00:33, 14.44it/s] 20%|â–ˆâ–‰        | 117/595 [00:09<00:33, 14.45it/s] 20%|â–ˆâ–ˆ        | 119/595 [00:09<00:30, 15.74it/s]                                                  20%|â–ˆâ–ˆ        | 119/595 [00:09<00:30, 15.74it/s][INFO|trainer.py:755] 2023-11-15 22:08:16,925 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModelForTokenClassification.forward` and have been ignored: text. If text are not expected by `PeftModelForTokenClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 22:08:16,927 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 22:08:16,927 >>   Num examples = 945
[INFO|trainer.py:3134] 2023-11-15 22:08:16,928 >>   Batch size = 8
{'loss': 0.6674, 'learning_rate': 0.0004, 'epoch': 1.0}

  0%|          | 0/119 [00:00<?, ?it/s][A
 10%|â–ˆ         | 12/119 [00:00<00:00, 115.36it/s][A
 20%|â–ˆâ–ˆ        | 24/119 [00:00<00:00, 111.00it/s][A
 30%|â–ˆâ–ˆâ–ˆ       | 36/119 [00:00<00:00, 109.73it/s][A
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 47/119 [00:00<00:00, 109.13it/s][A
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 58/119 [00:00<00:00, 108.14it/s][A
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 69/119 [00:00<00:00, 107.92it/s][A
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 80/119 [00:00<00:00, 107.47it/s][A
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 91/119 [00:00<00:00, 107.71it/s][A
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 102/119 [00:00<00:00, 107.86it/s][A
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 113/119 [00:01<00:00, 107.95it/s][A                                                 
                                                  [A 20%|â–ˆâ–ˆ        | 119/595 [00:10<00:30, 15.74it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 119/119 [00:01<00:00, 107.95it/s][A
                                                  [A 20%|â–ˆâ–ˆ        | 121/595 [00:10<01:52,  4.21it/s] 21%|â–ˆâ–ˆ        | 123/595 [00:10<01:28,  5.34it/s] 21%|â–ˆâ–ˆ        | 125/595 [00:10<01:11,  6.59it/s] 21%|â–ˆâ–ˆâ–       | 127/595 [00:10<00:59,  7.88it/s] 22%|â–ˆâ–ˆâ–       | 129/595 [00:11<00:51,  9.12it/s] 22%|â–ˆâ–ˆâ–       | 131/595 [00:11<00:45, 10.25it/s] 22%|â–ˆâ–ˆâ–       | 133/595 [00:11<00:41, 11.21it/s] 23%|â–ˆâ–ˆâ–Ž       | 135/595 [00:11<00:38, 12.02it/s] 23%|â–ˆâ–ˆâ–Ž       | 137/595 [00:11<00:36, 12.66it/s] 23%|â–ˆâ–ˆâ–Ž       | 139/595 [00:11<00:34, 13.15it/s] 24%|â–ˆâ–ˆâ–Ž       | 141/595 [00:11<00:33, 13.51it/s] 24%|â–ˆâ–ˆâ–       | 143/595 [00:11<00:32, 13.76it/s] 24%|â–ˆâ–ˆâ–       | 145/595 [00:12<00:32, 13.93it/s] 25%|â–ˆâ–ˆâ–       | 147/595 [00:12<00:31, 14.06it/s] 25%|â–ˆâ–ˆâ–Œ       | 149/595 [00:12<00:31, 14.16it/s] 25%|â–ˆâ–ˆâ–Œ       | 151/595 [00:12<00:31, 14.24it/s] 26%|â–ˆâ–ˆâ–Œ       | 153/595 [00:12<00:30, 14.31it/s] 26%|â–ˆâ–ˆâ–Œ       | 155/595 [00:12<00:30, 14.36it/s] 26%|â–ˆâ–ˆâ–‹       | 157/595 [00:12<00:30, 14.37it/s] 27%|â–ˆâ–ˆâ–‹       | 159/595 [00:13<00:30, 14.40it/s] 27%|â–ˆâ–ˆâ–‹       | 161/595 [00:13<00:30, 14.40it/s] 27%|â–ˆâ–ˆâ–‹       | 163/595 [00:13<00:30, 14.40it/s] 28%|â–ˆâ–ˆâ–Š       | 165/595 [00:13<00:29, 14.41it/s] 28%|â–ˆâ–ˆâ–Š       | 167/595 [00:13<00:29, 14.43it/s] 28%|â–ˆâ–ˆâ–Š       | 169/595 [00:13<00:29, 14.42it/s] 29%|â–ˆâ–ˆâ–Š       | 171/595 [00:13<00:29, 14.42it/s] 29%|â–ˆâ–ˆâ–‰       | 173/595 [00:14<00:29, 14.42it/s] 29%|â–ˆâ–ˆâ–‰       | 175/595 [00:14<00:29, 14.41it/s] 30%|â–ˆâ–ˆâ–‰       | 177/595 [00:14<00:29, 14.40it/s] 30%|â–ˆâ–ˆâ–ˆ       | 179/595 [00:14<00:28, 14.40it/s] 30%|â–ˆâ–ˆâ–ˆ       | 181/595 [00:14<00:28, 14.38it/s] 31%|â–ˆâ–ˆâ–ˆ       | 183/595 [00:14<00:28, 14.39it/s] 31%|â–ˆâ–ˆâ–ˆ       | 185/595 [00:14<00:28, 14.39it/s] 31%|â–ˆâ–ˆâ–ˆâ–      | 187/595 [00:15<00:28, 14.43it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 189/595 [00:15<00:28, 14.47it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 191/595 [00:15<00:28, 14.42it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 193/595 [00:15<00:27, 14.41it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 195/595 [00:15<00:27, 14.40it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 197/595 [00:15<00:27, 14.39it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 199/595 [00:15<00:27, 14.41it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 201/595 [00:16<00:27, 14.41it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 203/595 [00:16<00:27, 14.42it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 205/595 [00:16<00:27, 14.44it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 207/595 [00:16<00:26, 14.43it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 209/595 [00:16<00:26, 14.42it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 211/595 [00:16<00:26, 14.39it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 213/595 [00:16<00:26, 14.39it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 215/595 [00:16<00:26, 14.39it/s] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 217/595 [00:17<00:26, 14.39it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 219/595 [00:17<00:26, 14.40it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 221/595 [00:17<00:25, 14.42it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 223/595 [00:17<00:25, 14.44it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 225/595 [00:17<00:25, 14.44it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 227/595 [00:17<00:25, 14.42it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 229/595 [00:17<00:25, 14.39it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 231/595 [00:18<00:25, 14.38it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 233/595 [00:18<00:25, 14.39it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 235/595 [00:18<00:25, 14.40it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 237/595 [00:18<00:24, 14.46it/s]                                                  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 238/595 [00:18<00:24, 14.46it/s][INFO|trainer.py:755] 2023-11-15 22:08:26,287 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModelForTokenClassification.forward` and have been ignored: text. If text are not expected by `PeftModelForTokenClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 22:08:26,288 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 22:08:26,289 >>   Num examples = 945
[INFO|trainer.py:3134] 2023-11-15 22:08:26,289 >>   Batch size = 8
{'eval_loss': 0.6660851240158081, 'eval_accuracy': 0.765079365079365, 'eval_micro_f1': 0.765079365079365, 'eval_macro_f1': 0.6430835874110729, 'eval_runtime': 1.1413, 'eval_samples_per_second': 827.991, 'eval_steps_per_second': 104.265, 'epoch': 1.0}
{'loss': 0.515, 'learning_rate': 0.0003, 'epoch': 2.0}

  0%|          | 0/119 [00:00<?, ?it/s][A
 10%|â–ˆ         | 12/119 [00:00<00:00, 117.61it/s][A
 20%|â–ˆâ–ˆ        | 24/119 [00:00<00:00, 111.92it/s][A
 30%|â–ˆâ–ˆâ–ˆ       | 36/119 [00:00<00:00, 108.62it/s][A
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 47/119 [00:00<00:00, 108.26it/s][A
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 58/119 [00:00<00:00, 107.54it/s][A
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 69/119 [00:00<00:00, 107.49it/s][A
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 80/119 [00:00<00:00, 107.07it/s][A
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 91/119 [00:00<00:00, 107.23it/s][A
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 102/119 [00:00<00:00, 107.15it/s][A
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 113/119 [00:01<00:00, 107.19it/s][A                                                 
                                                  [A 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 238/595 [00:19<00:24, 14.46it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 119/119 [00:01<00:00, 107.19it/s][A
                                                  [A 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 239/595 [00:19<01:24,  4.23it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 241/595 [00:19<01:05,  5.36it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 243/595 [00:20<00:53,  6.60it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 245/595 [00:20<00:44,  7.88it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 247/595 [00:20<00:38,  9.12it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 249/595 [00:20<00:33, 10.26it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 251/595 [00:20<00:30, 11.23it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 253/595 [00:20<00:28, 12.02it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 255/595 [00:20<00:26, 12.63it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 257/595 [00:21<00:25, 13.10it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 259/595 [00:21<00:25, 13.44it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 261/595 [00:21<00:24, 13.70it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 263/595 [00:21<00:23, 13.89it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 265/595 [00:21<00:23, 14.03it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 267/595 [00:21<00:23, 14.14it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 269/595 [00:21<00:22, 14.22it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 271/595 [00:21<00:22, 14.28it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 273/595 [00:22<00:22, 14.29it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 275/595 [00:22<00:22, 14.32it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 277/595 [00:22<00:22, 14.34it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 279/595 [00:22<00:22, 14.33it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 281/595 [00:22<00:21, 14.31it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 283/595 [00:22<00:21, 14.31it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 285/595 [00:22<00:21, 14.31it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 287/595 [00:23<00:21, 14.31it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 289/595 [00:23<00:21, 14.31it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 291/595 [00:23<00:21, 14.32it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 293/595 [00:23<00:21, 14.31it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 295/595 [00:23<00:20, 14.32it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 297/595 [00:23<00:20, 14.32it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 299/595 [00:23<00:20, 14.31it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 301/595 [00:24<00:20, 14.31it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 303/595 [00:24<00:20, 14.30it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 305/595 [00:24<00:20, 14.32it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 307/595 [00:24<00:20, 14.32it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 309/595 [00:24<00:19, 14.33it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 311/595 [00:24<00:19, 14.34it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 313/595 [00:24<00:19, 14.35it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 315/595 [00:25<00:19, 14.33it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 317/595 [00:25<00:19, 14.34it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 319/595 [00:25<00:19, 14.35it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 321/595 [00:25<00:19, 14.38it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 323/595 [00:25<00:18, 14.39it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 325/595 [00:25<00:18, 14.39it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 327/595 [00:25<00:18, 14.37it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 329/595 [00:26<00:18, 14.35it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 331/595 [00:26<00:18, 14.33it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 333/595 [00:26<00:18, 14.33it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 335/595 [00:26<00:18, 14.33it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 337/595 [00:26<00:18, 14.32it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 339/595 [00:26<00:17, 14.32it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 341/595 [00:26<00:17, 14.30it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 343/595 [00:27<00:17, 14.30it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 345/595 [00:27<00:17, 14.30it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 347/595 [00:27<00:17, 14.31it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 349/595 [00:27<00:17, 14.27it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 351/595 [00:27<00:17, 14.28it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 353/595 [00:27<00:16, 14.27it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 355/595 [00:27<00:16, 14.27it/s]                                                  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 357/595 [00:27<00:16, 14.27it/s][INFO|trainer.py:755] 2023-11-15 22:08:35,700 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModelForTokenClassification.forward` and have been ignored: text. If text are not expected by `PeftModelForTokenClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 22:08:35,701 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 22:08:35,701 >>   Num examples = 945
[INFO|trainer.py:3134] 2023-11-15 22:08:35,702 >>   Batch size = 8
{'eval_loss': 0.530953586101532, 'eval_accuracy': 0.7798941798941799, 'eval_micro_f1': 0.7798941798941799, 'eval_macro_f1': 0.6700933008110495, 'eval_runtime': 1.1443, 'eval_samples_per_second': 825.802, 'eval_steps_per_second': 103.99, 'epoch': 2.0}
{'loss': 0.4366, 'learning_rate': 0.0002, 'epoch': 3.0}

  0%|          | 0/119 [00:00<?, ?it/s][A
 10%|â–ˆ         | 12/119 [00:00<00:00, 118.23it/s][A
 20%|â–ˆâ–ˆ        | 24/119 [00:00<00:00, 110.56it/s][A
 30%|â–ˆâ–ˆâ–ˆ       | 36/119 [00:00<00:00, 108.97it/s][A
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 47/119 [00:00<00:00, 108.36it/s][A
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 58/119 [00:00<00:00, 107.95it/s][A
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 69/119 [00:00<00:00, 107.62it/s][A
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 80/119 [00:00<00:00, 107.42it/s][A
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 91/119 [00:00<00:00, 107.34it/s][A
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 102/119 [00:00<00:00, 107.29it/s][A
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 113/119 [00:01<00:00, 107.25it/s][A                                                 
                                                  [A 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 357/595 [00:29<00:16, 14.27it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 119/119 [00:01<00:00, 107.25it/s][A
                                                  [A 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 358/595 [00:29<00:50,  4.65it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 360/595 [00:29<00:41,  5.70it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 362/595 [00:29<00:34,  6.85it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 364/595 [00:29<00:28,  8.05it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 366/595 [00:29<00:24,  9.21it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 368/595 [00:29<00:22, 10.28it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 370/595 [00:30<00:20, 11.21it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 372/595 [00:30<00:18, 11.97it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 374/595 [00:30<00:17, 12.57it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 376/595 [00:30<00:16, 13.05it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 378/595 [00:30<00:16, 13.41it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 380/595 [00:30<00:15, 13.65it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 382/595 [00:30<00:15, 13.84it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 384/595 [00:30<00:15, 13.94it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 386/595 [00:31<00:14, 14.03it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 388/595 [00:31<00:14, 14.12it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 390/595 [00:31<00:14, 14.19it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 392/595 [00:31<00:14, 14.22it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 394/595 [00:31<00:14, 14.24it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 396/595 [00:31<00:13, 14.26it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 398/595 [00:31<00:13, 14.26it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 400/595 [00:32<00:13, 14.28it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 402/595 [00:32<00:13, 14.28it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 404/595 [00:32<00:13, 14.28it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 406/595 [00:32<00:13, 14.28it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 408/595 [00:32<00:13, 14.29it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 410/595 [00:32<00:12, 14.29it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 412/595 [00:32<00:12, 14.29it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 414/595 [00:33<00:12, 14.24it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 416/595 [00:33<00:12, 14.28it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 418/595 [00:33<00:12, 14.25it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 420/595 [00:33<00:12, 14.29it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 422/595 [00:33<00:12, 14.30it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 424/595 [00:33<00:11, 14.31it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 426/595 [00:33<00:11, 14.31it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 428/595 [00:34<00:11, 14.31it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 430/595 [00:34<00:11, 14.30it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 432/595 [00:34<00:11, 14.30it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 434/595 [00:34<00:11, 14.29it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 436/595 [00:34<00:11, 14.29it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 438/595 [00:34<00:10, 14.29it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 440/595 [00:34<00:10, 14.28it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 442/595 [00:35<00:10, 14.26it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 444/595 [00:35<00:10, 14.29it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 446/595 [00:35<00:10, 14.30it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 448/595 [00:35<00:10, 14.28it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 450/595 [00:35<00:10, 14.29it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 452/595 [00:35<00:10, 14.30it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 454/595 [00:35<00:09, 14.30it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 456/595 [00:36<00:09, 14.30it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 458/595 [00:36<00:09, 14.28it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 460/595 [00:36<00:09, 14.26it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 462/595 [00:36<00:09, 14.24it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 464/595 [00:36<00:09, 14.24it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 466/595 [00:36<00:09, 14.27it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 468/595 [00:36<00:08, 14.30it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 470/595 [00:37<00:08, 14.32it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 472/595 [00:37<00:08, 14.32it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 474/595 [00:37<00:08, 14.33it/s]                                                  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 476/595 [00:37<00:08, 14.33it/s][INFO|trainer.py:755] 2023-11-15 22:08:45,138 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModelForTokenClassification.forward` and have been ignored: text. If text are not expected by `PeftModelForTokenClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 22:08:45,140 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 22:08:45,140 >>   Num examples = 945
[INFO|trainer.py:3134] 2023-11-15 22:08:45,140 >>   Batch size = 8
{'eval_loss': 0.5196322798728943, 'eval_accuracy': 0.8074074074074075, 'eval_micro_f1': 0.8074074074074074, 'eval_macro_f1': 0.7271111519285901, 'eval_runtime': 1.1446, 'eval_samples_per_second': 825.614, 'eval_steps_per_second': 103.966, 'epoch': 3.0}
{'loss': 0.3654, 'learning_rate': 0.0001, 'epoch': 4.0}

  0%|          | 0/119 [00:00<?, ?it/s][A
 10%|â–ˆ         | 12/119 [00:00<00:00, 118.57it/s][A
 20%|â–ˆâ–ˆ        | 24/119 [00:00<00:00, 111.25it/s][A
 30%|â–ˆâ–ˆâ–ˆ       | 36/119 [00:00<00:00, 108.40it/s][A
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 47/119 [00:00<00:00, 107.89it/s][A
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 58/119 [00:00<00:00, 107.58it/s][A
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 69/119 [00:00<00:00, 107.36it/s][A
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 80/119 [00:00<00:00, 107.23it/s][A
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 91/119 [00:00<00:00, 106.70it/s][A
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 102/119 [00:00<00:00, 106.62it/s][A
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 113/119 [00:01<00:00, 106.41it/s][A                                                 
                                                  [A 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 476/595 [00:38<00:08, 14.33it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 119/119 [00:01<00:00, 106.41it/s][A
                                                  [A 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 477/595 [00:38<00:25,  4.64it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 479/595 [00:38<00:20,  5.68it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 481/595 [00:38<00:16,  6.83it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 483/595 [00:39<00:13,  8.02it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 485/595 [00:39<00:11,  9.19it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 487/595 [00:39<00:10, 10.25it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 489/595 [00:39<00:09, 11.17it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 491/595 [00:39<00:08, 11.94it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 493/595 [00:39<00:08, 12.55it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 495/595 [00:39<00:07, 13.02it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 497/595 [00:40<00:07, 13.38it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 499/595 [00:40<00:07, 13.62it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 501/595 [00:40<00:06, 13.83it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 503/595 [00:40<00:06, 13.97it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 505/595 [00:40<00:06, 14.07it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 507/595 [00:40<00:06, 14.13it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 509/595 [00:40<00:06, 14.18it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 511/595 [00:41<00:05, 14.21it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 513/595 [00:41<00:05, 14.23it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 515/595 [00:41<00:05, 14.25it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 517/595 [00:41<00:05, 14.26it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 519/595 [00:41<00:05, 14.26it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 521/595 [00:41<00:05, 14.28it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 523/595 [00:41<00:05, 14.28it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 525/595 [00:41<00:04, 14.28it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 527/595 [00:42<00:04, 14.29it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 529/595 [00:42<00:04, 14.29it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 531/595 [00:42<00:04, 14.29it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 533/595 [00:42<00:04, 14.29it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 535/595 [00:42<00:04, 14.29it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 537/595 [00:42<00:04, 14.28it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 539/595 [00:42<00:03, 14.29it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 541/595 [00:43<00:03, 14.29it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 543/595 [00:43<00:03, 14.30it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 545/595 [00:43<00:03, 14.29it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 547/595 [00:43<00:03, 14.29it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 549/595 [00:43<00:03, 14.29it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 551/595 [00:43<00:03, 14.29it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 553/595 [00:43<00:02, 14.27it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 555/595 [00:44<00:02, 14.27it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 557/595 [00:44<00:02, 14.28it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 559/595 [00:44<00:02, 14.29it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 561/595 [00:44<00:02, 14.30it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 563/595 [00:44<00:02, 14.28it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 565/595 [00:44<00:02, 14.29it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 567/595 [00:44<00:01, 14.29it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 569/595 [00:45<00:01, 14.30it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 571/595 [00:45<00:01, 14.27it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 573/595 [00:45<00:01, 14.24it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 575/595 [00:45<00:01, 14.28it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 577/595 [00:45<00:01, 14.30it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 579/595 [00:45<00:01, 14.30it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 581/595 [00:45<00:00, 14.30it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 583/595 [00:46<00:00, 14.30it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 585/595 [00:46<00:00, 14.30it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 587/595 [00:46<00:00, 14.31it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 589/595 [00:46<00:00, 14.30it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 591/595 [00:46<00:00, 14.30it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 593/595 [00:46<00:00, 14.29it/s]                                                 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 595/595 [00:46<00:00, 14.29it/s][INFO|trainer.py:755] 2023-11-15 22:08:54,584 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModelForTokenClassification.forward` and have been ignored: text. If text are not expected by `PeftModelForTokenClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 22:08:54,586 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 22:08:54,586 >>   Num examples = 945
[INFO|trainer.py:3134] 2023-11-15 22:08:54,586 >>   Batch size = 8
{'eval_loss': 0.5148205161094666, 'eval_accuracy': 0.817989417989418, 'eval_micro_f1': 0.8179894179894182, 'eval_macro_f1': 0.7431785623845123, 'eval_runtime': 1.1488, 'eval_samples_per_second': 822.577, 'eval_steps_per_second': 103.584, 'epoch': 4.0}
{'loss': 0.3459, 'learning_rate': 0.0, 'epoch': 5.0}

  0%|          | 0/119 [00:00<?, ?it/s][A
 10%|â–ˆ         | 12/119 [00:00<00:00, 117.94it/s][A
 20%|â–ˆâ–ˆ        | 24/119 [00:00<00:00, 110.66it/s][A
 30%|â–ˆâ–ˆâ–ˆ       | 36/119 [00:00<00:00, 108.85it/s][A
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 47/119 [00:00<00:00, 107.24it/s][A
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 58/119 [00:00<00:00, 107.14it/s][A
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 69/119 [00:00<00:00, 107.04it/s][A
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 80/119 [00:00<00:00, 106.54it/s][A
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 91/119 [00:00<00:00, 106.52it/s][A
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 102/119 [00:00<00:00, 106.57it/s][A
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 113/119 [00:01<00:00, 106.76it/s][A                                                 
                                                  [A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 595/595 [00:47<00:00, 14.29it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 119/119 [00:01<00:00, 106.76it/s][A
                                                  [A[INFO|trainer.py:1963] 2023-11-15 22:08:55,737 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 595/595 [00:47<00:00, 14.29it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 595/595 [00:47<00:00, 12.40it/s]
[INFO|trainer.py:2855] 2023-11-15 22:08:55,739 >> Saving model checkpoint to ./result/restaurant_roberta-base_seed2_lora
[INFO|tokenization_utils_base.py:2235] 2023-11-15 22:08:55,846 >> tokenizer config file saved in ./result/restaurant_roberta-base_seed2_lora/tokenizer_config.json
[INFO|tokenization_utils_base.py:2242] 2023-11-15 22:08:55,848 >> Special tokens file saved in ./result/restaurant_roberta-base_seed2_lora/special_tokens_map.json
{'eval_loss': 0.4792897403240204, 'eval_accuracy': 0.816931216931217, 'eval_micro_f1': 0.816931216931217, 'eval_macro_f1': 0.7469986475067171, 'eval_runtime': 1.1475, 'eval_samples_per_second': 823.554, 'eval_steps_per_second': 103.707, 'epoch': 5.0}
{'train_runtime': 47.9973, 'train_samples_per_second': 393.46, 'train_steps_per_second': 12.397, 'train_loss': 0.46605958698176536, 'epoch': 5.0}
***** train metrics *****
  epoch                    =        5.0
  train_loss               =     0.4661
  train_runtime            = 0:00:47.99
  train_samples            =       3777
  train_samples_per_second =     393.46
  train_steps_per_second   =     12.397
11/15/2023 22:08:55 - INFO - __main__ - *** Evaluate ***
[INFO|trainer.py:755] 2023-11-15 22:08:55,938 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModelForTokenClassification.forward` and have been ignored: text. If text are not expected by `PeftModelForTokenClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 22:08:55,939 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 22:08:55,939 >>   Num examples = 945
[INFO|trainer.py:3134] 2023-11-15 22:08:55,940 >>   Batch size = 8
  0%|          | 0/119 [00:00<?, ?it/s] 10%|â–ˆ         | 12/119 [00:00<00:00, 118.04it/s] 20%|â–ˆâ–ˆ        | 24/119 [00:00<00:00, 111.94it/s] 30%|â–ˆâ–ˆâ–ˆ       | 36/119 [00:00<00:00, 109.79it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 47/119 [00:00<00:00, 108.67it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 58/119 [00:00<00:00, 107.78it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 69/119 [00:00<00:00, 107.72it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 80/119 [00:00<00:00, 107.45it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 91/119 [00:00<00:00, 107.50it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 102/119 [00:00<00:00, 107.31it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 113/119 [00:01<00:00, 107.25it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 119/119 [00:01<00:00, 105.69it/s]
***** eval metrics *****
  epoch                   =        5.0
  eval_accuracy           =     0.8169
  eval_loss               =     0.4793
  eval_macro_f1           =      0.747
  eval_micro_f1           =     0.8169
  eval_runtime            = 0:00:01.13
  eval_samples            =        945
  eval_samples_per_second =    829.828
  eval_steps_per_second   =    104.497
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                  eval/accuracy â–â–ƒâ–‡â–ˆâ–ˆâ–ˆ
wandb:                      eval/loss â–ˆâ–ƒâ–ƒâ–‚â–â–
wandb:                  eval/macro_f1 â–â–ƒâ–‡â–ˆâ–ˆâ–ˆ
wandb:                  eval/micro_f1 â–â–ƒâ–‡â–ˆâ–ˆâ–ˆ
wandb:                   eval/runtime â–ƒâ–…â–…â–ˆâ–‡â–
wandb:        eval/samples_per_second â–†â–„â–„â–â–‚â–ˆ
wandb:          eval/steps_per_second â–†â–„â–„â–â–‚â–ˆ
wandb:                    train/epoch â–â–â–ƒâ–ƒâ–…â–…â–†â–†â–ˆâ–ˆâ–ˆâ–ˆ
wandb:              train/global_step â–â–â–ƒâ–ƒâ–…â–…â–†â–†â–ˆâ–ˆâ–ˆâ–ˆ
wandb:            train/learning_rate â–ˆâ–†â–…â–ƒâ–
wandb:                     train/loss â–ˆâ–…â–ƒâ–â–
wandb:               train/total_flos â–
wandb:               train/train_loss â–
wandb:            train/train_runtime â–
wandb: train/train_samples_per_second â–
wandb:   train/train_steps_per_second â–
wandb: 
wandb: Run summary:
wandb:                  eval/accuracy 0.81693
wandb:                      eval/loss 0.47929
wandb:                  eval/macro_f1 0.747
wandb:                  eval/micro_f1 0.81693
wandb:                   eval/runtime 1.1388
wandb:        eval/samples_per_second 829.828
wandb:          eval/steps_per_second 104.497
wandb:                    train/epoch 5.0
wandb:              train/global_step 595
wandb:            train/learning_rate 0.0
wandb:                     train/loss 0.3459
wandb:               train/total_flos 629689029684480.0
wandb:               train/train_loss 0.46606
wandb:            train/train_runtime 47.9973
wandb: train/train_samples_per_second 393.46
wandb:   train/train_steps_per_second 12.397
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /ceph/home/wangyifei/a3/wandb/offline-run-20231115_220651-vvdhsodd
wandb: Find logs at: ./wandb/offline-run-20231115_220651-vvdhsodd/logs
(ModelArguments(model_name_or_path='roberta-base', cache_dir='./cache', use_adapter=False, use_lora=True), DataTrainingArguments(dataset_name='acl', max_seq_length=64), TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.0005,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=./result/acl_roberta-base_seed2_lora/runs/Nov15_22-09-07_node010,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.EPOCH,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=5.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
output_dir=./result/acl_roberta-base_seed2_lora,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
resume_from_checkpoint=None,
run_name=./result/acl_roberta-base_seed2_lora,
save_on_each_node=False,
save_safetensors=False,
save_steps=500,
save_strategy=IntervalStrategy.NO,
save_total_limit=None,
seed=333,
sharded_ddp=[],
skip_memory_metrics=True,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
))
11/15/2023 22:09:07 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False
11/15/2023 22:09:07 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.0005,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=./result/acl_roberta-base_seed2_lora/runs/Nov15_22-09-06_node010,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.EPOCH,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=5.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
output_dir=./result/acl_roberta-base_seed2_lora,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
resume_from_checkpoint=None,
run_name=./result/acl_roberta-base_seed2_lora,
save_on_each_node=False,
save_safetensors=False,
save_steps=500,
save_strategy=IntervalStrategy.NO,
save_total_limit=None,
seed=333,
sharded_ddp=[],
skip_memory_metrics=True,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
wandb: Tracking run with wandb version 0.15.12
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
Map:   0%|          | 0/11020 [00:00<?, ? examples/s]Map:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 4164/11020 [00:00<00:00, 41419.23 examples/s]Map:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 8463/11020 [00:00<00:00, 42334.10 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11020/11020 [00:00<00:00, 41402.07 examples/s]
[INFO|configuration_utils.py:715] 2023-11-15 22:09:23,403 >> loading configuration file config.json from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/config.json
[INFO|configuration_utils.py:775] 2023-11-15 22:09:23,412 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.33.3",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_auto.py:535] 2023-11-15 22:09:33,428 >> Could not locate the tokenizer configuration file, will try to use the model config instead.
[INFO|configuration_utils.py:715] 2023-11-15 22:09:43,445 >> loading configuration file config.json from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/config.json
[INFO|configuration_utils.py:775] 2023-11-15 22:09:43,446 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.33.3",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:1852] 2023-11-15 22:10:03,496 >> loading file vocab.json from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/vocab.json
[INFO|tokenization_utils_base.py:1852] 2023-11-15 22:10:03,496 >> loading file merges.txt from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/merges.txt
[INFO|tokenization_utils_base.py:1852] 2023-11-15 22:10:03,497 >> loading file tokenizer.json from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/tokenizer.json
[INFO|tokenization_utils_base.py:1852] 2023-11-15 22:10:03,497 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1852] 2023-11-15 22:10:03,497 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1852] 2023-11-15 22:10:03,497 >> loading file tokenizer_config.json from cache at None
[INFO|configuration_utils.py:715] 2023-11-15 22:10:03,498 >> loading configuration file config.json from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/config.json
[INFO|configuration_utils.py:775] 2023-11-15 22:10:03,499 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.33.3",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:2862] 2023-11-15 22:10:23,671 >> loading weights file model.safetensors from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/model.safetensors
[INFO|modeling_utils.py:3638] 2023-11-15 22:10:24,360 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3650] 2023-11-15 22:10:24,361 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
trainable params: 1,284,867 || all params: 125,830,662 || trainable%: 1.0211080348603745
model_max_length: 512
Running tokenizer on dataset:   0%|          | 0/8816 [00:00<?, ? examples/s]Running tokenizer on dataset:  34%|â–ˆâ–ˆâ–ˆâ–      | 3000/8816 [00:00<00:00, 19660.77 examples/s]Running tokenizer on dataset:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 7000/8816 [00:00<00:00, 21035.56 examples/s]Running tokenizer on dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8816/8816 [00:00<00:00, 21010.45 examples/s]
Running tokenizer on dataset:   0%|          | 0/2204 [00:00<?, ? examples/s]Running tokenizer on dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2204/2204 [00:00<00:00, 23140.29 examples/s]
11/15/2023 22:10:24 - INFO - __main__ - Sample 5747 of the training set: {'text': 'While the redox buffer pairs (e.g. GSH/GSSG, reduced PC/oxidised PC, and reduced Protein/oxidised Protein) can protect cells from oxidative damage (Tsuji et al., 2002), this produces an imbalance in the redox status that may lead to other unwanted effects such as changes in intracellular pH, whichâ€¦', 'label': 0, 'input_ids': [0, 5771, 5, 1275, 4325, 21944, 15029, 36, 242, 4, 571, 4, 272, 10237, 73, 534, 8108, 534, 6, 2906, 4985, 73, 4325, 808, 1720, 4985, 6, 8, 2906, 34786, 73, 4325, 808, 1720, 34786, 43, 64, 1744, 4590, 31, 46099, 1880, 36, 565, 9228, 5186, 4400, 1076, 482, 5241, 238, 42, 9108, 41, 27340, 11, 5, 1275, 4325, 2194, 14, 189, 483, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.
11/15/2023 22:10:24 - INFO - __main__ - Sample 5785 of the training set: {'text': 'BDNF has been shown to interact with several neurotrans-\nmitter systems, including the DA (Spenger et al., 1995), serotonin (5-HT) (Lyons et al., 1999; Rumajogee et al., 2002), and NPY systems (Barnea and Roberts, 2001; Nawa et al., 1993, 1994).', 'label': 0, 'input_ids': [0, 18941, 25356, 34, 57, 2343, 7, 10754, 19, 484, 44755, 12, 50118, 44370, 1743, 6, 217, 5, 9036, 36, 104, 9675, 2403, 4400, 1076, 482, 7969, 238, 43399, 36, 245, 12, 14469, 43, 36, 38683, 1790, 4400, 1076, 482, 6193, 131, 13772, 13745, 25601, 4400, 1076, 482, 5241, 238, 8, 26266, 975, 1743, 36, 14507, 22423, 8, 6274, 6, 5155, 131, 234, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.
11/15/2023 22:10:24 - INFO - __main__ - Sample 3534 of the training set: {'text': 'Studies have shown that alcohol consumption may contribute to the spread of HIV/AIDS by diminishing sexual inhibitions and interfering with oneâ€™s ability to adequately assess risk (Gordon et al. 1997; MacDonald et al. 2000a, b; Maisto et al. 2004).', 'label': 0, 'input_ids': [0, 46000, 33, 2343, 14, 3766, 4850, 189, 5042, 7, 5, 2504, 9, 7947, 73, 30968, 30, 28953, 1363, 38512, 8237, 8, 23524, 19, 65, 17, 27, 29, 1460, 7, 17327, 7118, 810, 36, 43226, 4400, 1076, 4, 7528, 131, 22207, 4400, 1076, 4, 3788, 102, 6, 741, 131, 3066, 661, 139, 4400, 1076, 4, 4482, 322, 2, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]}.
11/15/2023 22:10:25 - WARNING - accelerate.utils.other - Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[INFO|trainer.py:755] 2023-11-15 22:10:26,004 >> The following columns in the training set don't have a corresponding argument in `PeftModelForTokenClassification.forward` and have been ignored: text. If text are not expected by `PeftModelForTokenClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:1715] 2023-11-15 22:10:26,014 >> ***** Running training *****
[INFO|trainer.py:1716] 2023-11-15 22:10:26,014 >>   Num examples = 8,816
[INFO|trainer.py:1717] 2023-11-15 22:10:26,014 >>   Num Epochs = 5
[INFO|trainer.py:1718] 2023-11-15 22:10:26,015 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1721] 2023-11-15 22:10:26,015 >>   Total train batch size (w. parallel, distributed & accumulation) = 32
[INFO|trainer.py:1722] 2023-11-15 22:10:26,015 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:1723] 2023-11-15 22:10:26,016 >>   Total optimization steps = 1,380
[INFO|trainer.py:1724] 2023-11-15 22:10:26,016 >>   Number of trainable parameters = 1,284,867
[INFO|integration_utils.py:716] 2023-11-15 22:10:26,018 >> Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"
  0%|          | 0/1380 [00:00<?, ?it/s]  0%|          | 1/1380 [00:00<22:44,  1.01it/s]  0%|          | 3/1380 [00:01<07:04,  3.24it/s]  0%|          | 5/1380 [00:01<04:15,  5.39it/s]  1%|          | 7/1380 [00:01<03:07,  7.34it/s]  1%|          | 9/1380 [00:01<02:32,  8.98it/s]  1%|          | 11/1380 [00:01<02:12, 10.36it/s]  1%|          | 13/1380 [00:01<01:59, 11.44it/s]  1%|          | 15/1380 [00:01<01:51, 12.29it/s]  1%|          | 17/1380 [00:02<01:45, 12.88it/s]  1%|â–         | 19/1380 [00:02<01:42, 13.31it/s]  2%|â–         | 21/1380 [00:02<01:39, 13.62it/s]  2%|â–         | 23/1380 [00:02<01:37, 13.85it/s]  2%|â–         | 25/1380 [00:02<01:36, 14.02it/s]  2%|â–         | 27/1380 [00:02<01:35, 14.13it/s]  2%|â–         | 29/1380 [00:02<01:35, 14.20it/s]  2%|â–         | 31/1380 [00:03<01:34, 14.24it/s]  2%|â–         | 33/1380 [00:03<01:34, 14.27it/s]  3%|â–Ž         | 35/1380 [00:03<01:34, 14.29it/s]  3%|â–Ž         | 37/1380 [00:03<01:34, 14.24it/s]  3%|â–Ž         | 39/1380 [00:03<01:33, 14.27it/s]  3%|â–Ž         | 41/1380 [00:03<01:33, 14.29it/s]  3%|â–Ž         | 43/1380 [00:03<01:33, 14.29it/s]  3%|â–Ž         | 45/1380 [00:04<01:33, 14.30it/s]  3%|â–Ž         | 47/1380 [00:04<01:33, 14.24it/s]  4%|â–Ž         | 49/1380 [00:04<01:33, 14.26it/s]  4%|â–Ž         | 51/1380 [00:04<01:32, 14.30it/s]  4%|â–         | 53/1380 [00:04<01:32, 14.31it/s]  4%|â–         | 55/1380 [00:04<01:32, 14.32it/s]  4%|â–         | 57/1380 [00:04<01:32, 14.32it/s]  4%|â–         | 59/1380 [00:05<01:32, 14.32it/s]  4%|â–         | 61/1380 [00:05<01:32, 14.33it/s]  5%|â–         | 63/1380 [00:05<01:31, 14.34it/s]  5%|â–         | 65/1380 [00:05<01:31, 14.35it/s]  5%|â–         | 67/1380 [00:05<01:31, 14.35it/s]  5%|â–Œ         | 69/1380 [00:05<01:31, 14.35it/s]  5%|â–Œ         | 71/1380 [00:05<01:31, 14.36it/s]  5%|â–Œ         | 73/1380 [00:06<01:30, 14.37it/s]  5%|â–Œ         | 75/1380 [00:06<01:30, 14.38it/s]  6%|â–Œ         | 77/1380 [00:06<01:30, 14.39it/s]  6%|â–Œ         | 79/1380 [00:06<01:30, 14.40it/s]  6%|â–Œ         | 81/1380 [00:06<01:30, 14.37it/s]  6%|â–Œ         | 83/1380 [00:06<01:30, 14.35it/s]  6%|â–Œ         | 85/1380 [00:06<01:30, 14.34it/s]  6%|â–‹         | 87/1380 [00:06<01:30, 14.34it/s]  6%|â–‹         | 89/1380 [00:07<01:30, 14.34it/s]  7%|â–‹         | 91/1380 [00:07<01:29, 14.34it/s]  7%|â–‹         | 93/1380 [00:07<01:29, 14.34it/s]  7%|â–‹         | 95/1380 [00:07<01:29, 14.34it/s]  7%|â–‹         | 97/1380 [00:07<01:29, 14.34it/s]  7%|â–‹         | 99/1380 [00:07<01:29, 14.35it/s]  7%|â–‹         | 101/1380 [00:07<01:29, 14.34it/s]  7%|â–‹         | 103/1380 [00:08<01:29, 14.35it/s]  8%|â–Š         | 105/1380 [00:08<01:28, 14.35it/s]  8%|â–Š         | 107/1380 [00:08<01:28, 14.36it/s]  8%|â–Š         | 109/1380 [00:08<01:28, 14.37it/s]  8%|â–Š         | 111/1380 [00:08<01:28, 14.37it/s]  8%|â–Š         | 113/1380 [00:08<01:28, 14.37it/s]  8%|â–Š         | 115/1380 [00:08<01:28, 14.35it/s]  8%|â–Š         | 117/1380 [00:09<01:28, 14.33it/s]  9%|â–Š         | 119/1380 [00:09<01:28, 14.33it/s]  9%|â–‰         | 121/1380 [00:09<01:27, 14.32it/s]  9%|â–‰         | 123/1380 [00:09<01:27, 14.31it/s]  9%|â–‰         | 125/1380 [00:09<01:27, 14.30it/s]  9%|â–‰         | 127/1380 [00:09<01:27, 14.30it/s]  9%|â–‰         | 129/1380 [00:09<01:27, 14.30it/s]  9%|â–‰         | 131/1380 [00:10<01:27, 14.30it/s] 10%|â–‰         | 133/1380 [00:10<01:27, 14.28it/s] 10%|â–‰         | 135/1380 [00:10<01:27, 14.29it/s] 10%|â–‰         | 137/1380 [00:10<01:26, 14.30it/s] 10%|â–ˆ         | 139/1380 [00:10<01:26, 14.35it/s] 10%|â–ˆ         | 141/1380 [00:10<01:26, 14.37it/s] 10%|â–ˆ         | 143/1380 [00:10<01:25, 14.39it/s] 11%|â–ˆ         | 145/1380 [00:11<01:25, 14.41it/s] 11%|â–ˆ         | 147/1380 [00:11<01:25, 14.43it/s] 11%|â–ˆ         | 149/1380 [00:11<01:25, 14.45it/s] 11%|â–ˆ         | 151/1380 [00:11<01:25, 14.45it/s] 11%|â–ˆ         | 153/1380 [00:11<01:24, 14.44it/s] 11%|â–ˆ         | 155/1380 [00:11<01:25, 14.38it/s] 11%|â–ˆâ–        | 157/1380 [00:11<01:25, 14.38it/s] 12%|â–ˆâ–        | 159/1380 [00:11<01:24, 14.39it/s] 12%|â–ˆâ–        | 161/1380 [00:12<01:24, 14.40it/s] 12%|â–ˆâ–        | 163/1380 [00:12<01:24, 14.41it/s] 12%|â–ˆâ–        | 165/1380 [00:12<01:24, 14.43it/s] 12%|â–ˆâ–        | 167/1380 [00:12<01:24, 14.43it/s] 12%|â–ˆâ–        | 169/1380 [00:12<01:23, 14.43it/s] 12%|â–ˆâ–        | 171/1380 [00:12<01:23, 14.42it/s] 13%|â–ˆâ–Ž        | 173/1380 [00:12<01:23, 14.41it/s] 13%|â–ˆâ–Ž        | 175/1380 [00:13<01:23, 14.41it/s] 13%|â–ˆâ–Ž        | 177/1380 [00:13<01:23, 14.42it/s] 13%|â–ˆâ–Ž        | 179/1380 [00:13<01:23, 14.43it/s] 13%|â–ˆâ–Ž        | 181/1380 [00:13<01:23, 14.43it/s] 13%|â–ˆâ–Ž        | 183/1380 [00:13<01:22, 14.45it/s] 13%|â–ˆâ–Ž        | 185/1380 [00:13<01:22, 14.42it/s] 14%|â–ˆâ–Ž        | 187/1380 [00:13<01:22, 14.41it/s] 14%|â–ˆâ–Ž        | 189/1380 [00:14<01:22, 14.42it/s] 14%|â–ˆâ–        | 191/1380 [00:14<01:22, 14.40it/s] 14%|â–ˆâ–        | 193/1380 [00:14<01:22, 14.42it/s] 14%|â–ˆâ–        | 195/1380 [00:14<01:22, 14.42it/s] 14%|â–ˆâ–        | 197/1380 [00:14<01:21, 14.43it/s] 14%|â–ˆâ–        | 199/1380 [00:14<01:21, 14.43it/s] 15%|â–ˆâ–        | 201/1380 [00:14<01:21, 14.41it/s] 15%|â–ˆâ–        | 203/1380 [00:15<01:21, 14.40it/s] 15%|â–ˆâ–        | 205/1380 [00:15<01:21, 14.40it/s] 15%|â–ˆâ–Œ        | 207/1380 [00:15<01:21, 14.40it/s] 15%|â–ˆâ–Œ        | 209/1380 [00:15<01:21, 14.41it/s] 15%|â–ˆâ–Œ        | 211/1380 [00:15<01:21, 14.42it/s] 15%|â–ˆâ–Œ        | 213/1380 [00:15<01:20, 14.44it/s] 16%|â–ˆâ–Œ        | 215/1380 [00:15<01:20, 14.45it/s] 16%|â–ˆâ–Œ        | 217/1380 [00:16<01:20, 14.43it/s] 16%|â–ˆâ–Œ        | 219/1380 [00:16<01:20, 14.42it/s] 16%|â–ˆâ–Œ        | 221/1380 [00:16<01:20, 14.41it/s] 16%|â–ˆâ–Œ        | 223/1380 [00:16<01:20, 14.40it/s] 16%|â–ˆâ–‹        | 225/1380 [00:16<01:20, 14.40it/s] 16%|â–ˆâ–‹        | 227/1380 [00:16<01:20, 14.41it/s] 17%|â–ˆâ–‹        | 229/1380 [00:16<01:19, 14.42it/s] 17%|â–ˆâ–‹        | 231/1380 [00:16<01:19, 14.42it/s] 17%|â–ˆâ–‹        | 233/1380 [00:17<01:19, 14.41it/s] 17%|â–ˆâ–‹        | 235/1380 [00:17<01:19, 14.40it/s] 17%|â–ˆâ–‹        | 237/1380 [00:17<01:19, 14.39it/s] 17%|â–ˆâ–‹        | 239/1380 [00:17<01:19, 14.39it/s] 17%|â–ˆâ–‹        | 241/1380 [00:17<01:19, 14.40it/s] 18%|â–ˆâ–Š        | 243/1380 [00:17<01:18, 14.41it/s] 18%|â–ˆâ–Š        | 245/1380 [00:17<01:18, 14.41it/s] 18%|â–ˆâ–Š        | 247/1380 [00:18<01:18, 14.42it/s] 18%|â–ˆâ–Š        | 249/1380 [00:18<01:18, 14.44it/s] 18%|â–ˆâ–Š        | 251/1380 [00:18<01:18, 14.42it/s] 18%|â–ˆâ–Š        | 253/1380 [00:18<01:18, 14.40it/s] 18%|â–ˆâ–Š        | 255/1380 [00:18<01:18, 14.40it/s] 19%|â–ˆâ–Š        | 257/1380 [00:18<01:18, 14.40it/s] 19%|â–ˆâ–‰        | 259/1380 [00:18<01:17, 14.39it/s] 19%|â–ˆâ–‰        | 261/1380 [00:19<01:17, 14.39it/s] 19%|â–ˆâ–‰        | 263/1380 [00:19<01:17, 14.41it/s] 19%|â–ˆâ–‰        | 265/1380 [00:19<01:17, 14.41it/s] 19%|â–ˆâ–‰        | 267/1380 [00:19<01:17, 14.41it/s] 19%|â–ˆâ–‰        | 269/1380 [00:19<01:17, 14.40it/s] 20%|â–ˆâ–‰        | 271/1380 [00:19<01:17, 14.39it/s] 20%|â–ˆâ–‰        | 273/1380 [00:19<01:17, 14.37it/s] 20%|â–ˆâ–‰        | 275/1380 [00:20<01:16, 14.40it/s]                                                   20%|â–ˆâ–ˆ        | 276/1380 [00:20<01:16, 14.40it/s][INFO|trainer.py:755] 2023-11-15 22:10:46,110 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModelForTokenClassification.forward` and have been ignored: text. If text are not expected by `PeftModelForTokenClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 22:10:46,112 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 22:10:46,112 >>   Num examples = 2204
[INFO|trainer.py:3134] 2023-11-15 22:10:46,112 >>   Batch size = 8
{'loss': 0.494, 'learning_rate': 0.0004, 'epoch': 1.0}

  0%|          | 0/276 [00:00<?, ?it/s][A
  4%|â–         | 12/276 [00:00<00:02, 119.09it/s][A
  9%|â–Š         | 24/276 [00:00<00:02, 112.28it/s][A
 13%|â–ˆâ–Ž        | 36/276 [00:00<00:02, 110.35it/s][A
 17%|â–ˆâ–‹        | 48/276 [00:00<00:02, 109.31it/s][A
 21%|â–ˆâ–ˆâ–       | 59/276 [00:00<00:01, 108.69it/s][A
 25%|â–ˆâ–ˆâ–Œ       | 70/276 [00:00<00:01, 108.34it/s][A
 29%|â–ˆâ–ˆâ–‰       | 81/276 [00:00<00:01, 108.08it/s][A
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 92/276 [00:00<00:01, 107.88it/s][A
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 103/276 [00:00<00:01, 107.63it/s][A
 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 114/276 [00:01<00:01, 107.53it/s][A
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 125/276 [00:01<00:01, 107.52it/s][A
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 136/276 [00:01<00:01, 107.52it/s][A
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 147/276 [00:01<00:01, 107.51it/s][A
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 158/276 [00:01<00:01, 107.47it/s][A
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 169/276 [00:01<00:00, 107.41it/s][A
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 180/276 [00:01<00:00, 107.44it/s][A
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 191/276 [00:01<00:00, 107.47it/s][A
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 202/276 [00:01<00:00, 107.50it/s][A
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 213/276 [00:01<00:00, 107.56it/s][A
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 224/276 [00:02<00:00, 107.51it/s][A
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 235/276 [00:02<00:00, 107.49it/s][A
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 246/276 [00:02<00:00, 107.50it/s][A
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 257/276 [00:02<00:00, 107.43it/s][A
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 268/276 [00:02<00:00, 107.44it/s][A                                                  
                                                  [A 20%|â–ˆâ–ˆ        | 276/1380 [00:22<01:16, 14.40it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 276/276 [00:02<00:00, 107.44it/s][A
                                                  [A 20%|â–ˆâ–ˆ        | 277/1380 [00:22<08:25,  2.18it/s] 20%|â–ˆâ–ˆ        | 279/1380 [00:22<06:16,  2.93it/s] 20%|â–ˆâ–ˆ        | 281/1380 [00:23<04:45,  3.85it/s] 21%|â–ˆâ–ˆ        | 283/1380 [00:23<03:42,  4.93it/s] 21%|â–ˆâ–ˆ        | 285/1380 [00:23<02:58,  6.14it/s] 21%|â–ˆâ–ˆ        | 287/1380 [00:23<02:27,  7.41it/s] 21%|â–ˆâ–ˆ        | 289/1380 [00:23<02:05,  8.68it/s] 21%|â–ˆâ–ˆ        | 291/1380 [00:23<01:50,  9.85it/s] 21%|â–ˆâ–ˆ        | 293/1380 [00:23<01:39, 10.88it/s] 21%|â–ˆâ–ˆâ–       | 295/1380 [00:24<01:32, 11.73it/s] 22%|â–ˆâ–ˆâ–       | 297/1380 [00:24<01:27, 12.41it/s] 22%|â–ˆâ–ˆâ–       | 299/1380 [00:24<01:23, 12.93it/s] 22%|â–ˆâ–ˆâ–       | 301/1380 [00:24<01:21, 13.30it/s] 22%|â–ˆâ–ˆâ–       | 303/1380 [00:24<01:19, 13.58it/s] 22%|â–ˆâ–ˆâ–       | 305/1380 [00:24<01:17, 13.79it/s] 22%|â–ˆâ–ˆâ–       | 307/1380 [00:24<01:16, 13.94it/s] 22%|â–ˆâ–ˆâ–       | 309/1380 [00:25<01:16, 14.04it/s] 23%|â–ˆâ–ˆâ–Ž       | 311/1380 [00:25<01:15, 14.12it/s] 23%|â–ˆâ–ˆâ–Ž       | 313/1380 [00:25<01:15, 14.18it/s] 23%|â–ˆâ–ˆâ–Ž       | 315/1380 [00:25<01:14, 14.22it/s] 23%|â–ˆâ–ˆâ–Ž       | 317/1380 [00:25<01:14, 14.25it/s] 23%|â–ˆâ–ˆâ–Ž       | 319/1380 [00:25<01:14, 14.27it/s] 23%|â–ˆâ–ˆâ–Ž       | 321/1380 [00:25<01:14, 14.29it/s] 23%|â–ˆâ–ˆâ–Ž       | 323/1380 [00:25<01:13, 14.29it/s] 24%|â–ˆâ–ˆâ–Ž       | 325/1380 [00:26<01:13, 14.30it/s] 24%|â–ˆâ–ˆâ–Ž       | 327/1380 [00:26<01:13, 14.30it/s] 24%|â–ˆâ–ˆâ–       | 329/1380 [00:26<01:13, 14.31it/s] 24%|â–ˆâ–ˆâ–       | 331/1380 [00:26<01:13, 14.32it/s] 24%|â–ˆâ–ˆâ–       | 333/1380 [00:26<01:13, 14.33it/s] 24%|â–ˆâ–ˆâ–       | 335/1380 [00:26<01:12, 14.34it/s] 24%|â–ˆâ–ˆâ–       | 337/1380 [00:26<01:12, 14.34it/s] 25%|â–ˆâ–ˆâ–       | 339/1380 [00:27<01:12, 14.34it/s] 25%|â–ˆâ–ˆâ–       | 341/1380 [00:27<01:12, 14.34it/s] 25%|â–ˆâ–ˆâ–       | 343/1380 [00:27<01:12, 14.36it/s] 25%|â–ˆâ–ˆâ–Œ       | 345/1380 [00:27<01:12, 14.36it/s] 25%|â–ˆâ–ˆâ–Œ       | 347/1380 [00:27<01:11, 14.37it/s] 25%|â–ˆâ–ˆâ–Œ       | 349/1380 [00:27<01:11, 14.38it/s] 25%|â–ˆâ–ˆâ–Œ       | 351/1380 [00:27<01:11, 14.37it/s] 26%|â–ˆâ–ˆâ–Œ       | 353/1380 [00:28<01:11, 14.34it/s] 26%|â–ˆâ–ˆâ–Œ       | 355/1380 [00:28<01:11, 14.33it/s] 26%|â–ˆâ–ˆâ–Œ       | 357/1380 [00:28<01:11, 14.31it/s] 26%|â–ˆâ–ˆâ–Œ       | 359/1380 [00:28<01:11, 14.31it/s] 26%|â–ˆâ–ˆâ–Œ       | 361/1380 [00:28<01:11, 14.30it/s] 26%|â–ˆâ–ˆâ–‹       | 363/1380 [00:28<01:11, 14.31it/s] 26%|â–ˆâ–ˆâ–‹       | 365/1380 [00:28<01:10, 14.30it/s] 27%|â–ˆâ–ˆâ–‹       | 367/1380 [00:29<01:10, 14.30it/s] 27%|â–ˆâ–ˆâ–‹       | 369/1380 [00:29<01:10, 14.29it/s] 27%|â–ˆâ–ˆâ–‹       | 371/1380 [00:29<01:10, 14.29it/s] 27%|â–ˆâ–ˆâ–‹       | 373/1380 [00:29<01:10, 14.29it/s] 27%|â–ˆâ–ˆâ–‹       | 375/1380 [00:29<01:10, 14.25it/s] 27%|â–ˆâ–ˆâ–‹       | 377/1380 [00:29<01:10, 14.27it/s] 27%|â–ˆâ–ˆâ–‹       | 379/1380 [00:29<01:09, 14.30it/s] 28%|â–ˆâ–ˆâ–Š       | 381/1380 [00:30<01:09, 14.31it/s] 28%|â–ˆâ–ˆâ–Š       | 383/1380 [00:30<01:09, 14.30it/s] 28%|â–ˆâ–ˆâ–Š       | 385/1380 [00:30<01:09, 14.30it/s] 28%|â–ˆâ–ˆâ–Š       | 387/1380 [00:30<01:09, 14.30it/s] 28%|â–ˆâ–ˆâ–Š       | 389/1380 [00:30<01:09, 14.29it/s] 28%|â–ˆâ–ˆâ–Š       | 391/1380 [00:30<01:09, 14.30it/s] 28%|â–ˆâ–ˆâ–Š       | 393/1380 [00:30<01:09, 14.29it/s] 29%|â–ˆâ–ˆâ–Š       | 395/1380 [00:31<01:08, 14.29it/s] 29%|â–ˆâ–ˆâ–‰       | 397/1380 [00:31<01:08, 14.29it/s] 29%|â–ˆâ–ˆâ–‰       | 399/1380 [00:31<01:08, 14.29it/s] 29%|â–ˆâ–ˆâ–‰       | 401/1380 [00:31<01:08, 14.29it/s] 29%|â–ˆâ–ˆâ–‰       | 403/1380 [00:31<01:08, 14.28it/s] 29%|â–ˆâ–ˆâ–‰       | 405/1380 [00:31<01:08, 14.29it/s] 29%|â–ˆâ–ˆâ–‰       | 407/1380 [00:31<01:08, 14.29it/s] 30%|â–ˆâ–ˆâ–‰       | 409/1380 [00:31<01:07, 14.29it/s] 30%|â–ˆâ–ˆâ–‰       | 411/1380 [00:32<01:07, 14.29it/s] 30%|â–ˆâ–ˆâ–‰       | 413/1380 [00:32<01:07, 14.29it/s] 30%|â–ˆâ–ˆâ–ˆ       | 415/1380 [00:32<01:07, 14.30it/s] 30%|â–ˆâ–ˆâ–ˆ       | 417/1380 [00:32<01:07, 14.29it/s] 30%|â–ˆâ–ˆâ–ˆ       | 419/1380 [00:32<01:07, 14.29it/s] 31%|â–ˆâ–ˆâ–ˆ       | 421/1380 [00:32<01:07, 14.29it/s] 31%|â–ˆâ–ˆâ–ˆ       | 423/1380 [00:32<01:06, 14.29it/s] 31%|â–ˆâ–ˆâ–ˆ       | 425/1380 [00:33<01:06, 14.29it/s] 31%|â–ˆâ–ˆâ–ˆ       | 427/1380 [00:33<01:06, 14.29it/s] 31%|â–ˆâ–ˆâ–ˆ       | 429/1380 [00:33<01:06, 14.28it/s] 31%|â–ˆâ–ˆâ–ˆ       | 431/1380 [00:33<01:06, 14.29it/s] 31%|â–ˆâ–ˆâ–ˆâ–      | 433/1380 [00:33<01:06, 14.29it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 435/1380 [00:33<01:06, 14.29it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 437/1380 [00:33<01:05, 14.29it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 439/1380 [00:34<01:05, 14.29it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 441/1380 [00:34<01:05, 14.29it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 443/1380 [00:34<01:05, 14.29it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 445/1380 [00:34<01:05, 14.29it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 447/1380 [00:34<01:05, 14.28it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 449/1380 [00:34<01:05, 14.30it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 451/1380 [00:34<01:05, 14.24it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 453/1380 [00:35<01:05, 14.26it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 455/1380 [00:35<01:04, 14.27it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 457/1380 [00:35<01:04, 14.28it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 459/1380 [00:35<01:04, 14.30it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 461/1380 [00:35<01:04, 14.30it/s] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 463/1380 [00:35<01:04, 14.30it/s] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 465/1380 [00:35<01:04, 14.29it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 467/1380 [00:36<01:04, 14.26it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 469/1380 [00:36<01:03, 14.28it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 471/1380 [00:36<01:03, 14.29it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 473/1380 [00:36<01:03, 14.29it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 475/1380 [00:36<01:03, 14.30it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 477/1380 [00:36<01:03, 14.29it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 479/1380 [00:36<01:03, 14.30it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 481/1380 [00:37<01:02, 14.27it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 483/1380 [00:37<01:02, 14.28it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 485/1380 [00:37<01:02, 14.29it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 487/1380 [00:37<01:02, 14.29it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 489/1380 [00:37<01:02, 14.30it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 491/1380 [00:37<01:02, 14.28it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 493/1380 [00:37<01:02, 14.26it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 495/1380 [00:38<01:02, 14.26it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 497/1380 [00:38<01:01, 14.28it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 499/1380 [00:38<01:01, 14.30it/s] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 501/1380 [00:38<01:01, 14.28it/s] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 503/1380 [00:38<01:02, 14.11it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 505/1380 [00:38<01:02, 14.11it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 507/1380 [00:38<01:01, 14.13it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 509/1380 [00:39<01:01, 14.14it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 511/1380 [00:39<01:01, 14.18it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 513/1380 [00:39<01:01, 14.21it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 515/1380 [00:39<01:00, 14.21it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 517/1380 [00:39<01:00, 14.22it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 519/1380 [00:39<01:00, 14.24it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 521/1380 [00:39<01:00, 14.26it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 523/1380 [00:39<01:00, 14.27it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 525/1380 [00:40<00:59, 14.28it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 527/1380 [00:40<00:59, 14.27it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 529/1380 [00:40<00:59, 14.27it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 531/1380 [00:40<01:00, 14.12it/s] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 533/1380 [00:40<00:59, 14.14it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 535/1380 [00:40<01:00, 14.05it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 537/1380 [00:40<00:59, 14.10it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 539/1380 [00:41<00:59, 14.14it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 541/1380 [00:41<00:59, 14.01it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 543/1380 [00:41<00:59, 14.02it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 545/1380 [00:41<00:59, 14.08it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 547/1380 [00:41<00:58, 14.13it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 549/1380 [00:41<00:59, 14.04it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 551/1380 [00:41<00:58, 14.13it/s]                                                   40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 552/1380 [00:42<00:58, 14.13it/s][INFO|trainer.py:755] 2023-11-15 22:11:08,033 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModelForTokenClassification.forward` and have been ignored: text. If text are not expected by `PeftModelForTokenClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 22:11:08,034 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 22:11:08,035 >>   Num examples = 2204
[INFO|trainer.py:3134] 2023-11-15 22:11:08,035 >>   Batch size = 8
{'eval_loss': 0.4047123193740845, 'eval_accuracy': 0.8402903811252269, 'eval_micro_f1': 0.8402903811252269, 'eval_macro_f1': 0.8094134227428839, 'eval_runtime': 2.6098, 'eval_samples_per_second': 844.505, 'eval_steps_per_second': 105.755, 'epoch': 1.0}
{'loss': 0.3748, 'learning_rate': 0.0003, 'epoch': 2.0}

  0%|          | 0/276 [00:00<?, ?it/s][A
  4%|â–         | 12/276 [00:00<00:02, 117.56it/s][A
  9%|â–Š         | 24/276 [00:00<00:02, 108.34it/s][A
 13%|â–ˆâ–Ž        | 35/276 [00:00<00:02, 107.46it/s][A
 17%|â–ˆâ–‹        | 46/276 [00:00<00:02, 106.87it/s][A
 21%|â–ˆâ–ˆ        | 57/276 [00:00<00:02, 106.90it/s][A
 25%|â–ˆâ–ˆâ–       | 68/276 [00:00<00:01, 106.88it/s][A
 29%|â–ˆâ–ˆâ–Š       | 79/276 [00:00<00:01, 106.63it/s][A
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 90/276 [00:00<00:01, 106.33it/s][A
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 101/276 [00:00<00:01, 106.01it/s][A
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 112/276 [00:01<00:01, 106.30it/s][A
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 123/276 [00:01<00:01, 106.57it/s][A
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 134/276 [00:01<00:01, 106.63it/s][A
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 145/276 [00:01<00:01, 106.42it/s][A
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 156/276 [00:01<00:01, 106.25it/s][A
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 167/276 [00:01<00:01, 106.41it/s][A
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 178/276 [00:01<00:00, 106.45it/s][A
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 189/276 [00:01<00:00, 106.18it/s][A
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 200/276 [00:01<00:00, 106.38it/s][A
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 211/276 [00:01<00:00, 106.39it/s][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 222/276 [00:02<00:00, 106.22it/s][A
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 233/276 [00:02<00:00, 106.32it/s][A
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 244/276 [00:02<00:00, 105.75it/s][A
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 255/276 [00:02<00:00, 105.68it/s][A
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 266/276 [00:02<00:00, 106.00it/s][A                                                  
                                                  [A 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 552/1380 [00:44<00:58, 14.13it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 276/276 [00:02<00:00, 106.00it/s][A
                                                  [A 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 553/1380 [00:44<06:23,  2.16it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 555/1380 [00:44<04:45,  2.89it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 557/1380 [00:45<03:36,  3.80it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 559/1380 [00:45<02:48,  4.88it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 561/1380 [00:45<02:14,  6.08it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 563/1380 [00:45<01:51,  7.35it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 565/1380 [00:45<01:34,  8.60it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 567/1380 [00:45<01:23,  9.77it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 569/1380 [00:45<01:15, 10.79it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 571/1380 [00:45<01:09, 11.65it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 573/1380 [00:46<01:05, 12.33it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 575/1380 [00:46<01:02, 12.85it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 577/1380 [00:46<01:00, 13.26it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 579/1380 [00:46<00:59, 13.56it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 581/1380 [00:46<00:58, 13.77it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 583/1380 [00:46<00:57, 13.93it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 585/1380 [00:46<00:56, 14.01it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 587/1380 [00:47<00:56, 14.10it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 589/1380 [00:47<00:55, 14.17it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 591/1380 [00:47<00:55, 14.21it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 593/1380 [00:47<00:55, 14.23it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 595/1380 [00:47<00:55, 14.24it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 597/1380 [00:47<00:54, 14.26it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 599/1380 [00:47<00:54, 14.27it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 601/1380 [00:48<00:54, 14.28it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 603/1380 [00:48<00:54, 14.27it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 605/1380 [00:48<00:54, 14.29it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 607/1380 [00:48<00:54, 14.27it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 609/1380 [00:48<00:53, 14.29it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 611/1380 [00:48<00:53, 14.30it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 613/1380 [00:48<00:53, 14.30it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 615/1380 [00:49<00:53, 14.29it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 617/1380 [00:49<00:53, 14.30it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 619/1380 [00:49<00:53, 14.29it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 621/1380 [00:49<00:53, 14.28it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 623/1380 [00:49<00:52, 14.29it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 625/1380 [00:49<00:52, 14.29it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 627/1380 [00:49<00:52, 14.27it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 629/1380 [00:50<00:52, 14.28it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 631/1380 [00:50<00:52, 14.29it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 633/1380 [00:50<00:52, 14.29it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 635/1380 [00:50<00:52, 14.30it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 637/1380 [00:50<00:51, 14.30it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 639/1380 [00:50<00:51, 14.29it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 641/1380 [00:50<00:51, 14.29it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 643/1380 [00:51<00:51, 14.29it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 645/1380 [00:51<00:51, 14.29it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 647/1380 [00:51<00:51, 14.27it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 649/1380 [00:51<00:51, 14.28it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 651/1380 [00:51<00:51, 14.29it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 653/1380 [00:51<00:50, 14.29it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 655/1380 [00:51<00:50, 14.29it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 657/1380 [00:52<00:50, 14.28it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 659/1380 [00:52<00:50, 14.27it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 661/1380 [00:52<00:50, 14.24it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 663/1380 [00:52<00:50, 14.25it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 665/1380 [00:52<00:50, 14.26it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 667/1380 [00:52<00:49, 14.26it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 669/1380 [00:52<00:49, 14.24it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 671/1380 [00:52<00:49, 14.20it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 673/1380 [00:53<00:49, 14.20it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 675/1380 [00:53<00:49, 14.21it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 677/1380 [00:53<00:49, 14.21it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 679/1380 [00:53<00:49, 14.20it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 681/1380 [00:53<00:49, 14.21it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 683/1380 [00:53<00:49, 14.21it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 685/1380 [00:53<00:48, 14.23it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 687/1380 [00:54<00:48, 14.24it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 689/1380 [00:54<00:48, 14.24it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 691/1380 [00:54<00:48, 14.24it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 693/1380 [00:54<00:48, 14.25it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 695/1380 [00:54<00:48, 14.26it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 697/1380 [00:54<00:47, 14.26it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 699/1380 [00:54<00:47, 14.26it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 701/1380 [00:55<00:47, 14.25it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 703/1380 [00:55<00:47, 14.24it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 705/1380 [00:55<00:47, 14.25it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 707/1380 [00:55<00:47, 14.25it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 709/1380 [00:55<00:47, 14.25it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 711/1380 [00:55<00:47, 14.20it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 713/1380 [00:55<00:46, 14.20it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 715/1380 [00:56<00:46, 14.19it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 717/1380 [00:56<00:46, 14.20it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 719/1380 [00:56<00:46, 14.22it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 721/1380 [00:56<00:46, 14.21it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 723/1380 [00:56<00:46, 14.23it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 725/1380 [00:56<00:46, 14.23it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 727/1380 [00:56<00:45, 14.24it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 729/1380 [00:57<00:45, 14.24it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 731/1380 [00:57<00:45, 14.25it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 733/1380 [00:57<00:45, 14.25it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 735/1380 [00:57<00:45, 14.25it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 737/1380 [00:57<00:45, 14.24it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 739/1380 [00:57<00:45, 14.24it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 741/1380 [00:57<00:44, 14.22it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 743/1380 [00:58<00:44, 14.21it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 745/1380 [00:58<00:44, 14.21it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 747/1380 [00:58<00:44, 14.20it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 749/1380 [00:58<00:44, 14.21it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 751/1380 [00:58<00:44, 14.22it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 753/1380 [00:58<00:44, 14.24it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 755/1380 [00:58<00:43, 14.25it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 757/1380 [00:59<00:43, 14.24it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 759/1380 [00:59<00:43, 14.24it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 761/1380 [00:59<00:43, 14.24it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 763/1380 [00:59<00:43, 14.24it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 765/1380 [00:59<00:43, 14.24it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 767/1380 [00:59<00:43, 14.25it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 769/1380 [00:59<00:42, 14.22it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 771/1380 [01:00<00:42, 14.19it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 773/1380 [01:00<00:42, 14.19it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 775/1380 [01:00<00:42, 14.20it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 777/1380 [01:00<00:42, 14.21it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 779/1380 [01:00<00:42, 14.21it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 781/1380 [01:00<00:42, 14.23it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 783/1380 [01:00<00:41, 14.24it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 785/1380 [01:01<00:41, 14.25it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 787/1380 [01:01<00:41, 14.27it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 789/1380 [01:01<00:41, 14.26it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 791/1380 [01:01<00:41, 14.27it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 793/1380 [01:01<00:41, 14.28it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 795/1380 [01:01<00:40, 14.29it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 797/1380 [01:01<00:40, 14.28it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 799/1380 [01:01<00:40, 14.27it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 801/1380 [01:02<00:40, 14.28it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 803/1380 [01:02<00:40, 14.29it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 805/1380 [01:02<00:40, 14.29it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 807/1380 [01:02<00:40, 14.25it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 809/1380 [01:02<00:40, 14.24it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 811/1380 [01:02<00:39, 14.24it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 813/1380 [01:02<00:39, 14.25it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 815/1380 [01:03<00:39, 14.24it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 817/1380 [01:03<00:40, 14.04it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 819/1380 [01:03<00:39, 14.11it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 821/1380 [01:03<00:39, 14.11it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 823/1380 [01:03<00:39, 14.13it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 825/1380 [01:03<00:39, 14.16it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 827/1380 [01:03<00:38, 14.21it/s]                                                   60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 828/1380 [01:03<00:38, 14.21it/s][INFO|trainer.py:755] 2023-11-15 22:11:30,021 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModelForTokenClassification.forward` and have been ignored: text. If text are not expected by `PeftModelForTokenClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 22:11:30,022 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 22:11:30,023 >>   Num examples = 2204
[INFO|trainer.py:3134] 2023-11-15 22:11:30,023 >>   Batch size = 8
{'eval_loss': 0.3946365714073181, 'eval_accuracy': 0.8502722323049002, 'eval_micro_f1': 0.8502722323049003, 'eval_macro_f1': 0.8311814459377196, 'eval_runtime': 2.643, 'eval_samples_per_second': 833.887, 'eval_steps_per_second': 104.425, 'epoch': 2.0}
{'loss': 0.3415, 'learning_rate': 0.0002, 'epoch': 3.0}

  0%|          | 0/276 [00:00<?, ?it/s][A
  4%|â–         | 12/276 [00:00<00:02, 117.46it/s][A
  9%|â–Š         | 24/276 [00:00<00:02, 110.65it/s][A
 13%|â–ˆâ–Ž        | 36/276 [00:00<00:02, 108.89it/s][A
 17%|â–ˆâ–‹        | 47/276 [00:00<00:02, 107.71it/s][A
 21%|â–ˆâ–ˆ        | 58/276 [00:00<00:02, 107.27it/s][A
 25%|â–ˆâ–ˆâ–Œ       | 69/276 [00:00<00:01, 107.06it/s][A
 29%|â–ˆâ–ˆâ–‰       | 80/276 [00:00<00:01, 106.96it/s][A
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 91/276 [00:00<00:01, 106.84it/s][A
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 102/276 [00:00<00:01, 106.81it/s][A
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 113/276 [00:01<00:01, 106.61it/s][A
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 124/276 [00:01<00:01, 105.65it/s][A
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 135/276 [00:01<00:01, 105.88it/s][A
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 146/276 [00:01<00:01, 104.09it/s][A
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 157/276 [00:01<00:01, 104.62it/s][A
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 168/276 [00:01<00:01, 105.25it/s][A
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 179/276 [00:01<00:00, 105.58it/s][A
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 190/276 [00:01<00:00, 100.72it/s][A
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 201/276 [00:01<00:00, 102.10it/s][A
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 212/276 [00:02<00:00, 103.39it/s][A
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 223/276 [00:02<00:00, 104.28it/s][A
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 234/276 [00:02<00:00, 104.92it/s][A
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 245/276 [00:02<00:00, 105.40it/s][A
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 256/276 [00:02<00:00, 105.61it/s][A
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 267/276 [00:02<00:00, 105.87it/s][A                                                  
                                                  [A 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 828/1380 [01:06<00:38, 14.21it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 276/276 [00:02<00:00, 105.87it/s][A
                                                  [A 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 829/1380 [01:06<04:17,  2.14it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 831/1380 [01:06<03:11,  2.87it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 833/1380 [01:07<02:24,  3.78it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 835/1380 [01:07<01:52,  4.85it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 837/1380 [01:07<01:29,  6.05it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 839/1380 [01:07<01:13,  7.31it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 841/1380 [01:07<01:02,  8.56it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 843/1380 [01:07<00:55,  9.73it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 845/1380 [01:07<00:49, 10.77it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 847/1380 [01:08<00:45, 11.63it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 849/1380 [01:08<00:43, 12.31it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 851/1380 [01:08<00:41, 12.85it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 853/1380 [01:08<00:39, 13.25it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 855/1380 [01:08<00:38, 13.55it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 857/1380 [01:08<00:38, 13.75it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 859/1380 [01:08<00:37, 13.92it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 861/1380 [01:08<00:37, 14.02it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 863/1380 [01:09<00:36, 14.10it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 865/1380 [01:09<00:36, 14.16it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 867/1380 [01:09<00:36, 14.20it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 869/1380 [01:09<00:35, 14.23it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 871/1380 [01:09<00:35, 14.25it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 873/1380 [01:09<00:35, 14.26it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 875/1380 [01:09<00:35, 14.27it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 877/1380 [01:10<00:35, 14.28it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 879/1380 [01:10<00:35, 14.28it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 881/1380 [01:10<00:34, 14.28it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 883/1380 [01:10<00:34, 14.29it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 885/1380 [01:10<00:34, 14.26it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 887/1380 [01:10<00:34, 14.27it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 889/1380 [01:10<00:34, 14.28it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 891/1380 [01:11<00:34, 14.26it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 893/1380 [01:11<00:34, 14.24it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 895/1380 [01:11<00:34, 14.25it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 897/1380 [01:11<00:33, 14.25it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 899/1380 [01:11<00:33, 14.25it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 901/1380 [01:11<00:33, 14.25it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 903/1380 [01:11<00:33, 14.24it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 905/1380 [01:12<00:33, 14.24it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 907/1380 [01:12<00:33, 14.24it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 909/1380 [01:12<00:33, 14.24it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 911/1380 [01:12<00:32, 14.22it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 913/1380 [01:12<00:32, 14.23it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 915/1380 [01:12<00:32, 14.23it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 917/1380 [01:12<00:32, 14.25it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 919/1380 [01:13<00:32, 14.24it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 921/1380 [01:13<00:32, 14.26it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 923/1380 [01:13<00:32, 14.27it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 925/1380 [01:13<00:31, 14.26it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 927/1380 [01:13<00:31, 14.27it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 929/1380 [01:13<00:31, 14.27it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 931/1380 [01:13<00:31, 14.27it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 933/1380 [01:14<00:31, 14.27it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 935/1380 [01:14<00:31, 14.27it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 937/1380 [01:14<00:31, 14.27it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 939/1380 [01:14<00:30, 14.27it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 941/1380 [01:14<00:30, 14.28it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 943/1380 [01:14<00:30, 14.28it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 945/1380 [01:14<00:30, 14.28it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 947/1380 [01:15<00:30, 14.27it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 949/1380 [01:15<00:30, 14.27it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 951/1380 [01:15<00:30, 14.28it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 953/1380 [01:15<00:29, 14.28it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 955/1380 [01:15<00:29, 14.27it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 957/1380 [01:15<00:29, 14.26it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 959/1380 [01:15<00:29, 14.25it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 961/1380 [01:15<00:29, 14.26it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 963/1380 [01:16<00:29, 14.25it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 965/1380 [01:16<00:29, 14.25it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 967/1380 [01:16<00:29, 14.24it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 969/1380 [01:16<00:28, 14.23it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 971/1380 [01:16<00:28, 14.23it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 973/1380 [01:16<00:28, 14.23it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 975/1380 [01:16<00:28, 14.21it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 977/1380 [01:17<00:28, 14.23it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 979/1380 [01:17<00:28, 14.24it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 981/1380 [01:17<00:27, 14.25it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 983/1380 [01:17<00:27, 14.26it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 985/1380 [01:17<00:27, 14.27it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 987/1380 [01:17<00:27, 14.27it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 989/1380 [01:17<00:27, 14.28it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 991/1380 [01:18<00:27, 14.27it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 993/1380 [01:18<00:27, 14.28it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 995/1380 [01:18<00:26, 14.28it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 997/1380 [01:18<00:26, 14.29it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 999/1380 [01:18<00:26, 14.28it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1001/1380 [01:18<00:26, 14.29it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1003/1380 [01:18<00:26, 14.28it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1005/1380 [01:19<00:26, 14.29it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1007/1380 [01:19<00:26, 14.28it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1009/1380 [01:19<00:25, 14.29it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1011/1380 [01:19<00:25, 14.29it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1013/1380 [01:19<00:25, 14.28it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1015/1380 [01:19<00:25, 14.29it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1017/1380 [01:19<00:25, 14.29it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1019/1380 [01:20<00:25, 14.28it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1021/1380 [01:20<00:25, 14.28it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1023/1380 [01:20<00:24, 14.28it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1025/1380 [01:20<00:24, 14.28it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1027/1380 [01:20<00:24, 14.28it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1029/1380 [01:20<00:24, 14.28it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1031/1380 [01:20<00:24, 14.28it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1033/1380 [01:21<00:24, 14.27it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1035/1380 [01:21<00:24, 14.27it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1037/1380 [01:21<00:24, 14.28it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1039/1380 [01:21<00:23, 14.25it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1041/1380 [01:21<00:23, 14.25it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1043/1380 [01:21<00:23, 14.24it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1045/1380 [01:21<00:23, 14.24it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1047/1380 [01:22<00:23, 14.22it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1049/1380 [01:22<00:23, 14.22it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1051/1380 [01:22<00:23, 14.22it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1053/1380 [01:22<00:22, 14.22it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1055/1380 [01:22<00:22, 14.21it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1057/1380 [01:22<00:22, 14.22it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1059/1380 [01:22<00:22, 14.22it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1061/1380 [01:23<00:22, 14.21it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1063/1380 [01:23<00:22, 14.01it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1065/1380 [01:23<00:22, 14.08it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1067/1380 [01:23<00:22, 14.13it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1069/1380 [01:23<00:21, 14.17it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1071/1380 [01:23<00:21, 14.20it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1073/1380 [01:23<00:21, 14.23it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1075/1380 [01:23<00:21, 14.24it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1077/1380 [01:24<00:21, 14.25it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1079/1380 [01:24<00:21, 14.26it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1081/1380 [01:24<00:20, 14.27it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1083/1380 [01:24<00:20, 14.27it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1085/1380 [01:24<00:20, 14.29it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1087/1380 [01:24<00:20, 14.28it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1089/1380 [01:24<00:20, 14.28it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1091/1380 [01:25<00:20, 14.29it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1093/1380 [01:25<00:20, 14.29it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1095/1380 [01:25<00:19, 14.29it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1097/1380 [01:25<00:19, 14.29it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1099/1380 [01:25<00:19, 14.29it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1101/1380 [01:25<00:19, 14.29it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1103/1380 [01:25<00:19, 14.32it/s]                                                    80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1104/1380 [01:25<00:19, 14.32it/s][INFO|trainer.py:755] 2023-11-15 22:11:52,019 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModelForTokenClassification.forward` and have been ignored: text. If text are not expected by `PeftModelForTokenClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 22:11:52,021 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 22:11:52,021 >>   Num examples = 2204
[INFO|trainer.py:3134] 2023-11-15 22:11:52,021 >>   Batch size = 8
{'eval_loss': 0.37836259603500366, 'eval_accuracy': 0.8634301270417423, 'eval_micro_f1': 0.8634301270417423, 'eval_macro_f1': 0.847966547334495, 'eval_runtime': 2.6649, 'eval_samples_per_second': 827.059, 'eval_steps_per_second': 103.57, 'epoch': 3.0}
{'loss': 0.3063, 'learning_rate': 0.0001, 'epoch': 4.0}

  0%|          | 0/276 [00:00<?, ?it/s][A
  4%|â–         | 12/276 [00:00<00:02, 117.01it/s][A
  9%|â–Š         | 24/276 [00:00<00:02, 110.51it/s][A
 13%|â–ˆâ–Ž        | 36/276 [00:00<00:02, 108.62it/s][A
 17%|â–ˆâ–‹        | 47/276 [00:00<00:02, 107.81it/s][A
 21%|â–ˆâ–ˆ        | 58/276 [00:00<00:02, 107.25it/s][A
 25%|â–ˆâ–ˆâ–Œ       | 69/276 [00:00<00:01, 106.98it/s][A
 29%|â–ˆâ–ˆâ–‰       | 80/276 [00:00<00:01, 106.78it/s][A
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 91/276 [00:00<00:01, 106.62it/s][A
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 102/276 [00:00<00:01, 106.57it/s][A
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 113/276 [00:01<00:01, 106.51it/s][A
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 124/276 [00:01<00:01, 106.41it/s][A
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 135/276 [00:01<00:01, 106.41it/s][A
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 146/276 [00:01<00:01, 106.42it/s][A
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 157/276 [00:01<00:01, 106.35it/s][A
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 168/276 [00:01<00:01, 106.36it/s][A
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 179/276 [00:01<00:00, 106.37it/s][A
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 190/276 [00:01<00:00, 106.32it/s][A
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 201/276 [00:01<00:00, 106.30it/s][A
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 212/276 [00:01<00:00, 106.32it/s][A
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 223/276 [00:02<00:00, 106.35it/s][A
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 234/276 [00:02<00:00, 106.41it/s][A
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 245/276 [00:02<00:00, 106.38it/s][A
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 256/276 [00:02<00:00, 106.37it/s][A
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 267/276 [00:02<00:00, 106.43it/s][A                                                   
                                                  [A 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1104/1380 [01:28<00:19, 14.32it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 276/276 [00:02<00:00, 106.43it/s][A
                                                  [A 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1105/1380 [01:28<02:07,  2.16it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1107/1380 [01:28<01:34,  2.90it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1109/1380 [01:28<01:11,  3.81it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1111/1380 [01:29<00:55,  4.89it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1113/1380 [01:29<00:43,  6.08it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1115/1380 [01:29<00:36,  7.34it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1117/1380 [01:29<00:30,  8.59it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1119/1380 [01:29<00:26,  9.75it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1121/1380 [01:29<00:24, 10.76it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1123/1380 [01:29<00:22, 11.62it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1125/1380 [01:30<00:20, 12.31it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1127/1380 [01:30<00:19, 12.84it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1129/1380 [01:30<00:18, 13.24it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1131/1380 [01:30<00:18, 13.52it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1133/1380 [01:30<00:17, 13.73it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1135/1380 [01:30<00:17, 13.89it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1137/1380 [01:30<00:17, 13.99it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1139/1380 [01:31<00:17, 14.07it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1141/1380 [01:31<00:16, 14.12it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1143/1380 [01:31<00:16, 14.15it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1145/1380 [01:31<00:16, 14.17it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1147/1380 [01:31<00:16, 14.19it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1149/1380 [01:31<00:16, 14.19it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1151/1380 [01:31<00:16, 14.20it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1153/1380 [01:32<00:15, 14.20it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1155/1380 [01:32<00:15, 14.20it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1157/1380 [01:32<00:15, 14.20it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1159/1380 [01:32<00:15, 14.22it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1161/1380 [01:32<00:15, 14.23it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1163/1380 [01:32<00:15, 14.25it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1165/1380 [01:32<00:15, 14.26it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1167/1380 [01:33<00:14, 14.27it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1169/1380 [01:33<00:14, 14.26it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1171/1380 [01:33<00:14, 14.27it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1173/1380 [01:33<00:14, 14.27it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1175/1380 [01:33<00:14, 14.27it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1177/1380 [01:33<00:14, 14.26it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1179/1380 [01:33<00:14, 14.26it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1181/1380 [01:34<00:13, 14.25it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1183/1380 [01:34<00:13, 14.21it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1185/1380 [01:34<00:13, 14.21it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1187/1380 [01:34<00:13, 14.21it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1189/1380 [01:34<00:13, 14.20it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1191/1380 [01:34<00:13, 14.17it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1193/1380 [01:34<00:13, 14.13it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1195/1380 [01:35<00:13, 14.15it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1197/1380 [01:35<00:12, 14.17it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1199/1380 [01:35<00:12, 14.19it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1201/1380 [01:35<00:12, 14.19it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1203/1380 [01:35<00:12, 14.19it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1205/1380 [01:35<00:12, 14.20it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1207/1380 [01:35<00:12, 14.20it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1209/1380 [01:36<00:12, 14.20it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1211/1380 [01:36<00:11, 14.19it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1213/1380 [01:36<00:11, 14.20it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1215/1380 [01:36<00:11, 14.22it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1217/1380 [01:36<00:11, 14.23it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1219/1380 [01:36<00:11, 14.24it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1221/1380 [01:36<00:11, 14.24it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1223/1380 [01:37<00:11, 14.25it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1225/1380 [01:37<00:10, 14.25it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1227/1380 [01:37<00:10, 14.25it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1229/1380 [01:37<00:10, 14.26it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1231/1380 [01:37<00:10, 14.26it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1233/1380 [01:37<00:10, 14.26it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1235/1380 [01:37<00:10, 14.25it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1237/1380 [01:37<00:10, 14.23it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1239/1380 [01:38<00:09, 14.23it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1241/1380 [01:38<00:09, 14.22it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1243/1380 [01:38<00:09, 14.22it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1245/1380 [01:38<00:09, 14.21it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1247/1380 [01:38<00:09, 14.20it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1249/1380 [01:38<00:09, 14.21it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1251/1380 [01:38<00:09, 14.21it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1253/1380 [01:39<00:08, 14.19it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1255/1380 [01:39<00:08, 14.20it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1257/1380 [01:39<00:08, 14.18it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1259/1380 [01:39<00:08, 14.19it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1261/1380 [01:39<00:08, 14.19it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1263/1380 [01:39<00:08, 14.19it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1265/1380 [01:39<00:08, 14.18it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1267/1380 [01:40<00:07, 14.18it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1269/1380 [01:40<00:07, 14.19it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1271/1380 [01:40<00:07, 14.20it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1273/1380 [01:40<00:07, 14.22it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1275/1380 [01:40<00:07, 14.22it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1277/1380 [01:40<00:07, 14.22it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1279/1380 [01:40<00:07, 14.23it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1281/1380 [01:41<00:06, 14.22it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1283/1380 [01:41<00:06, 14.22it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1285/1380 [01:41<00:06, 14.21it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1287/1380 [01:41<00:06, 14.20it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1289/1380 [01:41<00:06, 14.19it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1291/1380 [01:41<00:06, 14.19it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1293/1380 [01:41<00:06, 14.19it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1295/1380 [01:42<00:05, 14.21it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1297/1380 [01:42<00:05, 14.22it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1299/1380 [01:42<00:05, 14.23it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1301/1380 [01:42<00:05, 14.23it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1303/1380 [01:42<00:05, 14.24it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1305/1380 [01:42<00:05, 14.24it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1307/1380 [01:42<00:05, 14.26it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1309/1380 [01:43<00:04, 14.25it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1311/1380 [01:43<00:04, 14.25it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1313/1380 [01:43<00:04, 14.24it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1315/1380 [01:43<00:04, 14.23it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1317/1380 [01:43<00:04, 14.23it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1319/1380 [01:43<00:04, 14.22it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1321/1380 [01:43<00:04, 14.21it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1323/1380 [01:44<00:04, 14.20it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1325/1380 [01:44<00:03, 14.20it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1327/1380 [01:44<00:03, 14.20it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1329/1380 [01:44<00:03, 14.21it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1331/1380 [01:44<00:03, 14.22it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1333/1380 [01:44<00:03, 14.21it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1335/1380 [01:44<00:03, 14.21it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1337/1380 [01:45<00:03, 14.21it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1339/1380 [01:45<00:02, 14.21it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1341/1380 [01:45<00:02, 14.17it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1343/1380 [01:45<00:02, 14.17it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1345/1380 [01:45<00:02, 14.18it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1347/1380 [01:45<00:02, 14.18it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1349/1380 [01:45<00:02, 14.19it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1351/1380 [01:46<00:02, 14.21it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1353/1380 [01:46<00:01, 14.22it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1355/1380 [01:46<00:01, 14.23it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1357/1380 [01:46<00:01, 14.23it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1359/1380 [01:46<00:01, 14.24it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1361/1380 [01:46<00:01, 14.24it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1363/1380 [01:46<00:01, 14.25it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1365/1380 [01:46<00:01, 14.24it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1367/1380 [01:47<00:00, 14.22it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1369/1380 [01:47<00:00, 14.21it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1371/1380 [01:47<00:00, 14.21it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1373/1380 [01:47<00:00, 14.21it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1375/1380 [01:47<00:00, 14.18it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1377/1380 [01:47<00:00, 14.19it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1379/1380 [01:47<00:00, 14.22it/s]                                                   100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1380/1380 [01:48<00:00, 14.22it/s][INFO|trainer.py:755] 2023-11-15 22:12:14,043 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModelForTokenClassification.forward` and have been ignored: text. If text are not expected by `PeftModelForTokenClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 22:12:14,044 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 22:12:14,045 >>   Num examples = 2204
[INFO|trainer.py:3134] 2023-11-15 22:12:14,045 >>   Batch size = 8
{'eval_loss': 0.3668396770954132, 'eval_accuracy': 0.8616152450090744, 'eval_micro_f1': 0.8616152450090744, 'eval_macro_f1': 0.8460090111773848, 'eval_runtime': 2.6376, 'eval_samples_per_second': 835.615, 'eval_steps_per_second': 104.641, 'epoch': 4.0}
{'loss': 0.2786, 'learning_rate': 0.0, 'epoch': 5.0}

  0%|          | 0/276 [00:00<?, ?it/s][A
  4%|â–         | 12/276 [00:00<00:02, 117.31it/s][A
  9%|â–Š         | 24/276 [00:00<00:02, 110.55it/s][A
 13%|â–ˆâ–Ž        | 36/276 [00:00<00:02, 108.59it/s][A
 17%|â–ˆâ–‹        | 47/276 [00:00<00:02, 107.74it/s][A
 21%|â–ˆâ–ˆ        | 58/276 [00:00<00:02, 107.16it/s][A
 25%|â–ˆâ–ˆâ–Œ       | 69/276 [00:00<00:01, 106.82it/s][A
 29%|â–ˆâ–ˆâ–‰       | 80/276 [00:00<00:01, 106.68it/s][A
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 91/276 [00:00<00:01, 106.51it/s][A
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 102/276 [00:00<00:01, 106.35it/s][A
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 113/276 [00:01<00:01, 106.20it/s][A
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 124/276 [00:01<00:01, 106.20it/s][A
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 135/276 [00:01<00:01, 106.23it/s][A
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 146/276 [00:01<00:01, 106.20it/s][A
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 157/276 [00:01<00:01, 106.20it/s][A
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 168/276 [00:01<00:01, 106.24it/s][A
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 179/276 [00:01<00:00, 106.18it/s][A
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 190/276 [00:01<00:00, 106.23it/s][A
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 201/276 [00:01<00:00, 106.19it/s][A
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 212/276 [00:01<00:00, 106.07it/s][A
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 223/276 [00:02<00:00, 106.05it/s][A
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 234/276 [00:02<00:00, 106.04it/s][A
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 245/276 [00:02<00:00, 106.13it/s][A
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 256/276 [00:02<00:00, 106.09it/s][A
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 267/276 [00:02<00:00, 106.08it/s][A                                                   
                                                  [A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1380/1380 [01:50<00:00, 14.22it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 276/276 [00:02<00:00, 106.08it/s][A
                                                  [A[INFO|trainer.py:1963] 2023-11-15 22:12:16,690 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                   100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1380/1380 [01:50<00:00, 14.22it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1380/1380 [01:50<00:00, 12.47it/s]
[INFO|trainer.py:2855] 2023-11-15 22:12:16,693 >> Saving model checkpoint to ./result/acl_roberta-base_seed2_lora
[INFO|tokenization_utils_base.py:2235] 2023-11-15 22:12:16,802 >> tokenizer config file saved in ./result/acl_roberta-base_seed2_lora/tokenizer_config.json
[INFO|tokenization_utils_base.py:2242] 2023-11-15 22:12:16,804 >> Special tokens file saved in ./result/acl_roberta-base_seed2_lora/special_tokens_map.json
{'eval_loss': 0.3885091543197632, 'eval_accuracy': 0.8620689655172413, 'eval_micro_f1': 0.8620689655172413, 'eval_macro_f1': 0.8466590734760419, 'eval_runtime': 2.6422, 'eval_samples_per_second': 834.159, 'eval_steps_per_second': 104.459, 'epoch': 5.0}
{'train_runtime': 110.6739, 'train_samples_per_second': 398.287, 'train_steps_per_second': 12.469, 'train_loss': 0.35903359012327335, 'epoch': 5.0}
***** train metrics *****
  epoch                    =        5.0
  train_loss               =      0.359
  train_runtime            = 0:01:50.67
  train_samples            =       8816
  train_samples_per_second =    398.287
  train_steps_per_second   =     12.469
11/15/2023 22:12:16 - INFO - __main__ - *** Evaluate ***
[INFO|trainer.py:755] 2023-11-15 22:12:16,894 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModelForTokenClassification.forward` and have been ignored: text. If text are not expected by `PeftModelForTokenClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 22:12:16,895 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 22:12:16,896 >>   Num examples = 2204
[INFO|trainer.py:3134] 2023-11-15 22:12:16,896 >>   Batch size = 8
  0%|          | 0/276 [00:00<?, ?it/s]  4%|â–         | 12/276 [00:00<00:02, 117.51it/s]  9%|â–Š         | 24/276 [00:00<00:02, 111.04it/s] 13%|â–ˆâ–Ž        | 36/276 [00:00<00:02, 109.10it/s] 17%|â–ˆâ–‹        | 47/276 [00:00<00:02, 108.25it/s] 21%|â–ˆâ–ˆ        | 58/276 [00:00<00:02, 107.69it/s] 25%|â–ˆâ–ˆâ–Œ       | 69/276 [00:00<00:01, 107.48it/s] 29%|â–ˆâ–ˆâ–‰       | 80/276 [00:00<00:01, 107.38it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 91/276 [00:00<00:01, 107.16it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 102/276 [00:00<00:01, 106.67it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 113/276 [00:01<00:01, 106.60it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 124/276 [00:01<00:01, 106.49it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 135/276 [00:01<00:01, 105.98it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 146/276 [00:01<00:01, 106.08it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 157/276 [00:01<00:01, 106.19it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 168/276 [00:01<00:01, 106.32it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 179/276 [00:01<00:00, 106.10it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 190/276 [00:01<00:00, 105.91it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 201/276 [00:01<00:00, 105.30it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 212/276 [00:01<00:00, 105.09it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 223/276 [00:02<00:00, 105.42it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 234/276 [00:02<00:00, 105.31it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 245/276 [00:02<00:00, 105.64it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 256/276 [00:02<00:00, 105.79it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 267/276 [00:02<00:00, 106.01it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 276/276 [00:02<00:00, 104.91it/s]
***** eval metrics *****
  epoch                   =        5.0
  eval_accuracy           =     0.8621
  eval_loss               =     0.3885
  eval_macro_f1           =     0.8467
  eval_micro_f1           =     0.8621
  eval_runtime            = 0:00:02.64
  eval_samples            =       2204
  eval_samples_per_second =     833.76
  eval_steps_per_second   =    104.409
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                  eval/accuracy â–â–„â–ˆâ–‡â–ˆâ–ˆ
wandb:                      eval/loss â–ˆâ–†â–ƒâ–â–…â–…
wandb:                  eval/macro_f1 â–â–…â–ˆâ–ˆâ–ˆâ–ˆ
wandb:                  eval/micro_f1 â–â–„â–ˆâ–‡â–ˆâ–ˆ
wandb:                   eval/runtime â–â–…â–ˆâ–…â–…â–…
wandb:        eval/samples_per_second â–ˆâ–„â–â–„â–„â–„
wandb:          eval/steps_per_second â–ˆâ–„â–â–„â–„â–„
wandb:                    train/epoch â–â–â–ƒâ–ƒâ–…â–…â–†â–†â–ˆâ–ˆâ–ˆâ–ˆ
wandb:              train/global_step â–â–â–ƒâ–ƒâ–…â–…â–†â–†â–ˆâ–ˆâ–ˆâ–ˆ
wandb:            train/learning_rate â–ˆâ–†â–…â–ƒâ–
wandb:                     train/loss â–ˆâ–„â–ƒâ–‚â–
wandb:               train/total_flos â–
wandb:               train/train_loss â–
wandb:            train/train_runtime â–
wandb: train/train_samples_per_second â–
wandb:   train/train_steps_per_second â–
wandb: 
wandb: Run summary:
wandb:                  eval/accuracy 0.86207
wandb:                      eval/loss 0.38851
wandb:                  eval/macro_f1 0.84666
wandb:                  eval/micro_f1 0.86207
wandb:                   eval/runtime 2.6434
wandb:        eval/samples_per_second 833.76
wandb:          eval/steps_per_second 104.409
wandb:                    train/epoch 5.0
wandb:              train/global_step 1380
wandb:            train/learning_rate 0.0
wandb:                     train/loss 0.2786
wandb:               train/total_flos 1469774552739840.0
wandb:               train/train_loss 0.35903
wandb:            train/train_runtime 110.6739
wandb: train/train_samples_per_second 398.287
wandb:   train/train_steps_per_second 12.469
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /ceph/home/wangyifei/a3/wandb/offline-run-20231115_220908-ytiahqyq
wandb: Find logs at: ./wandb/offline-run-20231115_220908-ytiahqyq/logs
(ModelArguments(model_name_or_path='roberta-base', cache_dir='./cache', use_adapter=False, use_lora=True), DataTrainingArguments(dataset_name='agnews_sup', max_seq_length=64), TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.0005,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=./result/agnews_sup_roberta-base_seed2_lora/runs/Nov15_22-12-29_node010,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.EPOCH,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=5.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
output_dir=./result/agnews_sup_roberta-base_seed2_lora,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
resume_from_checkpoint=None,
run_name=./result/agnews_sup_roberta-base_seed2_lora,
save_on_each_node=False,
save_safetensors=False,
save_steps=500,
save_strategy=IntervalStrategy.NO,
save_total_limit=None,
seed=333,
sharded_ddp=[],
skip_memory_metrics=True,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
))
11/15/2023 22:12:29 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False
11/15/2023 22:12:29 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.0005,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=./result/agnews_sup_roberta-base_seed2_lora/runs/Nov15_22-12-28_node010,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.EPOCH,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=5.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
output_dir=./result/agnews_sup_roberta-base_seed2_lora,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
resume_from_checkpoint=None,
run_name=./result/agnews_sup_roberta-base_seed2_lora,
save_on_each_node=False,
save_safetensors=False,
save_steps=500,
save_strategy=IntervalStrategy.NO,
save_total_limit=None,
seed=333,
sharded_ddp=[],
skip_memory_metrics=True,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
wandb: Tracking run with wandb version 0.15.12
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[INFO|configuration_utils.py:715] 2023-11-15 22:12:44,857 >> loading configuration file config.json from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/config.json
[INFO|configuration_utils.py:775] 2023-11-15 22:12:44,867 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.33.3",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_auto.py:535] 2023-11-15 22:12:54,882 >> Could not locate the tokenizer configuration file, will try to use the model config instead.
[INFO|configuration_utils.py:715] 2023-11-15 22:13:04,893 >> loading configuration file config.json from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/config.json
[INFO|configuration_utils.py:775] 2023-11-15 22:13:04,894 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.33.3",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:1852] 2023-11-15 22:13:24,940 >> loading file vocab.json from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/vocab.json
[INFO|tokenization_utils_base.py:1852] 2023-11-15 22:13:24,941 >> loading file merges.txt from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/merges.txt
[INFO|tokenization_utils_base.py:1852] 2023-11-15 22:13:24,941 >> loading file tokenizer.json from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/tokenizer.json
[INFO|tokenization_utils_base.py:1852] 2023-11-15 22:13:24,941 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1852] 2023-11-15 22:13:24,942 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1852] 2023-11-15 22:13:24,942 >> loading file tokenizer_config.json from cache at None
[INFO|configuration_utils.py:715] 2023-11-15 22:13:24,943 >> loading configuration file config.json from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/config.json
[INFO|configuration_utils.py:775] 2023-11-15 22:13:24,944 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.33.3",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:2862] 2023-11-15 22:13:45,155 >> loading weights file model.safetensors from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/model.safetensors
[INFO|modeling_utils.py:3638] 2023-11-15 22:13:45,867 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3650] 2023-11-15 22:13:45,868 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
trainable params: 1,285,636 || all params: 125,832,200 || trainable%: 1.0217066855701482
model_max_length: 512
Running tokenizer on dataset:   0%|          | 0/6840 [00:00<?, ? examples/s]Running tokenizer on dataset:  29%|â–ˆâ–ˆâ–‰       | 2000/6840 [00:00<00:00, 17613.21 examples/s]Running tokenizer on dataset:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 4000/6840 [00:00<00:00, 17715.81 examples/s]Running tokenizer on dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6840/6840 [00:00<00:00, 19455.61 examples/s]Running tokenizer on dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6840/6840 [00:00<00:00, 18840.93 examples/s]
Running tokenizer on dataset:   0%|          | 0/760 [00:00<?, ? examples/s]Running tokenizer on dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 760/760 [00:00<00:00, 22956.67 examples/s]
11/15/2023 22:13:46 - INFO - __main__ - Sample 4545 of the training set: {'text': "Yankees' Brown Has Successful Surgery Kevin Brown had successful surgery on his broken left hand Sunday and vowed to pitch again for the Yankees this season.", 'label': 0, 'input_ids': [0, 43033, 41563, 108, 1547, 6233, 14361, 2650, 26793, 2363, 1547, 56, 1800, 3012, 15, 39, 3187, 314, 865, 395, 8, 7588, 7, 3242, 456, 13, 5, 6742, 42, 191, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
11/15/2023 22:13:46 - INFO - __main__ - Sample 2873 of the training set: {'text': 'Bush shields shrimp industry The Bush administration yesterday said Chinese and Vietnamese shrimp are sold at unfairly low prices in the United States, siding with US fishermen as they try to fend off overseas competition.', 'label': 1, 'input_ids': [0, 43294, 31768, 22126, 539, 20, 3516, 942, 2350, 26, 1111, 8, 16859, 22126, 32, 1088, 23, 19106, 614, 850, 11, 5, 315, 532, 6, 579, 8231, 19, 382, 16516, 25, 51, 860, 7, 26885, 160, 4886, 1465, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
11/15/2023 22:13:46 - INFO - __main__ - Sample 2892 of the training set: {'text': 'How the credit policy will affect you The Reserve Bank of India announced the mid-term review of its monetary policy on Tuesday. Though the central Bank kept away from the much expected interest rate hike, the policy contained recommendations ', 'label': 1, 'input_ids': [0, 6179, 5, 1361, 714, 40, 3327, 47, 20, 3965, 788, 9, 666, 585, 5, 1084, 12, 1279, 1551, 9, 63, 5775, 714, 15, 294, 4, 3791, 5, 1353, 788, 1682, 409, 31, 5, 203, 421, 773, 731, 5960, 6, 5, 714, 5558, 4664, 1437, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
11/15/2023 22:13:46 - WARNING - accelerate.utils.other - Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[INFO|trainer.py:755] 2023-11-15 22:13:47,492 >> The following columns in the training set don't have a corresponding argument in `PeftModelForTokenClassification.forward` and have been ignored: text. If text are not expected by `PeftModelForTokenClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:1715] 2023-11-15 22:13:47,502 >> ***** Running training *****
[INFO|trainer.py:1716] 2023-11-15 22:13:47,503 >>   Num examples = 6,840
[INFO|trainer.py:1717] 2023-11-15 22:13:47,503 >>   Num Epochs = 5
[INFO|trainer.py:1718] 2023-11-15 22:13:47,503 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1721] 2023-11-15 22:13:47,504 >>   Total train batch size (w. parallel, distributed & accumulation) = 32
[INFO|trainer.py:1722] 2023-11-15 22:13:47,504 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:1723] 2023-11-15 22:13:47,504 >>   Total optimization steps = 1,070
[INFO|trainer.py:1724] 2023-11-15 22:13:47,505 >>   Number of trainable parameters = 1,285,636
[INFO|integration_utils.py:716] 2023-11-15 22:13:47,506 >> Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"
  0%|          | 0/1070 [00:00<?, ?it/s]  0%|          | 1/1070 [00:01<18:23,  1.03s/it]  0%|          | 3/1070 [00:01<05:41,  3.12it/s]  0%|          | 5/1070 [00:01<03:23,  5.23it/s]  1%|          | 7/1070 [00:01<02:28,  7.16it/s]  1%|          | 9/1070 [00:01<02:00,  8.84it/s]  1%|          | 11/1070 [00:01<01:43, 10.21it/s]  1%|          | 13/1070 [00:01<01:33, 11.27it/s]  1%|â–         | 15/1070 [00:02<01:27, 12.11it/s]  2%|â–         | 17/1070 [00:02<01:22, 12.72it/s]  2%|â–         | 19/1070 [00:02<01:19, 13.16it/s]  2%|â–         | 21/1070 [00:02<01:17, 13.50it/s]  2%|â–         | 23/1070 [00:02<01:16, 13.73it/s]  2%|â–         | 25/1070 [00:02<01:15, 13.87it/s]  3%|â–Ž         | 27/1070 [00:02<01:14, 13.95it/s]  3%|â–Ž         | 29/1070 [00:02<01:14, 14.05it/s]  3%|â–Ž         | 31/1070 [00:03<01:13, 14.08it/s]  3%|â–Ž         | 33/1070 [00:03<01:13, 14.04it/s]  3%|â–Ž         | 35/1070 [00:03<01:13, 14.10it/s]  3%|â–Ž         | 37/1070 [00:03<01:13, 14.09it/s]  4%|â–Ž         | 39/1070 [00:03<01:13, 14.05it/s]  4%|â–         | 41/1070 [00:03<01:13, 14.01it/s]  4%|â–         | 43/1070 [00:03<01:13, 14.04it/s]  4%|â–         | 45/1070 [00:04<01:12, 14.12it/s]  4%|â–         | 47/1070 [00:04<01:12, 14.11it/s]  5%|â–         | 49/1070 [00:04<01:12, 14.16it/s]  5%|â–         | 51/1070 [00:04<01:11, 14.19it/s]  5%|â–         | 53/1070 [00:04<01:11, 14.23it/s]  5%|â–Œ         | 55/1070 [00:04<01:11, 14.24it/s]  5%|â–Œ         | 57/1070 [00:04<01:11, 14.25it/s]  6%|â–Œ         | 59/1070 [00:05<01:10, 14.27it/s]  6%|â–Œ         | 61/1070 [00:05<01:10, 14.28it/s]  6%|â–Œ         | 63/1070 [00:05<01:10, 14.27it/s]  6%|â–Œ         | 65/1070 [00:05<01:10, 14.26it/s]  6%|â–‹         | 67/1070 [00:05<01:10, 14.27it/s]  6%|â–‹         | 69/1070 [00:05<01:10, 14.27it/s]  7%|â–‹         | 71/1070 [00:05<01:10, 14.26it/s]  7%|â–‹         | 73/1070 [00:06<01:09, 14.27it/s]  7%|â–‹         | 75/1070 [00:06<01:09, 14.26it/s]  7%|â–‹         | 77/1070 [00:06<01:09, 14.23it/s]  7%|â–‹         | 79/1070 [00:06<01:09, 14.25it/s]  8%|â–Š         | 81/1070 [00:06<01:09, 14.26it/s]  8%|â–Š         | 83/1070 [00:06<01:09, 14.26it/s]  8%|â–Š         | 85/1070 [00:06<01:09, 14.25it/s]  8%|â–Š         | 87/1070 [00:07<01:08, 14.27it/s]  8%|â–Š         | 89/1070 [00:07<01:08, 14.29it/s]  9%|â–Š         | 91/1070 [00:07<01:08, 14.31it/s]  9%|â–Š         | 93/1070 [00:07<01:08, 14.32it/s]  9%|â–‰         | 95/1070 [00:07<01:07, 14.34it/s]  9%|â–‰         | 97/1070 [00:07<01:07, 14.33it/s]  9%|â–‰         | 99/1070 [00:07<01:07, 14.33it/s]  9%|â–‰         | 101/1070 [00:08<01:07, 14.32it/s] 10%|â–‰         | 103/1070 [00:08<01:07, 14.32it/s] 10%|â–‰         | 105/1070 [00:08<01:07, 14.30it/s] 10%|â–ˆ         | 107/1070 [00:08<01:07, 14.27it/s] 10%|â–ˆ         | 109/1070 [00:08<01:07, 14.27it/s] 10%|â–ˆ         | 111/1070 [00:08<01:09, 13.81it/s] 11%|â–ˆ         | 113/1070 [00:08<01:09, 13.85it/s] 11%|â–ˆ         | 115/1070 [00:09<01:08, 13.84it/s] 11%|â–ˆ         | 117/1070 [00:09<01:08, 13.96it/s] 11%|â–ˆ         | 119/1070 [00:09<01:07, 14.02it/s] 11%|â–ˆâ–        | 121/1070 [00:09<01:07, 14.06it/s] 11%|â–ˆâ–        | 123/1070 [00:09<01:07, 14.11it/s] 12%|â–ˆâ–        | 125/1070 [00:09<01:06, 14.16it/s] 12%|â–ˆâ–        | 127/1070 [00:09<01:06, 14.19it/s] 12%|â–ˆâ–        | 129/1070 [00:10<01:06, 14.20it/s] 12%|â–ˆâ–        | 131/1070 [00:10<01:06, 14.22it/s] 12%|â–ˆâ–        | 133/1070 [00:10<01:05, 14.21it/s] 13%|â–ˆâ–Ž        | 135/1070 [00:10<01:05, 14.24it/s] 13%|â–ˆâ–Ž        | 137/1070 [00:10<01:05, 14.27it/s] 13%|â–ˆâ–Ž        | 139/1070 [00:10<01:05, 14.28it/s] 13%|â–ˆâ–Ž        | 141/1070 [00:10<01:05, 14.28it/s] 13%|â–ˆâ–Ž        | 143/1070 [00:11<01:04, 14.28it/s] 14%|â–ˆâ–Ž        | 145/1070 [00:11<01:04, 14.29it/s] 14%|â–ˆâ–Ž        | 147/1070 [00:11<01:04, 14.22it/s] 14%|â–ˆâ–        | 149/1070 [00:11<01:04, 14.22it/s] 14%|â–ˆâ–        | 151/1070 [00:11<01:04, 14.16it/s] 14%|â–ˆâ–        | 153/1070 [00:11<01:04, 14.20it/s] 14%|â–ˆâ–        | 155/1070 [00:11<01:04, 14.23it/s] 15%|â–ˆâ–        | 157/1070 [00:12<01:04, 14.24it/s] 15%|â–ˆâ–        | 159/1070 [00:12<01:03, 14.29it/s] 15%|â–ˆâ–Œ        | 161/1070 [00:12<01:03, 14.33it/s] 15%|â–ˆâ–Œ        | 163/1070 [00:12<01:03, 14.36it/s] 15%|â–ˆâ–Œ        | 165/1070 [00:12<01:02, 14.37it/s] 16%|â–ˆâ–Œ        | 167/1070 [00:12<01:02, 14.39it/s] 16%|â–ˆâ–Œ        | 169/1070 [00:12<01:02, 14.38it/s] 16%|â–ˆâ–Œ        | 171/1070 [00:12<01:02, 14.37it/s] 16%|â–ˆâ–Œ        | 173/1070 [00:13<01:02, 14.36it/s] 16%|â–ˆâ–‹        | 175/1070 [00:13<01:02, 14.36it/s] 17%|â–ˆâ–‹        | 177/1070 [00:13<01:02, 14.36it/s] 17%|â–ˆâ–‹        | 179/1070 [00:13<01:02, 14.35it/s] 17%|â–ˆâ–‹        | 181/1070 [00:13<01:01, 14.35it/s] 17%|â–ˆâ–‹        | 183/1070 [00:13<01:01, 14.37it/s] 17%|â–ˆâ–‹        | 185/1070 [00:13<01:01, 14.38it/s] 17%|â–ˆâ–‹        | 187/1070 [00:14<01:01, 14.40it/s] 18%|â–ˆâ–Š        | 189/1070 [00:14<01:01, 14.41it/s] 18%|â–ˆâ–Š        | 191/1070 [00:14<01:00, 14.43it/s] 18%|â–ˆâ–Š        | 193/1070 [00:14<01:00, 14.40it/s] 18%|â–ˆâ–Š        | 195/1070 [00:14<01:00, 14.39it/s] 18%|â–ˆâ–Š        | 197/1070 [00:14<01:00, 14.38it/s] 19%|â–ˆâ–Š        | 199/1070 [00:14<01:00, 14.38it/s] 19%|â–ˆâ–‰        | 201/1070 [00:15<01:00, 14.38it/s] 19%|â–ˆâ–‰        | 203/1070 [00:15<01:00, 14.38it/s] 19%|â–ˆâ–‰        | 205/1070 [00:15<01:00, 14.39it/s] 19%|â–ˆâ–‰        | 207/1070 [00:15<00:59, 14.39it/s] 20%|â–ˆâ–‰        | 209/1070 [00:15<00:59, 14.40it/s] 20%|â–ˆâ–‰        | 211/1070 [00:15<00:59, 14.40it/s] 20%|â–ˆâ–‰        | 213/1070 [00:15<00:59, 14.40it/s]                                                   20%|â–ˆâ–ˆ        | 214/1070 [00:15<00:59, 14.40it/s][INFO|trainer.py:755] 2023-11-15 22:14:03,459 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModelForTokenClassification.forward` and have been ignored: text. If text are not expected by `PeftModelForTokenClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 22:14:03,461 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 22:14:03,461 >>   Num examples = 760
[INFO|trainer.py:3134] 2023-11-15 22:14:03,461 >>   Batch size = 8
{'loss': 0.4341, 'learning_rate': 0.0004, 'epoch': 1.0}

  0%|          | 0/95 [00:00<?, ?it/s][A
 13%|â–ˆâ–Ž        | 12/95 [00:00<00:00, 117.75it/s][A
 25%|â–ˆâ–ˆâ–Œ       | 24/95 [00:00<00:00, 111.38it/s][A
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 36/95 [00:00<00:00, 109.23it/s][A
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 47/95 [00:00<00:00, 108.38it/s][A
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 58/95 [00:00<00:00, 107.81it/s][A
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 69/95 [00:00<00:00, 107.65it/s][A
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 80/95 [00:00<00:00, 107.52it/s][A
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 91/95 [00:00<00:00, 107.43it/s][A                                                  
                                                [A 20%|â–ˆâ–ˆ        | 214/1070 [00:16<00:59, 14.40it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 95/95 [00:00<00:00, 107.43it/s][A
                                                [A 20%|â–ˆâ–ˆ        | 215/1070 [00:16<02:56,  4.85it/s] 20%|â–ˆâ–ˆ        | 217/1070 [00:17<02:21,  6.05it/s] 20%|â–ˆâ–ˆ        | 219/1070 [00:17<01:56,  7.32it/s] 21%|â–ˆâ–ˆ        | 221/1070 [00:17<01:38,  8.58it/s] 21%|â–ˆâ–ˆ        | 223/1070 [00:17<01:26,  9.75it/s] 21%|â–ˆâ–ˆ        | 225/1070 [00:17<01:18, 10.79it/s] 21%|â–ˆâ–ˆ        | 227/1070 [00:17<01:12, 11.65it/s] 21%|â–ˆâ–ˆâ–       | 229/1070 [00:17<01:08, 12.33it/s] 22%|â–ˆâ–ˆâ–       | 231/1070 [00:18<01:05, 12.86it/s] 22%|â–ˆâ–ˆâ–       | 233/1070 [00:18<01:03, 13.27it/s] 22%|â–ˆâ–ˆâ–       | 235/1070 [00:18<01:01, 13.57it/s] 22%|â–ˆâ–ˆâ–       | 237/1070 [00:18<01:00, 13.78it/s] 22%|â–ˆâ–ˆâ–       | 239/1070 [00:18<00:59, 13.93it/s] 23%|â–ˆâ–ˆâ–Ž       | 241/1070 [00:18<00:59, 14.04it/s] 23%|â–ˆâ–ˆâ–Ž       | 243/1070 [00:18<00:58, 14.12it/s] 23%|â–ˆâ–ˆâ–Ž       | 245/1070 [00:19<00:58, 14.18it/s] 23%|â–ˆâ–ˆâ–Ž       | 247/1070 [00:19<00:57, 14.21it/s] 23%|â–ˆâ–ˆâ–Ž       | 249/1070 [00:19<00:57, 14.24it/s] 23%|â–ˆâ–ˆâ–Ž       | 251/1070 [00:19<00:57, 14.26it/s] 24%|â–ˆâ–ˆâ–Ž       | 253/1070 [00:19<00:57, 14.29it/s] 24%|â–ˆâ–ˆâ–       | 255/1070 [00:19<00:57, 14.29it/s] 24%|â–ˆâ–ˆâ–       | 257/1070 [00:19<00:56, 14.30it/s] 24%|â–ˆâ–ˆâ–       | 259/1070 [00:20<00:56, 14.31it/s] 24%|â–ˆâ–ˆâ–       | 261/1070 [00:20<00:56, 14.31it/s] 25%|â–ˆâ–ˆâ–       | 263/1070 [00:20<00:56, 14.33it/s] 25%|â–ˆâ–ˆâ–       | 265/1070 [00:20<00:56, 14.33it/s] 25%|â–ˆâ–ˆâ–       | 267/1070 [00:20<00:55, 14.34it/s] 25%|â–ˆâ–ˆâ–Œ       | 269/1070 [00:20<00:55, 14.35it/s] 25%|â–ˆâ–ˆâ–Œ       | 271/1070 [00:20<00:55, 14.33it/s] 26%|â–ˆâ–ˆâ–Œ       | 273/1070 [00:20<00:55, 14.34it/s] 26%|â–ˆâ–ˆâ–Œ       | 275/1070 [00:21<00:55, 14.34it/s] 26%|â–ˆâ–ˆâ–Œ       | 277/1070 [00:21<00:55, 14.34it/s] 26%|â–ˆâ–ˆâ–Œ       | 279/1070 [00:21<00:55, 14.34it/s] 26%|â–ˆâ–ˆâ–‹       | 281/1070 [00:21<00:55, 14.33it/s] 26%|â–ˆâ–ˆâ–‹       | 283/1070 [00:21<00:54, 14.32it/s] 27%|â–ˆâ–ˆâ–‹       | 285/1070 [00:21<00:54, 14.30it/s] 27%|â–ˆâ–ˆâ–‹       | 287/1070 [00:21<00:54, 14.30it/s] 27%|â–ˆâ–ˆâ–‹       | 289/1070 [00:22<00:54, 14.30it/s] 27%|â–ˆâ–ˆâ–‹       | 291/1070 [00:22<00:54, 14.30it/s] 27%|â–ˆâ–ˆâ–‹       | 293/1070 [00:22<00:54, 14.30it/s] 28%|â–ˆâ–ˆâ–Š       | 295/1070 [00:22<00:54, 14.29it/s] 28%|â–ˆâ–ˆâ–Š       | 297/1070 [00:22<00:54, 14.29it/s] 28%|â–ˆâ–ˆâ–Š       | 299/1070 [00:22<00:53, 14.29it/s] 28%|â–ˆâ–ˆâ–Š       | 301/1070 [00:22<00:53, 14.29it/s] 28%|â–ˆâ–ˆâ–Š       | 303/1070 [00:23<00:53, 14.29it/s] 29%|â–ˆâ–ˆâ–Š       | 305/1070 [00:23<00:53, 14.29it/s] 29%|â–ˆâ–ˆâ–Š       | 307/1070 [00:23<00:53, 14.29it/s] 29%|â–ˆâ–ˆâ–‰       | 309/1070 [00:23<00:53, 14.29it/s] 29%|â–ˆâ–ˆâ–‰       | 311/1070 [00:23<00:53, 14.29it/s] 29%|â–ˆâ–ˆâ–‰       | 313/1070 [00:23<00:52, 14.30it/s] 29%|â–ˆâ–ˆâ–‰       | 315/1070 [00:23<00:52, 14.35it/s] 30%|â–ˆâ–ˆâ–‰       | 317/1070 [00:24<00:52, 14.34it/s] 30%|â–ˆâ–ˆâ–‰       | 319/1070 [00:24<00:52, 14.32it/s] 30%|â–ˆâ–ˆâ–ˆ       | 321/1070 [00:24<00:52, 14.31it/s] 30%|â–ˆâ–ˆâ–ˆ       | 323/1070 [00:24<00:52, 14.30it/s] 30%|â–ˆâ–ˆâ–ˆ       | 325/1070 [00:24<00:52, 14.30it/s] 31%|â–ˆâ–ˆâ–ˆ       | 327/1070 [00:24<00:52, 14.29it/s] 31%|â–ˆâ–ˆâ–ˆ       | 329/1070 [00:24<00:51, 14.29it/s] 31%|â–ˆâ–ˆâ–ˆ       | 331/1070 [00:25<00:51, 14.30it/s] 31%|â–ˆâ–ˆâ–ˆ       | 333/1070 [00:25<00:51, 14.29it/s] 31%|â–ˆâ–ˆâ–ˆâ–      | 335/1070 [00:25<00:51, 14.29it/s] 31%|â–ˆâ–ˆâ–ˆâ–      | 337/1070 [00:25<00:51, 14.28it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 339/1070 [00:25<00:51, 14.27it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 341/1070 [00:25<00:51, 14.27it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 343/1070 [00:25<00:50, 14.27it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 345/1070 [00:26<00:50, 14.27it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 347/1070 [00:26<00:50, 14.27it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 349/1070 [00:26<00:50, 14.27it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 351/1070 [00:26<00:50, 14.28it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 353/1070 [00:26<00:50, 14.28it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 355/1070 [00:26<00:50, 14.28it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 357/1070 [00:26<00:49, 14.28it/s] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 359/1070 [00:27<00:49, 14.28it/s] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 361/1070 [00:27<00:49, 14.29it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 363/1070 [00:27<00:49, 14.29it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 365/1070 [00:27<00:49, 14.29it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 367/1070 [00:27<00:49, 14.27it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 369/1070 [00:27<00:49, 14.29it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 371/1070 [00:27<00:48, 14.29it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 373/1070 [00:27<00:48, 14.29it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 375/1070 [00:28<00:48, 14.27it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 377/1070 [00:28<00:48, 14.28it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 379/1070 [00:28<00:48, 14.28it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 381/1070 [00:28<00:48, 14.28it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 383/1070 [00:28<00:48, 14.29it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 385/1070 [00:28<00:47, 14.30it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 387/1070 [00:28<00:47, 14.30it/s] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 389/1070 [00:29<00:47, 14.30it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 391/1070 [00:29<00:47, 14.29it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 393/1070 [00:29<00:47, 14.30it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 395/1070 [00:29<00:47, 14.30it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 397/1070 [00:29<00:47, 14.29it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 399/1070 [00:29<00:46, 14.29it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 401/1070 [00:29<00:46, 14.29it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 403/1070 [00:30<00:46, 14.29it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 405/1070 [00:30<00:46, 14.29it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 407/1070 [00:30<00:46, 14.28it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 409/1070 [00:30<00:46, 14.29it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 411/1070 [00:30<00:46, 14.29it/s] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 413/1070 [00:30<00:45, 14.29it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 415/1070 [00:30<00:45, 14.29it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 417/1070 [00:31<00:45, 14.29it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 419/1070 [00:31<00:45, 14.28it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 421/1070 [00:31<00:45, 14.29it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 423/1070 [00:31<00:45, 14.29it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 425/1070 [00:31<00:45, 14.29it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 427/1070 [00:31<00:44, 14.30it/s]                                                   40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 428/1070 [00:31<00:44, 14.30it/s][INFO|trainer.py:755] 2023-11-15 22:14:19,336 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModelForTokenClassification.forward` and have been ignored: text. If text are not expected by `PeftModelForTokenClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 22:14:19,338 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 22:14:19,338 >>   Num examples = 760
[INFO|trainer.py:3134] 2023-11-15 22:14:19,339 >>   Batch size = 8
{'eval_loss': 0.29392367601394653, 'eval_accuracy': 0.9013157894736842, 'eval_micro_f1': 0.9013157894736842, 'eval_macro_f1': 0.8970164805601726, 'eval_runtime': 0.9201, 'eval_samples_per_second': 826.028, 'eval_steps_per_second': 103.254, 'epoch': 1.0}
{'loss': 0.2661, 'learning_rate': 0.0003, 'epoch': 2.0}

  0%|          | 0/95 [00:00<?, ?it/s][A
 13%|â–ˆâ–Ž        | 12/95 [00:00<00:00, 116.99it/s][A
 25%|â–ˆâ–ˆâ–Œ       | 24/95 [00:00<00:00, 110.49it/s][A
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 36/95 [00:00<00:00, 108.60it/s][A
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 47/95 [00:00<00:00, 107.56it/s][A
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 58/95 [00:00<00:00, 107.16it/s][A
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 69/95 [00:00<00:00, 106.67it/s][A
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 80/95 [00:00<00:00, 106.60it/s][A
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 91/95 [00:00<00:00, 106.54it/s][A                                                  
                                                [A 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 428/1070 [00:32<00:44, 14.30it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 95/95 [00:00<00:00, 106.54it/s][A
                                                [A 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 429/1070 [00:32<02:13,  4.81it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 431/1070 [00:32<01:46,  6.00it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 433/1070 [00:33<01:27,  7.27it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 435/1070 [00:33<01:14,  8.52it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 437/1070 [00:33<01:05,  9.70it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 439/1070 [00:33<00:58, 10.74it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 441/1070 [00:33<00:54, 11.61it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 443/1070 [00:33<00:50, 12.30it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 445/1070 [00:33<00:48, 12.84it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 447/1070 [00:34<00:47, 13.24it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 449/1070 [00:34<00:45, 13.53it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 451/1070 [00:34<00:45, 13.75it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 453/1070 [00:34<00:44, 13.91it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 455/1070 [00:34<00:43, 14.02it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 457/1070 [00:34<00:43, 14.10it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 459/1070 [00:34<00:43, 14.16it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 461/1070 [00:35<00:42, 14.20it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 463/1070 [00:35<00:42, 14.22it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 465/1070 [00:35<00:42, 14.23it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 467/1070 [00:35<00:42, 14.25it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 469/1070 [00:35<00:42, 14.26it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 471/1070 [00:35<00:41, 14.27it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 473/1070 [00:35<00:41, 14.28it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 475/1070 [00:36<00:41, 14.28it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 477/1070 [00:36<00:41, 14.28it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 479/1070 [00:36<00:41, 14.28it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 481/1070 [00:36<00:41, 14.29it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 483/1070 [00:36<00:41, 14.29it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 485/1070 [00:36<00:40, 14.29it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 487/1070 [00:36<00:40, 14.29it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 489/1070 [00:37<00:40, 14.29it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 491/1070 [00:37<00:40, 14.28it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 493/1070 [00:37<00:40, 14.29it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 495/1070 [00:37<00:40, 14.29it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 497/1070 [00:37<00:40, 14.29it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 499/1070 [00:37<00:39, 14.29it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 501/1070 [00:37<00:39, 14.29it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 503/1070 [00:38<00:39, 14.29it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 505/1070 [00:38<00:39, 14.29it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 507/1070 [00:38<00:39, 14.29it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 509/1070 [00:38<00:39, 14.29it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 511/1070 [00:38<00:39, 14.29it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 513/1070 [00:38<00:38, 14.29it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 515/1070 [00:38<00:38, 14.30it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 517/1070 [00:38<00:38, 14.29it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 519/1070 [00:39<00:38, 14.29it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 521/1070 [00:39<00:38, 14.29it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 523/1070 [00:39<00:38, 14.29it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 525/1070 [00:39<00:38, 14.29it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 527/1070 [00:39<00:38, 14.29it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 529/1070 [00:39<00:37, 14.29it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 531/1070 [00:39<00:37, 14.29it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 533/1070 [00:40<00:37, 14.29it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 535/1070 [00:40<00:37, 14.29it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 537/1070 [00:40<00:37, 14.28it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 539/1070 [00:40<00:37, 14.28it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 541/1070 [00:40<00:37, 14.29it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 543/1070 [00:40<00:36, 14.29it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 545/1070 [00:40<00:36, 14.29it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 547/1070 [00:41<00:36, 14.25it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 549/1070 [00:41<00:36, 14.27it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 551/1070 [00:41<00:36, 14.28it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 553/1070 [00:41<00:36, 14.27it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 555/1070 [00:41<00:36, 14.27it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 557/1070 [00:41<00:35, 14.27it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 559/1070 [00:41<00:35, 14.27it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 561/1070 [00:42<00:35, 14.27it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 563/1070 [00:42<00:35, 14.26it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 565/1070 [00:42<00:35, 14.26it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 567/1070 [00:42<00:35, 14.25it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 569/1070 [00:42<00:35, 14.26it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 571/1070 [00:42<00:35, 14.25it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 573/1070 [00:42<00:34, 14.23it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 575/1070 [00:43<00:34, 14.23it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 577/1070 [00:43<00:34, 14.23it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 579/1070 [00:43<00:34, 14.21it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 581/1070 [00:43<00:34, 14.21it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 583/1070 [00:43<00:34, 14.22it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 585/1070 [00:43<00:34, 14.24it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 587/1070 [00:43<00:33, 14.24it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 589/1070 [00:44<00:33, 14.25it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 591/1070 [00:44<00:33, 14.26it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 593/1070 [00:44<00:33, 14.27it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 595/1070 [00:44<00:33, 14.27it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 597/1070 [00:44<00:33, 14.28it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 599/1070 [00:44<00:32, 14.28it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 601/1070 [00:44<00:32, 14.28it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 603/1070 [00:45<00:32, 14.29it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 605/1070 [00:45<00:32, 14.29it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 607/1070 [00:45<00:32, 14.29it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 609/1070 [00:45<00:32, 14.29it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 611/1070 [00:45<00:32, 14.28it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 613/1070 [00:45<00:31, 14.29it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 615/1070 [00:45<00:31, 14.28it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 617/1070 [00:45<00:31, 14.29it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 619/1070 [00:46<00:31, 14.29it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 621/1070 [00:46<00:31, 14.29it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 623/1070 [00:46<00:31, 14.29it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 625/1070 [00:46<00:31, 14.28it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 627/1070 [00:46<00:31, 14.28it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 629/1070 [00:46<00:30, 14.28it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 631/1070 [00:46<00:30, 14.27it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 633/1070 [00:47<00:30, 14.27it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 635/1070 [00:47<00:30, 14.27it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 637/1070 [00:47<00:30, 14.26it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 639/1070 [00:47<00:30, 14.27it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 641/1070 [00:47<00:30, 14.28it/s]                                                   60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 642/1070 [00:47<00:29, 14.28it/s][INFO|trainer.py:755] 2023-11-15 22:14:35,243 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModelForTokenClassification.forward` and have been ignored: text. If text are not expected by `PeftModelForTokenClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 22:14:35,245 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 22:14:35,245 >>   Num examples = 760
[INFO|trainer.py:3134] 2023-11-15 22:14:35,245 >>   Batch size = 8
{'eval_loss': 0.27519461512565613, 'eval_accuracy': 0.9078947368421053, 'eval_micro_f1': 0.9078947368421053, 'eval_macro_f1': 0.904993008713521, 'eval_runtime': 0.9288, 'eval_samples_per_second': 818.271, 'eval_steps_per_second': 102.284, 'epoch': 2.0}
{'loss': 0.2236, 'learning_rate': 0.0002, 'epoch': 3.0}

  0%|          | 0/95 [00:00<?, ?it/s][A
 13%|â–ˆâ–Ž        | 12/95 [00:00<00:00, 116.58it/s][A
 25%|â–ˆâ–ˆâ–Œ       | 24/95 [00:00<00:00, 110.10it/s][A
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 36/95 [00:00<00:00, 108.20it/s][A
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 47/95 [00:00<00:00, 107.36it/s][A
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 58/95 [00:00<00:00, 106.81it/s][A
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 69/95 [00:00<00:00, 106.59it/s][A
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 80/95 [00:00<00:00, 106.38it/s][A
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 91/95 [00:00<00:00, 106.18it/s][A                                                  
                                                [A 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 642/1070 [00:48<00:29, 14.28it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 95/95 [00:00<00:00, 106.18it/s][A
                                                [A 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 643/1070 [00:48<01:28,  4.81it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 645/1070 [00:48<01:10,  6.00it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 647/1070 [00:49<00:58,  7.26it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 649/1070 [00:49<00:49,  8.51it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 651/1070 [00:49<00:43,  9.68it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 653/1070 [00:49<00:38, 10.71it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 655/1070 [00:49<00:35, 11.57it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 657/1070 [00:49<00:33, 12.26it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 659/1070 [00:49<00:32, 12.78it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 661/1070 [00:50<00:31, 13.18it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 663/1070 [00:50<00:30, 13.46it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 665/1070 [00:50<00:29, 13.68it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 667/1070 [00:50<00:29, 13.84it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 669/1070 [00:50<00:28, 13.96it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 671/1070 [00:50<00:28, 14.04it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 673/1070 [00:50<00:28, 14.10it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 675/1070 [00:50<00:27, 14.15it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 677/1070 [00:51<00:27, 14.18it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 679/1070 [00:51<00:27, 14.20it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 681/1070 [00:51<00:27, 14.23it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 683/1070 [00:51<00:27, 14.24it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 685/1070 [00:51<00:27, 14.24it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 687/1070 [00:51<00:26, 14.24it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 689/1070 [00:51<00:26, 14.24it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 691/1070 [00:52<00:26, 14.24it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 693/1070 [00:52<00:26, 14.24it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 695/1070 [00:52<00:26, 14.23it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 697/1070 [00:52<00:26, 14.21it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 699/1070 [00:52<00:26, 14.20it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 701/1070 [00:52<00:26, 14.19it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 703/1070 [00:52<00:25, 14.21it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 705/1070 [00:53<00:25, 14.23it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 707/1070 [00:53<00:25, 14.24it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 709/1070 [00:53<00:25, 14.25it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 711/1070 [00:53<00:25, 14.25it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 713/1070 [00:53<00:25, 14.25it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 715/1070 [00:53<00:24, 14.26it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 717/1070 [00:53<00:24, 14.25it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 719/1070 [00:54<00:24, 14.25it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 721/1070 [00:54<00:24, 14.26it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 723/1070 [00:54<00:24, 14.26it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 725/1070 [00:54<00:24, 14.25it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 727/1070 [00:54<00:24, 14.24it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 729/1070 [00:54<00:23, 14.23it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 731/1070 [00:54<00:23, 14.23it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 733/1070 [00:55<00:23, 14.21it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 735/1070 [00:55<00:23, 14.21it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 737/1070 [00:55<00:23, 14.19it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 739/1070 [00:55<00:23, 14.20it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 741/1070 [00:55<00:23, 14.21it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 743/1070 [00:55<00:22, 14.23it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 745/1070 [00:55<00:22, 14.24it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 747/1070 [00:56<00:22, 14.25it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 749/1070 [00:56<00:22, 14.25it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 751/1070 [00:56<00:22, 14.24it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 753/1070 [00:56<00:22, 14.25it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 755/1070 [00:56<00:22, 14.24it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 757/1070 [00:56<00:22, 14.23it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 759/1070 [00:56<00:21, 14.22it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 761/1070 [00:57<00:21, 14.22it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 763/1070 [00:57<00:21, 14.21it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 765/1070 [00:57<00:21, 14.20it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 767/1070 [00:57<00:21, 14.18it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 769/1070 [00:57<00:21, 14.20it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 771/1070 [00:57<00:21, 14.21it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 773/1070 [00:57<00:20, 14.22it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 775/1070 [00:58<00:20, 14.24it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 777/1070 [00:58<00:20, 14.24it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 779/1070 [00:58<00:20, 14.22it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 781/1070 [00:58<00:20, 14.23it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 783/1070 [00:58<00:20, 14.24it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 785/1070 [00:58<00:20, 14.25it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 787/1070 [00:58<00:19, 14.25it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 789/1070 [00:58<00:19, 14.25it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 791/1070 [00:59<00:19, 14.26it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 793/1070 [00:59<00:19, 14.25it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 795/1070 [00:59<00:19, 14.25it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 797/1070 [00:59<00:19, 14.24it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 799/1070 [00:59<00:19, 14.23it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 801/1070 [00:59<00:18, 14.23it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 803/1070 [00:59<00:18, 14.22it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 805/1070 [01:00<00:18, 14.21it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 807/1070 [01:00<00:18, 14.21it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 809/1070 [01:00<00:18, 14.22it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 811/1070 [01:00<00:18, 14.23it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 813/1070 [01:00<00:18, 14.24it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 815/1070 [01:00<00:17, 14.24it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 817/1070 [01:00<00:17, 14.25it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 819/1070 [01:01<00:17, 14.25it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 821/1070 [01:01<00:17, 14.25it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 823/1070 [01:01<00:17, 14.26it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 825/1070 [01:01<00:17, 14.24it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 827/1070 [01:01<00:17, 14.24it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 829/1070 [01:01<00:16, 14.24it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 831/1070 [01:01<00:16, 14.21it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 833/1070 [01:02<00:16, 14.20it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 835/1070 [01:02<00:16, 14.20it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 837/1070 [01:02<00:16, 14.18it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 839/1070 [01:02<00:16, 14.20it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 841/1070 [01:02<00:16, 14.21it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 843/1070 [01:02<00:15, 14.24it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 845/1070 [01:02<00:15, 14.24it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 847/1070 [01:03<00:15, 14.25it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 849/1070 [01:03<00:15, 14.26it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 851/1070 [01:03<00:15, 14.26it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 853/1070 [01:03<00:15, 14.26it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 855/1070 [01:03<00:15, 14.27it/s]                                                   80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 856/1070 [01:03<00:14, 14.27it/s][INFO|trainer.py:755] 2023-11-15 22:14:51,197 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModelForTokenClassification.forward` and have been ignored: text. If text are not expected by `PeftModelForTokenClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 22:14:51,198 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 22:14:51,199 >>   Num examples = 760
[INFO|trainer.py:3134] 2023-11-15 22:14:51,199 >>   Batch size = 8
{'eval_loss': 0.2897641658782959, 'eval_accuracy': 0.9052631578947369, 'eval_micro_f1': 0.9052631578947369, 'eval_macro_f1': 0.9025757204871363, 'eval_runtime': 0.9274, 'eval_samples_per_second': 819.491, 'eval_steps_per_second': 102.436, 'epoch': 3.0}
{'loss': 0.1868, 'learning_rate': 0.0001, 'epoch': 4.0}

  0%|          | 0/95 [00:00<?, ?it/s][A
 13%|â–ˆâ–Ž        | 12/95 [00:00<00:00, 116.12it/s][A
 25%|â–ˆâ–ˆâ–Œ       | 24/95 [00:00<00:00, 109.89it/s][A
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 36/95 [00:00<00:00, 108.14it/s][A
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 47/95 [00:00<00:00, 107.28it/s][A
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 58/95 [00:00<00:00, 106.79it/s][A
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 69/95 [00:00<00:00, 106.48it/s][A
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 80/95 [00:00<00:00, 106.29it/s][A
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 91/95 [00:00<00:00, 106.16it/s][A                                                  
                                                [A 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 856/1070 [01:04<00:14, 14.27it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 95/95 [00:00<00:00, 106.16it/s][A
                                                [A 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 857/1070 [01:04<00:44,  4.81it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 859/1070 [01:04<00:35,  6.00it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 861/1070 [01:04<00:28,  7.26it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 863/1070 [01:05<00:24,  8.51it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 865/1070 [01:05<00:21,  9.68it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 867/1070 [01:05<00:18, 10.71it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 869/1070 [01:05<00:17, 11.57it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 871/1070 [01:05<00:16, 12.26it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 873/1070 [01:05<00:15, 12.79it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 875/1070 [01:05<00:14, 13.18it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 877/1070 [01:06<00:14, 13.47it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 879/1070 [01:06<00:13, 13.67it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 881/1070 [01:06<00:13, 13.83it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 883/1070 [01:06<00:13, 13.95it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 885/1070 [01:06<00:13, 14.03it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 887/1070 [01:06<00:12, 14.09it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 889/1070 [01:06<00:12, 14.13it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 891/1070 [01:07<00:12, 14.17it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 893/1070 [01:07<00:12, 14.20it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 895/1070 [01:07<00:12, 14.22it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 897/1070 [01:07<00:12, 14.21it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 899/1070 [01:07<00:12, 14.21it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 901/1070 [01:07<00:11, 14.21it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 903/1070 [01:07<00:11, 14.22it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 905/1070 [01:08<00:11, 14.21it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 907/1070 [01:08<00:11, 14.21it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 909/1070 [01:08<00:11, 14.20it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 911/1070 [01:08<00:11, 14.19it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 913/1070 [01:08<00:11, 14.21it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 915/1070 [01:08<00:10, 14.23it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 917/1070 [01:08<00:10, 14.24it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 919/1070 [01:09<00:10, 14.23it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 921/1070 [01:09<00:10, 14.23it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 923/1070 [01:09<00:10, 14.23it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 925/1070 [01:09<00:10, 14.23it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 927/1070 [01:09<00:10, 14.23it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 929/1070 [01:09<00:09, 14.23it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 931/1070 [01:09<00:09, 14.23it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 933/1070 [01:10<00:09, 14.20it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 935/1070 [01:10<00:09, 14.20it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 937/1070 [01:10<00:09, 14.19it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 939/1070 [01:10<00:09, 14.19it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 941/1070 [01:10<00:09, 14.19it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 943/1070 [01:10<00:08, 14.21it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 945/1070 [01:10<00:08, 14.22it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 947/1070 [01:11<00:08, 14.23it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 949/1070 [01:11<00:08, 14.24it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 951/1070 [01:11<00:08, 14.24it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 953/1070 [01:11<00:08, 14.23it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 955/1070 [01:11<00:08, 14.23it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 957/1070 [01:11<00:07, 14.23it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 959/1070 [01:11<00:07, 14.22it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 961/1070 [01:12<00:07, 14.20it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 963/1070 [01:12<00:07, 14.19it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 965/1070 [01:12<00:07, 14.19it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 967/1070 [01:12<00:07, 14.20it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 969/1070 [01:12<00:07, 14.21it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 971/1070 [01:12<00:06, 14.23it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 973/1070 [01:12<00:06, 14.23it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 975/1070 [01:12<00:06, 14.24it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 977/1070 [01:13<00:06, 14.24it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 979/1070 [01:13<00:06, 14.25it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 981/1070 [01:13<00:06, 14.25it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 983/1070 [01:13<00:06, 14.25it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 985/1070 [01:13<00:05, 14.25it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 987/1070 [01:13<00:05, 14.25it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 989/1070 [01:13<00:05, 14.24it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 991/1070 [01:14<00:05, 14.23it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 993/1070 [01:14<00:05, 14.22it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 995/1070 [01:14<00:05, 14.22it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 997/1070 [01:14<00:05, 14.21it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 999/1070 [01:14<00:05, 14.20it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1001/1070 [01:14<00:04, 14.20it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1003/1070 [01:14<00:04, 14.20it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1005/1070 [01:15<00:04, 14.22it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1007/1070 [01:15<00:04, 14.24it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1009/1070 [01:15<00:04, 14.23it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1011/1070 [01:15<00:04, 14.24it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1013/1070 [01:15<00:04, 14.24it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1015/1070 [01:15<00:03, 14.23it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1017/1070 [01:15<00:03, 14.23it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1019/1070 [01:16<00:03, 14.23it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1021/1070 [01:16<00:03, 14.23it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1023/1070 [01:16<00:03, 14.23it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1025/1070 [01:16<00:03, 14.21it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1027/1070 [01:16<00:03, 14.21it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1029/1070 [01:16<00:02, 14.19it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1031/1070 [01:16<00:02, 14.20it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1033/1070 [01:17<00:02, 14.21it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1035/1070 [01:17<00:02, 14.22it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1037/1070 [01:17<00:02, 14.23it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1039/1070 [01:17<00:02, 14.23it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1041/1070 [01:17<00:02, 14.23it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1043/1070 [01:17<00:01, 14.23it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1045/1070 [01:17<00:01, 14.22it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1047/1070 [01:18<00:01, 14.22it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1049/1070 [01:18<00:01, 14.22it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1051/1070 [01:18<00:01, 14.22it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1053/1070 [01:18<00:01, 14.21it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1055/1070 [01:18<00:01, 14.20it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1057/1070 [01:18<00:00, 14.19it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1059/1070 [01:18<00:00, 14.20it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1061/1070 [01:19<00:00, 14.21it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1063/1070 [01:19<00:00, 14.23it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1065/1070 [01:19<00:00, 14.21it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1067/1070 [01:19<00:00, 14.22it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1069/1070 [01:19<00:00, 14.22it/s]                                                   100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1070/1070 [01:19<00:00, 14.22it/s][INFO|trainer.py:755] 2023-11-15 22:15:07,164 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModelForTokenClassification.forward` and have been ignored: text. If text are not expected by `PeftModelForTokenClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 22:15:07,165 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 22:15:07,166 >>   Num examples = 760
[INFO|trainer.py:3134] 2023-11-15 22:15:07,166 >>   Batch size = 8
{'eval_loss': 0.26805463433265686, 'eval_accuracy': 0.9131578947368421, 'eval_micro_f1': 0.9131578947368421, 'eval_macro_f1': 0.9107531562109016, 'eval_runtime': 0.9276, 'eval_samples_per_second': 819.294, 'eval_steps_per_second': 102.412, 'epoch': 4.0}
{'loss': 0.1586, 'learning_rate': 0.0, 'epoch': 5.0}

  0%|          | 0/95 [00:00<?, ?it/s][A
 13%|â–ˆâ–Ž        | 12/95 [00:00<00:00, 116.08it/s][A
 25%|â–ˆâ–ˆâ–Œ       | 24/95 [00:00<00:00, 109.93it/s][A
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 36/95 [00:00<00:00, 108.00it/s][A
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 47/95 [00:00<00:00, 107.20it/s][A
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 58/95 [00:00<00:00, 106.74it/s][A
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 69/95 [00:00<00:00, 106.46it/s][A
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 80/95 [00:00<00:00, 106.21it/s][A
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 91/95 [00:00<00:00, 106.11it/s][A                                                   
                                                [A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1070/1070 [01:20<00:00, 14.22it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 95/95 [00:00<00:00, 106.11it/s][A
                                                [A[INFO|trainer.py:1963] 2023-11-15 22:15:08,098 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                   100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1070/1070 [01:20<00:00, 14.22it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1070/1070 [01:20<00:00, 13.28it/s]
[INFO|trainer.py:2855] 2023-11-15 22:15:08,101 >> Saving model checkpoint to ./result/agnews_sup_roberta-base_seed2_lora
[INFO|tokenization_utils_base.py:2235] 2023-11-15 22:15:08,211 >> tokenizer config file saved in ./result/agnews_sup_roberta-base_seed2_lora/tokenizer_config.json
[INFO|tokenization_utils_base.py:2242] 2023-11-15 22:15:08,213 >> Special tokens file saved in ./result/agnews_sup_roberta-base_seed2_lora/special_tokens_map.json
{'eval_loss': 0.29138389229774475, 'eval_accuracy': 0.9144736842105263, 'eval_micro_f1': 0.9144736842105263, 'eval_macro_f1': 0.9120615829713433, 'eval_runtime': 0.9287, 'eval_samples_per_second': 818.379, 'eval_steps_per_second': 102.297, 'epoch': 5.0}
{'train_runtime': 80.5932, 'train_samples_per_second': 424.353, 'train_steps_per_second': 13.277, 'train_loss': 0.25380944225275626, 'epoch': 5.0}
***** train metrics *****
  epoch                    =        5.0
  train_loss               =     0.2538
  train_runtime            = 0:01:20.59
  train_samples            =       6840
  train_samples_per_second =    424.353
  train_steps_per_second   =     13.277
11/15/2023 22:15:08 - INFO - __main__ - *** Evaluate ***
[INFO|trainer.py:755] 2023-11-15 22:15:08,317 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModelForTokenClassification.forward` and have been ignored: text. If text are not expected by `PeftModelForTokenClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 22:15:08,319 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 22:15:08,319 >>   Num examples = 760
[INFO|trainer.py:3134] 2023-11-15 22:15:08,319 >>   Batch size = 8
  0%|          | 0/95 [00:00<?, ?it/s] 13%|â–ˆâ–Ž        | 12/95 [00:00<00:00, 116.87it/s] 25%|â–ˆâ–ˆâ–Œ       | 24/95 [00:00<00:00, 110.66it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 36/95 [00:00<00:00, 108.73it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 47/95 [00:00<00:00, 107.88it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 58/95 [00:00<00:00, 107.33it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 69/95 [00:00<00:00, 107.07it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 80/95 [00:00<00:00, 106.92it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 91/95 [00:00<00:00, 106.82it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 95/95 [00:00<00:00, 104.79it/s]
***** eval metrics *****
  epoch                   =        5.0
  eval_accuracy           =     0.9145
  eval_loss               =     0.2914
  eval_macro_f1           =     0.9121
  eval_micro_f1           =     0.9145
  eval_runtime            = 0:00:00.91
  eval_samples            =        760
  eval_samples_per_second =    826.305
  eval_steps_per_second   =    103.288
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                  eval/accuracy â–â–„â–ƒâ–‡â–ˆâ–ˆ
wandb:                      eval/loss â–ˆâ–ƒâ–‡â–â–‡â–‡
wandb:                  eval/macro_f1 â–â–…â–„â–‡â–ˆâ–ˆ
wandb:                  eval/micro_f1 â–â–„â–ƒâ–‡â–ˆâ–ˆ
wandb:                   eval/runtime â–â–ˆâ–‡â–‡â–ˆâ–
wandb:        eval/samples_per_second â–ˆâ–â–‚â–‚â–â–ˆ
wandb:          eval/steps_per_second â–ˆâ–â–‚â–‚â–â–ˆ
wandb:                    train/epoch â–â–â–ƒâ–ƒâ–…â–…â–†â–†â–ˆâ–ˆâ–ˆâ–ˆ
wandb:              train/global_step â–â–â–ƒâ–ƒâ–„â–„â–†â–†â–ˆâ–ˆâ–ˆâ–ˆ
wandb:            train/learning_rate â–ˆâ–†â–…â–ƒâ–
wandb:                     train/loss â–ˆâ–„â–ƒâ–‚â–
wandb:               train/total_flos â–
wandb:               train/train_loss â–
wandb:            train/train_runtime â–
wandb: train/train_samples_per_second â–
wandb:   train/train_steps_per_second â–
wandb: 
wandb: Run summary:
wandb:                  eval/accuracy 0.91447
wandb:                      eval/loss 0.29138
wandb:                  eval/macro_f1 0.91206
wandb:                  eval/micro_f1 0.91447
wandb:                   eval/runtime 0.9198
wandb:        eval/samples_per_second 826.305
wandb:          eval/steps_per_second 103.288
wandb:                    train/epoch 5.0
wandb:              train/global_step 1070
wandb:            train/learning_rate 0.0
wandb:                     train/loss 0.1586
wandb:               train/total_flos 1140362523648000.0
wandb:               train/train_loss 0.25381
wandb:            train/train_runtime 80.5932
wandb: train/train_samples_per_second 424.353
wandb:   train/train_steps_per_second 13.277
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /ceph/home/wangyifei/a3/wandb/offline-run-20231115_221230-dzwcr0dh
wandb: Find logs at: ./wandb/offline-run-20231115_221230-dzwcr0dh/logs
(ModelArguments(model_name_or_path='roberta-base', cache_dir='./cache', use_adapter=False, use_lora=True), DataTrainingArguments(dataset_name='restaurant', max_seq_length=64), TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.0005,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=./result/restaurant_roberta-base_seed3_lora/runs/Nov15_22-15-19_node010,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.EPOCH,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=5.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
output_dir=./result/restaurant_roberta-base_seed3_lora,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
resume_from_checkpoint=None,
run_name=./result/restaurant_roberta-base_seed3_lora,
save_on_each_node=False,
save_safetensors=False,
save_steps=500,
save_strategy=IntervalStrategy.NO,
save_total_limit=None,
seed=444,
sharded_ddp=[],
skip_memory_metrics=True,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
))
11/15/2023 22:15:19 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False
11/15/2023 22:15:19 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.0005,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=./result/restaurant_roberta-base_seed3_lora/runs/Nov15_22-15-18_node010,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.EPOCH,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=5.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
output_dir=./result/restaurant_roberta-base_seed3_lora,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
resume_from_checkpoint=None,
run_name=./result/restaurant_roberta-base_seed3_lora,
save_on_each_node=False,
save_safetensors=False,
save_steps=500,
save_strategy=IntervalStrategy.NO,
save_total_limit=None,
seed=444,
sharded_ddp=[],
skip_memory_metrics=True,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
wandb: Tracking run with wandb version 0.15.12
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
Map:   0%|          | 0/4722 [00:00<?, ? examples/s]Map:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4097/4722 [00:00<00:00, 40720.63 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4722/4722 [00:00<00:00, 40310.15 examples/s]
[INFO|configuration_utils.py:715] 2023-11-15 22:15:34,999 >> loading configuration file config.json from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/config.json
[INFO|configuration_utils.py:775] 2023-11-15 22:15:35,009 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.33.3",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_auto.py:535] 2023-11-15 22:15:45,024 >> Could not locate the tokenizer configuration file, will try to use the model config instead.
[INFO|configuration_utils.py:715] 2023-11-15 22:15:55,041 >> loading configuration file config.json from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/config.json
[INFO|configuration_utils.py:775] 2023-11-15 22:15:55,042 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.33.3",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:1852] 2023-11-15 22:16:15,090 >> loading file vocab.json from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/vocab.json
[INFO|tokenization_utils_base.py:1852] 2023-11-15 22:16:15,090 >> loading file merges.txt from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/merges.txt
[INFO|tokenization_utils_base.py:1852] 2023-11-15 22:16:15,090 >> loading file tokenizer.json from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/tokenizer.json
[INFO|tokenization_utils_base.py:1852] 2023-11-15 22:16:15,091 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1852] 2023-11-15 22:16:15,091 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1852] 2023-11-15 22:16:15,091 >> loading file tokenizer_config.json from cache at None
[INFO|configuration_utils.py:715] 2023-11-15 22:16:15,093 >> loading configuration file config.json from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/config.json
[INFO|configuration_utils.py:775] 2023-11-15 22:16:15,093 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.33.3",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:2862] 2023-11-15 22:16:35,303 >> loading weights file model.safetensors from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/model.safetensors
[INFO|modeling_utils.py:3638] 2023-11-15 22:16:36,002 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3650] 2023-11-15 22:16:36,003 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
trainable params: 1,284,867 || all params: 125,830,662 || trainable%: 1.0211080348603745
model_max_length: 512
Running tokenizer on dataset:   0%|          | 0/3777 [00:00<?, ? examples/s]Running tokenizer on dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3777/3777 [00:00<00:00, 24181.14 examples/s]Running tokenizer on dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3777/3777 [00:00<00:00, 23902.83 examples/s]
Running tokenizer on dataset:   0%|          | 0/945 [00:00<?, ? examples/s]Running tokenizer on dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 945/945 [00:00<00:00, 29375.80 examples/s]
11/15/2023 22:16:36 - INFO - __main__ - Sample 1265 of the training set: {'text': 'cuisine <SEP> I love when restaurants think using fancy expensive ingrediants makes the food fine cuisine, even with no idea how to use them.', 'label': 0, 'input_ids': [0, 16312, 40116, 28696, 3388, 510, 15698, 38, 657, 77, 4329, 206, 634, 13185, 3214, 49567, 40679, 817, 5, 689, 2051, 18196, 6, 190, 19, 117, 1114, 141, 7, 304, 106, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
11/15/2023 22:16:36 - INFO - __main__ - Sample 1178 of the training set: {'text': "table <SEP> I went with 5 friends and we lingered at the table for a bit and didn't feel rushed at all even though there was a wait.", 'label': 1, 'input_ids': [0, 14595, 28696, 3388, 510, 15698, 38, 439, 19, 195, 964, 8, 52, 24433, 3215, 23, 5, 2103, 13, 10, 828, 8, 399, 75, 619, 6022, 23, 70, 190, 600, 89, 21, 10, 2067, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
11/15/2023 22:16:36 - INFO - __main__ - Sample 54 of the training set: {'text': "Halibut <SEP> The Halibut was too salty, dessert was so so (don't waste any of your calories) and service was poor.", 'label': 2, 'input_ids': [0, 40306, 1452, 1182, 28696, 3388, 510, 15698, 20, 6579, 1452, 1182, 21, 350, 31924, 6, 17927, 21, 98, 98, 36, 7254, 75, 3844, 143, 9, 110, 12951, 43, 8, 544, 21, 2129, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
11/15/2023 22:16:36 - WARNING - accelerate.utils.other - Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[INFO|trainer.py:755] 2023-11-15 22:16:37,287 >> The following columns in the training set don't have a corresponding argument in `PeftModelForTokenClassification.forward` and have been ignored: text. If text are not expected by `PeftModelForTokenClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:1715] 2023-11-15 22:16:37,297 >> ***** Running training *****
[INFO|trainer.py:1716] 2023-11-15 22:16:37,297 >>   Num examples = 3,777
[INFO|trainer.py:1717] 2023-11-15 22:16:37,298 >>   Num Epochs = 5
[INFO|trainer.py:1718] 2023-11-15 22:16:37,298 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1721] 2023-11-15 22:16:37,298 >>   Total train batch size (w. parallel, distributed & accumulation) = 32
[INFO|trainer.py:1722] 2023-11-15 22:16:37,299 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:1723] 2023-11-15 22:16:37,299 >>   Total optimization steps = 595
[INFO|trainer.py:1724] 2023-11-15 22:16:37,300 >>   Number of trainable parameters = 1,284,867
[INFO|integration_utils.py:716] 2023-11-15 22:16:37,301 >> Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"
  0%|          | 0/595 [00:00<?, ?it/s]  0%|          | 1/595 [00:00<09:46,  1.01it/s]  1%|          | 3/595 [00:01<03:02,  3.24it/s]  1%|          | 5/595 [00:01<01:49,  5.39it/s]  1%|          | 7/595 [00:01<01:19,  7.35it/s]  2%|â–         | 9/595 [00:01<01:04,  9.04it/s]  2%|â–         | 11/595 [00:01<00:56, 10.41it/s]  2%|â–         | 13/595 [00:01<00:50, 11.48it/s]  3%|â–Ž         | 15/595 [00:01<00:47, 12.20it/s]  3%|â–Ž         | 17/595 [00:02<00:45, 12.84it/s]  3%|â–Ž         | 19/595 [00:02<00:43, 13.30it/s]  4%|â–Ž         | 21/595 [00:02<00:42, 13.60it/s]  4%|â–         | 23/595 [00:02<00:41, 13.85it/s]  4%|â–         | 25/595 [00:02<00:40, 14.04it/s]  5%|â–         | 27/595 [00:02<00:40, 14.13it/s]  5%|â–         | 29/595 [00:02<00:39, 14.24it/s]  5%|â–Œ         | 31/595 [00:03<00:39, 14.29it/s]  6%|â–Œ         | 33/595 [00:03<00:39, 14.33it/s]  6%|â–Œ         | 35/595 [00:03<00:38, 14.36it/s]  6%|â–Œ         | 37/595 [00:03<00:38, 14.37it/s]  7%|â–‹         | 39/595 [00:03<00:38, 14.40it/s]  7%|â–‹         | 41/595 [00:03<00:38, 14.43it/s]  7%|â–‹         | 43/595 [00:03<00:38, 14.46it/s]  8%|â–Š         | 45/595 [00:04<00:38, 14.44it/s]  8%|â–Š         | 47/595 [00:04<00:37, 14.43it/s]  8%|â–Š         | 49/595 [00:04<00:37, 14.43it/s]  9%|â–Š         | 51/595 [00:04<00:37, 14.43it/s]  9%|â–‰         | 53/595 [00:04<00:37, 14.44it/s]  9%|â–‰         | 55/595 [00:04<00:37, 14.46it/s] 10%|â–‰         | 57/595 [00:04<00:37, 14.45it/s] 10%|â–‰         | 59/595 [00:05<00:37, 14.44it/s] 10%|â–ˆ         | 61/595 [00:05<00:36, 14.43it/s] 11%|â–ˆ         | 63/595 [00:05<00:36, 14.43it/s] 11%|â–ˆ         | 65/595 [00:05<00:36, 14.43it/s] 11%|â–ˆâ–        | 67/595 [00:05<00:36, 14.41it/s] 12%|â–ˆâ–        | 69/595 [00:05<00:36, 14.44it/s] 12%|â–ˆâ–        | 71/595 [00:05<00:36, 14.46it/s] 12%|â–ˆâ–        | 73/595 [00:05<00:36, 14.44it/s] 13%|â–ˆâ–Ž        | 75/595 [00:06<00:36, 14.43it/s] 13%|â–ˆâ–Ž        | 77/595 [00:06<00:35, 14.42it/s] 13%|â–ˆâ–Ž        | 79/595 [00:06<00:35, 14.42it/s] 14%|â–ˆâ–Ž        | 81/595 [00:06<00:35, 14.42it/s] 14%|â–ˆâ–        | 83/595 [00:06<00:35, 14.43it/s] 14%|â–ˆâ–        | 85/595 [00:06<00:35, 14.44it/s] 15%|â–ˆâ–        | 87/595 [00:06<00:35, 14.40it/s] 15%|â–ˆâ–        | 89/595 [00:07<00:35, 14.38it/s] 15%|â–ˆâ–Œ        | 91/595 [00:07<00:35, 14.39it/s] 16%|â–ˆâ–Œ        | 93/595 [00:07<00:34, 14.38it/s] 16%|â–ˆâ–Œ        | 95/595 [00:07<00:34, 14.39it/s] 16%|â–ˆâ–‹        | 97/595 [00:07<00:34, 14.39it/s] 17%|â–ˆâ–‹        | 99/595 [00:07<00:34, 14.39it/s] 17%|â–ˆâ–‹        | 101/595 [00:07<00:34, 14.41it/s] 17%|â–ˆâ–‹        | 103/595 [00:08<00:34, 14.41it/s] 18%|â–ˆâ–Š        | 105/595 [00:08<00:34, 14.39it/s] 18%|â–ˆâ–Š        | 107/595 [00:08<00:33, 14.42it/s] 18%|â–ˆâ–Š        | 109/595 [00:08<00:33, 14.41it/s] 19%|â–ˆâ–Š        | 111/595 [00:08<00:33, 14.40it/s] 19%|â–ˆâ–‰        | 113/595 [00:08<00:33, 14.39it/s] 19%|â–ˆâ–‰        | 115/595 [00:08<00:33, 14.41it/s] 20%|â–ˆâ–‰        | 117/595 [00:09<00:33, 14.41it/s]                                                  20%|â–ˆâ–ˆ        | 119/595 [00:09<00:33, 14.41it/s][INFO|trainer.py:755] 2023-11-15 22:16:46,438 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModelForTokenClassification.forward` and have been ignored: text. If text are not expected by `PeftModelForTokenClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 22:16:46,439 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 22:16:46,439 >>   Num examples = 945
[INFO|trainer.py:3134] 2023-11-15 22:16:46,440 >>   Batch size = 8
{'loss': 0.7154, 'learning_rate': 0.0004, 'epoch': 1.0}

  0%|          | 0/119 [00:00<?, ?it/s][A
 10%|â–ˆ         | 12/119 [00:00<00:00, 119.76it/s][A
 20%|â–ˆâ–ˆ        | 24/119 [00:00<00:00, 112.94it/s][A
 30%|â–ˆâ–ˆâ–ˆ       | 36/119 [00:00<00:00, 111.02it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 48/119 [00:00<00:00, 110.03it/s][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 60/119 [00:00<00:00, 108.48it/s][A
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 71/119 [00:00<00:00, 108.53it/s][A
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 82/119 [00:00<00:00, 108.35it/s][A
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 93/119 [00:00<00:00, 108.40it/s][A
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 104/119 [00:00<00:00, 108.25it/s][A
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 115/119 [00:01<00:00, 108.28it/s][A                                                 
                                                  [A 20%|â–ˆâ–ˆ        | 119/595 [00:10<00:33, 14.41it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 119/119 [00:01<00:00, 108.28it/s][A
                                                  [A 20%|â–ˆâ–ˆ        | 120/595 [00:10<01:41,  4.70it/s] 21%|â–ˆâ–ˆ        | 122/595 [00:10<01:22,  5.74it/s] 21%|â–ˆâ–ˆ        | 124/595 [00:10<01:08,  6.91it/s] 21%|â–ˆâ–ˆ        | 126/595 [00:10<00:57,  8.12it/s] 22%|â–ˆâ–ˆâ–       | 128/595 [00:10<00:50,  9.30it/s] 22%|â–ˆâ–ˆâ–       | 130/595 [00:11<00:44, 10.38it/s] 22%|â–ˆâ–ˆâ–       | 132/595 [00:11<00:40, 11.32it/s] 23%|â–ˆâ–ˆâ–Ž       | 134/595 [00:11<00:38, 12.11it/s] 23%|â–ˆâ–ˆâ–Ž       | 136/595 [00:11<00:36, 12.74it/s] 23%|â–ˆâ–ˆâ–Ž       | 138/595 [00:11<00:34, 13.20it/s] 24%|â–ˆâ–ˆâ–Ž       | 140/595 [00:11<00:33, 13.55it/s] 24%|â–ˆâ–ˆâ–       | 142/595 [00:11<00:32, 13.80it/s] 24%|â–ˆâ–ˆâ–       | 144/595 [00:12<00:32, 13.99it/s] 25%|â–ˆâ–ˆâ–       | 146/595 [00:12<00:31, 14.14it/s] 25%|â–ˆâ–ˆâ–       | 148/595 [00:12<00:31, 14.26it/s] 25%|â–ˆâ–ˆâ–Œ       | 150/595 [00:12<00:31, 14.33it/s] 26%|â–ˆâ–ˆâ–Œ       | 152/595 [00:12<00:30, 14.36it/s] 26%|â–ˆâ–ˆâ–Œ       | 154/595 [00:12<00:30, 14.39it/s] 26%|â–ˆâ–ˆâ–Œ       | 156/595 [00:12<00:30, 14.42it/s] 27%|â–ˆâ–ˆâ–‹       | 158/595 [00:12<00:30, 14.43it/s] 27%|â–ˆâ–ˆâ–‹       | 160/595 [00:13<00:30, 14.46it/s] 27%|â–ˆâ–ˆâ–‹       | 162/595 [00:13<00:29, 14.46it/s] 28%|â–ˆâ–ˆâ–Š       | 164/595 [00:13<00:29, 14.45it/s] 28%|â–ˆâ–ˆâ–Š       | 166/595 [00:13<00:29, 14.41it/s] 28%|â–ˆâ–ˆâ–Š       | 168/595 [00:13<00:29, 14.42it/s] 29%|â–ˆâ–ˆâ–Š       | 170/595 [00:13<00:29, 14.44it/s] 29%|â–ˆâ–ˆâ–‰       | 172/595 [00:13<00:29, 14.47it/s] 29%|â–ˆâ–ˆâ–‰       | 174/595 [00:14<00:29, 14.48it/s] 30%|â–ˆâ–ˆâ–‰       | 176/595 [00:14<00:28, 14.45it/s] 30%|â–ˆâ–ˆâ–‰       | 178/595 [00:14<00:28, 14.44it/s] 30%|â–ˆâ–ˆâ–ˆ       | 180/595 [00:14<00:28, 14.45it/s] 31%|â–ˆâ–ˆâ–ˆ       | 182/595 [00:14<00:28, 14.47it/s] 31%|â–ˆâ–ˆâ–ˆ       | 184/595 [00:14<00:28, 14.48it/s] 31%|â–ˆâ–ˆâ–ˆâ–      | 186/595 [00:14<00:28, 14.47it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 188/595 [00:15<00:28, 14.46it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 190/595 [00:15<00:28, 14.46it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 192/595 [00:15<00:27, 14.46it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 194/595 [00:15<00:27, 14.46it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 196/595 [00:15<00:27, 14.45it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 198/595 [00:15<00:27, 14.48it/s] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 200/595 [00:15<00:27, 14.46it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 202/595 [00:16<00:27, 14.45it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 204/595 [00:16<00:27, 14.46it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 206/595 [00:16<00:26, 14.44it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 208/595 [00:16<00:26, 14.46it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 210/595 [00:16<00:26, 14.46it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 212/595 [00:16<00:26, 14.45it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 214/595 [00:16<00:26, 14.42it/s] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 216/595 [00:16<00:26, 14.42it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 218/595 [00:17<00:26, 14.40it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 220/595 [00:17<00:26, 14.41it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 222/595 [00:17<00:25, 14.43it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 224/595 [00:17<00:25, 14.46it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 226/595 [00:17<00:25, 14.47it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 228/595 [00:17<00:25, 14.45it/s] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 230/595 [00:17<00:25, 14.45it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 232/595 [00:18<00:25, 14.44it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 234/595 [00:18<00:24, 14.46it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 236/595 [00:18<00:24, 14.47it/s]                                                  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 238/595 [00:18<00:24, 14.47it/s][INFO|trainer.py:755] 2023-11-15 22:16:55,767 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModelForTokenClassification.forward` and have been ignored: text. If text are not expected by `PeftModelForTokenClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 22:16:55,768 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 22:16:55,769 >>   Num examples = 945
[INFO|trainer.py:3134] 2023-11-15 22:16:55,769 >>   Batch size = 8
{'eval_loss': 0.554571807384491, 'eval_accuracy': 0.780952380952381, 'eval_micro_f1': 0.780952380952381, 'eval_macro_f1': 0.6862528067229913, 'eval_runtime': 1.1312, 'eval_samples_per_second': 835.396, 'eval_steps_per_second': 105.198, 'epoch': 1.0}
{'loss': 0.4977, 'learning_rate': 0.0003, 'epoch': 2.0}

  0%|          | 0/119 [00:00<?, ?it/s][A
 11%|â–ˆ         | 13/119 [00:00<00:00, 119.47it/s][A
 21%|â–ˆâ–ˆ        | 25/119 [00:00<00:00, 113.30it/s][A
 31%|â–ˆâ–ˆâ–ˆ       | 37/119 [00:00<00:00, 111.37it/s][A
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 49/119 [00:00<00:00, 110.45it/s][A
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 61/119 [00:00<00:00, 109.73it/s][A
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 72/119 [00:00<00:00, 109.26it/s][A
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 83/119 [00:00<00:00, 109.19it/s][A
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 94/119 [00:00<00:00, 108.87it/s][A
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 105/119 [00:00<00:00, 108.87it/s][A
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 116/119 [00:01<00:00, 108.80it/s][A                                                 
                                                  [A 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 238/595 [00:19<00:24, 14.47it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 119/119 [00:01<00:00, 108.80it/s][A
                                                  [A 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 239/595 [00:19<01:15,  4.73it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 241/595 [00:19<01:01,  5.78it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 243/595 [00:19<00:50,  6.94it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 245/595 [00:20<00:42,  8.15it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 247/595 [00:20<00:37,  9.33it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 249/595 [00:20<00:33, 10.40it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 251/595 [00:20<00:30, 11.35it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 253/595 [00:20<00:28, 12.13it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 255/595 [00:20<00:26, 12.74it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 257/595 [00:20<00:25, 13.19it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 259/595 [00:21<00:24, 13.53it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 261/595 [00:21<00:24, 13.79it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 263/595 [00:21<00:23, 13.98it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 265/595 [00:21<00:23, 14.12it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 267/595 [00:21<00:23, 14.21it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 269/595 [00:21<00:22, 14.26it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 271/595 [00:21<00:22, 14.30it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 273/595 [00:22<00:22, 14.33it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 275/595 [00:22<00:22, 14.34it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 277/595 [00:22<00:22, 14.37it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 279/595 [00:22<00:22, 14.35it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 281/595 [00:22<00:21, 14.35it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 283/595 [00:22<00:21, 14.37it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 285/595 [00:22<00:21, 14.40it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 287/595 [00:22<00:21, 14.43it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 289/595 [00:23<00:21, 14.43it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 291/595 [00:23<00:21, 14.37it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 293/595 [00:23<00:21, 14.38it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 295/595 [00:23<00:20, 14.38it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 297/595 [00:23<00:20, 14.39it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 299/595 [00:23<00:20, 14.40it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 301/595 [00:23<00:20, 14.41it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 303/595 [00:24<00:20, 14.43it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 305/595 [00:24<00:20, 14.42it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 307/595 [00:24<00:19, 14.41it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 309/595 [00:24<00:19, 14.41it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 311/595 [00:24<00:19, 14.39it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 313/595 [00:24<00:19, 14.38it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 315/595 [00:24<00:19, 14.40it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 317/595 [00:25<00:19, 14.39it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 319/595 [00:25<00:19, 14.40it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 321/595 [00:25<00:18, 14.43it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 323/595 [00:25<00:18, 14.45it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 325/595 [00:25<00:18, 14.43it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 327/595 [00:25<00:18, 14.40it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 329/595 [00:25<00:18, 14.40it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 331/595 [00:26<00:18, 14.41it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 333/595 [00:26<00:18, 14.42it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 335/595 [00:26<00:18, 14.43it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 337/595 [00:26<00:17, 14.42it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 339/595 [00:26<00:17, 14.41it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 341/595 [00:26<00:17, 14.40it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 343/595 [00:26<00:17, 14.37it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 345/595 [00:27<00:17, 14.37it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 347/595 [00:27<00:17, 14.36it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 349/595 [00:27<00:17, 14.34it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 351/595 [00:27<00:17, 14.34it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 353/595 [00:27<00:16, 14.35it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 355/595 [00:27<00:16, 14.36it/s]                                                  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 357/595 [00:27<00:16, 14.36it/s][INFO|trainer.py:755] 2023-11-15 22:17:05,119 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModelForTokenClassification.forward` and have been ignored: text. If text are not expected by `PeftModelForTokenClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 22:17:05,120 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 22:17:05,121 >>   Num examples = 945
[INFO|trainer.py:3134] 2023-11-15 22:17:05,121 >>   Batch size = 8
{'eval_loss': 0.45989131927490234, 'eval_accuracy': 0.8095238095238095, 'eval_micro_f1': 0.8095238095238095, 'eval_macro_f1': 0.7398927730592421, 'eval_runtime': 1.1253, 'eval_samples_per_second': 839.805, 'eval_steps_per_second': 105.753, 'epoch': 2.0}
{'loss': 0.4227, 'learning_rate': 0.0002, 'epoch': 3.0}

  0%|          | 0/119 [00:00<?, ?it/s][A
 10%|â–ˆ         | 12/119 [00:00<00:00, 119.76it/s][A
 20%|â–ˆâ–ˆ        | 24/119 [00:00<00:00, 112.99it/s][A
 30%|â–ˆâ–ˆâ–ˆ       | 36/119 [00:00<00:00, 110.95it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 48/119 [00:00<00:00, 110.03it/s][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 60/119 [00:00<00:00, 109.14it/s][A
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 71/119 [00:00<00:00, 108.91it/s][A
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 82/119 [00:00<00:00, 108.50it/s][A
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 93/119 [00:00<00:00, 108.43it/s][A
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 104/119 [00:00<00:00, 108.28it/s][A
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 115/119 [00:01<00:00, 108.27it/s][A                                                 
                                                  [A 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 357/595 [00:28<00:16, 14.36it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 119/119 [00:01<00:00, 108.27it/s][A
                                                  [A 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 358/595 [00:29<00:50,  4.71it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 360/595 [00:29<00:40,  5.76it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 362/595 [00:29<00:33,  6.91it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 364/595 [00:29<00:28,  8.11it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 366/595 [00:29<00:24,  9.28it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 368/595 [00:29<00:21, 10.34it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 370/595 [00:29<00:19, 11.25it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 372/595 [00:30<00:18, 12.01it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 374/595 [00:30<00:17, 12.62it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 376/595 [00:30<00:16, 13.08it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 378/595 [00:30<00:16, 13.43it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 380/595 [00:30<00:15, 13.69it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 382/595 [00:30<00:15, 13.86it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 384/595 [00:30<00:15, 14.01it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 386/595 [00:30<00:14, 14.11it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 388/595 [00:31<00:14, 14.19it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 390/595 [00:31<00:14, 14.25it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 392/595 [00:31<00:14, 14.27it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 394/595 [00:31<00:14, 14.26it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 396/595 [00:31<00:13, 14.29it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 398/595 [00:31<00:13, 14.32it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 400/595 [00:31<00:13, 14.36it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 402/595 [00:32<00:13, 14.39it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 404/595 [00:32<00:13, 14.38it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 406/595 [00:32<00:13, 14.36it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 408/595 [00:32<00:13, 14.36it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 410/595 [00:32<00:12, 14.36it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 412/595 [00:32<00:12, 14.35it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 414/595 [00:32<00:12, 14.34it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 416/595 [00:33<00:12, 14.34it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 418/595 [00:33<00:12, 14.35it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 420/595 [00:33<00:12, 14.36it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 422/595 [00:33<00:12, 14.36it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 424/595 [00:33<00:11, 14.37it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 426/595 [00:33<00:11, 14.38it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 428/595 [00:33<00:11, 14.39it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 430/595 [00:34<00:11, 14.38it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 432/595 [00:34<00:11, 14.37it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 434/595 [00:34<00:11, 14.36it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 436/595 [00:34<00:11, 14.35it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 438/595 [00:34<00:10, 14.33it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 440/595 [00:34<00:10, 14.32it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 442/595 [00:34<00:10, 14.32it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 444/595 [00:35<00:10, 14.32it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 446/595 [00:35<00:10, 14.31it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 448/595 [00:35<00:10, 14.31it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 450/595 [00:35<00:10, 14.32it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 452/595 [00:35<00:09, 14.33it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 454/595 [00:35<00:09, 14.34it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 456/595 [00:35<00:09, 14.34it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 458/595 [00:35<00:09, 14.35it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 460/595 [00:36<00:09, 14.36it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 462/595 [00:36<00:09, 14.37it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 464/595 [00:36<00:09, 14.39it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 466/595 [00:36<00:08, 14.40it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 468/595 [00:36<00:08, 14.40it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 470/595 [00:36<00:08, 14.38it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 472/595 [00:36<00:08, 14.32it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 474/595 [00:37<00:08, 14.34it/s]                                                  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 476/595 [00:37<00:08, 14.34it/s][INFO|trainer.py:755] 2023-11-15 22:17:14,510 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModelForTokenClassification.forward` and have been ignored: text. If text are not expected by `PeftModelForTokenClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 22:17:14,512 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 22:17:14,512 >>   Num examples = 945
[INFO|trainer.py:3134] 2023-11-15 22:17:14,512 >>   Batch size = 8
{'eval_loss': 0.43941494822502136, 'eval_accuracy': 0.8243386243386244, 'eval_micro_f1': 0.8243386243386244, 'eval_macro_f1': 0.7363471375384399, 'eval_runtime': 1.1297, 'eval_samples_per_second': 836.475, 'eval_steps_per_second': 105.334, 'epoch': 3.0}
{'loss': 0.3425, 'learning_rate': 0.0001, 'epoch': 4.0}

  0%|          | 0/119 [00:00<?, ?it/s][A
 10%|â–ˆ         | 12/119 [00:00<00:00, 118.29it/s][A
 20%|â–ˆâ–ˆ        | 24/119 [00:00<00:00, 112.23it/s][A
 30%|â–ˆâ–ˆâ–ˆ       | 36/119 [00:00<00:00, 109.92it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 48/119 [00:00<00:00, 109.35it/s][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 59/119 [00:00<00:00, 108.88it/s][A
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 70/119 [00:00<00:00, 108.62it/s][A
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 81/119 [00:00<00:00, 108.40it/s][A
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 92/119 [00:00<00:00, 108.25it/s][A
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 103/119 [00:00<00:00, 108.04it/s][A
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 114/119 [00:01<00:00, 107.92it/s][A                                                 
                                                  [A 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 476/595 [00:38<00:08, 14.34it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 119/119 [00:01<00:00, 107.92it/s][A
                                                  [A 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 477/595 [00:38<00:25,  4.69it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 479/595 [00:38<00:20,  5.73it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 481/595 [00:38<00:16,  6.88it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 483/595 [00:38<00:13,  8.08it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 485/595 [00:38<00:11,  9.25it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 487/595 [00:39<00:10, 10.31it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 489/595 [00:39<00:09, 11.24it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 491/595 [00:39<00:08, 12.01it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 493/595 [00:39<00:08, 12.61it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 495/595 [00:39<00:07, 13.08it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 497/595 [00:39<00:07, 13.42it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 499/595 [00:39<00:07, 13.68it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 501/595 [00:40<00:06, 13.87it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 503/595 [00:40<00:06, 13.97it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 505/595 [00:40<00:06, 14.08it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 507/595 [00:40<00:06, 14.14it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 509/595 [00:40<00:06, 14.21it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 511/595 [00:40<00:05, 14.24it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 513/595 [00:40<00:05, 14.27it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 515/595 [00:41<00:05, 14.29it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 517/595 [00:41<00:05, 14.30it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 519/595 [00:41<00:05, 14.32it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 521/595 [00:41<00:05, 14.35it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 523/595 [00:41<00:05, 14.36it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 525/595 [00:41<00:04, 14.34it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 527/595 [00:41<00:04, 14.36it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 529/595 [00:42<00:04, 14.37it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 531/595 [00:42<00:04, 14.37it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 533/595 [00:42<00:04, 14.36it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 535/595 [00:42<00:04, 14.34it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 537/595 [00:42<00:04, 14.32it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 539/595 [00:42<00:03, 14.30it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 541/595 [00:42<00:03, 14.29it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 543/595 [00:43<00:03, 14.31it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 545/595 [00:43<00:03, 14.31it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 547/595 [00:43<00:03, 14.30it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 549/595 [00:43<00:03, 14.31it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 551/595 [00:43<00:03, 14.30it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 553/595 [00:43<00:02, 14.30it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 555/595 [00:43<00:02, 14.30it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 557/595 [00:44<00:02, 14.31it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 559/595 [00:44<00:02, 14.31it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 561/595 [00:44<00:02, 14.30it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 563/595 [00:44<00:02, 14.29it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 565/595 [00:44<00:02, 14.30it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 567/595 [00:44<00:01, 14.28it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 569/595 [00:44<00:01, 14.27it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 571/595 [00:44<00:01, 14.27it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 573/595 [00:45<00:01, 14.29it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 575/595 [00:45<00:01, 14.29it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 577/595 [00:45<00:01, 14.28it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 579/595 [00:45<00:01, 14.28it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 581/595 [00:45<00:00, 14.29it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 583/595 [00:45<00:00, 14.29it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 585/595 [00:45<00:00, 14.26it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 587/595 [00:46<00:00, 14.29it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 589/595 [00:46<00:00, 14.26it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 591/595 [00:46<00:00, 14.25it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 593/595 [00:46<00:00, 14.29it/s]                                                 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 595/595 [00:46<00:00, 14.29it/s][INFO|trainer.py:755] 2023-11-15 22:17:23,928 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModelForTokenClassification.forward` and have been ignored: text. If text are not expected by `PeftModelForTokenClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 22:17:23,929 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 22:17:23,930 >>   Num examples = 945
[INFO|trainer.py:3134] 2023-11-15 22:17:23,930 >>   Batch size = 8
{'eval_loss': 0.40954452753067017, 'eval_accuracy': 0.8486772486772487, 'eval_micro_f1': 0.8486772486772488, 'eval_macro_f1': 0.7886029297775279, 'eval_runtime': 1.1343, 'eval_samples_per_second': 833.124, 'eval_steps_per_second': 104.912, 'epoch': 4.0}
{'loss': 0.3071, 'learning_rate': 0.0, 'epoch': 5.0}

  0%|          | 0/119 [00:00<?, ?it/s][A
 10%|â–ˆ         | 12/119 [00:00<00:00, 119.50it/s][A
 20%|â–ˆâ–ˆ        | 24/119 [00:00<00:00, 112.36it/s][A
 30%|â–ˆâ–ˆâ–ˆ       | 36/119 [00:00<00:00, 110.32it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 48/119 [00:00<00:00, 109.37it/s][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 59/119 [00:00<00:00, 108.89it/s][A
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 70/119 [00:00<00:00, 108.60it/s][A
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 81/119 [00:00<00:00, 108.47it/s][A
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 92/119 [00:00<00:00, 108.33it/s][A
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 103/119 [00:00<00:00, 107.92it/s][A
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 114/119 [00:01<00:00, 107.84it/s][A                                                 
                                                  [A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 595/595 [00:47<00:00, 14.29it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 119/119 [00:01<00:00, 107.84it/s][A
                                                  [A[INFO|trainer.py:1963] 2023-11-15 22:17:25,066 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 595/595 [00:47<00:00, 14.29it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 595/595 [00:47<00:00, 12.46it/s]
[INFO|trainer.py:2855] 2023-11-15 22:17:25,069 >> Saving model checkpoint to ./result/restaurant_roberta-base_seed3_lora
[INFO|tokenization_utils_base.py:2235] 2023-11-15 22:17:25,177 >> tokenizer config file saved in ./result/restaurant_roberta-base_seed3_lora/tokenizer_config.json
[INFO|tokenization_utils_base.py:2242] 2023-11-15 22:17:25,179 >> Special tokens file saved in ./result/restaurant_roberta-base_seed3_lora/special_tokens_map.json
{'eval_loss': 0.409467875957489, 'eval_accuracy': 0.8455026455026455, 'eval_micro_f1': 0.8455026455026455, 'eval_macro_f1': 0.7878748601400538, 'eval_runtime': 1.133, 'eval_samples_per_second': 834.094, 'eval_steps_per_second': 105.034, 'epoch': 5.0}
{'train_runtime': 47.7664, 'train_samples_per_second': 395.361, 'train_steps_per_second': 12.456, 'train_loss': 0.45708024481765364, 'epoch': 5.0}
***** train metrics *****
  epoch                    =        5.0
  train_loss               =     0.4571
  train_runtime            = 0:00:47.76
  train_samples            =       3777
  train_samples_per_second =    395.361
  train_steps_per_second   =     12.456
11/15/2023 22:17:25 - INFO - __main__ - *** Evaluate ***
[INFO|trainer.py:755] 2023-11-15 22:17:25,269 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModelForTokenClassification.forward` and have been ignored: text. If text are not expected by `PeftModelForTokenClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 22:17:25,270 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 22:17:25,270 >>   Num examples = 945
[INFO|trainer.py:3134] 2023-11-15 22:17:25,271 >>   Batch size = 8
  0%|          | 0/119 [00:00<?, ?it/s] 10%|â–ˆ         | 12/119 [00:00<00:00, 119.06it/s] 20%|â–ˆâ–ˆ        | 24/119 [00:00<00:00, 112.89it/s] 30%|â–ˆâ–ˆâ–ˆ       | 36/119 [00:00<00:00, 110.97it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 48/119 [00:00<00:00, 110.18it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 60/119 [00:00<00:00, 109.70it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 71/119 [00:00<00:00, 109.46it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 82/119 [00:00<00:00, 109.22it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 93/119 [00:00<00:00, 108.64it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 104/119 [00:00<00:00, 108.68it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 115/119 [00:01<00:00, 108.19it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 119/119 [00:01<00:00, 106.88it/s]
***** eval metrics *****
  epoch                   =        5.0
  eval_accuracy           =     0.8455
  eval_loss               =     0.4095
  eval_macro_f1           =     0.7879
  eval_micro_f1           =     0.8455
  eval_runtime            = 0:00:01.12
  eval_samples            =        945
  eval_samples_per_second =    839.316
  eval_steps_per_second   =    105.692
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                  eval/accuracy â–â–„â–…â–ˆâ–ˆâ–ˆ
wandb:                      eval/loss â–ˆâ–ƒâ–‚â–â–â–
wandb:                  eval/macro_f1 â–â–…â–„â–ˆâ–ˆâ–ˆ
wandb:                  eval/micro_f1 â–â–„â–…â–ˆâ–ˆâ–ˆ
wandb:                   eval/runtime â–†â–â–„â–ˆâ–‡â–
wandb:        eval/samples_per_second â–ƒâ–ˆâ–…â–â–‚â–‡
wandb:          eval/steps_per_second â–ƒâ–ˆâ–…â–â–‚â–‡
wandb:                    train/epoch â–â–â–ƒâ–ƒâ–…â–…â–†â–†â–ˆâ–ˆâ–ˆâ–ˆ
wandb:              train/global_step â–â–â–ƒâ–ƒâ–…â–…â–†â–†â–ˆâ–ˆâ–ˆâ–ˆ
wandb:            train/learning_rate â–ˆâ–†â–…â–ƒâ–
wandb:                     train/loss â–ˆâ–„â–ƒâ–‚â–
wandb:               train/total_flos â–
wandb:               train/train_loss â–
wandb:            train/train_runtime â–
wandb: train/train_samples_per_second â–
wandb:   train/train_steps_per_second â–
wandb: 
wandb: Run summary:
wandb:                  eval/accuracy 0.8455
wandb:                      eval/loss 0.40947
wandb:                  eval/macro_f1 0.78787
wandb:                  eval/micro_f1 0.8455
wandb:                   eval/runtime 1.1259
wandb:        eval/samples_per_second 839.316
wandb:          eval/steps_per_second 105.692
wandb:                    train/epoch 5.0
wandb:              train/global_step 595
wandb:            train/learning_rate 0.0
wandb:                     train/loss 0.3071
wandb:               train/total_flos 629689029684480.0
wandb:               train/train_loss 0.45708
wandb:            train/train_runtime 47.7664
wandb: train/train_samples_per_second 395.361
wandb:   train/train_steps_per_second 12.456
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /ceph/home/wangyifei/a3/wandb/offline-run-20231115_221520-udgpt7rk
wandb: Find logs at: ./wandb/offline-run-20231115_221520-udgpt7rk/logs
(ModelArguments(model_name_or_path='roberta-base', cache_dir='./cache', use_adapter=False, use_lora=True), DataTrainingArguments(dataset_name='acl', max_seq_length=64), TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.0005,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=./result/acl_roberta-base_seed3_lora/runs/Nov15_22-17-35_node010,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.EPOCH,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=5.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
output_dir=./result/acl_roberta-base_seed3_lora,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
resume_from_checkpoint=None,
run_name=./result/acl_roberta-base_seed3_lora,
save_on_each_node=False,
save_safetensors=False,
save_steps=500,
save_strategy=IntervalStrategy.NO,
save_total_limit=None,
seed=444,
sharded_ddp=[],
skip_memory_metrics=True,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
))
11/15/2023 22:17:35 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False
11/15/2023 22:17:35 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.0005,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=./result/acl_roberta-base_seed3_lora/runs/Nov15_22-17-35_node010,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.EPOCH,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=5.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
output_dir=./result/acl_roberta-base_seed3_lora,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
resume_from_checkpoint=None,
run_name=./result/acl_roberta-base_seed3_lora,
save_on_each_node=False,
save_safetensors=False,
save_steps=500,
save_strategy=IntervalStrategy.NO,
save_total_limit=None,
seed=444,
sharded_ddp=[],
skip_memory_metrics=True,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
wandb: Tracking run with wandb version 0.15.12
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
Map:   0%|          | 0/11020 [00:00<?, ? examples/s]Map:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 4150/11020 [00:00<00:00, 40578.43 examples/s]Map:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 8349/11020 [00:00<00:00, 41399.60 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11020/11020 [00:00<00:00, 40555.22 examples/s]
[INFO|configuration_utils.py:715] 2023-11-15 22:17:51,673 >> loading configuration file config.json from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/config.json
[INFO|configuration_utils.py:775] 2023-11-15 22:17:51,681 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.33.3",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_auto.py:535] 2023-11-15 22:18:01,697 >> Could not locate the tokenizer configuration file, will try to use the model config instead.
[INFO|configuration_utils.py:715] 2023-11-15 22:18:11,714 >> loading configuration file config.json from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/config.json
[INFO|configuration_utils.py:775] 2023-11-15 22:18:11,714 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.33.3",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:1852] 2023-11-15 22:18:31,761 >> loading file vocab.json from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/vocab.json
[INFO|tokenization_utils_base.py:1852] 2023-11-15 22:18:31,761 >> loading file merges.txt from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/merges.txt
[INFO|tokenization_utils_base.py:1852] 2023-11-15 22:18:31,762 >> loading file tokenizer.json from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/tokenizer.json
[INFO|tokenization_utils_base.py:1852] 2023-11-15 22:18:31,762 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1852] 2023-11-15 22:18:31,762 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1852] 2023-11-15 22:18:31,763 >> loading file tokenizer_config.json from cache at None
[INFO|configuration_utils.py:715] 2023-11-15 22:18:31,764 >> loading configuration file config.json from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/config.json
[INFO|configuration_utils.py:775] 2023-11-15 22:18:31,764 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.33.3",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:2862] 2023-11-15 22:18:51,932 >> loading weights file model.safetensors from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/model.safetensors
[INFO|modeling_utils.py:3638] 2023-11-15 22:18:52,631 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3650] 2023-11-15 22:18:52,631 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
trainable params: 1,284,867 || all params: 125,830,662 || trainable%: 1.0211080348603745
model_max_length: 512
Running tokenizer on dataset:   0%|          | 0/8816 [00:00<?, ? examples/s]Running tokenizer on dataset:  23%|â–ˆâ–ˆâ–Ž       | 2000/8816 [00:00<00:00, 18806.60 examples/s]Running tokenizer on dataset:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 6000/8816 [00:00<00:00, 20944.40 examples/s]Running tokenizer on dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8816/8816 [00:00<00:00, 21496.75 examples/s]Running tokenizer on dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8816/8816 [00:00<00:00, 21081.80 examples/s]
Running tokenizer on dataset:   0%|          | 0/2204 [00:00<?, ? examples/s]Running tokenizer on dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2204/2204 [00:00<00:00, 22823.10 examples/s]
11/15/2023 22:18:53 - INFO - __main__ - Sample 5060 of the training set: {'text': 'After GABA immunostaining, as elaborated in earlier studies (Domenici et al. 1988; Granda and Crossland 1989), the cell bodies, fibers, and numerous terminals showed GABA-like immunoreactivity in the Imc nucleus.', 'label': 0, 'input_ids': [0, 4993, 47644, 13998, 2603, 8173, 6, 25, 35838, 11, 656, 3218, 36, 495, 14900, 13850, 4400, 1076, 4, 11151, 131, 2974, 5219, 8, 4415, 1245, 10206, 238, 5, 3551, 3738, 6, 32902, 6, 8, 3617, 20531, 969, 47644, 12, 3341, 13998, 1688, 30280, 11, 5, 5902, 438, 38531, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
11/15/2023 22:18:53 - INFO - __main__ - Sample 4715 of the training set: {'text': 'The dynamic nature of the Dlg â€˜supertertiaryâ€™ core structure suggest precise regulatory inputs have likely evolved to control its signaling output [25, 26].', 'label': 0, 'input_ids': [0, 133, 6878, 2574, 9, 5, 211, 462, 571, 44, 711, 16101, 1334, 90, 17174, 17, 27, 2731, 3184, 3608, 12548, 4099, 16584, 33, 533, 12236, 7, 797, 63, 22436, 4195, 646, 1244, 6, 973, 8174, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
11/15/2023 22:18:53 - INFO - __main__ - Sample 216 of the training set: {'text': 'C57 mice, however, have been reported to be more sensitive to the incentive properties of other drugs of abuse including amphetamine, cocaine, methamphetamine, or nicotine than DBA mice (for review see Crawley et al. 1997; Cabib et al. 2000; Orsini et al. 2004; 2005; Grabus et al. 2006).', 'label': 0, 'input_ids': [0, 347, 4390, 15540, 6, 959, 6, 33, 57, 431, 7, 28, 55, 5685, 7, 5, 10814, 3611, 9, 97, 2196, 9, 2134, 217, 28127, 45634, 6, 9890, 6, 19118, 6, 50, 27730, 87, 211, 3813, 15540, 36, 1990, 1551, 192, 28040, 607, 4400, 1076, 4, 7528, 131, 8434, 1452, 4400, 1076, 4, 3788, 131, 1793, 29, 2531, 4400, 1076, 4, 4482, 131, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.
11/15/2023 22:18:53 - WARNING - accelerate.utils.other - Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[INFO|trainer.py:755] 2023-11-15 22:18:54,248 >> The following columns in the training set don't have a corresponding argument in `PeftModelForTokenClassification.forward` and have been ignored: text. If text are not expected by `PeftModelForTokenClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:1715] 2023-11-15 22:18:54,258 >> ***** Running training *****
[INFO|trainer.py:1716] 2023-11-15 22:18:54,259 >>   Num examples = 8,816
[INFO|trainer.py:1717] 2023-11-15 22:18:54,259 >>   Num Epochs = 5
[INFO|trainer.py:1718] 2023-11-15 22:18:54,259 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1721] 2023-11-15 22:18:54,260 >>   Total train batch size (w. parallel, distributed & accumulation) = 32
[INFO|trainer.py:1722] 2023-11-15 22:18:54,260 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:1723] 2023-11-15 22:18:54,260 >>   Total optimization steps = 1,380
[INFO|trainer.py:1724] 2023-11-15 22:18:54,261 >>   Number of trainable parameters = 1,284,867
[INFO|integration_utils.py:716] 2023-11-15 22:18:54,262 >> Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"
  0%|          | 0/1380 [00:00<?, ?it/s]  0%|          | 1/1380 [00:01<23:19,  1.02s/it]  0%|          | 3/1380 [00:01<07:13,  3.18it/s]  0%|          | 5/1380 [00:01<04:19,  5.31it/s]  1%|          | 7/1380 [00:01<03:09,  7.25it/s]  1%|          | 9/1380 [00:01<02:33,  8.92it/s]  1%|          | 11/1380 [00:01<02:12, 10.30it/s]  1%|          | 13/1380 [00:01<01:59, 11.40it/s]  1%|          | 15/1380 [00:01<01:51, 12.23it/s]  1%|          | 17/1380 [00:02<01:46, 12.83it/s]  1%|â–         | 19/1380 [00:02<01:42, 13.27it/s]  2%|â–         | 21/1380 [00:02<01:40, 13.57it/s]  2%|â–         | 23/1380 [00:02<01:38, 13.80it/s]  2%|â–         | 25/1380 [00:02<01:36, 13.99it/s]  2%|â–         | 27/1380 [00:02<01:35, 14.13it/s]  2%|â–         | 29/1380 [00:02<01:35, 14.21it/s]  2%|â–         | 31/1380 [00:03<01:34, 14.30it/s]  2%|â–         | 33/1380 [00:03<01:34, 14.23it/s]  3%|â–Ž         | 35/1380 [00:03<01:34, 14.27it/s]  3%|â–Ž         | 37/1380 [00:03<01:33, 14.33it/s]  3%|â–Ž         | 39/1380 [00:03<01:33, 14.36it/s]  3%|â–Ž         | 41/1380 [00:03<01:33, 14.36it/s]  3%|â–Ž         | 43/1380 [00:03<01:33, 14.35it/s]  3%|â–Ž         | 45/1380 [00:04<01:33, 14.35it/s]  3%|â–Ž         | 47/1380 [00:04<01:32, 14.36it/s]  4%|â–Ž         | 49/1380 [00:04<01:32, 14.37it/s]  4%|â–Ž         | 51/1380 [00:04<01:32, 14.38it/s]  4%|â–         | 53/1380 [00:04<01:32, 14.37it/s]  4%|â–         | 55/1380 [00:04<01:32, 14.38it/s]  4%|â–         | 57/1380 [00:04<01:31, 14.39it/s]  4%|â–         | 59/1380 [00:05<01:31, 14.39it/s]  4%|â–         | 61/1380 [00:05<01:31, 14.39it/s]  5%|â–         | 63/1380 [00:05<01:31, 14.38it/s]  5%|â–         | 65/1380 [00:05<01:31, 14.36it/s]  5%|â–         | 67/1380 [00:05<01:31, 14.33it/s]  5%|â–Œ         | 69/1380 [00:05<01:31, 14.34it/s]  5%|â–Œ         | 71/1380 [00:05<01:31, 14.34it/s]  5%|â–Œ         | 73/1380 [00:06<01:31, 14.35it/s]  5%|â–Œ         | 75/1380 [00:06<01:30, 14.35it/s]  6%|â–Œ         | 77/1380 [00:06<01:30, 14.35it/s]  6%|â–Œ         | 79/1380 [00:06<01:30, 14.36it/s]  6%|â–Œ         | 81/1380 [00:06<01:30, 14.37it/s]  6%|â–Œ         | 83/1380 [00:06<01:30, 14.37it/s]  6%|â–Œ         | 85/1380 [00:06<01:30, 14.38it/s]  6%|â–‹         | 87/1380 [00:06<01:29, 14.37it/s]  6%|â–‹         | 89/1380 [00:07<01:29, 14.40it/s]  7%|â–‹         | 91/1380 [00:07<01:29, 14.37it/s]  7%|â–‹         | 93/1380 [00:07<01:29, 14.38it/s]  7%|â–‹         | 95/1380 [00:07<01:29, 14.36it/s]  7%|â–‹         | 97/1380 [00:07<01:29, 14.35it/s]  7%|â–‹         | 99/1380 [00:07<01:29, 14.35it/s]  7%|â–‹         | 101/1380 [00:07<01:29, 14.35it/s]  7%|â–‹         | 103/1380 [00:08<01:28, 14.36it/s]  8%|â–Š         | 105/1380 [00:08<01:28, 14.36it/s]  8%|â–Š         | 107/1380 [00:08<01:28, 14.37it/s]  8%|â–Š         | 109/1380 [00:08<01:28, 14.38it/s]  8%|â–Š         | 111/1380 [00:08<01:28, 14.38it/s]  8%|â–Š         | 113/1380 [00:08<01:28, 14.38it/s]  8%|â–Š         | 115/1380 [00:08<01:27, 14.39it/s]  8%|â–Š         | 117/1380 [00:09<01:27, 14.38it/s]  9%|â–Š         | 119/1380 [00:09<01:27, 14.37it/s]  9%|â–‰         | 121/1380 [00:09<01:27, 14.36it/s]  9%|â–‰         | 123/1380 [00:09<01:27, 14.35it/s]  9%|â–‰         | 125/1380 [00:09<01:27, 14.35it/s]  9%|â–‰         | 127/1380 [00:09<01:27, 14.34it/s]  9%|â–‰         | 129/1380 [00:09<01:27, 14.34it/s]  9%|â–‰         | 131/1380 [00:10<01:27, 14.35it/s] 10%|â–‰         | 133/1380 [00:10<01:26, 14.34it/s] 10%|â–‰         | 135/1380 [00:10<01:27, 14.31it/s] 10%|â–‰         | 137/1380 [00:10<01:26, 14.32it/s] 10%|â–ˆ         | 139/1380 [00:10<01:26, 14.32it/s] 10%|â–ˆ         | 141/1380 [00:10<01:26, 14.32it/s] 10%|â–ˆ         | 143/1380 [00:10<01:26, 14.33it/s] 11%|â–ˆ         | 145/1380 [00:11<01:26, 14.33it/s] 11%|â–ˆ         | 147/1380 [00:11<01:26, 14.30it/s] 11%|â–ˆ         | 149/1380 [00:11<01:25, 14.32it/s] 11%|â–ˆ         | 151/1380 [00:11<01:25, 14.34it/s] 11%|â–ˆ         | 153/1380 [00:11<01:25, 14.36it/s] 11%|â–ˆ         | 155/1380 [00:11<01:25, 14.38it/s] 11%|â–ˆâ–        | 157/1380 [00:11<01:25, 14.38it/s] 12%|â–ˆâ–        | 159/1380 [00:12<01:25, 14.36it/s] 12%|â–ˆâ–        | 161/1380 [00:12<01:24, 14.35it/s] 12%|â–ˆâ–        | 163/1380 [00:12<01:24, 14.33it/s] 12%|â–ˆâ–        | 165/1380 [00:12<01:24, 14.33it/s] 12%|â–ˆâ–        | 167/1380 [00:12<01:24, 14.31it/s] 12%|â–ˆâ–        | 169/1380 [00:12<01:24, 14.31it/s] 12%|â–ˆâ–        | 171/1380 [00:12<01:24, 14.30it/s] 13%|â–ˆâ–Ž        | 173/1380 [00:12<01:24, 14.33it/s] 13%|â–ˆâ–Ž        | 175/1380 [00:13<01:23, 14.35it/s] 13%|â–ˆâ–Ž        | 177/1380 [00:13<01:23, 14.37it/s] 13%|â–ˆâ–Ž        | 179/1380 [00:13<01:23, 14.38it/s] 13%|â–ˆâ–Ž        | 181/1380 [00:13<01:23, 14.39it/s] 13%|â–ˆâ–Ž        | 183/1380 [00:13<01:23, 14.39it/s] 13%|â–ˆâ–Ž        | 185/1380 [00:13<01:22, 14.42it/s] 14%|â–ˆâ–Ž        | 187/1380 [00:13<01:22, 14.43it/s] 14%|â–ˆâ–Ž        | 189/1380 [00:14<01:22, 14.43it/s] 14%|â–ˆâ–        | 191/1380 [00:14<01:22, 14.43it/s] 14%|â–ˆâ–        | 193/1380 [00:14<01:22, 14.40it/s] 14%|â–ˆâ–        | 195/1380 [00:14<01:22, 14.40it/s] 14%|â–ˆâ–        | 197/1380 [00:14<01:22, 14.41it/s] 14%|â–ˆâ–        | 199/1380 [00:14<01:21, 14.44it/s] 15%|â–ˆâ–        | 201/1380 [00:14<01:21, 14.46it/s] 15%|â–ˆâ–        | 203/1380 [00:15<01:21, 14.44it/s] 15%|â–ˆâ–        | 205/1380 [00:15<01:21, 14.43it/s] 15%|â–ˆâ–Œ        | 207/1380 [00:15<01:21, 14.42it/s] 15%|â–ˆâ–Œ        | 209/1380 [00:15<01:21, 14.42it/s] 15%|â–ˆâ–Œ        | 211/1380 [00:15<01:21, 14.43it/s] 15%|â–ˆâ–Œ        | 213/1380 [00:15<01:20, 14.44it/s] 16%|â–ˆâ–Œ        | 215/1380 [00:15<01:20, 14.45it/s] 16%|â–ˆâ–Œ        | 217/1380 [00:16<01:20, 14.43it/s] 16%|â–ˆâ–Œ        | 219/1380 [00:16<01:20, 14.41it/s] 16%|â–ˆâ–Œ        | 221/1380 [00:16<01:20, 14.40it/s] 16%|â–ˆâ–Œ        | 223/1380 [00:16<01:20, 14.39it/s] 16%|â–ˆâ–‹        | 225/1380 [00:16<01:20, 14.39it/s] 16%|â–ˆâ–‹        | 227/1380 [00:16<01:20, 14.41it/s] 17%|â–ˆâ–‹        | 229/1380 [00:16<01:19, 14.41it/s] 17%|â–ˆâ–‹        | 231/1380 [00:17<01:19, 14.42it/s] 17%|â–ˆâ–‹        | 233/1380 [00:17<01:19, 14.44it/s] 17%|â–ˆâ–‹        | 235/1380 [00:17<01:19, 14.43it/s] 17%|â–ˆâ–‹        | 237/1380 [00:17<01:19, 14.42it/s] 17%|â–ˆâ–‹        | 239/1380 [00:17<01:19, 14.41it/s] 17%|â–ˆâ–‹        | 241/1380 [00:17<01:19, 14.41it/s] 18%|â–ˆâ–Š        | 243/1380 [00:17<01:18, 14.41it/s] 18%|â–ˆâ–Š        | 245/1380 [00:17<01:18, 14.43it/s] 18%|â–ˆâ–Š        | 247/1380 [00:18<01:18, 14.44it/s] 18%|â–ˆâ–Š        | 249/1380 [00:18<01:18, 14.43it/s] 18%|â–ˆâ–Š        | 251/1380 [00:18<01:18, 14.41it/s] 18%|â–ˆâ–Š        | 253/1380 [00:18<01:18, 14.40it/s] 18%|â–ˆâ–Š        | 255/1380 [00:18<01:18, 14.38it/s] 19%|â–ˆâ–Š        | 257/1380 [00:18<01:18, 14.39it/s] 19%|â–ˆâ–‰        | 259/1380 [00:18<01:17, 14.39it/s] 19%|â–ˆâ–‰        | 261/1380 [00:19<01:17, 14.41it/s] 19%|â–ˆâ–‰        | 263/1380 [00:19<01:17, 14.42it/s] 19%|â–ˆâ–‰        | 265/1380 [00:19<01:17, 14.41it/s] 19%|â–ˆâ–‰        | 267/1380 [00:19<01:17, 14.44it/s] 19%|â–ˆâ–‰        | 269/1380 [00:19<01:17, 14.43it/s] 20%|â–ˆâ–‰        | 271/1380 [00:19<01:16, 14.41it/s] 20%|â–ˆâ–‰        | 273/1380 [00:19<01:16, 14.41it/s] 20%|â–ˆâ–‰        | 275/1380 [00:20<01:16, 14.43it/s]                                                   20%|â–ˆâ–ˆ        | 276/1380 [00:20<01:16, 14.43it/s][INFO|trainer.py:755] 2023-11-15 22:19:14,374 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModelForTokenClassification.forward` and have been ignored: text. If text are not expected by `PeftModelForTokenClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 22:19:14,375 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 22:19:14,375 >>   Num examples = 2204
[INFO|trainer.py:3134] 2023-11-15 22:19:14,375 >>   Batch size = 8
{'loss': 0.5221, 'learning_rate': 0.0004, 'epoch': 1.0}

  0%|          | 0/276 [00:00<?, ?it/s][A
  4%|â–         | 12/276 [00:00<00:02, 118.93it/s][A
  9%|â–Š         | 24/276 [00:00<00:02, 111.72it/s][A
 13%|â–ˆâ–Ž        | 36/276 [00:00<00:02, 110.05it/s][A
 17%|â–ˆâ–‹        | 48/276 [00:00<00:02, 108.35it/s][A
 21%|â–ˆâ–ˆâ–       | 59/276 [00:00<00:02, 107.29it/s][A
 25%|â–ˆâ–ˆâ–Œ       | 70/276 [00:00<00:01, 107.50it/s][A
 29%|â–ˆâ–ˆâ–‰       | 81/276 [00:00<00:01, 107.58it/s][A
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 92/276 [00:00<00:01, 107.74it/s][A
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 103/276 [00:00<00:01, 107.69it/s][A
 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 114/276 [00:01<00:01, 107.72it/s][A
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 125/276 [00:01<00:01, 107.84it/s][A
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 136/276 [00:01<00:01, 107.82it/s][A
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 147/276 [00:01<00:01, 107.83it/s][A
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 158/276 [00:01<00:01, 107.67it/s][A
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 169/276 [00:01<00:00, 107.68it/s][A
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 180/276 [00:01<00:00, 107.79it/s][A
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 191/276 [00:01<00:00, 107.76it/s][A
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 202/276 [00:01<00:00, 107.59it/s][A
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 213/276 [00:01<00:00, 107.59it/s][A
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 224/276 [00:02<00:00, 107.67it/s][A
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 235/276 [00:02<00:00, 107.55it/s][A
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 246/276 [00:02<00:00, 107.52it/s][A
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 257/276 [00:02<00:00, 107.64it/s][A
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 268/276 [00:02<00:00, 107.52it/s][A                                                  
                                                  [A 20%|â–ˆâ–ˆ        | 276/1380 [00:22<01:16, 14.43it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 276/276 [00:02<00:00, 107.52it/s][A
                                                  [A 20%|â–ˆâ–ˆ        | 277/1380 [00:22<08:25,  2.18it/s] 20%|â–ˆâ–ˆ        | 279/1380 [00:22<06:16,  2.92it/s] 20%|â–ˆâ–ˆ        | 281/1380 [00:23<04:46,  3.84it/s] 21%|â–ˆâ–ˆ        | 283/1380 [00:23<03:43,  4.92it/s] 21%|â–ˆâ–ˆ        | 285/1380 [00:23<02:58,  6.12it/s] 21%|â–ˆâ–ˆ        | 287/1380 [00:23<02:27,  7.39it/s] 21%|â–ˆâ–ˆ        | 289/1380 [00:23<02:06,  8.64it/s] 21%|â–ˆâ–ˆ        | 291/1380 [00:23<01:51,  9.80it/s] 21%|â–ˆâ–ˆ        | 293/1380 [00:23<01:40, 10.82it/s] 21%|â–ˆâ–ˆâ–       | 295/1380 [00:24<01:32, 11.67it/s] 22%|â–ˆâ–ˆâ–       | 297/1380 [00:24<01:27, 12.35it/s] 22%|â–ˆâ–ˆâ–       | 299/1380 [00:24<01:23, 12.88it/s] 22%|â–ˆâ–ˆâ–       | 301/1380 [00:24<01:21, 13.28it/s] 22%|â–ˆâ–ˆâ–       | 303/1380 [00:24<01:19, 13.56it/s] 22%|â–ˆâ–ˆâ–       | 305/1380 [00:24<01:18, 13.76it/s] 22%|â–ˆâ–ˆâ–       | 307/1380 [00:24<01:17, 13.93it/s] 22%|â–ˆâ–ˆâ–       | 309/1380 [00:25<01:16, 14.04it/s] 23%|â–ˆâ–ˆâ–Ž       | 311/1380 [00:25<01:15, 14.11it/s] 23%|â–ˆâ–ˆâ–Ž       | 313/1380 [00:25<01:15, 14.16it/s] 23%|â–ˆâ–ˆâ–Ž       | 315/1380 [00:25<01:15, 14.20it/s] 23%|â–ˆâ–ˆâ–Ž       | 317/1380 [00:25<01:14, 14.23it/s] 23%|â–ˆâ–ˆâ–Ž       | 319/1380 [00:25<01:14, 14.25it/s] 23%|â–ˆâ–ˆâ–Ž       | 321/1380 [00:25<01:14, 14.26it/s] 23%|â–ˆâ–ˆâ–Ž       | 323/1380 [00:26<01:14, 14.27it/s] 24%|â–ˆâ–ˆâ–Ž       | 325/1380 [00:26<01:13, 14.27it/s] 24%|â–ˆâ–ˆâ–Ž       | 327/1380 [00:26<01:13, 14.28it/s] 24%|â–ˆâ–ˆâ–       | 329/1380 [00:26<01:13, 14.28it/s] 24%|â–ˆâ–ˆâ–       | 331/1380 [00:26<01:13, 14.29it/s] 24%|â–ˆâ–ˆâ–       | 333/1380 [00:26<01:13, 14.28it/s] 24%|â–ˆâ–ˆâ–       | 335/1380 [00:26<01:13, 14.26it/s] 24%|â–ˆâ–ˆâ–       | 337/1380 [00:26<01:13, 14.28it/s] 25%|â–ˆâ–ˆâ–       | 339/1380 [00:27<01:12, 14.29it/s] 25%|â–ˆâ–ˆâ–       | 341/1380 [00:27<01:12, 14.29it/s] 25%|â–ˆâ–ˆâ–       | 343/1380 [00:27<01:12, 14.30it/s] 25%|â–ˆâ–ˆâ–Œ       | 345/1380 [00:27<01:12, 14.29it/s] 25%|â–ˆâ–ˆâ–Œ       | 347/1380 [00:27<01:12, 14.29it/s] 25%|â–ˆâ–ˆâ–Œ       | 349/1380 [00:27<01:12, 14.29it/s] 25%|â–ˆâ–ˆâ–Œ       | 351/1380 [00:27<01:11, 14.30it/s] 26%|â–ˆâ–ˆâ–Œ       | 353/1380 [00:28<01:12, 14.26it/s] 26%|â–ˆâ–ˆâ–Œ       | 355/1380 [00:28<01:11, 14.28it/s] 26%|â–ˆâ–ˆâ–Œ       | 357/1380 [00:28<01:11, 14.30it/s] 26%|â–ˆâ–ˆâ–Œ       | 359/1380 [00:28<01:11, 14.29it/s] 26%|â–ˆâ–ˆâ–Œ       | 361/1380 [00:28<01:11, 14.28it/s] 26%|â–ˆâ–ˆâ–‹       | 363/1380 [00:28<01:11, 14.28it/s] 26%|â–ˆâ–ˆâ–‹       | 365/1380 [00:28<01:11, 14.28it/s] 27%|â–ˆâ–ˆâ–‹       | 367/1380 [00:29<01:10, 14.28it/s] 27%|â–ˆâ–ˆâ–‹       | 369/1380 [00:29<01:10, 14.29it/s] 27%|â–ˆâ–ˆâ–‹       | 371/1380 [00:29<01:10, 14.28it/s] 27%|â–ˆâ–ˆâ–‹       | 373/1380 [00:29<01:10, 14.28it/s] 27%|â–ˆâ–ˆâ–‹       | 375/1380 [00:29<01:10, 14.28it/s] 27%|â–ˆâ–ˆâ–‹       | 377/1380 [00:29<01:10, 14.24it/s] 27%|â–ˆâ–ˆâ–‹       | 379/1380 [00:29<01:10, 14.26it/s] 28%|â–ˆâ–ˆâ–Š       | 381/1380 [00:30<01:10, 14.27it/s] 28%|â–ˆâ–ˆâ–Š       | 383/1380 [00:30<01:09, 14.28it/s] 28%|â–ˆâ–ˆâ–Š       | 385/1380 [00:30<01:09, 14.28it/s] 28%|â–ˆâ–ˆâ–Š       | 387/1380 [00:30<01:09, 14.29it/s] 28%|â–ˆâ–ˆâ–Š       | 389/1380 [00:30<01:09, 14.27it/s] 28%|â–ˆâ–ˆâ–Š       | 391/1380 [00:30<01:09, 14.27it/s] 28%|â–ˆâ–ˆâ–Š       | 393/1380 [00:30<01:09, 14.28it/s] 29%|â–ˆâ–ˆâ–Š       | 395/1380 [00:31<01:08, 14.28it/s] 29%|â–ˆâ–ˆâ–‰       | 397/1380 [00:31<01:08, 14.26it/s] 29%|â–ˆâ–ˆâ–‰       | 399/1380 [00:31<01:08, 14.26it/s] 29%|â–ˆâ–ˆâ–‰       | 401/1380 [00:31<01:08, 14.26it/s] 29%|â–ˆâ–ˆâ–‰       | 403/1380 [00:31<01:08, 14.25it/s] 29%|â–ˆâ–ˆâ–‰       | 405/1380 [00:31<01:08, 14.24it/s] 29%|â–ˆâ–ˆâ–‰       | 407/1380 [00:31<01:08, 14.24it/s] 30%|â–ˆâ–ˆâ–‰       | 409/1380 [00:32<01:08, 14.24it/s] 30%|â–ˆâ–ˆâ–‰       | 411/1380 [00:32<01:08, 14.22it/s] 30%|â–ˆâ–ˆâ–‰       | 413/1380 [00:32<01:08, 14.22it/s] 30%|â–ˆâ–ˆâ–ˆ       | 415/1380 [00:32<01:08, 14.19it/s] 30%|â–ˆâ–ˆâ–ˆ       | 417/1380 [00:32<01:07, 14.21it/s] 30%|â–ˆâ–ˆâ–ˆ       | 419/1380 [00:32<01:07, 14.23it/s] 31%|â–ˆâ–ˆâ–ˆ       | 421/1380 [00:32<01:07, 14.25it/s] 31%|â–ˆâ–ˆâ–ˆ       | 423/1380 [00:33<01:07, 14.26it/s] 31%|â–ˆâ–ˆâ–ˆ       | 425/1380 [00:33<01:06, 14.26it/s] 31%|â–ˆâ–ˆâ–ˆ       | 427/1380 [00:33<01:06, 14.27it/s] 31%|â–ˆâ–ˆâ–ˆ       | 429/1380 [00:33<01:06, 14.28it/s] 31%|â–ˆâ–ˆâ–ˆ       | 431/1380 [00:33<01:06, 14.30it/s] 31%|â–ˆâ–ˆâ–ˆâ–      | 433/1380 [00:33<01:06, 14.30it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 435/1380 [00:33<01:06, 14.29it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 437/1380 [00:34<01:05, 14.29it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 439/1380 [00:34<01:05, 14.27it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 441/1380 [00:34<01:05, 14.28it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 443/1380 [00:34<01:05, 14.29it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 445/1380 [00:34<01:05, 14.29it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 447/1380 [00:34<01:05, 14.29it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 449/1380 [00:34<01:05, 14.29it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 451/1380 [00:34<01:05, 14.25it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 453/1380 [00:35<01:05, 14.25it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 455/1380 [00:35<01:04, 14.25it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 457/1380 [00:35<01:04, 14.26it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 459/1380 [00:35<01:04, 14.28it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 461/1380 [00:35<01:04, 14.28it/s] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 463/1380 [00:35<01:04, 14.28it/s] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 465/1380 [00:35<01:04, 14.28it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 467/1380 [00:36<01:03, 14.28it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 469/1380 [00:36<01:03, 14.28it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 471/1380 [00:36<01:03, 14.27it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 473/1380 [00:36<01:03, 14.28it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 475/1380 [00:36<01:03, 14.28it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 477/1380 [00:36<01:03, 14.29it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 479/1380 [00:36<01:03, 14.29it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 481/1380 [00:37<01:02, 14.29it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 483/1380 [00:37<01:02, 14.29it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 485/1380 [00:37<01:02, 14.28it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 487/1380 [00:37<01:02, 14.29it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 489/1380 [00:37<01:02, 14.29it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 491/1380 [00:37<01:02, 14.29it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 493/1380 [00:37<01:02, 14.27it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 495/1380 [00:38<01:01, 14.28it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 497/1380 [00:38<01:01, 14.26it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 499/1380 [00:38<01:01, 14.25it/s] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 501/1380 [00:38<01:01, 14.26it/s] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 503/1380 [00:38<01:01, 14.22it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 505/1380 [00:38<01:01, 14.22it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 507/1380 [00:38<01:01, 14.21it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 509/1380 [00:39<01:01, 14.20it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 511/1380 [00:39<01:01, 14.20it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 513/1380 [00:39<01:01, 14.21it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 515/1380 [00:39<01:00, 14.21it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 517/1380 [00:39<01:00, 14.21it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 519/1380 [00:39<01:00, 14.23it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 521/1380 [00:39<01:00, 14.24it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 523/1380 [00:40<01:00, 14.20it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 525/1380 [00:40<01:00, 14.22it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 527/1380 [00:40<00:59, 14.23it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 529/1380 [00:40<00:59, 14.23it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 531/1380 [00:40<00:59, 14.22it/s] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 533/1380 [00:40<00:59, 14.22it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 535/1380 [00:40<00:59, 14.20it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 537/1380 [00:41<00:59, 14.20it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 539/1380 [00:41<00:59, 14.21it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 541/1380 [00:41<00:58, 14.23it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 543/1380 [00:41<00:58, 14.24it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 545/1380 [00:41<00:58, 14.22it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 547/1380 [00:41<00:58, 14.24it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 549/1380 [00:41<00:58, 14.25it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 551/1380 [00:42<00:58, 14.28it/s]                                                   40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 552/1380 [00:42<00:57, 14.28it/s][INFO|trainer.py:755] 2023-11-15 22:19:36,311 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModelForTokenClassification.forward` and have been ignored: text. If text are not expected by `PeftModelForTokenClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 22:19:36,312 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 22:19:36,313 >>   Num examples = 2204
[INFO|trainer.py:3134] 2023-11-15 22:19:36,313 >>   Batch size = 8
{'eval_loss': 0.3926643431186676, 'eval_accuracy': 0.853448275862069, 'eval_micro_f1': 0.853448275862069, 'eval_macro_f1': 0.8309040902532153, 'eval_runtime': 2.6101, 'eval_samples_per_second': 844.408, 'eval_steps_per_second': 105.743, 'epoch': 1.0}
{'loss': 0.3847, 'learning_rate': 0.0003, 'epoch': 2.0}

  0%|          | 0/276 [00:00<?, ?it/s][A
  4%|â–         | 12/276 [00:00<00:02, 117.37it/s][A
  9%|â–Š         | 24/276 [00:00<00:02, 110.97it/s][A
 13%|â–ˆâ–Ž        | 36/276 [00:00<00:02, 108.97it/s][A
 17%|â–ˆâ–‹        | 47/276 [00:00<00:02, 107.98it/s][A
 21%|â–ˆâ–ˆ        | 58/276 [00:00<00:02, 107.53it/s][A
 25%|â–ˆâ–ˆâ–Œ       | 69/276 [00:00<00:01, 107.32it/s][A
 29%|â–ˆâ–ˆâ–‰       | 80/276 [00:00<00:01, 107.20it/s][A
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 91/276 [00:00<00:01, 107.05it/s][A
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 102/276 [00:00<00:01, 106.75it/s][A
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 113/276 [00:01<00:01, 106.52it/s][A
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 124/276 [00:01<00:01, 106.54it/s][A
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 135/276 [00:01<00:01, 106.52it/s][A
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 146/276 [00:01<00:01, 106.58it/s][A
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 157/276 [00:01<00:01, 106.40it/s][A
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 168/276 [00:01<00:01, 106.47it/s][A
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 179/276 [00:01<00:00, 106.59it/s][A
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 190/276 [00:01<00:00, 106.55it/s][A
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 201/276 [00:01<00:00, 106.59it/s][A
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 212/276 [00:01<00:00, 106.65it/s][A
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 223/276 [00:02<00:00, 106.43it/s][A
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 234/276 [00:02<00:00, 106.52it/s][A
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 245/276 [00:02<00:00, 106.44it/s][A
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 256/276 [00:02<00:00, 106.52it/s][A
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 267/276 [00:02<00:00, 106.36it/s][A                                                  
                                                  [A 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 552/1380 [00:44<00:57, 14.28it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 276/276 [00:02<00:00, 106.36it/s][A
                                                  [A 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 553/1380 [00:44<06:21,  2.17it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 555/1380 [00:44<04:43,  2.91it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 557/1380 [00:45<03:35,  3.82it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 559/1380 [00:45<02:47,  4.89it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 561/1380 [00:45<02:14,  6.09it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 563/1380 [00:45<01:51,  7.35it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 565/1380 [00:45<01:34,  8.59it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 567/1380 [00:45<01:23,  9.75it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 569/1380 [00:45<01:15, 10.76it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 571/1380 [00:46<01:09, 11.59it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 573/1380 [00:46<01:05, 12.27it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 575/1380 [00:46<01:02, 12.80it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 577/1380 [00:46<01:00, 13.21it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 579/1380 [00:46<00:59, 13.52it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 581/1380 [00:46<00:58, 13.73it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 583/1380 [00:46<00:57, 13.89it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 585/1380 [00:47<00:56, 13.99it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 587/1380 [00:47<00:56, 14.05it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 589/1380 [00:47<00:56, 14.10it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 591/1380 [00:47<00:55, 14.14it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 593/1380 [00:47<00:55, 14.16it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 595/1380 [00:47<00:55, 14.14it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 597/1380 [00:47<00:55, 14.12it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 599/1380 [00:47<00:55, 14.15it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 601/1380 [00:48<00:54, 14.18it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 603/1380 [00:48<00:54, 14.19it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 605/1380 [00:48<00:54, 14.21it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 607/1380 [00:48<00:54, 14.21it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 609/1380 [00:48<00:54, 14.21it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 611/1380 [00:48<00:54, 14.22it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 613/1380 [00:48<00:53, 14.20it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 615/1380 [00:49<00:53, 14.20it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 617/1380 [00:49<00:53, 14.17it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 619/1380 [00:49<00:53, 14.15it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 621/1380 [00:49<00:53, 14.16it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 623/1380 [00:49<00:53, 14.19it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 625/1380 [00:49<00:53, 14.21it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 627/1380 [00:49<00:52, 14.23it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 629/1380 [00:50<00:52, 14.24it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 631/1380 [00:50<00:52, 14.24it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 633/1380 [00:50<00:52, 14.22it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 635/1380 [00:50<00:52, 14.20it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 637/1380 [00:50<00:52, 14.19it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 639/1380 [00:50<00:52, 14.19it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 641/1380 [00:50<00:52, 14.18it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 643/1380 [00:51<00:52, 14.17it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 645/1380 [00:51<00:51, 14.18it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 647/1380 [00:51<00:51, 14.17it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 649/1380 [00:51<00:51, 14.20it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 651/1380 [00:51<00:51, 14.21it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 653/1380 [00:51<00:51, 14.22it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 655/1380 [00:51<00:50, 14.22it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 657/1380 [00:52<00:50, 14.20it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 659/1380 [00:52<00:50, 14.19it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 661/1380 [00:52<00:50, 14.18it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 663/1380 [00:52<00:50, 14.15it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 665/1380 [00:52<00:50, 14.17it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 667/1380 [00:52<00:50, 14.14it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 669/1380 [00:52<00:50, 14.18it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 671/1380 [00:53<00:49, 14.19it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 673/1380 [00:53<00:49, 14.20it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 675/1380 [00:53<00:49, 14.14it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 677/1380 [00:53<00:49, 14.13it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 679/1380 [00:53<00:49, 14.15it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 681/1380 [00:53<00:49, 14.17it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 683/1380 [00:53<00:49, 14.19it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 685/1380 [00:54<00:48, 14.21it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 687/1380 [00:54<00:48, 14.21it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 689/1380 [00:54<00:48, 14.21it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 691/1380 [00:54<00:48, 14.21it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 693/1380 [00:54<00:48, 14.20it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 695/1380 [00:54<00:48, 14.15it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 697/1380 [00:54<00:48, 14.16it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 699/1380 [00:55<00:48, 14.16it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 701/1380 [00:55<00:47, 14.17it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 703/1380 [00:55<00:47, 14.19it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 705/1380 [00:55<00:47, 14.17it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 707/1380 [00:55<00:47, 14.18it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 709/1380 [00:55<00:47, 14.18it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 711/1380 [00:55<00:47, 14.15it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 713/1380 [00:56<00:47, 14.15it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 715/1380 [00:56<00:46, 14.16it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 717/1380 [00:56<00:46, 14.17it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 719/1380 [00:56<00:46, 14.15it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 721/1380 [00:56<00:46, 14.19it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 723/1380 [00:56<00:46, 14.19it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 725/1380 [00:56<00:46, 14.19it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 727/1380 [00:57<00:46, 14.19it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 729/1380 [00:57<00:45, 14.19it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 731/1380 [00:57<00:45, 14.19it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 733/1380 [00:57<00:45, 14.17it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 735/1380 [00:57<00:45, 14.17it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 737/1380 [00:57<00:45, 14.19it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 739/1380 [00:57<00:45, 14.20it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 741/1380 [00:57<00:44, 14.22it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 743/1380 [00:58<00:44, 14.23it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 745/1380 [00:58<00:44, 14.24it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 747/1380 [00:58<00:44, 14.24it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 749/1380 [00:58<00:44, 14.24it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 751/1380 [00:58<00:44, 14.24it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 753/1380 [00:58<00:44, 14.23it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 755/1380 [00:58<00:43, 14.23it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 757/1380 [00:59<00:43, 14.22it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 759/1380 [00:59<00:43, 14.21it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 761/1380 [00:59<00:43, 14.20it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 763/1380 [00:59<00:43, 14.19it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 765/1380 [00:59<00:43, 14.18it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 767/1380 [00:59<00:43, 14.21it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 769/1380 [00:59<00:43, 14.21it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 771/1380 [01:00<00:42, 14.21it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 773/1380 [01:00<00:42, 14.20it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 775/1380 [01:00<00:42, 14.20it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 777/1380 [01:00<00:42, 14.20it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 779/1380 [01:00<00:42, 14.18it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 781/1380 [01:00<00:42, 14.16it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 783/1380 [01:00<00:42, 14.15it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 785/1380 [01:01<00:41, 14.18it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 787/1380 [01:01<00:41, 14.20it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 789/1380 [01:01<00:41, 14.21it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 791/1380 [01:01<00:41, 14.22it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 793/1380 [01:01<00:41, 14.21it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 795/1380 [01:01<00:41, 14.21it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 797/1380 [01:01<00:41, 14.21it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 799/1380 [01:02<00:40, 14.22it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 801/1380 [01:02<00:40, 14.20it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 803/1380 [01:02<00:40, 14.20it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 805/1380 [01:02<00:40, 14.17it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 807/1380 [01:02<00:40, 14.15it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 809/1380 [01:02<00:40, 14.17it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 811/1380 [01:02<00:40, 14.18it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 813/1380 [01:03<00:39, 14.19it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 815/1380 [01:03<00:39, 14.18it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 817/1380 [01:03<00:39, 14.18it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 819/1380 [01:03<00:39, 14.17it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 821/1380 [01:03<00:39, 14.17it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 823/1380 [01:03<00:39, 14.12it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 825/1380 [01:03<00:39, 14.15it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 827/1380 [01:04<00:38, 14.19it/s]                                                   60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 828/1380 [01:04<00:38, 14.19it/s][INFO|trainer.py:755] 2023-11-15 22:19:58,365 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModelForTokenClassification.forward` and have been ignored: text. If text are not expected by `PeftModelForTokenClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 22:19:58,367 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 22:19:58,367 >>   Num examples = 2204
[INFO|trainer.py:3134] 2023-11-15 22:19:58,368 >>   Batch size = 8
{'eval_loss': 0.37955382466316223, 'eval_accuracy': 0.8616152450090744, 'eval_micro_f1': 0.8616152450090744, 'eval_macro_f1': 0.8393433891482367, 'eval_runtime': 2.6326, 'eval_samples_per_second': 837.182, 'eval_steps_per_second': 104.838, 'epoch': 2.0}
{'loss': 0.3443, 'learning_rate': 0.0002, 'epoch': 3.0}

  0%|          | 0/276 [00:00<?, ?it/s][A
  4%|â–         | 12/276 [00:00<00:02, 117.13it/s][A
  9%|â–Š         | 24/276 [00:00<00:02, 110.63it/s][A
 13%|â–ˆâ–Ž        | 36/276 [00:00<00:02, 108.66it/s][A
 17%|â–ˆâ–‹        | 47/276 [00:00<00:02, 107.71it/s][A
 21%|â–ˆâ–ˆ        | 58/276 [00:00<00:02, 107.08it/s][A
 25%|â–ˆâ–ˆâ–Œ       | 69/276 [00:00<00:01, 106.81it/s][A
 29%|â–ˆâ–ˆâ–‰       | 80/276 [00:00<00:01, 106.61it/s][A
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 91/276 [00:00<00:01, 106.59it/s][A
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 102/276 [00:00<00:01, 106.53it/s][A
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 113/276 [00:01<00:01, 106.49it/s][A
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 124/276 [00:01<00:01, 106.36it/s][A
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 135/276 [00:01<00:01, 106.43it/s][A
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 146/276 [00:01<00:01, 106.26it/s][A
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 157/276 [00:01<00:01, 106.29it/s][A
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 168/276 [00:01<00:01, 106.18it/s][A
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 179/276 [00:01<00:00, 104.69it/s][A
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 190/276 [00:01<00:00, 100.61it/s][A
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 201/276 [00:01<00:00, 100.42it/s][A
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 212/276 [00:02<00:00, 101.38it/s][A
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 223/276 [00:02<00:00, 102.37it/s][A
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 234/276 [00:02<00:00, 102.51it/s][A
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 245/276 [00:02<00:00, 101.59it/s][A
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 256/276 [00:02<00:00, 102.96it/s][A
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 267/276 [00:02<00:00, 103.93it/s][A                                                  
                                                  [A 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 828/1380 [01:06<00:38, 14.19it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 276/276 [00:02<00:00, 103.93it/s][A
                                                  [A 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 829/1380 [01:06<04:18,  2.13it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 831/1380 [01:07<03:12,  2.86it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 833/1380 [01:07<02:25,  3.76it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 835/1380 [01:07<01:53,  4.82it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 837/1380 [01:07<01:30,  6.01it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 839/1380 [01:07<01:14,  7.26it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 841/1380 [01:07<01:03,  8.52it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 843/1380 [01:07<00:55,  9.68it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 845/1380 [01:07<00:49, 10.71it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 847/1380 [01:08<00:46, 11.57it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 849/1380 [01:08<00:43, 12.23it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 851/1380 [01:08<00:41, 12.75it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 853/1380 [01:08<00:40, 13.15it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 855/1380 [01:08<00:39, 13.43it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 857/1380 [01:08<00:38, 13.64it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 859/1380 [01:08<00:37, 13.80it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 861/1380 [01:09<00:37, 13.93it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 863/1380 [01:09<00:36, 14.02it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 865/1380 [01:09<00:36, 14.08it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 867/1380 [01:09<00:36, 14.11it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 869/1380 [01:09<00:36, 14.13it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 871/1380 [01:09<00:35, 14.15it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 873/1380 [01:09<00:35, 14.16it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 875/1380 [01:10<00:35, 14.16it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 877/1380 [01:10<00:35, 14.15it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 879/1380 [01:10<00:35, 14.17it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 881/1380 [01:10<00:35, 14.18it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 883/1380 [01:10<00:35, 14.19it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 885/1380 [01:10<00:34, 14.20it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 887/1380 [01:10<00:34, 14.16it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 889/1380 [01:11<00:34, 14.16it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 891/1380 [01:11<00:34, 14.16it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 893/1380 [01:11<00:34, 14.16it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 895/1380 [01:11<00:34, 14.17it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 897/1380 [01:11<00:34, 14.19it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 899/1380 [01:11<00:33, 14.21it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 901/1380 [01:11<00:33, 14.20it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 903/1380 [01:12<00:33, 14.21it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 905/1380 [01:12<00:33, 14.21it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 907/1380 [01:12<00:33, 14.22it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 909/1380 [01:12<00:33, 14.22it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 911/1380 [01:12<00:32, 14.22it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 913/1380 [01:12<00:32, 14.20it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 915/1380 [01:12<00:32, 14.20it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 917/1380 [01:13<00:32, 14.16it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 919/1380 [01:13<00:32, 14.17it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 921/1380 [01:13<00:32, 14.19it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 923/1380 [01:13<00:32, 14.20it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 925/1380 [01:13<00:32, 14.20it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 927/1380 [01:13<00:31, 14.19it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 929/1380 [01:13<00:31, 14.19it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 931/1380 [01:14<00:31, 14.19it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 933/1380 [01:14<00:31, 14.17it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 935/1380 [01:14<00:31, 14.14it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 937/1380 [01:14<00:31, 14.16it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 939/1380 [01:14<00:31, 14.18it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 941/1380 [01:14<00:30, 14.19it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 943/1380 [01:14<00:30, 14.20it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 945/1380 [01:15<00:30, 14.19it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 947/1380 [01:15<00:30, 14.18it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 949/1380 [01:15<00:30, 14.18it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 951/1380 [01:15<00:30, 14.17it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 953/1380 [01:15<00:30, 14.16it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 955/1380 [01:15<00:29, 14.17it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 957/1380 [01:15<00:29, 14.18it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 959/1380 [01:16<00:29, 14.19it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 961/1380 [01:16<00:29, 14.19it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 963/1380 [01:16<00:29, 14.20it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 965/1380 [01:16<00:29, 14.18it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 967/1380 [01:16<00:29, 14.15it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 969/1380 [01:16<00:29, 14.13it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 971/1380 [01:16<00:28, 14.15it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 973/1380 [01:17<00:28, 14.17it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 975/1380 [01:17<00:28, 14.18it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 977/1380 [01:17<00:28, 14.19it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 979/1380 [01:17<00:28, 14.20it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 981/1380 [01:17<00:28, 14.20it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 983/1380 [01:17<00:27, 14.19it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 985/1380 [01:17<00:27, 14.19it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 987/1380 [01:17<00:27, 14.17it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 989/1380 [01:18<00:27, 14.16it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 991/1380 [01:18<00:27, 14.17it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 993/1380 [01:18<00:27, 14.18it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 995/1380 [01:18<00:27, 14.18it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 997/1380 [01:18<00:27, 14.18it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 999/1380 [01:18<00:26, 14.17it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1001/1380 [01:18<00:26, 14.17it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1003/1380 [01:19<00:26, 14.16it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1005/1380 [01:19<00:26, 14.14it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1007/1380 [01:19<00:26, 14.15it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1009/1380 [01:19<00:26, 14.18it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1011/1380 [01:19<00:26, 14.17it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1013/1380 [01:19<00:25, 14.19it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1015/1380 [01:19<00:25, 14.21it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1017/1380 [01:20<00:25, 14.20it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1019/1380 [01:20<00:25, 14.20it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1021/1380 [01:20<00:25, 14.20it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1023/1380 [01:20<00:25, 14.19it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1025/1380 [01:20<00:25, 14.19it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1027/1380 [01:20<00:24, 14.18it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1029/1380 [01:20<00:24, 14.19it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1031/1380 [01:21<00:24, 14.19it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1033/1380 [01:21<00:24, 14.20it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1035/1380 [01:21<00:24, 14.21it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1037/1380 [01:21<00:24, 14.21it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1039/1380 [01:21<00:24, 14.20it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1041/1380 [01:21<00:23, 14.19it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1043/1380 [01:21<00:23, 14.17it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1045/1380 [01:22<00:23, 14.14it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1047/1380 [01:22<00:23, 14.15it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1049/1380 [01:22<00:23, 14.18it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1051/1380 [01:22<00:23, 14.20it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1053/1380 [01:22<00:23, 14.21it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1055/1380 [01:22<00:22, 14.21it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1057/1380 [01:22<00:22, 14.20it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1059/1380 [01:23<00:22, 14.20it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1061/1380 [01:23<00:23, 13.85it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1063/1380 [01:23<00:22, 13.91it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1065/1380 [01:23<00:22, 13.97it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1067/1380 [01:23<00:22, 14.04it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1069/1380 [01:23<00:22, 14.10it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1071/1380 [01:23<00:21, 14.13it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1073/1380 [01:24<00:21, 14.10it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1075/1380 [01:24<00:21, 14.08it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1077/1380 [01:24<00:21, 14.10it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1079/1380 [01:24<00:21, 14.12it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1081/1380 [01:24<00:21, 14.14it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1083/1380 [01:24<00:20, 14.15it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1085/1380 [01:24<00:20, 14.14it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1087/1380 [01:25<00:20, 14.13it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1089/1380 [01:25<00:20, 14.12it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1091/1380 [01:25<00:20, 14.12it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1093/1380 [01:25<00:20, 14.15it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1095/1380 [01:25<00:20, 14.17it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1097/1380 [01:25<00:19, 14.18it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1099/1380 [01:25<00:19, 14.19it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1101/1380 [01:26<00:19, 14.19it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1103/1380 [01:26<00:19, 14.21it/s]                                                    80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1104/1380 [01:26<00:19, 14.21it/s][INFO|trainer.py:755] 2023-11-15 22:20:20,501 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModelForTokenClassification.forward` and have been ignored: text. If text are not expected by `PeftModelForTokenClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 22:20:20,503 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 22:20:20,503 >>   Num examples = 2204
[INFO|trainer.py:3134] 2023-11-15 22:20:20,504 >>   Batch size = 8
{'eval_loss': 0.42392265796661377, 'eval_accuracy': 0.8452813067150635, 'eval_micro_f1': 0.8452813067150635, 'eval_macro_f1': 0.8317964298603701, 'eval_runtime': 2.6802, 'eval_samples_per_second': 822.325, 'eval_steps_per_second': 102.977, 'epoch': 3.0}
{'loss': 0.3126, 'learning_rate': 0.0001, 'epoch': 4.0}

  0%|          | 0/276 [00:00<?, ?it/s][A
  4%|â–         | 12/276 [00:00<00:02, 117.08it/s][A
  9%|â–Š         | 24/276 [00:00<00:02, 110.53it/s][A
 13%|â–ˆâ–Ž        | 36/276 [00:00<00:02, 108.58it/s][A
 17%|â–ˆâ–‹        | 47/276 [00:00<00:02, 107.72it/s][A
 21%|â–ˆâ–ˆ        | 58/276 [00:00<00:02, 106.85it/s][A
 25%|â–ˆâ–ˆâ–Œ       | 69/276 [00:00<00:01, 106.66it/s][A
 29%|â–ˆâ–ˆâ–‰       | 80/276 [00:00<00:01, 105.81it/s][A
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 91/276 [00:00<00:01, 105.86it/s][A
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 102/276 [00:00<00:01, 106.01it/s][A
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 113/276 [00:01<00:01, 106.22it/s][A
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 124/276 [00:01<00:01, 103.37it/s][A
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 135/276 [00:01<00:01, 104.24it/s][A
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 146/276 [00:01<00:01, 104.92it/s][A
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 157/276 [00:01<00:01, 105.34it/s][A
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 168/276 [00:01<00:01, 104.69it/s][A
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 179/276 [00:01<00:00, 105.18it/s][A
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 190/276 [00:01<00:00, 105.46it/s][A
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 201/276 [00:01<00:00, 105.54it/s][A
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 212/276 [00:02<00:00, 103.84it/s][A
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 223/276 [00:02<00:00, 104.51it/s][A
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 234/276 [00:02<00:00, 104.98it/s][A
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 245/276 [00:02<00:00, 105.34it/s][A
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 256/276 [00:02<00:00, 105.58it/s][A
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 267/276 [00:02<00:00, 105.82it/s][A                                                   
                                                  [A 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1104/1380 [01:28<00:19, 14.21it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 276/276 [00:02<00:00, 105.82it/s][A
                                                  [A 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1105/1380 [01:28<02:08,  2.14it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1107/1380 [01:29<01:35,  2.87it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1109/1380 [01:29<01:11,  3.77it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1111/1380 [01:29<00:55,  4.84it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1113/1380 [01:29<00:44,  6.03it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1115/1380 [01:29<00:36,  7.28it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1117/1380 [01:29<00:30,  8.53it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1119/1380 [01:29<00:26,  9.69it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1121/1380 [01:30<00:24, 10.72it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1123/1380 [01:30<00:22, 11.57it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1125/1380 [01:30<00:20, 12.25it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1127/1380 [01:30<00:19, 12.77it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1129/1380 [01:30<00:19, 13.17it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1131/1380 [01:30<00:18, 13.46it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1133/1380 [01:30<00:18, 13.66it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1135/1380 [01:31<00:17, 13.80it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1137/1380 [01:31<00:17, 13.89it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1139/1380 [01:31<00:17, 13.99it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1141/1380 [01:31<00:17, 14.05it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1143/1380 [01:31<00:16, 14.08it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1145/1380 [01:31<00:16, 14.10it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1147/1380 [01:31<00:16, 14.10it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1149/1380 [01:32<00:16, 14.07it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1151/1380 [01:32<00:16, 14.11it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1153/1380 [01:32<00:16, 14.14it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1155/1380 [01:32<00:15, 14.15it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1157/1380 [01:32<00:15, 14.09it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1159/1380 [01:32<00:15, 14.10it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1161/1380 [01:32<00:15, 14.12it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1163/1380 [01:33<00:15, 14.14it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1165/1380 [01:33<00:15, 14.14it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1167/1380 [01:33<00:15, 14.14it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1169/1380 [01:33<00:14, 14.14it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1171/1380 [01:33<00:14, 14.14it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1173/1380 [01:33<00:14, 14.14it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1175/1380 [01:33<00:14, 14.13it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1177/1380 [01:34<00:14, 14.15it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1179/1380 [01:34<00:14, 14.17it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1181/1380 [01:34<00:14, 14.17it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1183/1380 [01:34<00:13, 14.18it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1185/1380 [01:34<00:13, 14.13it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1187/1380 [01:34<00:13, 14.13it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1189/1380 [01:34<00:13, 14.12it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1191/1380 [01:35<00:13, 14.14it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1193/1380 [01:35<00:13, 14.16it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1195/1380 [01:35<00:13, 14.13it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1197/1380 [01:35<00:12, 14.13it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1199/1380 [01:35<00:12, 14.13it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1201/1380 [01:35<00:12, 14.13it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1203/1380 [01:35<00:12, 14.13it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1205/1380 [01:36<00:12, 14.13it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1207/1380 [01:36<00:12, 14.14it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1209/1380 [01:36<00:12, 14.16it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1211/1380 [01:36<00:11, 14.16it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1213/1380 [01:36<00:11, 14.16it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1215/1380 [01:36<00:11, 14.15it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1217/1380 [01:36<00:11, 14.14it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1219/1380 [01:37<00:11, 14.16it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1221/1380 [01:37<00:11, 14.17it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1223/1380 [01:37<00:11, 14.15it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1225/1380 [01:37<00:10, 14.16it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1227/1380 [01:37<00:10, 14.15it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1229/1380 [01:37<00:10, 14.16it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1231/1380 [01:37<00:10, 14.16it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1233/1380 [01:38<00:10, 14.15it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1235/1380 [01:38<00:10, 14.16it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1237/1380 [01:38<00:10, 14.18it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1239/1380 [01:38<00:09, 14.19it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1241/1380 [01:38<00:09, 14.19it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1243/1380 [01:38<00:09, 14.18it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1245/1380 [01:38<00:09, 14.18it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1247/1380 [01:39<00:09, 14.17it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1249/1380 [01:39<00:09, 14.17it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1251/1380 [01:39<00:09, 14.16it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1253/1380 [01:39<00:08, 14.14it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1255/1380 [01:39<00:08, 14.16it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1257/1380 [01:39<00:08, 14.17it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1259/1380 [01:39<00:08, 14.17it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1261/1380 [01:40<00:08, 14.15it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1263/1380 [01:40<00:08, 14.15it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1265/1380 [01:40<00:08, 14.14it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1267/1380 [01:40<00:08, 14.12it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1269/1380 [01:40<00:07, 14.11it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1271/1380 [01:40<00:07, 14.12it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1273/1380 [01:40<00:07, 14.11it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1275/1380 [01:40<00:07, 14.12it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1277/1380 [01:41<00:07, 14.13it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1279/1380 [01:41<00:07, 14.15it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1281/1380 [01:41<00:06, 14.15it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1283/1380 [01:41<00:06, 14.16it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1285/1380 [01:41<00:06, 14.16it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1287/1380 [01:41<00:06, 14.16it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1289/1380 [01:41<00:06, 14.15it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1291/1380 [01:42<00:06, 14.15it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1293/1380 [01:42<00:06, 14.15it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1295/1380 [01:42<00:06, 14.16it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1297/1380 [01:42<00:05, 14.18it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1299/1380 [01:42<00:05, 14.18it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1301/1380 [01:42<00:05, 14.16it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1303/1380 [01:42<00:05, 14.16it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1305/1380 [01:43<00:05, 14.16it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1307/1380 [01:43<00:05, 14.16it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1309/1380 [01:43<00:05, 14.16it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1311/1380 [01:43<00:04, 14.15it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1313/1380 [01:43<00:04, 14.16it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1315/1380 [01:43<00:04, 14.11it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1317/1380 [01:43<00:04, 14.11it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1319/1380 [01:44<00:04, 14.11it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1321/1380 [01:44<00:04, 14.14it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1323/1380 [01:44<00:04, 14.16it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1325/1380 [01:44<00:03, 14.13it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1327/1380 [01:44<00:03, 14.14it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1329/1380 [01:44<00:03, 14.12it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1331/1380 [01:44<00:03, 14.13it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1333/1380 [01:45<00:03, 14.13it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1335/1380 [01:45<00:03, 14.15it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1337/1380 [01:45<00:03, 14.17it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1339/1380 [01:45<00:02, 14.17it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1341/1380 [01:45<00:02, 14.17it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1343/1380 [01:45<00:02, 14.17it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1345/1380 [01:45<00:02, 14.17it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1347/1380 [01:46<00:02, 14.16it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1349/1380 [01:46<00:02, 14.15it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1351/1380 [01:46<00:02, 14.15it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1353/1380 [01:46<00:01, 14.17it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1355/1380 [01:46<00:01, 14.16it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1357/1380 [01:46<00:01, 14.14it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1359/1380 [01:46<00:01, 14.14it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1361/1380 [01:47<00:01, 14.14it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1363/1380 [01:47<00:01, 14.14it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1365/1380 [01:47<00:01, 14.15it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1367/1380 [01:47<00:00, 14.16it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1369/1380 [01:47<00:00, 14.17it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1371/1380 [01:47<00:00, 14.19it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1373/1380 [01:47<00:00, 14.19it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1375/1380 [01:48<00:00, 14.18it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1377/1380 [01:48<00:00, 14.16it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1379/1380 [01:48<00:00, 14.14it/s]                                                   100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1380/1380 [01:48<00:00, 14.14it/s][INFO|trainer.py:755] 2023-11-15 22:20:42,646 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModelForTokenClassification.forward` and have been ignored: text. If text are not expected by `PeftModelForTokenClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 22:20:42,648 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 22:20:42,648 >>   Num examples = 2204
[INFO|trainer.py:3134] 2023-11-15 22:20:42,649 >>   Batch size = 8
{'eval_loss': 0.4111783504486084, 'eval_accuracy': 0.853448275862069, 'eval_micro_f1': 0.853448275862069, 'eval_macro_f1': 0.8368193019868168, 'eval_runtime': 2.6653, 'eval_samples_per_second': 826.911, 'eval_steps_per_second': 103.551, 'epoch': 4.0}
{'loss': 0.2884, 'learning_rate': 0.0, 'epoch': 5.0}

  0%|          | 0/276 [00:00<?, ?it/s][A
  4%|â–         | 12/276 [00:00<00:02, 116.47it/s][A
  9%|â–Š         | 24/276 [00:00<00:02, 110.23it/s][A
 13%|â–ˆâ–Ž        | 36/276 [00:00<00:02, 108.25it/s][A
 17%|â–ˆâ–‹        | 47/276 [00:00<00:02, 107.58it/s][A
 21%|â–ˆâ–ˆ        | 58/276 [00:00<00:02, 107.08it/s][A
 25%|â–ˆâ–ˆâ–Œ       | 69/276 [00:00<00:01, 106.60it/s][A
 29%|â–ˆâ–ˆâ–‰       | 80/276 [00:00<00:01, 106.54it/s][A
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 91/276 [00:00<00:01, 106.46it/s][A
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 102/276 [00:00<00:01, 106.44it/s][A
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 113/276 [00:01<00:01, 106.37it/s][A
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 124/276 [00:01<00:01, 106.33it/s][A
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 135/276 [00:01<00:01, 106.14it/s][A
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 146/276 [00:01<00:01, 106.17it/s][A
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 157/276 [00:01<00:01, 106.11it/s][A
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 168/276 [00:01<00:01, 106.14it/s][A
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 179/276 [00:01<00:00, 106.00it/s][A
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 190/276 [00:01<00:00, 105.83it/s][A
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 201/276 [00:01<00:00, 105.80it/s][A
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 212/276 [00:01<00:00, 105.68it/s][A
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 223/276 [00:02<00:00, 105.67it/s][A
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 234/276 [00:02<00:00, 105.63it/s][A
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 245/276 [00:02<00:00, 105.63it/s][A
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 256/276 [00:02<00:00, 105.63it/s][A
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 267/276 [00:02<00:00, 105.67it/s][A                                                   
                                                  [A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1380/1380 [01:51<00:00, 14.14it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 276/276 [00:02<00:00, 105.67it/s][A
                                                  [A[INFO|trainer.py:1963] 2023-11-15 22:20:45,301 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                   100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1380/1380 [01:51<00:00, 14.14it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1380/1380 [01:51<00:00, 12.43it/s]
[INFO|trainer.py:2855] 2023-11-15 22:20:45,304 >> Saving model checkpoint to ./result/acl_roberta-base_seed3_lora
[INFO|tokenization_utils_base.py:2235] 2023-11-15 22:20:45,416 >> tokenizer config file saved in ./result/acl_roberta-base_seed3_lora/tokenizer_config.json
[INFO|tokenization_utils_base.py:2242] 2023-11-15 22:20:45,418 >> Special tokens file saved in ./result/acl_roberta-base_seed3_lora/special_tokens_map.json
{'eval_loss': 0.3845891058444977, 'eval_accuracy': 0.8611615245009074, 'eval_micro_f1': 0.8611615245009075, 'eval_macro_f1': 0.8430556206589, 'eval_runtime': 2.6491, 'eval_samples_per_second': 831.974, 'eval_steps_per_second': 104.185, 'epoch': 5.0}
{'train_runtime': 111.04, 'train_samples_per_second': 396.974, 'train_steps_per_second': 12.428, 'train_loss': 0.3703971420509228, 'epoch': 5.0}
***** train metrics *****
  epoch                    =        5.0
  train_loss               =     0.3704
  train_runtime            = 0:01:51.04
  train_samples            =       8816
  train_samples_per_second =    396.974
  train_steps_per_second   =     12.428
11/15/2023 22:20:45 - INFO - __main__ - *** Evaluate ***
[INFO|trainer.py:755] 2023-11-15 22:20:45,513 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModelForTokenClassification.forward` and have been ignored: text. If text are not expected by `PeftModelForTokenClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 22:20:45,514 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 22:20:45,515 >>   Num examples = 2204
[INFO|trainer.py:3134] 2023-11-15 22:20:45,515 >>   Batch size = 8
  0%|          | 0/276 [00:00<?, ?it/s]  4%|â–         | 12/276 [00:00<00:02, 116.86it/s]  9%|â–Š         | 24/276 [00:00<00:02, 110.60it/s] 13%|â–ˆâ–Ž        | 36/276 [00:00<00:02, 108.68it/s] 17%|â–ˆâ–‹        | 47/276 [00:00<00:02, 107.83it/s] 21%|â–ˆâ–ˆ        | 58/276 [00:00<00:02, 107.26it/s] 25%|â–ˆâ–ˆâ–Œ       | 69/276 [00:00<00:01, 107.15it/s] 29%|â–ˆâ–ˆâ–‰       | 80/276 [00:00<00:01, 107.17it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 91/276 [00:00<00:01, 107.04it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 102/276 [00:00<00:01, 106.74it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 113/276 [00:01<00:01, 106.40it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 124/276 [00:01<00:01, 106.20it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 135/276 [00:01<00:01, 106.36it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 146/276 [00:01<00:01, 106.57it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 157/276 [00:01<00:01, 106.71it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 168/276 [00:01<00:01, 106.64it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 179/276 [00:01<00:00, 106.77it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 190/276 [00:01<00:00, 106.87it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 201/276 [00:01<00:00, 106.77it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 212/276 [00:01<00:00, 106.77it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 223/276 [00:02<00:00, 106.67it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 234/276 [00:02<00:00, 106.82it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 245/276 [00:02<00:00, 106.87it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 256/276 [00:02<00:00, 106.98it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 267/276 [00:02<00:00, 106.82it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 276/276 [00:02<00:00, 105.46it/s]
***** eval metrics *****
  epoch                   =        5.0
  eval_accuracy           =     0.8612
  eval_loss               =     0.3846
  eval_macro_f1           =     0.8431
  eval_micro_f1           =     0.8612
  eval_runtime            = 0:00:02.63
  eval_samples            =       2204
  eval_samples_per_second =    837.952
  eval_steps_per_second   =    104.934
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                  eval/accuracy â–…â–ˆâ–â–…â–ˆâ–ˆ
wandb:                      eval/loss â–ƒâ–â–ˆâ–†â–‚â–‚
wandb:                  eval/macro_f1 â–â–†â–‚â–„â–ˆâ–ˆ
wandb:                  eval/micro_f1 â–…â–ˆâ–â–…â–ˆâ–ˆ
wandb:                   eval/runtime â–â–ƒâ–ˆâ–‡â–…â–ƒ
wandb:        eval/samples_per_second â–ˆâ–†â–â–‚â–„â–†
wandb:          eval/steps_per_second â–ˆâ–†â–â–‚â–„â–†
wandb:                    train/epoch â–â–â–ƒâ–ƒâ–…â–…â–†â–†â–ˆâ–ˆâ–ˆâ–ˆ
wandb:              train/global_step â–â–â–ƒâ–ƒâ–…â–…â–†â–†â–ˆâ–ˆâ–ˆâ–ˆ
wandb:            train/learning_rate â–ˆâ–†â–…â–ƒâ–
wandb:                     train/loss â–ˆâ–„â–ƒâ–‚â–
wandb:               train/total_flos â–
wandb:               train/train_loss â–
wandb:            train/train_runtime â–
wandb: train/train_samples_per_second â–
wandb:   train/train_steps_per_second â–
wandb: 
wandb: Run summary:
wandb:                  eval/accuracy 0.86116
wandb:                      eval/loss 0.38459
wandb:                  eval/macro_f1 0.84306
wandb:                  eval/micro_f1 0.86116
wandb:                   eval/runtime 2.6302
wandb:        eval/samples_per_second 837.952
wandb:          eval/steps_per_second 104.934
wandb:                    train/epoch 5.0
wandb:              train/global_step 1380
wandb:            train/learning_rate 0.0
wandb:                     train/loss 0.2884
wandb:               train/total_flos 1469774552739840.0
wandb:               train/train_loss 0.3704
wandb:            train/train_runtime 111.04
wandb: train/train_samples_per_second 396.974
wandb:   train/train_steps_per_second 12.428
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /ceph/home/wangyifei/a3/wandb/offline-run-20231115_221737-egm4r2tu
wandb: Find logs at: ./wandb/offline-run-20231115_221737-egm4r2tu/logs
(ModelArguments(model_name_or_path='roberta-base', cache_dir='./cache', use_adapter=False, use_lora=True), DataTrainingArguments(dataset_name='agnews_sup', max_seq_length=64), TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.0005,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=./result/agnews_sup_roberta-base_seed3_lora/runs/Nov15_22-20-57_node010,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.EPOCH,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=5.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
output_dir=./result/agnews_sup_roberta-base_seed3_lora,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
resume_from_checkpoint=None,
run_name=./result/agnews_sup_roberta-base_seed3_lora,
save_on_each_node=False,
save_safetensors=False,
save_steps=500,
save_strategy=IntervalStrategy.NO,
save_total_limit=None,
seed=444,
sharded_ddp=[],
skip_memory_metrics=True,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
))
11/15/2023 22:20:57 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False
11/15/2023 22:20:57 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.0005,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=./result/agnews_sup_roberta-base_seed3_lora/runs/Nov15_22-20-57_node010,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.EPOCH,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=5.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
output_dir=./result/agnews_sup_roberta-base_seed3_lora,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
resume_from_checkpoint=None,
run_name=./result/agnews_sup_roberta-base_seed3_lora,
save_on_each_node=False,
save_safetensors=False,
save_steps=500,
save_strategy=IntervalStrategy.NO,
save_total_limit=None,
seed=444,
sharded_ddp=[],
skip_memory_metrics=True,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
wandb: Tracking run with wandb version 0.15.12
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[INFO|configuration_utils.py:715] 2023-11-15 22:21:13,148 >> loading configuration file config.json from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/config.json
[INFO|configuration_utils.py:775] 2023-11-15 22:21:13,157 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.33.3",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_auto.py:535] 2023-11-15 22:21:23,173 >> Could not locate the tokenizer configuration file, will try to use the model config instead.
[INFO|configuration_utils.py:715] 2023-11-15 22:21:33,234 >> loading configuration file config.json from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/config.json
[INFO|configuration_utils.py:775] 2023-11-15 22:21:33,235 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.33.3",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:1852] 2023-11-15 22:21:53,282 >> loading file vocab.json from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/vocab.json
[INFO|tokenization_utils_base.py:1852] 2023-11-15 22:21:53,283 >> loading file merges.txt from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/merges.txt
[INFO|tokenization_utils_base.py:1852] 2023-11-15 22:21:53,283 >> loading file tokenizer.json from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/tokenizer.json
[INFO|tokenization_utils_base.py:1852] 2023-11-15 22:21:53,283 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1852] 2023-11-15 22:21:53,283 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1852] 2023-11-15 22:21:53,284 >> loading file tokenizer_config.json from cache at None
[INFO|configuration_utils.py:715] 2023-11-15 22:21:53,285 >> loading configuration file config.json from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/config.json
[INFO|configuration_utils.py:775] 2023-11-15 22:21:53,286 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.33.3",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:2862] 2023-11-15 22:22:13,459 >> loading weights file model.safetensors from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/model.safetensors
[INFO|modeling_utils.py:3638] 2023-11-15 22:22:14,187 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3650] 2023-11-15 22:22:14,189 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
trainable params: 1,285,636 || all params: 125,832,200 || trainable%: 1.0217066855701482
model_max_length: 512
Running tokenizer on dataset:   0%|          | 0/6840 [00:00<?, ? examples/s]Running tokenizer on dataset:  29%|â–ˆâ–ˆâ–‰       | 2000/6840 [00:00<00:00, 17129.85 examples/s]Running tokenizer on dataset:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 5000/6840 [00:00<00:00, 19005.14 examples/s]Running tokenizer on dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6840/6840 [00:00<00:00, 19158.46 examples/s]
Running tokenizer on dataset:   0%|          | 0/760 [00:00<?, ? examples/s]Running tokenizer on dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 760/760 [00:00<00:00, 22008.83 examples/s]
11/15/2023 22:22:14 - INFO - __main__ - Sample 2530 of the training set: {'text': 'Invasion of the Data Snatchers (washingtonpost.com) washingtonpost.com - Think your PC is safe? Think again. A new study indicates your home computer is likely bogged down with spyware, viruses and other scourges wrought by hackers and PC pranksters. Ignorance may be bliss for some people, but for computer users, not knowing can be costly and inefficient.', 'label': 2, 'input_ids': [0, 42782, 27720, 9, 5, 5423, 7500, 415, 7873, 36, 605, 40886, 7049, 4, 175, 43, 14784, 1054, 7049, 4, 175, 111, 9387, 110, 4985, 16, 1522, 116, 9387, 456, 4, 83, 92, 892, 8711, 110, 184, 3034, 16, 533, 28423, 4462, 159, 19, 10258, 10680, 6, 21717, 8, 97, 2850, 2126, 5641, 37058, 30, 11344, 8, 4985, 25828, 9230, 4, 18762, 368, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.
11/15/2023 22:22:14 - INFO - __main__ - Sample 2357 of the training set: {'text': 'Corning begins work on Taiwan LCD facility Encouraged by the demand for LCDs, glass maker Corning on Thursday said it has broken ground for a second manufacturing facility in Taiwan.', 'label': 2, 'input_ids': [0, 15228, 3509, 3772, 173, 15, 6951, 20808, 2122, 14813, 2126, 4628, 30, 5, 1077, 13, 20808, 29, 6, 4049, 4403, 2812, 3509, 15, 296, 26, 24, 34, 3187, 1255, 13, 10, 200, 3021, 2122, 11, 6951, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
11/15/2023 22:22:14 - INFO - __main__ - Sample 108 of the training set: {'text': "In Asia, Powell defends N. Korea policy SEOUL -- Secretary of State Colin L. Powell yesterday sought to fend off complaints from key partners in the effort to end North Korea's nuclear programs that the Bush administration has not been sufficiently creative or willing to compromise in the negotiations.", 'label': 3, 'input_ids': [0, 1121, 1817, 6, 8274, 24951, 234, 4, 1101, 714, 6324, 5061, 574, 480, 1863, 9, 331, 8718, 226, 4, 8274, 2350, 2952, 7, 26885, 160, 4496, 31, 762, 2567, 11, 5, 1351, 7, 253, 369, 1101, 18, 1748, 1767, 14, 5, 3516, 942, 34, 45, 57, 21547, 3904, 50, 2882, 7, 7932, 11, 5, 3377, 4, 2, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]}.
11/15/2023 22:22:14 - WARNING - accelerate.utils.other - Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[INFO|trainer.py:755] 2023-11-15 22:22:16,084 >> The following columns in the training set don't have a corresponding argument in `PeftModelForTokenClassification.forward` and have been ignored: text. If text are not expected by `PeftModelForTokenClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:1715] 2023-11-15 22:22:16,094 >> ***** Running training *****
[INFO|trainer.py:1716] 2023-11-15 22:22:16,094 >>   Num examples = 6,840
[INFO|trainer.py:1717] 2023-11-15 22:22:16,095 >>   Num Epochs = 5
[INFO|trainer.py:1718] 2023-11-15 22:22:16,095 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1721] 2023-11-15 22:22:16,095 >>   Total train batch size (w. parallel, distributed & accumulation) = 32
[INFO|trainer.py:1722] 2023-11-15 22:22:16,095 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:1723] 2023-11-15 22:22:16,096 >>   Total optimization steps = 1,070
[INFO|trainer.py:1724] 2023-11-15 22:22:16,097 >>   Number of trainable parameters = 1,285,636
[INFO|integration_utils.py:716] 2023-11-15 22:22:16,098 >> Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"
  0%|          | 0/1070 [00:00<?, ?it/s]  0%|          | 1/1070 [00:01<18:03,  1.01s/it]  0%|          | 3/1070 [00:01<05:35,  3.18it/s]  0%|          | 5/1070 [00:01<03:21,  5.29it/s]  1%|          | 7/1070 [00:01<02:27,  7.23it/s]  1%|          | 9/1070 [00:01<01:59,  8.88it/s]  1%|          | 11/1070 [00:01<01:43, 10.25it/s]  1%|          | 13/1070 [00:01<01:33, 11.31it/s]  1%|â–         | 15/1070 [00:01<01:26, 12.13it/s]  2%|â–         | 17/1070 [00:02<01:22, 12.72it/s]  2%|â–         | 19/1070 [00:02<01:19, 13.17it/s]  2%|â–         | 21/1070 [00:02<01:17, 13.50it/s]  2%|â–         | 23/1070 [00:02<01:16, 13.72it/s]  2%|â–         | 25/1070 [00:02<01:15, 13.90it/s]  3%|â–Ž         | 27/1070 [00:02<01:14, 14.03it/s]  3%|â–Ž         | 29/1070 [00:02<01:13, 14.11it/s]  3%|â–Ž         | 31/1070 [00:03<01:13, 14.19it/s]  3%|â–Ž         | 33/1070 [00:03<01:12, 14.24it/s]  3%|â–Ž         | 35/1070 [00:03<01:12, 14.27it/s]  3%|â–Ž         | 37/1070 [00:03<01:12, 14.27it/s]  4%|â–Ž         | 39/1070 [00:03<01:12, 14.30it/s]  4%|â–         | 41/1070 [00:03<01:11, 14.31it/s]  4%|â–         | 43/1070 [00:03<01:11, 14.32it/s]  4%|â–         | 45/1070 [00:04<01:11, 14.30it/s]  4%|â–         | 47/1070 [00:04<01:11, 14.32it/s]  5%|â–         | 49/1070 [00:04<01:11, 14.25it/s]  5%|â–         | 51/1070 [00:04<01:11, 14.28it/s]  5%|â–         | 53/1070 [00:04<01:11, 14.30it/s]  5%|â–Œ         | 55/1070 [00:04<01:10, 14.30it/s]  5%|â–Œ         | 57/1070 [00:04<01:10, 14.28it/s]  6%|â–Œ         | 59/1070 [00:05<01:10, 14.28it/s]  6%|â–Œ         | 61/1070 [00:05<01:10, 14.29it/s]  6%|â–Œ         | 63/1070 [00:05<01:10, 14.30it/s]  6%|â–Œ         | 65/1070 [00:05<01:10, 14.29it/s]  6%|â–‹         | 67/1070 [00:05<01:10, 14.30it/s]  6%|â–‹         | 69/1070 [00:05<01:09, 14.31it/s]  7%|â–‹         | 71/1070 [00:05<01:09, 14.32it/s]  7%|â–‹         | 73/1070 [00:06<01:09, 14.32it/s]  7%|â–‹         | 75/1070 [00:06<01:09, 14.33it/s]  7%|â–‹         | 77/1070 [00:06<01:09, 14.33it/s]  7%|â–‹         | 79/1070 [00:06<01:09, 14.34it/s]  8%|â–Š         | 81/1070 [00:06<01:08, 14.34it/s]  8%|â–Š         | 83/1070 [00:06<01:08, 14.34it/s]  8%|â–Š         | 85/1070 [00:06<01:08, 14.34it/s]  8%|â–Š         | 87/1070 [00:07<01:08, 14.33it/s]  8%|â–Š         | 89/1070 [00:07<01:08, 14.29it/s]  9%|â–Š         | 91/1070 [00:07<01:08, 14.30it/s]  9%|â–Š         | 93/1070 [00:07<01:08, 14.31it/s]  9%|â–‰         | 95/1070 [00:07<01:08, 14.31it/s]  9%|â–‰         | 97/1070 [00:07<01:08, 14.31it/s]  9%|â–‰         | 99/1070 [00:07<01:07, 14.30it/s]  9%|â–‰         | 101/1070 [00:08<01:07, 14.29it/s] 10%|â–‰         | 103/1070 [00:08<01:07, 14.30it/s] 10%|â–‰         | 105/1070 [00:08<01:07, 14.29it/s] 10%|â–ˆ         | 107/1070 [00:08<01:07, 14.29it/s] 10%|â–ˆ         | 109/1070 [00:08<01:07, 14.28it/s] 10%|â–ˆ         | 111/1070 [00:08<01:07, 14.28it/s] 11%|â–ˆ         | 113/1070 [00:08<01:06, 14.30it/s] 11%|â–ˆ         | 115/1070 [00:08<01:06, 14.30it/s] 11%|â–ˆ         | 117/1070 [00:09<01:06, 14.30it/s] 11%|â–ˆ         | 119/1070 [00:09<01:06, 14.29it/s] 11%|â–ˆâ–        | 121/1070 [00:09<01:06, 14.29it/s] 11%|â–ˆâ–        | 123/1070 [00:09<01:06, 14.29it/s] 12%|â–ˆâ–        | 125/1070 [00:09<01:06, 14.29it/s] 12%|â–ˆâ–        | 127/1070 [00:09<01:05, 14.31it/s] 12%|â–ˆâ–        | 129/1070 [00:09<01:05, 14.33it/s] 12%|â–ˆâ–        | 131/1070 [00:10<01:05, 14.34it/s] 12%|â–ˆâ–        | 133/1070 [00:10<01:05, 14.32it/s] 13%|â–ˆâ–Ž        | 135/1070 [00:10<01:05, 14.34it/s] 13%|â–ˆâ–Ž        | 137/1070 [00:10<01:04, 14.36it/s] 13%|â–ˆâ–Ž        | 139/1070 [00:10<01:04, 14.37it/s] 13%|â–ˆâ–Ž        | 141/1070 [00:10<01:04, 14.36it/s] 13%|â–ˆâ–Ž        | 143/1070 [00:10<01:04, 14.37it/s] 14%|â–ˆâ–Ž        | 145/1070 [00:11<01:04, 14.38it/s] 14%|â–ˆâ–Ž        | 147/1070 [00:11<01:04, 14.41it/s] 14%|â–ˆâ–        | 149/1070 [00:11<01:03, 14.40it/s] 14%|â–ˆâ–        | 151/1070 [00:11<01:03, 14.39it/s] 14%|â–ˆâ–        | 153/1070 [00:11<01:03, 14.37it/s] 14%|â–ˆâ–        | 155/1070 [00:11<01:03, 14.36it/s] 15%|â–ˆâ–        | 157/1070 [00:11<01:03, 14.36it/s] 15%|â–ˆâ–        | 159/1070 [00:12<01:03, 14.36it/s] 15%|â–ˆâ–Œ        | 161/1070 [00:12<01:03, 14.37it/s] 15%|â–ˆâ–Œ        | 163/1070 [00:12<01:03, 14.36it/s] 15%|â–ˆâ–Œ        | 165/1070 [00:12<01:02, 14.37it/s] 16%|â–ˆâ–Œ        | 167/1070 [00:12<01:02, 14.39it/s] 16%|â–ˆâ–Œ        | 169/1070 [00:12<01:02, 14.45it/s] 16%|â–ˆâ–Œ        | 171/1070 [00:12<01:02, 14.41it/s] 16%|â–ˆâ–Œ        | 173/1070 [00:13<01:02, 14.38it/s] 16%|â–ˆâ–‹        | 175/1070 [00:13<01:02, 14.37it/s] 17%|â–ˆâ–‹        | 177/1070 [00:13<01:02, 14.35it/s] 17%|â–ˆâ–‹        | 179/1070 [00:13<01:02, 14.33it/s] 17%|â–ˆâ–‹        | 181/1070 [00:13<01:02, 14.34it/s] 17%|â–ˆâ–‹        | 183/1070 [00:13<01:01, 14.34it/s] 17%|â–ˆâ–‹        | 185/1070 [00:13<01:01, 14.35it/s] 17%|â–ˆâ–‹        | 187/1070 [00:13<01:01, 14.36it/s] 18%|â–ˆâ–Š        | 189/1070 [00:14<01:01, 14.35it/s] 18%|â–ˆâ–Š        | 191/1070 [00:14<01:01, 14.37it/s] 18%|â–ˆâ–Š        | 193/1070 [00:14<01:00, 14.39it/s] 18%|â–ˆâ–Š        | 195/1070 [00:14<01:00, 14.40it/s] 18%|â–ˆâ–Š        | 197/1070 [00:14<01:00, 14.43it/s] 19%|â–ˆâ–Š        | 199/1070 [00:14<01:00, 14.43it/s] 19%|â–ˆâ–‰        | 201/1070 [00:14<01:00, 14.39it/s] 19%|â–ˆâ–‰        | 203/1070 [00:15<01:00, 14.39it/s] 19%|â–ˆâ–‰        | 205/1070 [00:15<01:00, 14.37it/s] 19%|â–ˆâ–‰        | 207/1070 [00:15<01:00, 14.37it/s] 20%|â–ˆâ–‰        | 209/1070 [00:15<00:59, 14.38it/s] 20%|â–ˆâ–‰        | 211/1070 [00:15<00:59, 14.39it/s] 20%|â–ˆâ–‰        | 213/1070 [00:15<00:59, 14.38it/s]                                                   20%|â–ˆâ–ˆ        | 214/1070 [00:15<00:59, 14.38it/s][INFO|trainer.py:755] 2023-11-15 22:22:31,958 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModelForTokenClassification.forward` and have been ignored: text. If text are not expected by `PeftModelForTokenClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 22:22:31,960 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 22:22:31,960 >>   Num examples = 760
[INFO|trainer.py:3134] 2023-11-15 22:22:31,960 >>   Batch size = 8
{'loss': 0.4288, 'learning_rate': 0.0004, 'epoch': 1.0}

  0%|          | 0/95 [00:00<?, ?it/s][A
 13%|â–ˆâ–Ž        | 12/95 [00:00<00:00, 117.97it/s][A
 25%|â–ˆâ–ˆâ–Œ       | 24/95 [00:00<00:00, 111.70it/s][A
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 36/95 [00:00<00:00, 109.87it/s][A
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 48/95 [00:00<00:00, 108.92it/s][A
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 59/95 [00:00<00:00, 108.39it/s][A
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 70/95 [00:00<00:00, 108.20it/s][A
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 81/95 [00:00<00:00, 107.88it/s][A
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 92/95 [00:00<00:00, 107.93it/s][A                                                  
                                                [A 20%|â–ˆâ–ˆ        | 214/1070 [00:16<00:59, 14.38it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 95/95 [00:00<00:00, 107.93it/s][A
                                                [A 20%|â–ˆâ–ˆ        | 215/1070 [00:16<02:55,  4.86it/s] 20%|â–ˆâ–ˆ        | 217/1070 [00:16<02:20,  6.07it/s] 20%|â–ˆâ–ˆ        | 219/1070 [00:17<01:56,  7.34it/s] 21%|â–ˆâ–ˆ        | 221/1070 [00:17<01:38,  8.59it/s] 21%|â–ˆâ–ˆ        | 223/1070 [00:17<01:26,  9.76it/s] 21%|â–ˆâ–ˆ        | 225/1070 [00:17<01:18, 10.79it/s] 21%|â–ˆâ–ˆ        | 227/1070 [00:17<01:12, 11.64it/s] 21%|â–ˆâ–ˆâ–       | 229/1070 [00:17<01:08, 12.33it/s] 22%|â–ˆâ–ˆâ–       | 231/1070 [00:17<01:05, 12.86it/s] 22%|â–ˆâ–ˆâ–       | 233/1070 [00:18<01:03, 13.26it/s] 22%|â–ˆâ–ˆâ–       | 235/1070 [00:18<01:01, 13.55it/s] 22%|â–ˆâ–ˆâ–       | 237/1070 [00:18<01:00, 13.77it/s] 22%|â–ˆâ–ˆâ–       | 239/1070 [00:18<00:59, 13.92it/s] 23%|â–ˆâ–ˆâ–Ž       | 241/1070 [00:18<00:59, 14.01it/s] 23%|â–ˆâ–ˆâ–Ž       | 243/1070 [00:18<00:58, 14.09it/s] 23%|â–ˆâ–ˆâ–Ž       | 245/1070 [00:18<00:58, 14.14it/s] 23%|â–ˆâ–ˆâ–Ž       | 247/1070 [00:19<00:58, 14.18it/s] 23%|â–ˆâ–ˆâ–Ž       | 249/1070 [00:19<00:57, 14.23it/s] 23%|â–ˆâ–ˆâ–Ž       | 251/1070 [00:19<00:57, 14.25it/s] 24%|â–ˆâ–ˆâ–Ž       | 253/1070 [00:19<00:57, 14.25it/s] 24%|â–ˆâ–ˆâ–       | 255/1070 [00:19<00:57, 14.27it/s] 24%|â–ˆâ–ˆâ–       | 257/1070 [00:19<00:57, 14.25it/s] 24%|â–ˆâ–ˆâ–       | 259/1070 [00:19<00:56, 14.27it/s] 24%|â–ˆâ–ˆâ–       | 261/1070 [00:20<00:56, 14.28it/s] 25%|â–ˆâ–ˆâ–       | 263/1070 [00:20<00:56, 14.29it/s] 25%|â–ˆâ–ˆâ–       | 265/1070 [00:20<00:56, 14.29it/s] 25%|â–ˆâ–ˆâ–       | 267/1070 [00:20<00:56, 14.26it/s] 25%|â–ˆâ–ˆâ–Œ       | 269/1070 [00:20<00:56, 14.30it/s] 25%|â–ˆâ–ˆâ–Œ       | 271/1070 [00:20<00:55, 14.29it/s] 26%|â–ˆâ–ˆâ–Œ       | 273/1070 [00:20<00:55, 14.30it/s] 26%|â–ˆâ–ˆâ–Œ       | 275/1070 [00:21<00:55, 14.29it/s] 26%|â–ˆâ–ˆâ–Œ       | 277/1070 [00:21<00:55, 14.29it/s] 26%|â–ˆâ–ˆâ–Œ       | 279/1070 [00:21<00:55, 14.29it/s] 26%|â–ˆâ–ˆâ–‹       | 281/1070 [00:21<00:55, 14.29it/s] 26%|â–ˆâ–ˆâ–‹       | 283/1070 [00:21<00:55, 14.30it/s] 27%|â–ˆâ–ˆâ–‹       | 285/1070 [00:21<00:54, 14.28it/s] 27%|â–ˆâ–ˆâ–‹       | 287/1070 [00:21<00:54, 14.29it/s] 27%|â–ˆâ–ˆâ–‹       | 289/1070 [00:22<00:54, 14.30it/s] 27%|â–ˆâ–ˆâ–‹       | 291/1070 [00:22<00:54, 14.27it/s] 27%|â–ˆâ–ˆâ–‹       | 293/1070 [00:22<00:54, 14.29it/s] 28%|â–ˆâ–ˆâ–Š       | 295/1070 [00:22<00:54, 14.29it/s] 28%|â–ˆâ–ˆâ–Š       | 297/1070 [00:22<00:54, 14.29it/s] 28%|â–ˆâ–ˆâ–Š       | 299/1070 [00:22<00:53, 14.28it/s] 28%|â–ˆâ–ˆâ–Š       | 301/1070 [00:22<00:53, 14.29it/s] 28%|â–ˆâ–ˆâ–Š       | 303/1070 [00:23<00:53, 14.26it/s] 29%|â–ˆâ–ˆâ–Š       | 305/1070 [00:23<00:53, 14.28it/s] 29%|â–ˆâ–ˆâ–Š       | 307/1070 [00:23<00:53, 14.29it/s] 29%|â–ˆâ–ˆâ–‰       | 309/1070 [00:23<00:53, 14.31it/s] 29%|â–ˆâ–ˆâ–‰       | 311/1070 [00:23<00:53, 14.31it/s] 29%|â–ˆâ–ˆâ–‰       | 313/1070 [00:23<00:52, 14.30it/s] 29%|â–ˆâ–ˆâ–‰       | 315/1070 [00:23<00:52, 14.30it/s] 30%|â–ˆâ–ˆâ–‰       | 317/1070 [00:23<00:52, 14.30it/s] 30%|â–ˆâ–ˆâ–‰       | 319/1070 [00:24<00:52, 14.29it/s] 30%|â–ˆâ–ˆâ–ˆ       | 321/1070 [00:24<00:52, 14.29it/s] 30%|â–ˆâ–ˆâ–ˆ       | 323/1070 [00:24<00:52, 14.29it/s] 30%|â–ˆâ–ˆâ–ˆ       | 325/1070 [00:24<00:52, 14.29it/s] 31%|â–ˆâ–ˆâ–ˆ       | 327/1070 [00:24<00:51, 14.29it/s] 31%|â–ˆâ–ˆâ–ˆ       | 329/1070 [00:24<00:51, 14.28it/s] 31%|â–ˆâ–ˆâ–ˆ       | 331/1070 [00:24<00:51, 14.28it/s] 31%|â–ˆâ–ˆâ–ˆ       | 333/1070 [00:25<00:51, 14.28it/s] 31%|â–ˆâ–ˆâ–ˆâ–      | 335/1070 [00:25<00:51, 14.28it/s] 31%|â–ˆâ–ˆâ–ˆâ–      | 337/1070 [00:25<00:51, 14.28it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 339/1070 [00:25<00:51, 14.29it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 341/1070 [00:25<00:51, 14.27it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 343/1070 [00:25<00:50, 14.29it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 345/1070 [00:25<00:50, 14.30it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 347/1070 [00:26<00:50, 14.29it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 349/1070 [00:26<00:50, 14.28it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 351/1070 [00:26<00:50, 14.26it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 353/1070 [00:26<00:50, 14.24it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 355/1070 [00:26<00:50, 14.27it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 357/1070 [00:26<00:49, 14.30it/s] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 359/1070 [00:26<00:49, 14.31it/s] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 361/1070 [00:27<00:49, 14.30it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 363/1070 [00:27<00:49, 14.30it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 365/1070 [00:27<00:49, 14.31it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 367/1070 [00:27<00:49, 14.28it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 369/1070 [00:27<00:49, 14.30it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 371/1070 [00:27<00:48, 14.30it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 373/1070 [00:27<00:48, 14.30it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 375/1070 [00:28<00:48, 14.29it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 377/1070 [00:28<00:48, 14.30it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 379/1070 [00:28<00:48, 14.30it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 381/1070 [00:28<00:48, 14.29it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 383/1070 [00:28<00:48, 14.28it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 385/1070 [00:28<00:48, 14.24it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 387/1070 [00:28<00:47, 14.27it/s] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 389/1070 [00:29<00:47, 14.29it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 391/1070 [00:29<00:47, 14.30it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 393/1070 [00:29<00:47, 14.31it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 395/1070 [00:29<00:47, 14.30it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 397/1070 [00:29<00:47, 14.29it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 399/1070 [00:29<00:46, 14.30it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 401/1070 [00:29<00:46, 14.27it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 403/1070 [00:30<00:46, 14.27it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 405/1070 [00:30<00:46, 14.28it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 407/1070 [00:30<00:46, 14.24it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 409/1070 [00:30<00:46, 14.24it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 411/1070 [00:30<00:46, 14.27it/s] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 413/1070 [00:30<00:45, 14.28it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 415/1070 [00:30<00:45, 14.29it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 417/1070 [00:30<00:45, 14.29it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 419/1070 [00:31<00:45, 14.30it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 421/1070 [00:31<00:45, 14.32it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 423/1070 [00:31<00:45, 14.32it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 425/1070 [00:31<00:45, 14.31it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 427/1070 [00:31<00:44, 14.33it/s]                                                   40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 428/1070 [00:31<00:44, 14.33it/s][INFO|trainer.py:755] 2023-11-15 22:22:47,842 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModelForTokenClassification.forward` and have been ignored: text. If text are not expected by `PeftModelForTokenClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 22:22:47,843 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 22:22:47,843 >>   Num examples = 760
[INFO|trainer.py:3134] 2023-11-15 22:22:47,844 >>   Batch size = 8
{'eval_loss': 0.31539249420166016, 'eval_accuracy': 0.8973684210526316, 'eval_micro_f1': 0.8973684210526317, 'eval_macro_f1': 0.8938221673348704, 'eval_runtime': 0.9155, 'eval_samples_per_second': 830.159, 'eval_steps_per_second': 103.77, 'epoch': 1.0}
{'loss': 0.2716, 'learning_rate': 0.0003, 'epoch': 2.0}

  0%|          | 0/95 [00:00<?, ?it/s][A
 13%|â–ˆâ–Ž        | 12/95 [00:00<00:00, 116.94it/s][A
 25%|â–ˆâ–ˆâ–Œ       | 24/95 [00:00<00:00, 110.78it/s][A
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 36/95 [00:00<00:00, 108.78it/s][A
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 47/95 [00:00<00:00, 107.96it/s][A
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 58/95 [00:00<00:00, 107.53it/s][A
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 69/95 [00:00<00:00, 107.29it/s][A
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 80/95 [00:00<00:00, 106.97it/s][A
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 91/95 [00:00<00:00, 106.80it/s][A                                                  
                                                [A 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 428/1070 [00:32<00:44, 14.33it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 95/95 [00:00<00:00, 106.80it/s][A
                                                [A 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 429/1070 [00:32<02:12,  4.82it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 431/1070 [00:32<01:46,  6.02it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 433/1070 [00:33<01:27,  7.28it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 435/1070 [00:33<01:14,  8.53it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 437/1070 [00:33<01:05,  9.70it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 439/1070 [00:33<00:58, 10.72it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 441/1070 [00:33<00:54, 11.57it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 443/1070 [00:33<00:51, 12.27it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 445/1070 [00:33<00:48, 12.81it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 447/1070 [00:34<00:47, 13.22it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 449/1070 [00:34<00:45, 13.52it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 451/1070 [00:34<00:45, 13.75it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 453/1070 [00:34<00:44, 13.90it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 455/1070 [00:34<00:43, 14.01it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 457/1070 [00:34<00:43, 14.09it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 459/1070 [00:34<00:43, 14.14it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 461/1070 [00:34<00:42, 14.19it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 463/1070 [00:35<00:42, 14.19it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 465/1070 [00:35<00:42, 14.22it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 467/1070 [00:35<00:42, 14.25it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 469/1070 [00:35<00:42, 14.26it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 471/1070 [00:35<00:41, 14.28it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 473/1070 [00:35<00:41, 14.29it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 475/1070 [00:35<00:41, 14.28it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 477/1070 [00:36<00:41, 14.30it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 479/1070 [00:36<00:41, 14.30it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 481/1070 [00:36<00:41, 14.30it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 483/1070 [00:36<00:41, 14.29it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 485/1070 [00:36<00:40, 14.29it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 487/1070 [00:36<00:40, 14.25it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 489/1070 [00:36<00:40, 14.25it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 491/1070 [00:37<00:40, 14.26it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 493/1070 [00:37<00:40, 14.27it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 495/1070 [00:37<00:40, 14.27it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 497/1070 [00:37<00:40, 14.27it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 499/1070 [00:37<00:40, 14.27it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 501/1070 [00:37<00:39, 14.27it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 503/1070 [00:37<00:39, 14.28it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 505/1070 [00:38<00:39, 14.28it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 507/1070 [00:38<00:39, 14.26it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 509/1070 [00:38<00:39, 14.26it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 511/1070 [00:38<00:39, 14.26it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 513/1070 [00:38<00:39, 14.26it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 515/1070 [00:38<00:38, 14.26it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 517/1070 [00:38<00:38, 14.21it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 519/1070 [00:39<00:38, 14.21it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 521/1070 [00:39<00:38, 14.17it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 523/1070 [00:39<00:38, 14.19it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 525/1070 [00:39<00:38, 14.22it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 527/1070 [00:39<00:38, 14.23it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 529/1070 [00:39<00:37, 14.25it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 531/1070 [00:39<00:37, 14.26it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 533/1070 [00:40<00:37, 14.27it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 535/1070 [00:40<00:37, 14.27it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 537/1070 [00:40<00:37, 14.25it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 539/1070 [00:40<00:37, 14.27it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 541/1070 [00:40<00:37, 14.27it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 543/1070 [00:40<00:36, 14.28it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 545/1070 [00:40<00:36, 14.28it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 547/1070 [00:41<00:36, 14.29it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 549/1070 [00:41<00:36, 14.26it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 551/1070 [00:41<00:36, 14.26it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 553/1070 [00:41<00:36, 14.27it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 555/1070 [00:41<00:36, 14.25it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 557/1070 [00:41<00:35, 14.26it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 559/1070 [00:41<00:35, 14.26it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 561/1070 [00:41<00:35, 14.27it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 563/1070 [00:42<00:35, 14.26it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 565/1070 [00:42<00:35, 14.27it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 567/1070 [00:42<00:35, 14.26it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 569/1070 [00:42<00:35, 14.25it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 571/1070 [00:42<00:35, 14.23it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 573/1070 [00:42<00:34, 14.22it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 575/1070 [00:42<00:34, 14.23it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 577/1070 [00:43<00:34, 14.22it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 579/1070 [00:43<00:34, 14.20it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 581/1070 [00:43<00:34, 14.23it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 583/1070 [00:43<00:34, 14.21it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 585/1070 [00:43<00:34, 14.24it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 587/1070 [00:43<00:33, 14.25it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 589/1070 [00:43<00:33, 14.26it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 591/1070 [00:44<00:33, 14.27it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 593/1070 [00:44<00:33, 14.27it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 595/1070 [00:44<00:33, 14.28it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 597/1070 [00:44<00:33, 14.28it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 599/1070 [00:44<00:32, 14.29it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 601/1070 [00:44<00:32, 14.25it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 603/1070 [00:44<00:32, 14.25it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 605/1070 [00:45<00:32, 14.26it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 607/1070 [00:45<00:32, 14.27it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 609/1070 [00:45<00:32, 14.27it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 611/1070 [00:45<00:32, 14.27it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 613/1070 [00:45<00:32, 14.28it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 615/1070 [00:45<00:31, 14.28it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 617/1070 [00:45<00:31, 14.28it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 619/1070 [00:46<00:31, 14.33it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 621/1070 [00:46<00:31, 14.31it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 623/1070 [00:46<00:31, 14.31it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 625/1070 [00:46<00:31, 14.29it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 627/1070 [00:46<00:30, 14.29it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 629/1070 [00:46<00:30, 14.29it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 631/1070 [00:46<00:30, 14.29it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 633/1070 [00:47<00:30, 14.28it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 635/1070 [00:47<00:30, 14.27it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 637/1070 [00:47<00:30, 14.26it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 639/1070 [00:47<00:30, 14.23it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 641/1070 [00:47<00:30, 14.26it/s]                                                   60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 642/1070 [00:47<00:30, 14.26it/s][INFO|trainer.py:755] 2023-11-15 22:23:03,760 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModelForTokenClassification.forward` and have been ignored: text. If text are not expected by `PeftModelForTokenClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 22:23:03,761 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 22:23:03,761 >>   Num examples = 760
[INFO|trainer.py:3134] 2023-11-15 22:23:03,762 >>   Batch size = 8
{'eval_loss': 0.2739241421222687, 'eval_accuracy': 0.9131578947368421, 'eval_micro_f1': 0.9131578947368421, 'eval_macro_f1': 0.9099189898952998, 'eval_runtime': 0.9232, 'eval_samples_per_second': 823.218, 'eval_steps_per_second': 102.902, 'epoch': 2.0}
{'loss': 0.2254, 'learning_rate': 0.0002, 'epoch': 3.0}

  0%|          | 0/95 [00:00<?, ?it/s][A
 13%|â–ˆâ–Ž        | 12/95 [00:00<00:00, 116.91it/s][A
 25%|â–ˆâ–ˆâ–Œ       | 24/95 [00:00<00:00, 110.82it/s][A
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 36/95 [00:00<00:00, 108.91it/s][A
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 47/95 [00:00<00:00, 108.07it/s][A
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 58/95 [00:00<00:00, 107.34it/s][A
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 69/95 [00:00<00:00, 106.97it/s][A
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 80/95 [00:00<00:00, 106.83it/s][A
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 91/95 [00:00<00:00, 106.73it/s][A                                                  
                                                [A 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 642/1070 [00:48<00:30, 14.26it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 95/95 [00:00<00:00, 106.73it/s][A
                                                [A 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 643/1070 [00:48<01:28,  4.82it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 645/1070 [00:48<01:10,  6.01it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 647/1070 [00:48<00:58,  7.28it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 649/1070 [00:49<00:49,  8.52it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 651/1070 [00:49<00:43,  9.69it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 653/1070 [00:49<00:38, 10.72it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 655/1070 [00:49<00:35, 11.59it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 657/1070 [00:49<00:33, 12.28it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 659/1070 [00:49<00:32, 12.82it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 661/1070 [00:49<00:30, 13.23it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 663/1070 [00:50<00:30, 13.52it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 665/1070 [00:50<00:29, 13.73it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 667/1070 [00:50<00:29, 13.88it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 669/1070 [00:50<00:28, 13.98it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 671/1070 [00:50<00:28, 14.06it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 673/1070 [00:50<00:28, 14.12it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 675/1070 [00:50<00:27, 14.15it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 677/1070 [00:51<00:27, 14.17it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 679/1070 [00:51<00:27, 14.19it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 681/1070 [00:51<00:27, 14.20it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 683/1070 [00:51<00:27, 14.20it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 685/1070 [00:51<00:27, 14.20it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 687/1070 [00:51<00:26, 14.21it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 689/1070 [00:51<00:26, 14.22it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 691/1070 [00:52<00:26, 14.23it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 693/1070 [00:52<00:26, 14.24it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 695/1070 [00:52<00:26, 14.25it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 697/1070 [00:52<00:26, 14.25it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 699/1070 [00:52<00:26, 14.25it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 701/1070 [00:52<00:25, 14.23it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 703/1070 [00:52<00:25, 14.21it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 705/1070 [00:53<00:25, 14.19it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 707/1070 [00:53<00:25, 14.20it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 709/1070 [00:53<00:25, 14.15it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 711/1070 [00:53<00:25, 14.14it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 713/1070 [00:53<00:25, 14.16it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 715/1070 [00:53<00:25, 14.17it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 717/1070 [00:53<00:24, 14.18it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 719/1070 [00:54<00:24, 14.18it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 721/1070 [00:54<00:24, 14.18it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 723/1070 [00:54<00:24, 14.18it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 725/1070 [00:54<00:24, 14.17it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 727/1070 [00:54<00:24, 14.15it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 729/1070 [00:54<00:24, 14.16it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 731/1070 [00:54<00:23, 14.17it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 733/1070 [00:54<00:23, 14.18it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 735/1070 [00:55<00:23, 14.15it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 737/1070 [00:55<00:23, 14.12it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 739/1070 [00:55<00:23, 14.12it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 741/1070 [00:55<00:23, 14.12it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 743/1070 [00:55<00:23, 14.15it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 745/1070 [00:55<00:22, 14.16it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 747/1070 [00:55<00:22, 14.17it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 749/1070 [00:56<00:22, 14.16it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 751/1070 [00:56<00:22, 14.16it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 753/1070 [00:56<00:22, 14.16it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 755/1070 [00:56<00:22, 14.15it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 757/1070 [00:56<00:22, 14.16it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 759/1070 [00:56<00:21, 14.17it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 761/1070 [00:56<00:21, 14.16it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 763/1070 [00:57<00:21, 14.16it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 765/1070 [00:57<00:21, 14.16it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 767/1070 [00:57<00:21, 14.15it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 769/1070 [00:57<00:21, 14.15it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 771/1070 [00:57<00:21, 14.14it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 773/1070 [00:57<00:20, 14.15it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 775/1070 [00:57<00:20, 14.12it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 777/1070 [00:58<00:20, 14.09it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 779/1070 [00:58<00:20, 13.96it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 781/1070 [00:58<00:20, 14.01it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 783/1070 [00:58<00:20, 14.06it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 785/1070 [00:58<00:21, 13.55it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 787/1070 [00:58<00:20, 13.64it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 789/1070 [00:58<00:20, 13.79it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 791/1070 [00:59<00:20, 13.78it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 793/1070 [00:59<00:19, 13.85it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 795/1070 [00:59<00:19, 13.95it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 797/1070 [00:59<00:19, 14.02it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 799/1070 [00:59<00:19, 13.89it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 801/1070 [00:59<00:19, 13.96it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 803/1070 [00:59<00:19, 14.02it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 805/1070 [01:00<00:18, 14.06it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 807/1070 [01:00<00:18, 14.10it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 809/1070 [01:00<00:18, 14.12it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 811/1070 [01:00<00:18, 14.13it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 813/1070 [01:00<00:18, 14.13it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 815/1070 [01:00<00:18, 14.12it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 817/1070 [01:00<00:17, 14.14it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 819/1070 [01:01<00:17, 14.15it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 821/1070 [01:01<00:17, 14.15it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 823/1070 [01:01<00:17, 14.15it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 825/1070 [01:01<00:17, 14.15it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 827/1070 [01:01<00:17, 14.14it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 829/1070 [01:01<00:17, 14.13it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 831/1070 [01:01<00:16, 14.14it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 833/1070 [01:02<00:16, 14.16it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 835/1070 [01:02<00:16, 14.16it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 837/1070 [01:02<00:16, 14.17it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 839/1070 [01:02<00:16, 14.17it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 841/1070 [01:02<00:16, 14.16it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 843/1070 [01:02<00:16, 14.15it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 845/1070 [01:02<00:15, 14.14it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 847/1070 [01:03<00:15, 14.11it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 849/1070 [01:03<00:15, 14.12it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 851/1070 [01:03<00:15, 14.12it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 853/1070 [01:03<00:15, 14.12it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 855/1070 [01:03<00:15, 14.12it/s]                                                   80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 856/1070 [01:03<00:15, 14.12it/s][INFO|trainer.py:755] 2023-11-15 22:23:19,811 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModelForTokenClassification.forward` and have been ignored: text. If text are not expected by `PeftModelForTokenClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 22:23:19,812 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 22:23:19,813 >>   Num examples = 760
[INFO|trainer.py:3134] 2023-11-15 22:23:19,813 >>   Batch size = 8
{'eval_loss': 0.2654804587364197, 'eval_accuracy': 0.9052631578947369, 'eval_micro_f1': 0.9052631578947369, 'eval_macro_f1': 0.9025657880546132, 'eval_runtime': 0.9229, 'eval_samples_per_second': 823.488, 'eval_steps_per_second': 102.936, 'epoch': 3.0}
{'loss': 0.1944, 'learning_rate': 0.0001, 'epoch': 4.0}

  0%|          | 0/95 [00:00<?, ?it/s][A
 13%|â–ˆâ–Ž        | 12/95 [00:00<00:00, 115.89it/s][A
 25%|â–ˆâ–ˆâ–Œ       | 24/95 [00:00<00:00, 109.83it/s][A
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 36/95 [00:00<00:00, 106.93it/s][A
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 47/95 [00:00<00:00, 106.33it/s][A
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 58/95 [00:00<00:00, 106.15it/s][A
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 69/95 [00:00<00:00, 106.06it/s][A
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 80/95 [00:00<00:00, 106.02it/s][A
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 91/95 [00:00<00:00, 105.94it/s][A                                                  
                                                [A 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 856/1070 [01:04<00:15, 14.12it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 95/95 [00:00<00:00, 105.94it/s][A
                                                [A 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 857/1070 [01:04<00:44,  4.77it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 859/1070 [01:04<00:35,  5.94it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 861/1070 [01:05<00:29,  7.19it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 863/1070 [01:05<00:24,  8.44it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 865/1070 [01:05<00:21,  9.61it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 867/1070 [01:05<00:19, 10.63it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 869/1070 [01:05<00:17, 11.49it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 871/1070 [01:05<00:16, 12.18it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 873/1070 [01:05<00:15, 12.70it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 875/1070 [01:05<00:14, 13.11it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 877/1070 [01:06<00:14, 13.43it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 879/1070 [01:06<00:13, 13.66it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 881/1070 [01:06<00:13, 13.82it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 883/1070 [01:06<00:13, 13.93it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 885/1070 [01:06<00:13, 14.01it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 887/1070 [01:06<00:13, 14.03it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 889/1070 [01:06<00:12, 14.03it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 891/1070 [01:07<00:12, 14.07it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 893/1070 [01:07<00:12, 14.11it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 895/1070 [01:07<00:12, 14.13it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 897/1070 [01:07<00:12, 14.14it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 899/1070 [01:07<00:12, 14.13it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 901/1070 [01:07<00:11, 14.13it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 903/1070 [01:07<00:11, 14.13it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 905/1070 [01:08<00:11, 14.14it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 907/1070 [01:08<00:11, 14.16it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 909/1070 [01:08<00:11, 14.18it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 911/1070 [01:08<00:11, 14.19it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 913/1070 [01:08<00:11, 14.19it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 915/1070 [01:08<00:10, 14.19it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 917/1070 [01:08<00:10, 14.18it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 919/1070 [01:09<00:10, 14.17it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 921/1070 [01:09<00:10, 14.17it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 923/1070 [01:09<00:10, 14.16it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 925/1070 [01:09<00:10, 14.17it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 927/1070 [01:09<00:10, 14.18it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 929/1070 [01:09<00:09, 14.18it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 931/1070 [01:09<00:09, 14.18it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 933/1070 [01:10<00:09, 14.18it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 935/1070 [01:10<00:09, 14.17it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 937/1070 [01:10<00:09, 14.16it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 939/1070 [01:10<00:09, 14.15it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 941/1070 [01:10<00:09, 14.15it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 943/1070 [01:10<00:08, 14.16it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 945/1070 [01:10<00:08, 14.17it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 947/1070 [01:11<00:08, 14.20it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 949/1070 [01:11<00:08, 14.19it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 951/1070 [01:11<00:08, 14.18it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 953/1070 [01:11<00:08, 14.18it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 955/1070 [01:11<00:08, 14.16it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 957/1070 [01:11<00:07, 14.16it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 959/1070 [01:11<00:07, 14.17it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 961/1070 [01:12<00:07, 14.18it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 963/1070 [01:12<00:07, 14.18it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 965/1070 [01:12<00:07, 14.18it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 967/1070 [01:12<00:07, 14.18it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 969/1070 [01:12<00:07, 14.17it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 971/1070 [01:12<00:06, 14.16it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 973/1070 [01:12<00:06, 14.15it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 975/1070 [01:13<00:06, 14.16it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 977/1070 [01:13<00:06, 14.18it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 979/1070 [01:13<00:06, 14.19it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 981/1070 [01:13<00:06, 14.19it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 983/1070 [01:13<00:06, 14.19it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 985/1070 [01:13<00:05, 14.17it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 987/1070 [01:13<00:05, 14.17it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 989/1070 [01:14<00:05, 14.16it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 991/1070 [01:14<00:05, 14.14it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 993/1070 [01:14<00:05, 14.16it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 995/1070 [01:14<00:05, 14.17it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 997/1070 [01:14<00:05, 14.18it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 999/1070 [01:14<00:05, 14.14it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1001/1070 [01:14<00:04, 14.14it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1003/1070 [01:15<00:04, 14.14it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1005/1070 [01:15<00:04, 14.13it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1007/1070 [01:15<00:04, 14.14it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1009/1070 [01:15<00:04, 14.16it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1011/1070 [01:15<00:04, 14.17it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1013/1070 [01:15<00:04, 14.14it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1015/1070 [01:15<00:03, 14.14it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1017/1070 [01:16<00:03, 14.15it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1019/1070 [01:16<00:03, 14.14it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1021/1070 [01:16<00:03, 14.15it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1023/1070 [01:16<00:03, 14.16it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1025/1070 [01:16<00:03, 14.18it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1027/1070 [01:16<00:03, 14.18it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1029/1070 [01:16<00:02, 14.17it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1031/1070 [01:17<00:02, 14.16it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1033/1070 [01:17<00:02, 14.15it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1035/1070 [01:17<00:02, 14.15it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1037/1070 [01:17<00:02, 14.15it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1039/1070 [01:17<00:02, 14.16it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1041/1070 [01:17<00:02, 14.16it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1043/1070 [01:17<00:01, 14.17it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1045/1070 [01:17<00:01, 14.17it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1047/1070 [01:18<00:01, 14.17it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1049/1070 [01:18<00:01, 14.17it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1051/1070 [01:18<00:01, 14.15it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1053/1070 [01:18<00:01, 14.16it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1055/1070 [01:18<00:01, 14.17it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1057/1070 [01:18<00:00, 14.18it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1059/1070 [01:18<00:00, 14.18it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1061/1070 [01:19<00:00, 14.17it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1063/1070 [01:19<00:00, 14.16it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1065/1070 [01:19<00:00, 14.16it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1067/1070 [01:19<00:00, 14.13it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1069/1070 [01:19<00:00, 14.15it/s]                                                   100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1070/1070 [01:19<00:00, 14.15it/s][INFO|trainer.py:755] 2023-11-15 22:23:35,847 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModelForTokenClassification.forward` and have been ignored: text. If text are not expected by `PeftModelForTokenClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 22:23:35,849 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 22:23:35,850 >>   Num examples = 760
[INFO|trainer.py:3134] 2023-11-15 22:23:35,850 >>   Batch size = 8
{'eval_loss': 0.27661269903182983, 'eval_accuracy': 0.906578947368421, 'eval_micro_f1': 0.906578947368421, 'eval_macro_f1': 0.9038954543615684, 'eval_runtime': 0.9331, 'eval_samples_per_second': 814.529, 'eval_steps_per_second': 101.816, 'epoch': 4.0}
{'loss': 0.1647, 'learning_rate': 0.0, 'epoch': 5.0}

  0%|          | 0/95 [00:00<?, ?it/s][A
 13%|â–ˆâ–Ž        | 12/95 [00:00<00:00, 115.55it/s][A
 25%|â–ˆâ–ˆâ–Œ       | 24/95 [00:00<00:00, 109.80it/s][A
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 35/95 [00:00<00:00, 108.05it/s][A
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 46/95 [00:00<00:00, 107.22it/s][A
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 57/95 [00:00<00:00, 106.67it/s][A
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 68/95 [00:00<00:00, 106.43it/s][A
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 79/95 [00:00<00:00, 106.18it/s][A
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 90/95 [00:00<00:00, 106.08it/s][A                                                   
                                                [A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1070/1070 [01:20<00:00, 14.15it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 95/95 [00:00<00:00, 106.08it/s][A
                                                [A[INFO|trainer.py:1963] 2023-11-15 22:23:36,786 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                   100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1070/1070 [01:20<00:00, 14.15it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1070/1070 [01:20<00:00, 13.26it/s]
[INFO|trainer.py:2855] 2023-11-15 22:23:36,789 >> Saving model checkpoint to ./result/agnews_sup_roberta-base_seed3_lora
[INFO|tokenization_utils_base.py:2235] 2023-11-15 22:23:36,897 >> tokenizer config file saved in ./result/agnews_sup_roberta-base_seed3_lora/tokenizer_config.json
[INFO|tokenization_utils_base.py:2242] 2023-11-15 22:23:36,899 >> Special tokens file saved in ./result/agnews_sup_roberta-base_seed3_lora/special_tokens_map.json
{'eval_loss': 0.27896836400032043, 'eval_accuracy': 0.9092105263157895, 'eval_micro_f1': 0.9092105263157895, 'eval_macro_f1': 0.906309870461506, 'eval_runtime': 0.9327, 'eval_samples_per_second': 814.862, 'eval_steps_per_second': 101.858, 'epoch': 5.0}
{'train_runtime': 80.6894, 'train_samples_per_second': 423.848, 'train_steps_per_second': 13.261, 'train_loss': 0.2569741079740435, 'epoch': 5.0}
***** train metrics *****
  epoch                    =        5.0
  train_loss               =      0.257
  train_runtime            = 0:01:20.68
  train_samples            =       6840
  train_samples_per_second =    423.848
  train_steps_per_second   =     13.261
11/15/2023 22:23:37 - INFO - __main__ - *** Evaluate ***
[INFO|trainer.py:755] 2023-11-15 22:23:37,002 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModelForTokenClassification.forward` and have been ignored: text. If text are not expected by `PeftModelForTokenClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 22:23:37,004 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 22:23:37,004 >>   Num examples = 760
[INFO|trainer.py:3134] 2023-11-15 22:23:37,004 >>   Batch size = 8
  0%|          | 0/95 [00:00<?, ?it/s] 13%|â–ˆâ–Ž        | 12/95 [00:00<00:00, 117.50it/s] 25%|â–ˆâ–ˆâ–Œ       | 24/95 [00:00<00:00, 110.87it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 36/95 [00:00<00:00, 108.97it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 47/95 [00:00<00:00, 108.09it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 58/95 [00:00<00:00, 107.59it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 69/95 [00:00<00:00, 107.30it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 80/95 [00:00<00:00, 107.21it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 91/95 [00:00<00:00, 107.07it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 95/95 [00:00<00:00, 105.10it/s]
***** eval metrics *****
  epoch                   =        5.0
  eval_accuracy           =     0.9092
  eval_loss               =      0.279
  eval_macro_f1           =     0.9063
  eval_micro_f1           =     0.9092
  eval_runtime            = 0:00:00.91
  eval_samples            =        760
  eval_samples_per_second =    828.966
  eval_steps_per_second   =    103.621
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                  eval/accuracy â–â–ˆâ–…â–…â–†â–†
wandb:                      eval/loss â–ˆâ–‚â–â–ƒâ–ƒâ–ƒ
wandb:                  eval/macro_f1 â–â–ˆâ–…â–…â–†â–†
wandb:                  eval/micro_f1 â–â–ˆâ–…â–…â–†â–†
wandb:                   eval/runtime â–â–„â–„â–ˆâ–ˆâ–‚
wandb:        eval/samples_per_second â–ˆâ–…â–…â–â–â–‡
wandb:          eval/steps_per_second â–ˆâ–…â–…â–â–â–‡
wandb:                    train/epoch â–â–â–ƒâ–ƒâ–…â–…â–†â–†â–ˆâ–ˆâ–ˆâ–ˆ
wandb:              train/global_step â–â–â–ƒâ–ƒâ–„â–„â–†â–†â–ˆâ–ˆâ–ˆâ–ˆ
wandb:            train/learning_rate â–ˆâ–†â–…â–ƒâ–
wandb:                     train/loss â–ˆâ–„â–ƒâ–‚â–
wandb:               train/total_flos â–
wandb:               train/train_loss â–
wandb:            train/train_runtime â–
wandb: train/train_samples_per_second â–
wandb:   train/train_steps_per_second â–
wandb: 
wandb: Run summary:
wandb:                  eval/accuracy 0.90921
wandb:                      eval/loss 0.27897
wandb:                  eval/macro_f1 0.90631
wandb:                  eval/micro_f1 0.90921
wandb:                   eval/runtime 0.9168
wandb:        eval/samples_per_second 828.966
wandb:          eval/steps_per_second 103.621
wandb:                    train/epoch 5.0
wandb:              train/global_step 1070
wandb:            train/learning_rate 0.0
wandb:                     train/loss 0.1647
wandb:               train/total_flos 1140362523648000.0
wandb:               train/train_loss 0.25697
wandb:            train/train_runtime 80.6894
wandb: train/train_samples_per_second 423.848
wandb:   train/train_steps_per_second 13.261
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /ceph/home/wangyifei/a3/wandb/offline-run-20231115_222058-rzphlk2g
wandb: Find logs at: ./wandb/offline-run-20231115_222058-rzphlk2g/logs
(ModelArguments(model_name_or_path='roberta-base', cache_dir='./cache', use_adapter=False, use_lora=True), DataTrainingArguments(dataset_name='restaurant', max_seq_length=64), TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.0005,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=./result/restaurant_roberta-base_seed4_lora/runs/Nov15_22-23-47_node010,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.EPOCH,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=5.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
output_dir=./result/restaurant_roberta-base_seed4_lora,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
resume_from_checkpoint=None,
run_name=./result/restaurant_roberta-base_seed4_lora,
save_on_each_node=False,
save_safetensors=False,
save_steps=500,
save_strategy=IntervalStrategy.NO,
save_total_limit=None,
seed=555,
sharded_ddp=[],
skip_memory_metrics=True,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
))
11/15/2023 22:23:47 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False
11/15/2023 22:23:47 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.0005,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=./result/restaurant_roberta-base_seed4_lora/runs/Nov15_22-23-47_node010,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.EPOCH,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=5.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
output_dir=./result/restaurant_roberta-base_seed4_lora,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
resume_from_checkpoint=None,
run_name=./result/restaurant_roberta-base_seed4_lora,
save_on_each_node=False,
save_safetensors=False,
save_steps=500,
save_strategy=IntervalStrategy.NO,
save_total_limit=None,
seed=555,
sharded_ddp=[],
skip_memory_metrics=True,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
wandb: Tracking run with wandb version 0.15.12
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
Map:   0%|          | 0/4722 [00:00<?, ? examples/s]Map:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4159/4722 [00:00<00:00, 41314.04 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4722/4722 [00:00<00:00, 40650.69 examples/s]
[INFO|configuration_utils.py:715] 2023-11-15 22:24:03,322 >> loading configuration file config.json from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/config.json
[INFO|configuration_utils.py:775] 2023-11-15 22:24:03,331 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.33.3",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_auto.py:535] 2023-11-15 22:24:13,390 >> Could not locate the tokenizer configuration file, will try to use the model config instead.
[INFO|configuration_utils.py:715] 2023-11-15 22:24:23,399 >> loading configuration file config.json from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/config.json
[INFO|configuration_utils.py:775] 2023-11-15 22:24:23,400 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.33.3",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:1852] 2023-11-15 22:24:43,446 >> loading file vocab.json from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/vocab.json
[INFO|tokenization_utils_base.py:1852] 2023-11-15 22:24:43,447 >> loading file merges.txt from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/merges.txt
[INFO|tokenization_utils_base.py:1852] 2023-11-15 22:24:43,447 >> loading file tokenizer.json from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/tokenizer.json
[INFO|tokenization_utils_base.py:1852] 2023-11-15 22:24:43,447 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1852] 2023-11-15 22:24:43,448 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1852] 2023-11-15 22:24:43,448 >> loading file tokenizer_config.json from cache at None
[INFO|configuration_utils.py:715] 2023-11-15 22:24:43,449 >> loading configuration file config.json from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/config.json
[INFO|configuration_utils.py:775] 2023-11-15 22:24:43,450 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.33.3",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:2862] 2023-11-15 22:25:03,614 >> loading weights file model.safetensors from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/model.safetensors
[INFO|modeling_utils.py:3638] 2023-11-15 22:25:04,307 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3650] 2023-11-15 22:25:04,308 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
trainable params: 1,284,867 || all params: 125,830,662 || trainable%: 1.0211080348603745
model_max_length: 512
Running tokenizer on dataset:   0%|          | 0/3777 [00:00<?, ? examples/s]Running tokenizer on dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3777/3777 [00:00<00:00, 24572.65 examples/s]Running tokenizer on dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3777/3777 [00:00<00:00, 24133.84 examples/s]
Running tokenizer on dataset:   0%|          | 0/945 [00:00<?, ? examples/s]Running tokenizer on dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 945/945 [00:00<00:00, 32885.99 examples/s]
11/15/2023 22:25:04 - INFO - __main__ - Sample 791 of the training set: {'text': 'plain pizza <SEP> The plain pizza was soggy and the creative wild mushroom(third generation-Fornini) pizza we had was drenched with truffle oil in the middle( again making it soggy) and nothingon the rest.', 'label': 2, 'input_ids': [0, 21306, 9366, 28696, 3388, 510, 15698, 20, 10798, 9366, 21, 579, 2154, 4740, 8, 5, 3904, 3418, 30004, 1640, 12347, 2706, 12, 597, 4244, 2531, 43, 9366, 52, 56, 21, 385, 30388, 19, 2664, 15315, 681, 11, 5, 1692, 1640, 456, 442, 24, 579, 2154, 4740, 43, 8, 1085, 261, 5, 1079, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
11/15/2023 22:25:04 - INFO - __main__ - Sample 1124 of the training set: {'text': "lamb sausages <SEP> The dishes offered were unique, very tasty and fresh from the lamb sausages, sardines with biscuits, large whole shrimp to the amazing pistachio ice cream (the best and freshest I've ever had).", 'label': 0, 'input_ids': [0, 5112, 428, 2241, 687, 3443, 28696, 3388, 510, 15698, 20, 10230, 1661, 58, 2216, 6, 182, 22307, 8, 2310, 31, 5, 17988, 2241, 687, 3443, 6, 579, 1120, 3141, 19, 31729, 6, 739, 1086, 22126, 7, 5, 2770, 32617, 1488, 1020, 2480, 6353, 36, 627, 275, 8, 21862, 20921, 38, 348, 655, 56, 322, 2, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]}.
11/15/2023 22:25:04 - INFO - __main__ - Sample 659 of the training set: {'text': 'ingredients <SEP> Great value for the quality ingredients.', 'label': 0, 'input_ids': [0, 154, 48205, 28696, 3388, 510, 15698, 2860, 923, 13, 5, 1318, 7075, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
11/15/2023 22:25:04 - WARNING - accelerate.utils.other - Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[INFO|trainer.py:755] 2023-11-15 22:25:05,741 >> The following columns in the training set don't have a corresponding argument in `PeftModelForTokenClassification.forward` and have been ignored: text. If text are not expected by `PeftModelForTokenClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:1715] 2023-11-15 22:25:05,751 >> ***** Running training *****
[INFO|trainer.py:1716] 2023-11-15 22:25:05,752 >>   Num examples = 3,777
[INFO|trainer.py:1717] 2023-11-15 22:25:05,752 >>   Num Epochs = 5
[INFO|trainer.py:1718] 2023-11-15 22:25:05,752 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1721] 2023-11-15 22:25:05,752 >>   Total train batch size (w. parallel, distributed & accumulation) = 32
[INFO|trainer.py:1722] 2023-11-15 22:25:05,753 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:1723] 2023-11-15 22:25:05,753 >>   Total optimization steps = 595
[INFO|trainer.py:1724] 2023-11-15 22:25:05,754 >>   Number of trainable parameters = 1,284,867
[INFO|integration_utils.py:716] 2023-11-15 22:25:05,755 >> Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"
  0%|          | 0/595 [00:00<?, ?it/s]  0%|          | 1/595 [00:00<09:53,  1.00it/s]  1%|          | 3/595 [00:01<03:04,  3.21it/s]  1%|          | 5/595 [00:01<01:50,  5.32it/s]  1%|          | 7/595 [00:01<01:20,  7.27it/s]  2%|â–         | 9/595 [00:01<01:05,  8.93it/s]  2%|â–         | 11/595 [00:01<00:56, 10.29it/s]  2%|â–         | 13/595 [00:01<00:51, 11.35it/s]  3%|â–Ž         | 15/595 [00:01<00:47, 12.17it/s]  3%|â–Ž         | 17/595 [00:02<00:45, 12.78it/s]  3%|â–Ž         | 19/595 [00:02<00:43, 13.21it/s]  4%|â–Ž         | 21/595 [00:02<00:42, 13.57it/s]  4%|â–         | 23/595 [00:02<00:41, 13.80it/s]  4%|â–         | 25/595 [00:02<00:40, 14.00it/s]  5%|â–         | 27/595 [00:02<00:40, 14.14it/s]  5%|â–         | 29/595 [00:02<00:39, 14.21it/s]  5%|â–Œ         | 31/595 [00:03<00:39, 14.25it/s]  6%|â–Œ         | 33/595 [00:03<00:39, 14.29it/s]  6%|â–Œ         | 35/595 [00:03<00:39, 14.32it/s]  6%|â–Œ         | 37/595 [00:03<00:38, 14.34it/s]  7%|â–‹         | 39/595 [00:03<00:38, 14.35it/s]  7%|â–‹         | 41/595 [00:03<00:38, 14.37it/s]  7%|â–‹         | 43/595 [00:03<00:38, 14.39it/s]  8%|â–Š         | 45/595 [00:04<00:38, 14.41it/s]  8%|â–Š         | 47/595 [00:04<00:37, 14.44it/s]  8%|â–Š         | 49/595 [00:04<00:37, 14.42it/s]  9%|â–Š         | 51/595 [00:04<00:37, 14.39it/s]  9%|â–‰         | 53/595 [00:04<00:37, 14.38it/s]  9%|â–‰         | 55/595 [00:04<00:37, 14.37it/s] 10%|â–‰         | 57/595 [00:04<00:37, 14.38it/s] 10%|â–‰         | 59/595 [00:05<00:37, 14.38it/s] 10%|â–ˆ         | 61/595 [00:05<00:37, 14.35it/s] 11%|â–ˆ         | 63/595 [00:05<00:37, 14.35it/s] 11%|â–ˆ         | 65/595 [00:05<00:36, 14.37it/s] 11%|â–ˆâ–        | 67/595 [00:05<00:36, 14.34it/s] 12%|â–ˆâ–        | 69/595 [00:05<00:36, 14.36it/s] 12%|â–ˆâ–        | 71/595 [00:05<00:36, 14.39it/s] 12%|â–ˆâ–        | 73/595 [00:06<00:36, 14.39it/s] 13%|â–ˆâ–Ž        | 75/595 [00:06<00:36, 14.37it/s] 13%|â–ˆâ–Ž        | 77/595 [00:06<00:36, 14.34it/s] 13%|â–ˆâ–Ž        | 79/595 [00:06<00:35, 14.34it/s] 14%|â–ˆâ–Ž        | 81/595 [00:06<00:35, 14.33it/s] 14%|â–ˆâ–        | 83/595 [00:06<00:35, 14.34it/s] 14%|â–ˆâ–        | 85/595 [00:06<00:35, 14.34it/s] 15%|â–ˆâ–        | 87/595 [00:06<00:35, 14.33it/s] 15%|â–ˆâ–        | 89/595 [00:07<00:35, 14.35it/s] 15%|â–ˆâ–Œ        | 91/595 [00:07<00:35, 14.36it/s] 16%|â–ˆâ–Œ        | 93/595 [00:07<00:34, 14.37it/s] 16%|â–ˆâ–Œ        | 95/595 [00:07<00:34, 14.38it/s] 16%|â–ˆâ–‹        | 97/595 [00:07<00:34, 14.40it/s] 17%|â–ˆâ–‹        | 99/595 [00:07<00:34, 14.41it/s] 17%|â–ˆâ–‹        | 101/595 [00:07<00:34, 14.42it/s] 17%|â–ˆâ–‹        | 103/595 [00:08<00:34, 14.39it/s] 18%|â–ˆâ–Š        | 105/595 [00:08<00:34, 14.38it/s] 18%|â–ˆâ–Š        | 107/595 [00:08<00:33, 14.36it/s] 18%|â–ˆâ–Š        | 109/595 [00:08<00:33, 14.33it/s] 19%|â–ˆâ–Š        | 111/595 [00:08<00:33, 14.33it/s] 19%|â–ˆâ–‰        | 113/595 [00:08<00:33, 14.34it/s] 19%|â–ˆâ–‰        | 115/595 [00:08<00:33, 14.35it/s] 20%|â–ˆâ–‰        | 117/595 [00:09<00:33, 14.31it/s] 20%|â–ˆâ–ˆ        | 119/595 [00:09<00:30, 15.63it/s]                                                  20%|â–ˆâ–ˆ        | 119/595 [00:09<00:30, 15.63it/s][INFO|trainer.py:755] 2023-11-15 22:25:14,938 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModelForTokenClassification.forward` and have been ignored: text. If text are not expected by `PeftModelForTokenClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 22:25:14,940 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 22:25:14,940 >>   Num examples = 945
[INFO|trainer.py:3134] 2023-11-15 22:25:14,940 >>   Batch size = 8
{'loss': 0.7002, 'learning_rate': 0.0004, 'epoch': 1.0}

  0%|          | 0/119 [00:00<?, ?it/s][A
 10%|â–ˆ         | 12/119 [00:00<00:00, 119.10it/s][A
 20%|â–ˆâ–ˆ        | 24/119 [00:00<00:00, 112.15it/s][A
 30%|â–ˆâ–ˆâ–ˆ       | 36/119 [00:00<00:00, 109.96it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 48/119 [00:00<00:00, 108.80it/s][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 59/119 [00:00<00:00, 108.55it/s][A
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 70/119 [00:00<00:00, 108.52it/s][A
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 81/119 [00:00<00:00, 108.26it/s][A
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 92/119 [00:00<00:00, 108.30it/s][A
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 103/119 [00:00<00:00, 108.21it/s][A
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 114/119 [00:01<00:00, 108.25it/s][A                                                 
                                                  [A 20%|â–ˆâ–ˆ        | 119/595 [00:10<00:30, 15.63it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 119/119 [00:01<00:00, 108.25it/s][A
                                                  [A 20%|â–ˆâ–ˆ        | 121/595 [00:10<01:52,  4.22it/s] 21%|â–ˆâ–ˆ        | 123/595 [00:10<01:28,  5.36it/s] 21%|â–ˆâ–ˆ        | 125/595 [00:10<01:11,  6.60it/s] 21%|â–ˆâ–ˆâ–       | 127/595 [00:10<00:59,  7.87it/s] 22%|â–ˆâ–ˆâ–       | 129/595 [00:11<00:51,  9.11it/s] 22%|â–ˆâ–ˆâ–       | 131/595 [00:11<00:45, 10.24it/s] 22%|â–ˆâ–ˆâ–       | 133/595 [00:11<00:41, 11.21it/s] 23%|â–ˆâ–ˆâ–Ž       | 135/595 [00:11<00:38, 12.01it/s] 23%|â–ˆâ–ˆâ–Ž       | 137/595 [00:11<00:36, 12.65it/s] 23%|â–ˆâ–ˆâ–Ž       | 139/595 [00:11<00:34, 13.13it/s] 24%|â–ˆâ–ˆâ–Ž       | 141/595 [00:11<00:33, 13.47it/s] 24%|â–ˆâ–ˆâ–       | 143/595 [00:11<00:32, 13.72it/s] 24%|â–ˆâ–ˆâ–       | 145/595 [00:12<00:32, 13.91it/s] 25%|â–ˆâ–ˆâ–       | 147/595 [00:12<00:31, 14.03it/s] 25%|â–ˆâ–ˆâ–Œ       | 149/595 [00:12<00:31, 14.12it/s] 25%|â–ˆâ–ˆâ–Œ       | 151/595 [00:12<00:31, 14.17it/s] 26%|â–ˆâ–ˆâ–Œ       | 153/595 [00:12<00:31, 14.24it/s] 26%|â–ˆâ–ˆâ–Œ       | 155/595 [00:12<00:30, 14.28it/s] 26%|â–ˆâ–ˆâ–‹       | 157/595 [00:12<00:30, 14.31it/s] 27%|â–ˆâ–ˆâ–‹       | 159/595 [00:13<00:30, 14.33it/s] 27%|â–ˆâ–ˆâ–‹       | 161/595 [00:13<00:30, 14.34it/s] 27%|â–ˆâ–ˆâ–‹       | 163/595 [00:13<00:30, 14.37it/s] 28%|â–ˆâ–ˆâ–Š       | 165/595 [00:13<00:29, 14.39it/s] 28%|â–ˆâ–ˆâ–Š       | 167/595 [00:13<00:29, 14.41it/s] 28%|â–ˆâ–ˆâ–Š       | 169/595 [00:13<00:29, 14.36it/s] 29%|â–ˆâ–ˆâ–Š       | 171/595 [00:13<00:29, 14.36it/s] 29%|â–ˆâ–ˆâ–‰       | 173/595 [00:14<00:29, 14.35it/s] 29%|â–ˆâ–ˆâ–‰       | 175/595 [00:14<00:29, 14.35it/s] 30%|â–ˆâ–ˆâ–‰       | 177/595 [00:14<00:29, 14.35it/s] 30%|â–ˆâ–ˆâ–ˆ       | 179/595 [00:14<00:29, 14.34it/s] 30%|â–ˆâ–ˆâ–ˆ       | 181/595 [00:14<00:28, 14.33it/s] 31%|â–ˆâ–ˆâ–ˆ       | 183/595 [00:14<00:28, 14.35it/s] 31%|â–ˆâ–ˆâ–ˆ       | 185/595 [00:14<00:28, 14.34it/s] 31%|â–ˆâ–ˆâ–ˆâ–      | 187/595 [00:15<00:28, 14.35it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 189/595 [00:15<00:28, 14.37it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 191/595 [00:15<00:28, 14.38it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 193/595 [00:15<00:27, 14.39it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 195/595 [00:15<00:27, 14.39it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 197/595 [00:15<00:27, 14.36it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 199/595 [00:15<00:27, 14.34it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 201/595 [00:16<00:27, 14.35it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 203/595 [00:16<00:27, 14.34it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 205/595 [00:16<00:27, 14.33it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 207/595 [00:16<00:27, 14.33it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 209/595 [00:16<00:26, 14.32it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 211/595 [00:16<00:26, 14.33it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 213/595 [00:16<00:26, 14.32it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 215/595 [00:17<00:26, 14.29it/s] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 217/595 [00:17<00:26, 14.28it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 219/595 [00:17<00:26, 14.29it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 221/595 [00:17<00:26, 14.29it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 223/595 [00:17<00:26, 14.28it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 225/595 [00:17<00:25, 14.28it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 227/595 [00:17<00:25, 14.29it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 229/595 [00:17<00:25, 14.29it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 231/595 [00:18<00:25, 14.30it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 233/595 [00:18<00:25, 14.28it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 235/595 [00:18<00:25, 14.28it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 237/595 [00:18<00:24, 14.35it/s]                                                  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 238/595 [00:18<00:24, 14.35it/s][INFO|trainer.py:755] 2023-11-15 22:25:24,332 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModelForTokenClassification.forward` and have been ignored: text. If text are not expected by `PeftModelForTokenClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 22:25:24,334 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 22:25:24,335 >>   Num examples = 945
[INFO|trainer.py:3134] 2023-11-15 22:25:24,335 >>   Batch size = 8
{'eval_loss': 0.5917644500732422, 'eval_accuracy': 0.783068783068783, 'eval_micro_f1': 0.7830687830687831, 'eval_macro_f1': 0.7022570918784377, 'eval_runtime': 1.1337, 'eval_samples_per_second': 833.587, 'eval_steps_per_second': 104.97, 'epoch': 1.0}
{'loss': 0.4873, 'learning_rate': 0.0003, 'epoch': 2.0}

  0%|          | 0/119 [00:00<?, ?it/s][A
 10%|â–ˆ         | 12/119 [00:00<00:00, 118.13it/s][A
 20%|â–ˆâ–ˆ        | 24/119 [00:00<00:00, 112.13it/s][A
 30%|â–ˆâ–ˆâ–ˆ       | 36/119 [00:00<00:00, 110.27it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 48/119 [00:00<00:00, 109.36it/s][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 59/119 [00:00<00:00, 108.42it/s][A
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 70/119 [00:00<00:00, 107.99it/s][A
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 81/119 [00:00<00:00, 108.01it/s][A
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 92/119 [00:00<00:00, 108.05it/s][A
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 103/119 [00:00<00:00, 108.06it/s][A
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 114/119 [00:01<00:00, 107.91it/s][A                                                 
                                                  [A 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 238/595 [00:19<00:24, 14.35it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 119/119 [00:01<00:00, 107.91it/s][A
                                                  [A 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 239/595 [00:19<01:23,  4.25it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 241/595 [00:19<01:05,  5.39it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 243/595 [00:20<00:53,  6.63it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 245/595 [00:20<00:44,  7.90it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 247/595 [00:20<00:38,  9.13it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 249/595 [00:20<00:33, 10.23it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 251/595 [00:20<00:30, 11.18it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 253/595 [00:20<00:28, 11.97it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 255/595 [00:20<00:27, 12.59it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 257/595 [00:21<00:25, 13.05it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 259/595 [00:21<00:25, 13.40it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 261/595 [00:21<00:24, 13.66it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 263/595 [00:21<00:23, 13.84it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 265/595 [00:21<00:23, 13.97it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 267/595 [00:21<00:23, 14.06it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 269/595 [00:21<00:23, 14.12it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 271/595 [00:22<00:22, 14.18it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 273/595 [00:22<00:22, 14.22it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 275/595 [00:22<00:22, 14.24it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 277/595 [00:22<00:22, 14.26it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 279/595 [00:22<00:22, 14.27it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 281/595 [00:22<00:22, 14.27it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 283/595 [00:22<00:21, 14.28it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 285/595 [00:23<00:21, 14.28it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 287/595 [00:23<00:21, 14.28it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 289/595 [00:23<00:21, 14.28it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 291/595 [00:23<00:21, 14.27it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 293/595 [00:23<00:21, 14.28it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 295/595 [00:23<00:20, 14.29it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 297/595 [00:23<00:20, 14.30it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 299/595 [00:23<00:20, 14.29it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 301/595 [00:24<00:20, 14.30it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 303/595 [00:24<00:20, 14.29it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 305/595 [00:24<00:20, 14.29it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 307/595 [00:24<00:20, 14.28it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 309/595 [00:24<00:20, 14.29it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 311/595 [00:24<00:19, 14.26it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 313/595 [00:24<00:19, 14.27it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 315/595 [00:25<00:19, 14.29it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 317/595 [00:25<00:19, 14.30it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 319/595 [00:25<00:19, 14.30it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 321/595 [00:25<00:19, 14.30it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 323/595 [00:25<00:19, 14.29it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 325/595 [00:25<00:18, 14.30it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 327/595 [00:25<00:18, 14.29it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 329/595 [00:26<00:18, 14.29it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 331/595 [00:26<00:18, 14.29it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 333/595 [00:26<00:18, 14.29it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 335/595 [00:26<00:18, 14.29it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 337/595 [00:26<00:18, 14.29it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 339/595 [00:26<00:17, 14.29it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 341/595 [00:26<00:17, 14.29it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 343/595 [00:27<00:17, 14.27it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 345/595 [00:27<00:17, 14.25it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 347/595 [00:27<00:17, 14.27it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 349/595 [00:27<00:17, 14.29it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 351/595 [00:27<00:17, 14.29it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 353/595 [00:27<00:16, 14.29it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 355/595 [00:27<00:16, 14.30it/s]                                                  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 357/595 [00:28<00:16, 14.30it/s][INFO|trainer.py:755] 2023-11-15 22:25:33,760 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModelForTokenClassification.forward` and have been ignored: text. If text are not expected by `PeftModelForTokenClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 22:25:33,761 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 22:25:33,762 >>   Num examples = 945
[INFO|trainer.py:3134] 2023-11-15 22:25:33,762 >>   Batch size = 8
{'eval_loss': 0.4666450321674347, 'eval_accuracy': 0.8222222222222222, 'eval_micro_f1': 0.8222222222222222, 'eval_macro_f1': 0.7451663018148436, 'eval_runtime': 1.1363, 'eval_samples_per_second': 831.639, 'eval_steps_per_second': 104.725, 'epoch': 2.0}
{'loss': 0.3946, 'learning_rate': 0.0002, 'epoch': 3.0}

  0%|          | 0/119 [00:00<?, ?it/s][A
 10%|â–ˆ         | 12/119 [00:00<00:00, 118.81it/s][A
 20%|â–ˆâ–ˆ        | 24/119 [00:00<00:00, 111.81it/s][A
 30%|â–ˆâ–ˆâ–ˆ       | 36/119 [00:00<00:00, 109.84it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 48/119 [00:00<00:00, 108.86it/s][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 59/119 [00:00<00:00, 108.30it/s][A
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 70/119 [00:00<00:00, 107.70it/s][A
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 81/119 [00:00<00:00, 107.68it/s][A
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 92/119 [00:00<00:00, 107.59it/s][A
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 103/119 [00:00<00:00, 107.29it/s][A
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 114/119 [00:01<00:00, 107.24it/s][A                                                 
                                                  [A 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 357/595 [00:29<00:16, 14.30it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 119/119 [00:01<00:00, 107.24it/s][A
                                                  [A 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 358/595 [00:29<00:50,  4.67it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 360/595 [00:29<00:41,  5.71it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 362/595 [00:29<00:33,  6.87it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 364/595 [00:29<00:28,  8.06it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 366/595 [00:29<00:24,  9.23it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 368/595 [00:29<00:22, 10.30it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 370/595 [00:30<00:20, 11.23it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 372/595 [00:30<00:18, 12.00it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 374/595 [00:30<00:17, 12.61it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 376/595 [00:30<00:16, 13.08it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 378/595 [00:30<00:16, 13.43it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 380/595 [00:30<00:15, 13.68it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 382/595 [00:30<00:15, 13.87it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 384/595 [00:31<00:15, 14.01it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 386/595 [00:31<00:14, 14.09it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 388/595 [00:31<00:14, 14.18it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 390/595 [00:31<00:14, 14.19it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 392/595 [00:31<00:14, 14.24it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 394/595 [00:31<00:14, 14.27it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 396/595 [00:31<00:13, 14.28it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 398/595 [00:32<00:13, 14.29it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 400/595 [00:32<00:13, 14.27it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 402/595 [00:32<00:13, 14.25it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 404/595 [00:32<00:13, 14.27it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 406/595 [00:32<00:13, 14.28it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 408/595 [00:32<00:13, 14.30it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 410/595 [00:32<00:12, 14.27it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 412/595 [00:32<00:12, 14.30it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 414/595 [00:33<00:12, 14.31it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 416/595 [00:33<00:12, 14.30it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 418/595 [00:33<00:12, 14.31it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 420/595 [00:33<00:12, 14.28it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 422/595 [00:33<00:12, 14.30it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 424/595 [00:33<00:11, 14.30it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 426/595 [00:33<00:11, 14.29it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 428/595 [00:34<00:11, 14.26it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 430/595 [00:34<00:11, 14.29it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 432/595 [00:34<00:11, 14.30it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 434/595 [00:34<00:11, 14.30it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 436/595 [00:34<00:11, 14.30it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 438/595 [00:34<00:10, 14.30it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 440/595 [00:34<00:10, 14.30it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 442/595 [00:35<00:10, 14.25it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 444/595 [00:35<00:10, 14.29it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 446/595 [00:35<00:10, 14.30it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 448/595 [00:35<00:10, 14.30it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 450/595 [00:35<00:10, 14.31it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 452/595 [00:35<00:09, 14.30it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 454/595 [00:35<00:09, 14.29it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 456/595 [00:36<00:09, 14.29it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 458/595 [00:36<00:09, 14.30it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 460/595 [00:36<00:09, 14.29it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 462/595 [00:36<00:09, 14.29it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 464/595 [00:36<00:09, 14.25it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 466/595 [00:36<00:09, 14.28it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 468/595 [00:36<00:08, 14.30it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 470/595 [00:37<00:08, 14.31it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 472/595 [00:37<00:08, 14.30it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 474/595 [00:37<00:08, 14.29it/s]                                                  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 476/595 [00:37<00:08, 14.29it/s][INFO|trainer.py:755] 2023-11-15 22:25:43,188 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModelForTokenClassification.forward` and have been ignored: text. If text are not expected by `PeftModelForTokenClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 22:25:43,189 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 22:25:43,190 >>   Num examples = 945
[INFO|trainer.py:3134] 2023-11-15 22:25:43,190 >>   Batch size = 8
{'eval_loss': 0.48680439591407776, 'eval_accuracy': 0.8158730158730159, 'eval_micro_f1': 0.815873015873016, 'eval_macro_f1': 0.7204320851117282, 'eval_runtime': 1.1385, 'eval_samples_per_second': 830.02, 'eval_steps_per_second': 104.521, 'epoch': 3.0}
{'loss': 0.3413, 'learning_rate': 0.0001, 'epoch': 4.0}

  0%|          | 0/119 [00:00<?, ?it/s][A
 10%|â–ˆ         | 12/119 [00:00<00:00, 118.52it/s][A
 20%|â–ˆâ–ˆ        | 24/119 [00:00<00:00, 111.92it/s][A
 30%|â–ˆâ–ˆâ–ˆ       | 36/119 [00:00<00:00, 109.59it/s][A
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 47/119 [00:00<00:00, 108.30it/s][A
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 58/119 [00:00<00:00, 107.99it/s][A
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 69/119 [00:00<00:00, 107.77it/s][A
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 80/119 [00:00<00:00, 107.68it/s][A
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 91/119 [00:00<00:00, 107.30it/s][A
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 102/119 [00:00<00:00, 107.39it/s][A
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 113/119 [00:01<00:00, 107.42it/s][A                                                 
                                                  [A 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 476/595 [00:38<00:08, 14.29it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 119/119 [00:01<00:00, 107.42it/s][A
                                                  [A 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 477/595 [00:38<00:25,  4.67it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 479/595 [00:38<00:20,  5.71it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 481/595 [00:38<00:16,  6.86it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 483/595 [00:39<00:13,  8.05it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 485/595 [00:39<00:11,  9.21it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 487/595 [00:39<00:10, 10.27it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 489/595 [00:39<00:09, 11.18it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 491/595 [00:39<00:08, 11.94it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 493/595 [00:39<00:08, 12.55it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 495/595 [00:39<00:07, 13.02it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 497/595 [00:40<00:07, 13.38it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 499/595 [00:40<00:07, 13.64it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 501/595 [00:40<00:06, 13.81it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 503/595 [00:40<00:06, 13.95it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 505/595 [00:40<00:06, 14.06it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 507/595 [00:40<00:06, 14.13it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 509/595 [00:40<00:06, 14.14it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 511/595 [00:41<00:05, 14.20it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 513/595 [00:41<00:05, 14.23it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 515/595 [00:41<00:05, 14.25it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 517/595 [00:41<00:05, 14.27it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 519/595 [00:41<00:05, 14.27it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 521/595 [00:41<00:05, 14.29it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 523/595 [00:41<00:05, 14.27it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 525/595 [00:42<00:04, 14.28it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 527/595 [00:42<00:04, 14.29it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 529/595 [00:42<00:04, 14.30it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 531/595 [00:42<00:04, 14.29it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 533/595 [00:42<00:04, 14.30it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 535/595 [00:42<00:04, 14.30it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 537/595 [00:42<00:04, 14.29it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 539/595 [00:42<00:03, 14.29it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 541/595 [00:43<00:03, 14.28it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 543/595 [00:43<00:03, 14.23it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 545/595 [00:43<00:03, 14.26it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 547/595 [00:43<00:03, 14.28it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 549/595 [00:43<00:03, 14.27it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 551/595 [00:43<00:03, 14.29it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 553/595 [00:43<00:02, 14.29it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 555/595 [00:44<00:02, 14.30it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 557/595 [00:44<00:02, 14.29it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 559/595 [00:44<00:02, 14.29it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 561/595 [00:44<00:02, 14.29it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 563/595 [00:44<00:02, 14.28it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 565/595 [00:44<00:02, 14.29it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 567/595 [00:44<00:01, 14.29it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 569/595 [00:45<00:01, 14.30it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 571/595 [00:45<00:01, 14.30it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 573/595 [00:45<00:01, 14.30it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 575/595 [00:45<00:01, 14.30it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 577/595 [00:45<00:01, 14.30it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 579/595 [00:45<00:01, 14.30it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 581/595 [00:45<00:00, 14.29it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 583/595 [00:46<00:00, 14.30it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 585/595 [00:46<00:00, 14.30it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 587/595 [00:46<00:00, 14.30it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 589/595 [00:46<00:00, 14.30it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 591/595 [00:46<00:00, 14.29it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 593/595 [00:46<00:00, 14.29it/s]                                                 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 595/595 [00:46<00:00, 14.29it/s][INFO|trainer.py:755] 2023-11-15 22:25:52,625 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModelForTokenClassification.forward` and have been ignored: text. If text are not expected by `PeftModelForTokenClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 22:25:52,627 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 22:25:52,627 >>   Num examples = 945
[INFO|trainer.py:3134] 2023-11-15 22:25:52,627 >>   Batch size = 8
{'eval_loss': 0.45507922768592834, 'eval_accuracy': 0.8380952380952381, 'eval_micro_f1': 0.8380952380952381, 'eval_macro_f1': 0.7707221338732868, 'eval_runtime': 1.1401, 'eval_samples_per_second': 828.902, 'eval_steps_per_second': 104.38, 'epoch': 4.0}
{'loss': 0.3024, 'learning_rate': 0.0, 'epoch': 5.0}

  0%|          | 0/119 [00:00<?, ?it/s][A
 10%|â–ˆ         | 12/119 [00:00<00:00, 118.23it/s][A
 20%|â–ˆâ–ˆ        | 24/119 [00:00<00:00, 111.58it/s][A
 30%|â–ˆâ–ˆâ–ˆ       | 36/119 [00:00<00:00, 109.60it/s][A
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 47/119 [00:00<00:00, 108.67it/s][A
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 58/119 [00:00<00:00, 107.82it/s][A
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 69/119 [00:00<00:00, 107.55it/s][A
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 80/119 [00:00<00:00, 107.48it/s][A
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 91/119 [00:00<00:00, 107.39it/s][A
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 102/119 [00:00<00:00, 107.36it/s][A
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 113/119 [00:01<00:00, 107.32it/s][A                                                 
                                                  [A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 595/595 [00:48<00:00, 14.29it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 119/119 [00:01<00:00, 107.32it/s][A
                                                  [A[INFO|trainer.py:1963] 2023-11-15 22:25:53,770 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 595/595 [00:48<00:00, 14.29it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 595/595 [00:48<00:00, 12.39it/s]
[INFO|trainer.py:2855] 2023-11-15 22:25:53,773 >> Saving model checkpoint to ./result/restaurant_roberta-base_seed4_lora
[INFO|tokenization_utils_base.py:2235] 2023-11-15 22:25:53,881 >> tokenizer config file saved in ./result/restaurant_roberta-base_seed4_lora/tokenizer_config.json
[INFO|tokenization_utils_base.py:2242] 2023-11-15 22:25:53,883 >> Special tokens file saved in ./result/restaurant_roberta-base_seed4_lora/special_tokens_map.json
{'eval_loss': 0.4560333788394928, 'eval_accuracy': 0.837037037037037, 'eval_micro_f1': 0.837037037037037, 'eval_macro_f1': 0.76995124835458, 'eval_runtime': 1.1393, 'eval_samples_per_second': 829.431, 'eval_steps_per_second': 104.447, 'epoch': 5.0}
{'train_runtime': 48.0158, 'train_samples_per_second': 393.308, 'train_steps_per_second': 12.392, 'train_loss': 0.44517563250886294, 'epoch': 5.0}
***** train metrics *****
  epoch                    =        5.0
  train_loss               =     0.4452
  train_runtime            = 0:00:48.01
  train_samples            =       3777
  train_samples_per_second =    393.308
  train_steps_per_second   =     12.392
11/15/2023 22:25:53 - INFO - __main__ - *** Evaluate ***
[INFO|trainer.py:755] 2023-11-15 22:25:53,971 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModelForTokenClassification.forward` and have been ignored: text. If text are not expected by `PeftModelForTokenClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 22:25:53,973 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 22:25:53,973 >>   Num examples = 945
[INFO|trainer.py:3134] 2023-11-15 22:25:53,973 >>   Batch size = 8
  0%|          | 0/119 [00:00<?, ?it/s] 10%|â–ˆ         | 12/119 [00:00<00:00, 117.46it/s] 20%|â–ˆâ–ˆ        | 24/119 [00:00<00:00, 111.48it/s] 30%|â–ˆâ–ˆâ–ˆ       | 36/119 [00:00<00:00, 109.51it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 47/119 [00:00<00:00, 108.75it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 58/119 [00:00<00:00, 108.38it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 69/119 [00:00<00:00, 107.86it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 80/119 [00:00<00:00, 107.47it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 91/119 [00:00<00:00, 107.20it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 102/119 [00:00<00:00, 107.41it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 113/119 [00:01<00:00, 107.48it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 119/119 [00:01<00:00, 105.66it/s]
***** eval metrics *****
  epoch                   =        5.0
  eval_accuracy           =      0.837
  eval_loss               =      0.456
  eval_macro_f1           =       0.77
  eval_micro_f1           =      0.837
  eval_runtime            = 0:00:01.13
  eval_samples            =        945
  eval_samples_per_second =    829.801
  eval_steps_per_second   =    104.493
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                  eval/accuracy â–â–†â–…â–ˆâ–ˆâ–ˆ
wandb:                      eval/loss â–ˆâ–‚â–ƒâ–â–â–
wandb:                  eval/macro_f1 â–â–…â–ƒâ–ˆâ–ˆâ–ˆ
wandb:                  eval/micro_f1 â–â–†â–…â–ˆâ–ˆâ–ˆ
wandb:                   eval/runtime â–â–„â–†â–ˆâ–‡â–‡
wandb:        eval/samples_per_second â–ˆâ–…â–ƒâ–â–‚â–‚
wandb:          eval/steps_per_second â–ˆâ–…â–ƒâ–â–‚â–‚
wandb:                    train/epoch â–â–â–ƒâ–ƒâ–…â–…â–†â–†â–ˆâ–ˆâ–ˆâ–ˆ
wandb:              train/global_step â–â–â–ƒâ–ƒâ–…â–…â–†â–†â–ˆâ–ˆâ–ˆâ–ˆ
wandb:            train/learning_rate â–ˆâ–†â–…â–ƒâ–
wandb:                     train/loss â–ˆâ–„â–ƒâ–‚â–
wandb:               train/total_flos â–
wandb:               train/train_loss â–
wandb:            train/train_runtime â–
wandb: train/train_samples_per_second â–
wandb:   train/train_steps_per_second â–
wandb: 
wandb: Run summary:
wandb:                  eval/accuracy 0.83704
wandb:                      eval/loss 0.45603
wandb:                  eval/macro_f1 0.76995
wandb:                  eval/micro_f1 0.83704
wandb:                   eval/runtime 1.1388
wandb:        eval/samples_per_second 829.801
wandb:          eval/steps_per_second 104.493
wandb:                    train/epoch 5.0
wandb:              train/global_step 595
wandb:            train/learning_rate 0.0
wandb:                     train/loss 0.3024
wandb:               train/total_flos 629689029684480.0
wandb:               train/train_loss 0.44518
wandb:            train/train_runtime 48.0158
wandb: train/train_samples_per_second 393.308
wandb:   train/train_steps_per_second 12.392
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /ceph/home/wangyifei/a3/wandb/offline-run-20231115_222349-gam49m9j
wandb: Find logs at: ./wandb/offline-run-20231115_222349-gam49m9j/logs
(ModelArguments(model_name_or_path='roberta-base', cache_dir='./cache', use_adapter=False, use_lora=True), DataTrainingArguments(dataset_name='acl', max_seq_length=64), TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.0005,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=./result/acl_roberta-base_seed4_lora/runs/Nov15_22-26-04_node010,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.EPOCH,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=5.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
output_dir=./result/acl_roberta-base_seed4_lora,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
resume_from_checkpoint=None,
run_name=./result/acl_roberta-base_seed4_lora,
save_on_each_node=False,
save_safetensors=False,
save_steps=500,
save_strategy=IntervalStrategy.NO,
save_total_limit=None,
seed=555,
sharded_ddp=[],
skip_memory_metrics=True,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
))
11/15/2023 22:26:04 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False
11/15/2023 22:26:04 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.0005,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=./result/acl_roberta-base_seed4_lora/runs/Nov15_22-26-03_node010,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.EPOCH,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=5.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
output_dir=./result/acl_roberta-base_seed4_lora,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
resume_from_checkpoint=None,
run_name=./result/acl_roberta-base_seed4_lora,
save_on_each_node=False,
save_safetensors=False,
save_steps=500,
save_strategy=IntervalStrategy.NO,
save_total_limit=None,
seed=555,
sharded_ddp=[],
skip_memory_metrics=True,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
wandb: Tracking run with wandb version 0.15.12
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
Map:   0%|          | 0/11020 [00:00<?, ? examples/s]Map:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 4144/11020 [00:00<00:00, 41189.52 examples/s]Map:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 8411/11020 [00:00<00:00, 42050.30 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11020/11020 [00:00<00:00, 41512.96 examples/s]
[INFO|configuration_utils.py:715] 2023-11-15 22:26:20,019 >> loading configuration file config.json from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/config.json
[INFO|configuration_utils.py:775] 2023-11-15 22:26:20,027 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.33.3",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_auto.py:535] 2023-11-15 22:26:30,043 >> Could not locate the tokenizer configuration file, will try to use the model config instead.
[INFO|configuration_utils.py:715] 2023-11-15 22:26:40,101 >> loading configuration file config.json from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/config.json
[INFO|configuration_utils.py:775] 2023-11-15 22:26:40,101 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.33.3",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:1852] 2023-11-15 22:27:00,146 >> loading file vocab.json from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/vocab.json
[INFO|tokenization_utils_base.py:1852] 2023-11-15 22:27:00,146 >> loading file merges.txt from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/merges.txt
[INFO|tokenization_utils_base.py:1852] 2023-11-15 22:27:00,147 >> loading file tokenizer.json from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/tokenizer.json
[INFO|tokenization_utils_base.py:1852] 2023-11-15 22:27:00,147 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1852] 2023-11-15 22:27:00,147 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1852] 2023-11-15 22:27:00,148 >> loading file tokenizer_config.json from cache at None
[INFO|configuration_utils.py:715] 2023-11-15 22:27:00,149 >> loading configuration file config.json from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/config.json
[INFO|configuration_utils.py:775] 2023-11-15 22:27:00,149 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.33.3",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:2862] 2023-11-15 22:27:20,306 >> loading weights file model.safetensors from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/model.safetensors
[INFO|modeling_utils.py:3638] 2023-11-15 22:27:20,997 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3650] 2023-11-15 22:27:20,998 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
trainable params: 1,284,867 || all params: 125,830,662 || trainable%: 1.0211080348603745
model_max_length: 512
Running tokenizer on dataset:   0%|          | 0/8816 [00:00<?, ? examples/s]Running tokenizer on dataset:  23%|â–ˆâ–ˆâ–Ž       | 2000/8816 [00:00<00:00, 17090.45 examples/s]Running tokenizer on dataset:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 4000/8816 [00:00<00:00, 17973.35 examples/s]Running tokenizer on dataset:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 7000/8816 [00:00<00:00, 19092.22 examples/s]Running tokenizer on dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8816/8816 [00:00<00:00, 18770.59 examples/s]
Running tokenizer on dataset:   0%|          | 0/2204 [00:00<?, ? examples/s]Running tokenizer on dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2204/2204 [00:00<00:00, 20111.97 examples/s]Running tokenizer on dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2204/2204 [00:00<00:00, 19855.84 examples/s]
11/15/2023 22:27:21 - INFO - __main__ - Sample 3167 of the training set: {'text': 'Other studies showed that ACR could affect the cellular energy generation and the deficiency of energy induced the neurotoxicity [7, 8].', 'label': 0, 'input_ids': [0, 24989, 3218, 969, 14, 7224, 500, 115, 3327, 5, 19729, 1007, 2706, 8, 5, 30367, 9, 1007, 26914, 5, 44978, 46513, 646, 406, 6, 290, 8174, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
11/15/2023 22:27:21 - INFO - __main__ - Sample 4498 of the training set: {'text': 'Left renal glucose utilization and splanchnic glucose utilization (utilization) were calculated using the formula\n utilization 5 FEGlc 3 3Glc4a 3 R1H2PF (4)\n where R(H)PF equals either unilateral renal plasma flow or hepatic plasma flow.', 'label': 1, 'input_ids': [0, 39961, 39729, 26071, 21429, 8, 11743, 260, 13212, 636, 26071, 21429, 36, 32843, 1938, 43, 58, 9658, 634, 5, 9288, 50118, 21429, 195, 274, 7170, 45071, 155, 155, 16389, 438, 306, 102, 155, 248, 134, 725, 176, 16088, 36, 306, 43, 50118, 147, 248, 1640, 725, 43, 16088, 27601, 1169, 23077, 39729, 29051, 3041, 50, 45441, 5183, 29051, 3041, 4, 2, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]}.
11/15/2023 22:27:21 - INFO - __main__ - Sample 2638 of the training set: {'text': 'The randomized controlled trials (RCTs) which evaluated the efficacy of PEG for mechanical bowel preparation in prevention of postoperative complications in colorectal surgery were considered for inclusion.', 'label': 1, 'input_ids': [0, 133, 36861, 4875, 7341, 36, 500, 7164, 29, 43, 61, 15423, 5, 22081, 9, 221, 7170, 13, 12418, 29928, 7094, 11, 8555, 9, 618, 23655, 12385, 11, 11311, 1688, 3894, 337, 3012, 58, 1687, 13, 9290, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
11/15/2023 22:27:21 - WARNING - accelerate.utils.other - Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[INFO|trainer.py:755] 2023-11-15 22:27:22,708 >> The following columns in the training set don't have a corresponding argument in `PeftModelForTokenClassification.forward` and have been ignored: text. If text are not expected by `PeftModelForTokenClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:1715] 2023-11-15 22:27:22,718 >> ***** Running training *****
[INFO|trainer.py:1716] 2023-11-15 22:27:22,719 >>   Num examples = 8,816
[INFO|trainer.py:1717] 2023-11-15 22:27:22,719 >>   Num Epochs = 5
[INFO|trainer.py:1718] 2023-11-15 22:27:22,719 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1721] 2023-11-15 22:27:22,720 >>   Total train batch size (w. parallel, distributed & accumulation) = 32
[INFO|trainer.py:1722] 2023-11-15 22:27:22,720 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:1723] 2023-11-15 22:27:22,720 >>   Total optimization steps = 1,380
[INFO|trainer.py:1724] 2023-11-15 22:27:22,721 >>   Number of trainable parameters = 1,284,867
[INFO|integration_utils.py:716] 2023-11-15 22:27:22,723 >> Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"
  0%|          | 0/1380 [00:00<?, ?it/s]  0%|          | 1/1380 [00:01<23:26,  1.02s/it]  0%|          | 3/1380 [00:01<07:16,  3.15it/s]  0%|          | 5/1380 [00:01<04:20,  5.27it/s]  1%|          | 7/1380 [00:01<03:10,  7.20it/s]  1%|          | 9/1380 [00:01<02:34,  8.87it/s]  1%|          | 11/1380 [00:01<02:13, 10.23it/s]  1%|          | 13/1380 [00:01<02:00, 11.30it/s]  1%|          | 15/1380 [00:01<01:52, 12.14it/s]  1%|          | 17/1380 [00:02<01:46, 12.76it/s]  1%|â–         | 19/1380 [00:02<01:42, 13.23it/s]  2%|â–         | 21/1380 [00:02<01:40, 13.57it/s]  2%|â–         | 23/1380 [00:02<01:38, 13.77it/s]  2%|â–         | 25/1380 [00:02<01:36, 13.97it/s]  2%|â–         | 27/1380 [00:02<01:35, 14.12it/s]  2%|â–         | 29/1380 [00:02<01:34, 14.22it/s]  2%|â–         | 31/1380 [00:03<01:34, 14.27it/s]  2%|â–         | 33/1380 [00:03<01:34, 14.30it/s]  3%|â–Ž         | 35/1380 [00:03<01:34, 14.30it/s]  3%|â–Ž         | 37/1380 [00:03<01:33, 14.32it/s]  3%|â–Ž         | 39/1380 [00:03<01:33, 14.30it/s]  3%|â–Ž         | 41/1380 [00:03<01:33, 14.25it/s]  3%|â–Ž         | 43/1380 [00:03<01:33, 14.29it/s]  3%|â–Ž         | 45/1380 [00:04<01:33, 14.31it/s]  3%|â–Ž         | 47/1380 [00:04<01:33, 14.32it/s]  4%|â–Ž         | 49/1380 [00:04<01:32, 14.33it/s]  4%|â–Ž         | 51/1380 [00:04<01:33, 14.28it/s]  4%|â–         | 53/1380 [00:04<01:32, 14.31it/s]  4%|â–         | 55/1380 [00:04<01:32, 14.33it/s]  4%|â–         | 57/1380 [00:04<01:32, 14.32it/s]  4%|â–         | 59/1380 [00:05<01:32, 14.34it/s]  4%|â–         | 61/1380 [00:05<01:32, 14.32it/s]  5%|â–         | 63/1380 [00:05<01:32, 14.32it/s]  5%|â–         | 65/1380 [00:05<01:31, 14.35it/s]  5%|â–         | 67/1380 [00:05<01:31, 14.36it/s]  5%|â–Œ         | 69/1380 [00:05<01:31, 14.36it/s]  5%|â–Œ         | 71/1380 [00:05<01:31, 14.36it/s]  5%|â–Œ         | 73/1380 [00:06<01:30, 14.37it/s]  5%|â–Œ         | 75/1380 [00:06<01:30, 14.37it/s]  6%|â–Œ         | 77/1380 [00:06<01:30, 14.36it/s]  6%|â–Œ         | 79/1380 [00:06<01:30, 14.32it/s]  6%|â–Œ         | 81/1380 [00:06<01:30, 14.32it/s]  6%|â–Œ         | 83/1380 [00:06<01:30, 14.32it/s]  6%|â–Œ         | 85/1380 [00:06<01:30, 14.31it/s]  6%|â–‹         | 87/1380 [00:07<01:30, 14.30it/s]  6%|â–‹         | 89/1380 [00:07<01:30, 14.30it/s]  7%|â–‹         | 91/1380 [00:07<01:30, 14.30it/s]  7%|â–‹         | 93/1380 [00:07<01:30, 14.29it/s]  7%|â–‹         | 95/1380 [00:07<01:29, 14.29it/s]  7%|â–‹         | 97/1380 [00:07<01:29, 14.29it/s]  7%|â–‹         | 99/1380 [00:07<01:29, 14.29it/s]  7%|â–‹         | 101/1380 [00:07<01:29, 14.28it/s]  7%|â–‹         | 103/1380 [00:08<01:29, 14.29it/s]  8%|â–Š         | 105/1380 [00:08<01:29, 14.30it/s]  8%|â–Š         | 107/1380 [00:08<01:29, 14.30it/s]  8%|â–Š         | 109/1380 [00:08<01:28, 14.30it/s]  8%|â–Š         | 111/1380 [00:08<01:28, 14.27it/s]  8%|â–Š         | 113/1380 [00:08<01:28, 14.29it/s]  8%|â–Š         | 115/1380 [00:08<01:28, 14.28it/s]  8%|â–Š         | 117/1380 [00:09<01:28, 14.30it/s]  9%|â–Š         | 119/1380 [00:09<01:28, 14.30it/s]  9%|â–‰         | 121/1380 [00:09<01:27, 14.31it/s]  9%|â–‰         | 123/1380 [00:09<01:27, 14.31it/s]  9%|â–‰         | 125/1380 [00:09<01:27, 14.31it/s]  9%|â–‰         | 127/1380 [00:09<01:27, 14.28it/s]  9%|â–‰         | 129/1380 [00:09<01:27, 14.29it/s]  9%|â–‰         | 131/1380 [00:10<01:27, 14.30it/s] 10%|â–‰         | 133/1380 [00:10<01:27, 14.28it/s] 10%|â–‰         | 135/1380 [00:10<01:27, 14.27it/s] 10%|â–‰         | 137/1380 [00:10<01:27, 14.28it/s] 10%|â–ˆ         | 139/1380 [00:10<01:26, 14.28it/s] 10%|â–ˆ         | 141/1380 [00:10<01:26, 14.26it/s] 10%|â–ˆ         | 143/1380 [00:10<01:26, 14.28it/s] 11%|â–ˆ         | 145/1380 [00:11<01:26, 14.30it/s] 11%|â–ˆ         | 147/1380 [00:11<01:26, 14.30it/s] 11%|â–ˆ         | 149/1380 [00:11<01:26, 14.29it/s] 11%|â–ˆ         | 151/1380 [00:11<01:26, 14.28it/s] 11%|â–ˆ         | 153/1380 [00:11<01:25, 14.30it/s] 11%|â–ˆ         | 155/1380 [00:11<01:25, 14.29it/s] 11%|â–ˆâ–        | 157/1380 [00:11<01:25, 14.30it/s] 12%|â–ˆâ–        | 159/1380 [00:12<01:25, 14.28it/s] 12%|â–ˆâ–        | 161/1380 [00:12<01:25, 14.29it/s] 12%|â–ˆâ–        | 163/1380 [00:12<01:25, 14.30it/s] 12%|â–ˆâ–        | 165/1380 [00:12<01:24, 14.30it/s] 12%|â–ˆâ–        | 167/1380 [00:12<01:24, 14.27it/s] 12%|â–ˆâ–        | 169/1380 [00:12<01:24, 14.27it/s] 12%|â–ˆâ–        | 171/1380 [00:12<01:24, 14.29it/s] 13%|â–ˆâ–Ž        | 173/1380 [00:13<01:24, 14.30it/s] 13%|â–ˆâ–Ž        | 175/1380 [00:13<01:24, 14.30it/s] 13%|â–ˆâ–Ž        | 177/1380 [00:13<01:24, 14.30it/s] 13%|â–ˆâ–Ž        | 179/1380 [00:13<01:24, 14.29it/s] 13%|â–ˆâ–Ž        | 181/1380 [00:13<01:23, 14.29it/s] 13%|â–ˆâ–Ž        | 183/1380 [00:13<01:23, 14.30it/s] 13%|â–ˆâ–Ž        | 185/1380 [00:13<01:23, 14.26it/s] 14%|â–ˆâ–Ž        | 187/1380 [00:14<01:23, 14.26it/s] 14%|â–ˆâ–Ž        | 189/1380 [00:14<01:23, 14.28it/s] 14%|â–ˆâ–        | 191/1380 [00:14<01:23, 14.29it/s] 14%|â–ˆâ–        | 193/1380 [00:14<01:23, 14.30it/s] 14%|â–ˆâ–        | 195/1380 [00:14<01:22, 14.31it/s] 14%|â–ˆâ–        | 197/1380 [00:14<01:22, 14.29it/s] 14%|â–ˆâ–        | 199/1380 [00:14<01:22, 14.28it/s] 15%|â–ˆâ–        | 201/1380 [00:14<01:22, 14.29it/s] 15%|â–ˆâ–        | 203/1380 [00:15<01:22, 14.30it/s] 15%|â–ˆâ–        | 205/1380 [00:15<01:22, 14.30it/s] 15%|â–ˆâ–Œ        | 207/1380 [00:15<01:22, 14.30it/s] 15%|â–ˆâ–Œ        | 209/1380 [00:15<01:21, 14.29it/s] 15%|â–ˆâ–Œ        | 211/1380 [00:15<01:21, 14.29it/s] 15%|â–ˆâ–Œ        | 213/1380 [00:15<01:21, 14.29it/s] 16%|â–ˆâ–Œ        | 215/1380 [00:15<01:21, 14.30it/s] 16%|â–ˆâ–Œ        | 217/1380 [00:16<01:21, 14.27it/s] 16%|â–ˆâ–Œ        | 219/1380 [00:16<01:21, 14.28it/s] 16%|â–ˆâ–Œ        | 221/1380 [00:16<01:21, 14.27it/s] 16%|â–ˆâ–Œ        | 223/1380 [00:16<01:20, 14.31it/s] 16%|â–ˆâ–‹        | 225/1380 [00:16<01:20, 14.33it/s] 16%|â–ˆâ–‹        | 227/1380 [00:16<01:20, 14.35it/s] 17%|â–ˆâ–‹        | 229/1380 [00:16<01:20, 14.30it/s] 17%|â–ˆâ–‹        | 231/1380 [00:17<01:20, 14.31it/s] 17%|â–ˆâ–‹        | 233/1380 [00:17<01:20, 14.28it/s] 17%|â–ˆâ–‹        | 235/1380 [00:17<01:20, 14.30it/s] 17%|â–ˆâ–‹        | 237/1380 [00:17<01:19, 14.32it/s] 17%|â–ˆâ–‹        | 239/1380 [00:17<01:19, 14.34it/s] 17%|â–ˆâ–‹        | 241/1380 [00:17<01:19, 14.35it/s] 18%|â–ˆâ–Š        | 243/1380 [00:17<01:19, 14.36it/s] 18%|â–ˆâ–Š        | 245/1380 [00:18<01:18, 14.37it/s] 18%|â–ˆâ–Š        | 247/1380 [00:18<01:18, 14.39it/s] 18%|â–ˆâ–Š        | 249/1380 [00:18<01:18, 14.39it/s] 18%|â–ˆâ–Š        | 251/1380 [00:18<01:18, 14.42it/s] 18%|â–ˆâ–Š        | 253/1380 [00:18<01:18, 14.42it/s] 18%|â–ˆâ–Š        | 255/1380 [00:18<01:18, 14.39it/s] 19%|â–ˆâ–Š        | 257/1380 [00:18<01:18, 14.38it/s] 19%|â–ˆâ–‰        | 259/1380 [00:19<01:17, 14.38it/s] 19%|â–ˆâ–‰        | 261/1380 [00:19<01:17, 14.38it/s] 19%|â–ˆâ–‰        | 263/1380 [00:19<01:17, 14.34it/s] 19%|â–ˆâ–‰        | 265/1380 [00:19<01:17, 14.35it/s] 19%|â–ˆâ–‰        | 267/1380 [00:19<01:17, 14.34it/s] 19%|â–ˆâ–‰        | 269/1380 [00:19<01:17, 14.34it/s] 20%|â–ˆâ–‰        | 271/1380 [00:19<01:17, 14.36it/s] 20%|â–ˆâ–‰        | 273/1380 [00:20<01:17, 14.36it/s] 20%|â–ˆâ–‰        | 275/1380 [00:20<01:16, 14.38it/s]                                                   20%|â–ˆâ–ˆ        | 276/1380 [00:20<01:16, 14.38it/s][INFO|trainer.py:755] 2023-11-15 22:27:42,928 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModelForTokenClassification.forward` and have been ignored: text. If text are not expected by `PeftModelForTokenClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 22:27:42,929 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 22:27:42,930 >>   Num examples = 2204
[INFO|trainer.py:3134] 2023-11-15 22:27:42,930 >>   Batch size = 8
{'loss': 0.4884, 'learning_rate': 0.0004, 'epoch': 1.0}

  0%|          | 0/276 [00:00<?, ?it/s][A
  4%|â–         | 12/276 [00:00<00:02, 118.42it/s][A
  9%|â–Š         | 24/276 [00:00<00:02, 111.91it/s][A
 13%|â–ˆâ–Ž        | 36/276 [00:00<00:02, 109.94it/s][A
 17%|â–ˆâ–‹        | 48/276 [00:00<00:02, 109.04it/s][A
 21%|â–ˆâ–ˆâ–       | 59/276 [00:00<00:01, 108.51it/s][A
 25%|â–ˆâ–ˆâ–Œ       | 70/276 [00:00<00:01, 108.24it/s][A
 29%|â–ˆâ–ˆâ–‰       | 81/276 [00:00<00:01, 108.01it/s][A
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 92/276 [00:00<00:01, 107.55it/s][A
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 103/276 [00:00<00:01, 107.45it/s][A
 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 114/276 [00:01<00:01, 107.40it/s][A
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 125/276 [00:01<00:01, 107.31it/s][A
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 136/276 [00:01<00:01, 107.41it/s][A
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 147/276 [00:01<00:01, 107.49it/s][A
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 158/276 [00:01<00:01, 107.54it/s][A
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 169/276 [00:01<00:00, 107.55it/s][A
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 180/276 [00:01<00:00, 107.57it/s][A
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 191/276 [00:01<00:00, 107.33it/s][A
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 202/276 [00:01<00:00, 107.19it/s][A
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 213/276 [00:01<00:00, 107.26it/s][A
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 224/276 [00:02<00:00, 106.99it/s][A
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 235/276 [00:02<00:00, 107.14it/s][A
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 246/276 [00:02<00:00, 106.95it/s][A
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 257/276 [00:02<00:00, 107.06it/s][A
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 268/276 [00:02<00:00, 107.27it/s][A                                                  
                                                  [A 20%|â–ˆâ–ˆ        | 276/1380 [00:22<01:16, 14.38it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 276/276 [00:02<00:00, 107.27it/s][A
                                                  [A 20%|â–ˆâ–ˆ        | 277/1380 [00:22<08:26,  2.18it/s] 20%|â–ˆâ–ˆ        | 279/1380 [00:23<06:17,  2.92it/s] 20%|â–ˆâ–ˆ        | 281/1380 [00:23<04:46,  3.83it/s] 21%|â–ˆâ–ˆ        | 283/1380 [00:23<03:43,  4.91it/s] 21%|â–ˆâ–ˆ        | 285/1380 [00:23<02:58,  6.12it/s] 21%|â–ˆâ–ˆ        | 287/1380 [00:23<02:27,  7.39it/s] 21%|â–ˆâ–ˆ        | 289/1380 [00:23<02:06,  8.64it/s] 21%|â–ˆâ–ˆ        | 291/1380 [00:23<01:51,  9.80it/s] 21%|â–ˆâ–ˆ        | 293/1380 [00:24<01:40, 10.82it/s] 21%|â–ˆâ–ˆâ–       | 295/1380 [00:24<01:33, 11.64it/s] 22%|â–ˆâ–ˆâ–       | 297/1380 [00:24<01:27, 12.34it/s] 22%|â–ˆâ–ˆâ–       | 299/1380 [00:24<01:24, 12.86it/s] 22%|â–ˆâ–ˆâ–       | 301/1380 [00:24<01:21, 13.26it/s] 22%|â–ˆâ–ˆâ–       | 303/1380 [00:24<01:19, 13.56it/s] 22%|â–ˆâ–ˆâ–       | 305/1380 [00:24<01:18, 13.77it/s] 22%|â–ˆâ–ˆâ–       | 307/1380 [00:24<01:17, 13.92it/s] 22%|â–ˆâ–ˆâ–       | 309/1380 [00:25<01:16, 14.03it/s] 23%|â–ˆâ–ˆâ–Ž       | 311/1380 [00:25<01:15, 14.08it/s] 23%|â–ˆâ–ˆâ–Ž       | 313/1380 [00:25<01:15, 14.14it/s] 23%|â–ˆâ–ˆâ–Ž       | 315/1380 [00:25<01:15, 14.19it/s] 23%|â–ˆâ–ˆâ–Ž       | 317/1380 [00:25<01:14, 14.22it/s] 23%|â–ˆâ–ˆâ–Ž       | 319/1380 [00:25<01:14, 14.22it/s] 23%|â–ˆâ–ˆâ–Ž       | 321/1380 [00:25<01:14, 14.24it/s] 23%|â–ˆâ–ˆâ–Ž       | 323/1380 [00:26<01:14, 14.23it/s] 24%|â–ˆâ–ˆâ–Ž       | 325/1380 [00:26<01:14, 14.24it/s] 24%|â–ˆâ–ˆâ–Ž       | 327/1380 [00:26<01:14, 14.22it/s] 24%|â–ˆâ–ˆâ–       | 329/1380 [00:26<01:13, 14.23it/s] 24%|â–ˆâ–ˆâ–       | 331/1380 [00:26<01:13, 14.24it/s] 24%|â–ˆâ–ˆâ–       | 333/1380 [00:26<01:13, 14.21it/s] 24%|â–ˆâ–ˆâ–       | 335/1380 [00:26<01:13, 14.21it/s] 24%|â–ˆâ–ˆâ–       | 337/1380 [00:27<01:13, 14.20it/s] 25%|â–ˆâ–ˆâ–       | 339/1380 [00:27<01:13, 14.23it/s] 25%|â–ˆâ–ˆâ–       | 341/1380 [00:27<01:13, 14.22it/s] 25%|â–ˆâ–ˆâ–       | 343/1380 [00:27<01:12, 14.25it/s] 25%|â–ˆâ–ˆâ–Œ       | 345/1380 [00:27<01:12, 14.24it/s] 25%|â–ˆâ–ˆâ–Œ       | 347/1380 [00:27<01:12, 14.24it/s] 25%|â–ˆâ–ˆâ–Œ       | 349/1380 [00:27<01:12, 14.25it/s] 25%|â–ˆâ–ˆâ–Œ       | 351/1380 [00:28<01:12, 14.22it/s] 26%|â–ˆâ–ˆâ–Œ       | 353/1380 [00:28<01:12, 14.23it/s] 26%|â–ˆâ–ˆâ–Œ       | 355/1380 [00:28<01:11, 14.25it/s] 26%|â–ˆâ–ˆâ–Œ       | 357/1380 [00:28<01:11, 14.27it/s] 26%|â–ˆâ–ˆâ–Œ       | 359/1380 [00:28<01:11, 14.27it/s] 26%|â–ˆâ–ˆâ–Œ       | 361/1380 [00:28<01:11, 14.28it/s] 26%|â–ˆâ–ˆâ–‹       | 363/1380 [00:28<01:11, 14.27it/s] 26%|â–ˆâ–ˆâ–‹       | 365/1380 [00:29<01:11, 14.28it/s] 27%|â–ˆâ–ˆâ–‹       | 367/1380 [00:29<01:11, 14.26it/s] 27%|â–ˆâ–ˆâ–‹       | 369/1380 [00:29<01:11, 14.24it/s] 27%|â–ˆâ–ˆâ–‹       | 371/1380 [00:29<01:10, 14.25it/s] 27%|â–ˆâ–ˆâ–‹       | 373/1380 [00:29<01:10, 14.25it/s] 27%|â–ˆâ–ˆâ–‹       | 375/1380 [00:29<01:10, 14.24it/s] 27%|â–ˆâ–ˆâ–‹       | 377/1380 [00:29<01:10, 14.24it/s] 27%|â–ˆâ–ˆâ–‹       | 379/1380 [00:30<01:10, 14.24it/s] 28%|â–ˆâ–ˆâ–Š       | 381/1380 [00:30<01:10, 14.23it/s] 28%|â–ˆâ–ˆâ–Š       | 383/1380 [00:30<01:10, 14.18it/s] 28%|â–ˆâ–ˆâ–Š       | 385/1380 [00:30<01:10, 14.20it/s] 28%|â–ˆâ–ˆâ–Š       | 387/1380 [00:30<01:09, 14.20it/s] 28%|â–ˆâ–ˆâ–Š       | 389/1380 [00:30<01:09, 14.23it/s] 28%|â–ˆâ–ˆâ–Š       | 391/1380 [00:30<01:09, 14.25it/s] 28%|â–ˆâ–ˆâ–Š       | 393/1380 [00:31<01:09, 14.25it/s] 29%|â–ˆâ–ˆâ–Š       | 395/1380 [00:31<01:09, 14.27it/s] 29%|â–ˆâ–ˆâ–‰       | 397/1380 [00:31<01:08, 14.27it/s] 29%|â–ˆâ–ˆâ–‰       | 399/1380 [00:31<01:08, 14.27it/s] 29%|â–ˆâ–ˆâ–‰       | 401/1380 [00:31<01:08, 14.27it/s] 29%|â–ˆâ–ˆâ–‰       | 403/1380 [00:31<01:08, 14.28it/s] 29%|â–ˆâ–ˆâ–‰       | 405/1380 [00:31<01:08, 14.27it/s] 29%|â–ˆâ–ˆâ–‰       | 407/1380 [00:32<01:08, 14.28it/s] 30%|â–ˆâ–ˆâ–‰       | 409/1380 [00:32<01:08, 14.25it/s] 30%|â–ˆâ–ˆâ–‰       | 411/1380 [00:32<01:07, 14.25it/s] 30%|â–ˆâ–ˆâ–‰       | 413/1380 [00:32<01:07, 14.23it/s] 30%|â–ˆâ–ˆâ–ˆ       | 415/1380 [00:32<01:07, 14.22it/s] 30%|â–ˆâ–ˆâ–ˆ       | 417/1380 [00:32<01:07, 14.22it/s] 30%|â–ˆâ–ˆâ–ˆ       | 419/1380 [00:32<01:07, 14.22it/s] 31%|â–ˆâ–ˆâ–ˆ       | 421/1380 [00:32<01:07, 14.20it/s] 31%|â–ˆâ–ˆâ–ˆ       | 423/1380 [00:33<01:07, 14.18it/s] 31%|â–ˆâ–ˆâ–ˆ       | 425/1380 [00:33<01:07, 14.19it/s] 31%|â–ˆâ–ˆâ–ˆ       | 427/1380 [00:33<01:07, 14.16it/s] 31%|â–ˆâ–ˆâ–ˆ       | 429/1380 [00:33<01:06, 14.20it/s] 31%|â–ˆâ–ˆâ–ˆ       | 431/1380 [00:33<01:06, 14.23it/s] 31%|â–ˆâ–ˆâ–ˆâ–      | 433/1380 [00:33<01:06, 14.25it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 435/1380 [00:33<01:06, 14.26it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 437/1380 [00:34<01:06, 14.26it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 439/1380 [00:34<01:05, 14.26it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 441/1380 [00:34<01:05, 14.27it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 443/1380 [00:34<01:05, 14.27it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 445/1380 [00:34<01:05, 14.26it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 447/1380 [00:34<01:05, 14.26it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 449/1380 [00:34<01:05, 14.24it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 451/1380 [00:35<01:05, 14.22it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 453/1380 [00:35<01:05, 14.19it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 455/1380 [00:35<01:05, 14.15it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 457/1380 [00:35<01:05, 14.17it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 459/1380 [00:35<01:04, 14.18it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 461/1380 [00:35<01:04, 14.21it/s] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 463/1380 [00:35<01:04, 14.23it/s] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 465/1380 [00:36<01:04, 14.24it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 467/1380 [00:36<01:04, 14.24it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 469/1380 [00:36<01:04, 14.22it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 471/1380 [00:36<01:03, 14.23it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 473/1380 [00:36<01:03, 14.23it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 475/1380 [00:36<01:03, 14.22it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 477/1380 [00:36<01:03, 14.21it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 479/1380 [00:37<01:03, 14.18it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 481/1380 [00:37<01:03, 14.17it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 483/1380 [00:37<01:03, 14.07it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 485/1380 [00:37<01:04, 13.97it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 487/1380 [00:37<01:04, 13.94it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 489/1380 [00:37<01:03, 13.98it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 491/1380 [00:37<01:03, 13.99it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 493/1380 [00:38<01:03, 14.00it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 495/1380 [00:38<01:02, 14.07it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 497/1380 [00:38<01:02, 14.13it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 499/1380 [00:38<01:02, 14.16it/s] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 501/1380 [00:38<01:01, 14.19it/s] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 503/1380 [00:38<01:01, 14.20it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 505/1380 [00:38<01:01, 14.21it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 507/1380 [00:39<01:01, 14.21it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 509/1380 [00:39<01:01, 14.21it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 511/1380 [00:39<01:01, 14.19it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 513/1380 [00:39<01:01, 14.17it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 515/1380 [00:39<01:00, 14.19it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 517/1380 [00:39<01:00, 14.20it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 519/1380 [00:39<01:00, 14.22it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 521/1380 [00:40<01:00, 14.21it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 523/1380 [00:40<01:00, 14.23it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 525/1380 [00:40<01:00, 14.22it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 527/1380 [00:40<01:00, 14.21it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 529/1380 [00:40<00:59, 14.21it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 531/1380 [00:40<00:59, 14.20it/s] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 533/1380 [00:40<00:59, 14.19it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 535/1380 [00:41<00:59, 14.18it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 537/1380 [00:41<00:59, 14.18it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 539/1380 [00:41<00:59, 14.19it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 541/1380 [00:41<00:59, 14.18it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 543/1380 [00:41<00:59, 14.19it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 545/1380 [00:41<00:58, 14.19it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 547/1380 [00:41<00:58, 14.20it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 549/1380 [00:42<00:58, 14.22it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 551/1380 [00:42<00:58, 14.24it/s]                                                   40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 552/1380 [00:42<00:58, 14.24it/s][INFO|trainer.py:755] 2023-11-15 22:28:04,930 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModelForTokenClassification.forward` and have been ignored: text. If text are not expected by `PeftModelForTokenClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 22:28:04,932 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 22:28:04,932 >>   Num examples = 2204
[INFO|trainer.py:3134] 2023-11-15 22:28:04,933 >>   Batch size = 8
{'eval_loss': 0.463204950094223, 'eval_accuracy': 0.8316696914700544, 'eval_micro_f1': 0.8316696914700544, 'eval_macro_f1': 0.8268355199002491, 'eval_runtime': 2.6155, 'eval_samples_per_second': 842.67, 'eval_steps_per_second': 105.525, 'epoch': 1.0}
{'loss': 0.3669, 'learning_rate': 0.0003, 'epoch': 2.0}

  0%|          | 0/276 [00:00<?, ?it/s][A
  4%|â–         | 12/276 [00:00<00:02, 115.94it/s][A
  9%|â–Š         | 24/276 [00:00<00:02, 110.25it/s][A
 13%|â–ˆâ–Ž        | 36/276 [00:00<00:02, 108.35it/s][A
 17%|â–ˆâ–‹        | 47/276 [00:00<00:02, 107.19it/s][A
 21%|â–ˆâ–ˆ        | 58/276 [00:00<00:02, 106.60it/s][A
 25%|â–ˆâ–ˆâ–Œ       | 69/276 [00:00<00:01, 106.50it/s][A
 29%|â–ˆâ–ˆâ–‰       | 80/276 [00:00<00:01, 106.40it/s][A
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 91/276 [00:00<00:01, 106.31it/s][A
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 102/276 [00:00<00:01, 106.34it/s][A
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 113/276 [00:01<00:01, 106.17it/s][A
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 124/276 [00:01<00:01, 105.96it/s][A
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 135/276 [00:01<00:01, 105.85it/s][A
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 146/276 [00:01<00:01, 105.66it/s][A
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 157/276 [00:01<00:01, 105.71it/s][A
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 168/276 [00:01<00:01, 105.62it/s][A
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 179/276 [00:01<00:00, 105.78it/s][A
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 190/276 [00:01<00:00, 105.91it/s][A
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 201/276 [00:01<00:00, 105.98it/s][A
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 212/276 [00:01<00:00, 105.88it/s][A
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 223/276 [00:02<00:00, 105.76it/s][A
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 234/276 [00:02<00:00, 105.87it/s][A
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 245/276 [00:02<00:00, 105.94it/s][A
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 256/276 [00:02<00:00, 105.52it/s][A
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 267/276 [00:02<00:00, 105.46it/s][A                                                  
                                                  [A 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 552/1380 [00:44<00:58, 14.24it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 276/276 [00:02<00:00, 105.46it/s][A
                                                  [A 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 553/1380 [00:44<06:24,  2.15it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 555/1380 [00:45<04:46,  2.88it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 557/1380 [00:45<03:37,  3.79it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 559/1380 [00:45<02:48,  4.86it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 561/1380 [00:45<02:15,  6.06it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 563/1380 [00:45<01:51,  7.32it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 565/1380 [00:45<01:35,  8.56it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 567/1380 [00:45<01:23,  9.73it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 569/1380 [00:46<01:15, 10.74it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 571/1380 [00:46<01:09, 11.59it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 573/1380 [00:46<01:05, 12.27it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 575/1380 [00:46<01:02, 12.78it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 577/1380 [00:46<01:00, 13.17it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 579/1380 [00:46<00:59, 13.44it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 581/1380 [00:46<00:58, 13.66it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 583/1380 [00:47<00:57, 13.83it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 585/1380 [00:47<00:57, 13.94it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 587/1380 [00:47<00:56, 14.03it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 589/1380 [00:47<00:56, 14.08it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 591/1380 [00:47<00:56, 14.07it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 593/1380 [00:47<00:55, 14.09it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 595/1380 [00:47<00:55, 14.08it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 597/1380 [00:48<00:55, 14.12it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 599/1380 [00:48<00:55, 14.13it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 601/1380 [00:48<00:55, 14.15it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 603/1380 [00:48<00:54, 14.16it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 605/1380 [00:48<00:54, 14.17it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 607/1380 [00:48<00:54, 14.18it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 609/1380 [00:48<00:54, 14.17it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 611/1380 [00:49<00:54, 14.15it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 613/1380 [00:49<00:54, 14.13it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 615/1380 [00:49<00:54, 14.13it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 617/1380 [00:49<00:53, 14.13it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 619/1380 [00:49<00:53, 14.14it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 621/1380 [00:49<00:53, 14.13it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 623/1380 [00:49<00:53, 14.13it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 625/1380 [00:50<00:53, 14.13it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 627/1380 [00:50<00:53, 14.14it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 629/1380 [00:50<00:53, 14.17it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 631/1380 [00:50<00:52, 14.18it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 633/1380 [00:50<00:52, 14.20it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 635/1380 [00:50<00:52, 14.19it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 637/1380 [00:50<00:52, 14.18it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 639/1380 [00:51<00:52, 14.18it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 641/1380 [00:51<00:52, 14.17it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 643/1380 [00:51<00:52, 14.16it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 645/1380 [00:51<00:51, 14.17it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 647/1380 [00:51<00:51, 14.18it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 649/1380 [00:51<00:51, 14.15it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 651/1380 [00:51<00:51, 14.16it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 653/1380 [00:51<00:51, 14.17it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 655/1380 [00:52<00:51, 14.16it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 657/1380 [00:52<00:51, 14.15it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 659/1380 [00:52<00:50, 14.15it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 661/1380 [00:52<00:50, 14.18it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 663/1380 [00:52<00:50, 14.19it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 665/1380 [00:52<00:50, 14.20it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 667/1380 [00:52<00:50, 14.21it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 669/1380 [00:53<00:50, 14.18it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 671/1380 [00:53<00:49, 14.19it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 673/1380 [00:53<00:49, 14.19it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 675/1380 [00:53<00:49, 14.18it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 677/1380 [00:53<00:49, 14.18it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 679/1380 [00:53<00:49, 14.16it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 681/1380 [00:53<00:49, 14.18it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 683/1380 [00:54<00:49, 14.15it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 685/1380 [00:54<00:49, 14.16it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 687/1380 [00:54<00:48, 14.17it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 689/1380 [00:54<00:48, 14.16it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 691/1380 [00:54<00:48, 14.12it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 693/1380 [00:54<00:48, 14.14it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 695/1380 [00:54<00:48, 14.16it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 697/1380 [00:55<00:48, 14.17it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 699/1380 [00:55<00:48, 14.18it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 701/1380 [00:55<00:47, 14.19it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 703/1380 [00:55<00:47, 14.19it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 705/1380 [00:55<00:47, 14.19it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 707/1380 [00:55<00:47, 14.15it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 709/1380 [00:55<00:47, 14.15it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 711/1380 [00:56<00:47, 14.15it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 713/1380 [00:56<00:47, 14.16it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 715/1380 [00:56<00:46, 14.17it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 717/1380 [00:56<00:46, 14.18it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 719/1380 [00:56<00:46, 14.18it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 721/1380 [00:56<00:46, 14.17it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 723/1380 [00:56<00:46, 14.17it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 725/1380 [00:57<00:46, 14.08it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 727/1380 [00:57<00:46, 14.12it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 729/1380 [00:57<00:46, 14.15it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 731/1380 [00:57<00:45, 14.16it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 733/1380 [00:57<00:45, 14.16it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 735/1380 [00:57<00:45, 14.17it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 737/1380 [00:57<00:45, 14.14it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 739/1380 [00:58<00:45, 14.14it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 741/1380 [00:58<00:45, 14.12it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 743/1380 [00:58<00:45, 14.14it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 745/1380 [00:58<00:44, 14.14it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 747/1380 [00:58<00:44, 14.15it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 749/1380 [00:58<00:44, 14.15it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 751/1380 [00:58<00:44, 14.14it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 753/1380 [00:59<00:44, 14.16it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 755/1380 [00:59<00:44, 14.16it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 757/1380 [00:59<00:43, 14.16it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 759/1380 [00:59<00:43, 14.18it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 761/1380 [00:59<00:43, 14.18it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 763/1380 [00:59<00:43, 14.19it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 765/1380 [00:59<00:43, 14.16it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 767/1380 [01:00<00:43, 14.16it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 769/1380 [01:00<00:43, 14.16it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 771/1380 [01:00<00:42, 14.17it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 773/1380 [01:00<00:42, 14.17it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 775/1380 [01:00<00:42, 14.18it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 777/1380 [01:00<00:42, 14.19it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 779/1380 [01:00<00:42, 14.19it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 781/1380 [01:01<00:42, 14.19it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 783/1380 [01:01<00:42, 14.18it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 785/1380 [01:01<00:42, 14.15it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 787/1380 [01:01<00:41, 14.14it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 789/1380 [01:01<00:41, 14.17it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 791/1380 [01:01<00:41, 14.15it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 793/1380 [01:01<00:41, 14.17it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 795/1380 [01:02<00:41, 14.15it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 797/1380 [01:02<00:41, 14.15it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 799/1380 [01:02<00:41, 14.14it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 801/1380 [01:02<00:40, 14.13it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 803/1380 [01:02<00:40, 14.16it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 805/1380 [01:02<00:40, 14.17it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 807/1380 [01:02<00:40, 14.18it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 809/1380 [01:03<00:40, 14.17it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 811/1380 [01:03<00:40, 14.17it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 813/1380 [01:03<00:40, 14.15it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 815/1380 [01:03<00:39, 14.13it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 817/1380 [01:03<00:39, 14.14it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 819/1380 [01:03<00:39, 14.16it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 821/1380 [01:03<00:39, 14.15it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 823/1380 [01:03<00:39, 14.13it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 825/1380 [01:04<00:39, 14.14it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 827/1380 [01:04<00:39, 14.16it/s]                                                   60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 828/1380 [01:04<00:38, 14.16it/s][INFO|trainer.py:755] 2023-11-15 22:28:27,046 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModelForTokenClassification.forward` and have been ignored: text. If text are not expected by `PeftModelForTokenClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 22:28:27,047 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 22:28:27,048 >>   Num examples = 2204
[INFO|trainer.py:3134] 2023-11-15 22:28:27,048 >>   Batch size = 8
{'eval_loss': 0.4015752375125885, 'eval_accuracy': 0.8443738656987296, 'eval_micro_f1': 0.8443738656987296, 'eval_macro_f1': 0.8370993557212624, 'eval_runtime': 2.6523, 'eval_samples_per_second': 830.988, 'eval_steps_per_second': 104.062, 'epoch': 2.0}
{'loss': 0.332, 'learning_rate': 0.0002, 'epoch': 3.0}

  0%|          | 0/276 [00:00<?, ?it/s][A
  4%|â–         | 12/276 [00:00<00:02, 116.38it/s][A
  9%|â–Š         | 24/276 [00:00<00:02, 110.03it/s][A
 13%|â–ˆâ–Ž        | 36/276 [00:00<00:02, 108.00it/s][A
 17%|â–ˆâ–‹        | 47/276 [00:00<00:02, 107.33it/s][A
 21%|â–ˆâ–ˆ        | 58/276 [00:00<00:02, 106.93it/s][A
 25%|â–ˆâ–ˆâ–Œ       | 69/276 [00:00<00:01, 106.60it/s][A
 29%|â–ˆâ–ˆâ–‰       | 80/276 [00:00<00:01, 106.23it/s][A
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 91/276 [00:00<00:01, 106.12it/s][A
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 102/276 [00:00<00:01, 106.13it/s][A
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 113/276 [00:01<00:01, 106.02it/s][A
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 124/276 [00:01<00:01, 105.87it/s][A
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 135/276 [00:01<00:01, 105.93it/s][A
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 146/276 [00:01<00:01, 105.96it/s][A
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 157/276 [00:01<00:01, 106.02it/s][A
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 168/276 [00:01<00:01, 106.07it/s][A
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 179/276 [00:01<00:00, 106.14it/s][A
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 190/276 [00:01<00:00, 105.72it/s][A
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 201/276 [00:01<00:00, 105.71it/s][A
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 212/276 [00:01<00:00, 105.73it/s][A
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 223/276 [00:02<00:00, 105.73it/s][A
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 234/276 [00:02<00:00, 105.86it/s][A
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 245/276 [00:02<00:00, 105.83it/s][A
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 256/276 [00:02<00:00, 105.88it/s][A
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 267/276 [00:02<00:00, 105.86it/s][A                                                  
                                                  [A 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 828/1380 [01:06<00:38, 14.16it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 276/276 [00:02<00:00, 105.86it/s][A
                                                  [A 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 829/1380 [01:07<04:16,  2.15it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 831/1380 [01:07<03:10,  2.88it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 833/1380 [01:07<02:24,  3.79it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 835/1380 [01:07<01:52,  4.86it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 837/1380 [01:07<01:29,  6.05it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 839/1380 [01:07<01:14,  7.31it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 841/1380 [01:07<01:03,  8.55it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 843/1380 [01:08<00:55,  9.71it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 845/1380 [01:08<00:49, 10.72it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 847/1380 [01:08<00:46, 11.56it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 849/1380 [01:08<00:43, 12.23it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 851/1380 [01:08<00:41, 12.77it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 853/1380 [01:08<00:40, 13.17it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 855/1380 [01:08<00:38, 13.47it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 857/1380 [01:09<00:38, 13.68it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 859/1380 [01:09<00:37, 13.83it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 861/1380 [01:09<00:37, 13.94it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 863/1380 [01:09<00:36, 14.00it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 865/1380 [01:09<00:36, 14.04it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 867/1380 [01:09<00:36, 14.06it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 869/1380 [01:09<00:36, 14.10it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 871/1380 [01:10<00:36, 14.10it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 873/1380 [01:10<00:35, 14.13it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 875/1380 [01:10<00:35, 14.15it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 877/1380 [01:10<00:35, 14.16it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 879/1380 [01:10<00:35, 14.16it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 881/1380 [01:10<00:35, 14.14it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 883/1380 [01:10<00:35, 14.14it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 885/1380 [01:11<00:34, 14.16it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 887/1380 [01:11<00:34, 14.17it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 889/1380 [01:11<00:34, 14.18it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 891/1380 [01:11<00:34, 14.19it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 893/1380 [01:11<00:34, 14.19it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 895/1380 [01:11<00:34, 14.18it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 897/1380 [01:11<00:34, 14.17it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 899/1380 [01:11<00:33, 14.16it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 901/1380 [01:12<00:33, 14.17it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 903/1380 [01:12<00:33, 14.17it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 905/1380 [01:12<00:33, 14.18it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 907/1380 [01:12<00:33, 14.19it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 909/1380 [01:12<00:33, 14.20it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 911/1380 [01:12<00:33, 14.19it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 913/1380 [01:12<00:33, 14.15it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 915/1380 [01:13<00:32, 14.14it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 917/1380 [01:13<00:32, 14.14it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 919/1380 [01:13<00:32, 14.17it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 921/1380 [01:13<00:32, 14.18it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 923/1380 [01:13<00:32, 14.19it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 925/1380 [01:13<00:32, 14.20it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 927/1380 [01:13<00:31, 14.19it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 929/1380 [01:14<00:31, 14.18it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 931/1380 [01:14<00:31, 14.15it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 933/1380 [01:14<00:31, 14.14it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 935/1380 [01:14<00:31, 14.15it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 937/1380 [01:14<00:31, 14.16it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 939/1380 [01:14<00:31, 14.18it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 941/1380 [01:14<00:30, 14.19it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 943/1380 [01:15<00:30, 14.20it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 945/1380 [01:15<00:30, 14.20it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 947/1380 [01:15<00:30, 14.17it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 949/1380 [01:15<00:30, 14.16it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 951/1380 [01:15<00:30, 14.15it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 953/1380 [01:15<00:30, 14.16it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 955/1380 [01:15<00:29, 14.17it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 957/1380 [01:16<00:29, 14.18it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 959/1380 [01:16<00:29, 14.17it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 961/1380 [01:16<00:29, 14.17it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 963/1380 [01:16<00:29, 14.17it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 965/1380 [01:16<00:29, 14.16it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 967/1380 [01:16<00:29, 14.15it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 969/1380 [01:16<00:29, 14.16it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 971/1380 [01:17<00:28, 14.18it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 973/1380 [01:17<00:28, 14.19it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 975/1380 [01:17<00:28, 14.20it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 977/1380 [01:17<00:28, 14.20it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 979/1380 [01:17<00:28, 14.19it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 981/1380 [01:17<00:28, 14.19it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 983/1380 [01:17<00:27, 14.18it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 985/1380 [01:18<00:27, 14.17it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 987/1380 [01:18<00:27, 14.16it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 989/1380 [01:18<00:27, 14.16it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 991/1380 [01:18<00:27, 14.17it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 993/1380 [01:18<00:27, 14.18it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 995/1380 [01:18<00:27, 14.18it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 997/1380 [01:18<00:27, 14.18it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 999/1380 [01:19<00:26, 14.16it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1001/1380 [01:19<00:26, 14.15it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1003/1380 [01:19<00:26, 14.14it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1005/1380 [01:19<00:26, 14.15it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1007/1380 [01:19<00:26, 14.15it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1009/1380 [01:19<00:26, 14.16it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1011/1380 [01:19<00:26, 14.13it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1013/1380 [01:20<00:25, 14.13it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1015/1380 [01:20<00:25, 14.13it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1017/1380 [01:20<00:25, 14.14it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1019/1380 [01:20<00:25, 14.15it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1021/1380 [01:20<00:25, 14.16it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1023/1380 [01:20<00:25, 14.18it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1025/1380 [01:20<00:25, 14.15it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1027/1380 [01:21<00:24, 14.13it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1029/1380 [01:21<00:24, 14.11it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1031/1380 [01:21<00:24, 14.13it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1033/1380 [01:21<00:24, 14.15it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1035/1380 [01:21<00:24, 14.17it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1037/1380 [01:21<00:24, 14.17it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1039/1380 [01:21<00:24, 14.18it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1041/1380 [01:22<00:23, 14.17it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1043/1380 [01:22<00:23, 14.17it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1045/1380 [01:22<00:23, 14.16it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1047/1380 [01:22<00:23, 14.15it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1049/1380 [01:22<00:23, 14.13it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1051/1380 [01:22<00:23, 14.14it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1053/1380 [01:22<00:23, 14.15it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1055/1380 [01:23<00:22, 14.15it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1057/1380 [01:23<00:22, 14.14it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1059/1380 [01:23<00:23, 13.77it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1061/1380 [01:23<00:22, 13.87it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1063/1380 [01:23<00:22, 13.92it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1065/1380 [01:23<00:22, 13.98it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1067/1380 [01:23<00:22, 14.01it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1069/1380 [01:24<00:22, 14.06it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1071/1380 [01:24<00:21, 14.09it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1073/1380 [01:24<00:21, 14.10it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1075/1380 [01:24<00:21, 14.12it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1077/1380 [01:24<00:21, 14.13it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1079/1380 [01:24<00:21, 14.13it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1081/1380 [01:24<00:21, 14.13it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1083/1380 [01:24<00:20, 14.15it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1085/1380 [01:25<00:20, 14.17it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1087/1380 [01:25<00:20, 14.18it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1089/1380 [01:25<00:20, 14.17it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1091/1380 [01:25<00:20, 14.14it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1093/1380 [01:25<00:20, 14.14it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1095/1380 [01:25<00:20, 14.13it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1097/1380 [01:25<00:20, 14.14it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1099/1380 [01:26<00:19, 14.16it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1101/1380 [01:26<00:19, 14.16it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1103/1380 [01:26<00:19, 14.20it/s]                                                    80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1104/1380 [01:26<00:19, 14.20it/s][INFO|trainer.py:755] 2023-11-15 22:28:49,173 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModelForTokenClassification.forward` and have been ignored: text. If text are not expected by `PeftModelForTokenClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 22:28:49,174 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 22:28:49,175 >>   Num examples = 2204
[INFO|trainer.py:3134] 2023-11-15 22:28:49,175 >>   Batch size = 8
{'eval_loss': 0.3985525965690613, 'eval_accuracy': 0.8493647912885662, 'eval_micro_f1': 0.8493647912885661, 'eval_macro_f1': 0.8395145693954787, 'eval_runtime': 2.6495, 'eval_samples_per_second': 831.851, 'eval_steps_per_second': 104.17, 'epoch': 3.0}
{'loss': 0.3027, 'learning_rate': 0.0001, 'epoch': 4.0}

  0%|          | 0/276 [00:00<?, ?it/s][A
  4%|â–         | 12/276 [00:00<00:02, 115.50it/s][A
  9%|â–Š         | 24/276 [00:00<00:02, 109.88it/s][A
 13%|â–ˆâ–Ž        | 36/276 [00:00<00:02, 108.02it/s][A
 17%|â–ˆâ–‹        | 47/276 [00:00<00:02, 106.92it/s][A
 21%|â–ˆâ–ˆ        | 58/276 [00:00<00:02, 106.60it/s][A
 25%|â–ˆâ–ˆâ–Œ       | 69/276 [00:00<00:01, 106.27it/s][A
 29%|â–ˆâ–ˆâ–‰       | 80/276 [00:00<00:01, 106.04it/s][A
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 91/276 [00:00<00:01, 105.90it/s][A
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 102/276 [00:00<00:01, 105.72it/s][A
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 113/276 [00:01<00:01, 105.64it/s][A
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 124/276 [00:01<00:01, 105.31it/s][A
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 135/276 [00:01<00:01, 105.46it/s][A
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 146/276 [00:01<00:01, 105.39it/s][A
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 157/276 [00:01<00:01, 105.31it/s][A
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 168/276 [00:01<00:01, 105.25it/s][A
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 179/276 [00:01<00:00, 105.30it/s][A
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 190/276 [00:01<00:00, 105.43it/s][A
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 201/276 [00:01<00:00, 105.44it/s][A
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 212/276 [00:01<00:00, 105.55it/s][A
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 223/276 [00:02<00:00, 105.56it/s][A
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 234/276 [00:02<00:00, 105.60it/s][A
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 245/276 [00:02<00:00, 105.31it/s][A
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 256/276 [00:02<00:00, 105.34it/s][A
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 267/276 [00:02<00:00, 105.43it/s][A                                                   
                                                  [A 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1104/1380 [01:29<00:19, 14.20it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 276/276 [00:02<00:00, 105.43it/s][A
                                                  [A 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1105/1380 [01:29<02:08,  2.15it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1107/1380 [01:29<01:34,  2.88it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1109/1380 [01:29<01:11,  3.78it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1111/1380 [01:29<00:55,  4.85it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1113/1380 [01:29<00:44,  6.04it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1115/1380 [01:29<00:36,  7.30it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1117/1380 [01:30<00:30,  8.54it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1119/1380 [01:30<00:26,  9.69it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1121/1380 [01:30<00:24, 10.69it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1123/1380 [01:30<00:22, 11.54it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1125/1380 [01:30<00:20, 12.22it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1127/1380 [01:30<00:19, 12.74it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1129/1380 [01:30<00:19, 13.14it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1131/1380 [01:31<00:18, 13.41it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1133/1380 [01:31<00:18, 13.60it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1135/1380 [01:31<00:17, 13.77it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1137/1380 [01:31<00:17, 13.87it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1139/1380 [01:31<00:17, 13.94it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1141/1380 [01:31<00:17, 14.00it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1143/1380 [01:31<00:16, 14.04it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1145/1380 [01:32<00:16, 14.05it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1147/1380 [01:32<00:16, 14.08it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1149/1380 [01:32<00:16, 14.11it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1151/1380 [01:32<00:16, 14.14it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1153/1380 [01:32<00:16, 14.15it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1155/1380 [01:32<00:15, 14.16it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1157/1380 [01:32<00:15, 14.15it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1159/1380 [01:33<00:15, 14.14it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1161/1380 [01:33<00:15, 14.12it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1163/1380 [01:33<00:15, 14.14it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1165/1380 [01:33<00:15, 14.15it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1167/1380 [01:33<00:15, 14.15it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1169/1380 [01:33<00:14, 14.15it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1171/1380 [01:33<00:14, 14.13it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1173/1380 [01:33<00:14, 14.13it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1175/1380 [01:34<00:14, 14.14it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1177/1380 [01:34<00:14, 14.14it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1179/1380 [01:34<00:14, 14.16it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1181/1380 [01:34<00:14, 14.16it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1183/1380 [01:34<00:13, 14.17it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1185/1380 [01:34<00:13, 14.16it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1187/1380 [01:34<00:13, 14.15it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1189/1380 [01:35<00:13, 14.12it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1191/1380 [01:35<00:13, 14.15it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1193/1380 [01:35<00:13, 14.16it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1195/1380 [01:35<00:13, 14.13it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1197/1380 [01:35<00:12, 14.14it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1199/1380 [01:35<00:12, 14.14it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1201/1380 [01:35<00:12, 14.13it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1203/1380 [01:36<00:12, 14.12it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1205/1380 [01:36<00:12, 14.14it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1207/1380 [01:36<00:12, 14.14it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1209/1380 [01:36<00:12, 14.14it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1211/1380 [01:36<00:11, 14.14it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1213/1380 [01:36<00:11, 14.11it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1215/1380 [01:36<00:11, 14.12it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1217/1380 [01:37<00:11, 14.14it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1219/1380 [01:37<00:11, 14.14it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1221/1380 [01:37<00:11, 14.11it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1223/1380 [01:37<00:11, 14.12it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1225/1380 [01:37<00:10, 14.11it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1227/1380 [01:37<00:10, 14.12it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1229/1380 [01:37<00:10, 14.13it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1231/1380 [01:38<00:10, 14.15it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1233/1380 [01:38<00:10, 14.13it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1235/1380 [01:38<00:10, 14.12it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1237/1380 [01:38<00:10, 14.11it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1239/1380 [01:38<00:09, 14.12it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1241/1380 [01:38<00:09, 14.12it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1243/1380 [01:38<00:09, 14.11it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1245/1380 [01:39<00:09, 14.12it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1247/1380 [01:39<00:09, 14.13it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1249/1380 [01:39<00:09, 14.12it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1251/1380 [01:39<00:09, 14.12it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1253/1380 [01:39<00:08, 14.13it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1255/1380 [01:39<00:08, 14.15it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1257/1380 [01:39<00:08, 14.15it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1259/1380 [01:40<00:08, 14.14it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1261/1380 [01:40<00:08, 14.12it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1263/1380 [01:40<00:08, 14.12it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1265/1380 [01:40<00:08, 14.13it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1267/1380 [01:40<00:07, 14.14it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1269/1380 [01:40<00:07, 14.15it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1271/1380 [01:40<00:07, 14.14it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1273/1380 [01:41<00:07, 14.14it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1275/1380 [01:41<00:07, 14.14it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1277/1380 [01:41<00:07, 14.13it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1279/1380 [01:41<00:07, 14.14it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1281/1380 [01:41<00:06, 14.15it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1283/1380 [01:41<00:06, 14.16it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1285/1380 [01:41<00:06, 14.16it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1287/1380 [01:42<00:06, 14.16it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1289/1380 [01:42<00:06, 14.13it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1291/1380 [01:42<00:06, 14.11it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1293/1380 [01:42<00:06, 14.11it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1295/1380 [01:42<00:06, 14.14it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1297/1380 [01:42<00:05, 14.16it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1299/1380 [01:42<00:05, 14.13it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1301/1380 [01:43<00:05, 14.13it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1303/1380 [01:43<00:05, 14.13it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1305/1380 [01:43<00:05, 14.14it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1307/1380 [01:43<00:05, 14.14it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1309/1380 [01:43<00:05, 14.11it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1311/1380 [01:43<00:04, 14.12it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1313/1380 [01:43<00:04, 14.10it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1315/1380 [01:44<00:04, 14.10it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1317/1380 [01:44<00:04, 14.12it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1319/1380 [01:44<00:04, 14.12it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1321/1380 [01:44<00:04, 14.13it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1323/1380 [01:44<00:04, 14.15it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1325/1380 [01:44<00:03, 14.15it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1327/1380 [01:44<00:03, 14.11it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1329/1380 [01:45<00:03, 14.11it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1331/1380 [01:45<00:03, 14.11it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1333/1380 [01:45<00:03, 14.13it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1335/1380 [01:45<00:03, 14.14it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1337/1380 [01:45<00:03, 14.13it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1339/1380 [01:45<00:02, 14.13it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1341/1380 [01:45<00:02, 14.12it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1343/1380 [01:46<00:02, 14.11it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1345/1380 [01:46<00:02, 14.15it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1347/1380 [01:46<00:02, 14.13it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1349/1380 [01:46<00:02, 14.14it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1351/1380 [01:46<00:02, 14.10it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1353/1380 [01:46<00:01, 14.08it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1355/1380 [01:46<00:01, 14.10it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1357/1380 [01:47<00:01, 14.11it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1359/1380 [01:47<00:01, 14.11it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1361/1380 [01:47<00:01, 14.11it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1363/1380 [01:47<00:01, 14.09it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1365/1380 [01:47<00:01, 14.12it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1367/1380 [01:47<00:00, 14.14it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1369/1380 [01:47<00:00, 14.15it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1371/1380 [01:48<00:00, 14.15it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1373/1380 [01:48<00:00, 14.16it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1375/1380 [01:48<00:00, 14.15it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1377/1380 [01:48<00:00, 14.14it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1379/1380 [01:48<00:00, 14.16it/s]                                                   100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1380/1380 [01:48<00:00, 14.16it/s][INFO|trainer.py:755] 2023-11-15 22:29:11,334 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModelForTokenClassification.forward` and have been ignored: text. If text are not expected by `PeftModelForTokenClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 22:29:11,335 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 22:29:11,335 >>   Num examples = 2204
[INFO|trainer.py:3134] 2023-11-15 22:29:11,336 >>   Batch size = 8
{'eval_loss': 0.4033646285533905, 'eval_accuracy': 0.8470961887477314, 'eval_micro_f1': 0.8470961887477315, 'eval_macro_f1': 0.8376223806867434, 'eval_runtime': 2.6574, 'eval_samples_per_second': 829.385, 'eval_steps_per_second': 103.861, 'epoch': 4.0}
{'loss': 0.2791, 'learning_rate': 0.0, 'epoch': 5.0}

  0%|          | 0/276 [00:00<?, ?it/s][A
  4%|â–         | 12/276 [00:00<00:02, 116.52it/s][A
  9%|â–Š         | 24/276 [00:00<00:02, 110.13it/s][A
 13%|â–ˆâ–Ž        | 36/276 [00:00<00:02, 108.22it/s][A
 17%|â–ˆâ–‹        | 47/276 [00:00<00:02, 107.03it/s][A
 21%|â–ˆâ–ˆ        | 58/276 [00:00<00:02, 106.65it/s][A
 25%|â–ˆâ–ˆâ–Œ       | 69/276 [00:00<00:01, 106.00it/s][A
 29%|â–ˆâ–ˆâ–‰       | 80/276 [00:00<00:01, 105.94it/s][A
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 91/276 [00:00<00:01, 105.88it/s][A
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 102/276 [00:00<00:01, 105.92it/s][A
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 113/276 [00:01<00:01, 105.82it/s][A
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 124/276 [00:01<00:01, 105.07it/s][A
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 135/276 [00:01<00:01, 105.30it/s][A
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 146/276 [00:01<00:01, 105.34it/s][A
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 157/276 [00:01<00:01, 105.48it/s][A
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 168/276 [00:01<00:01, 105.43it/s][A
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 179/276 [00:01<00:00, 105.49it/s][A
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 190/276 [00:01<00:00, 105.58it/s][A
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 201/276 [00:01<00:00, 105.64it/s][A
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 212/276 [00:01<00:00, 105.77it/s][A
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 223/276 [00:02<00:00, 105.78it/s][A
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 234/276 [00:02<00:00, 105.38it/s][A
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 245/276 [00:02<00:00, 105.44it/s][A
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 256/276 [00:02<00:00, 105.49it/s][A
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 267/276 [00:02<00:00, 105.44it/s][A                                                   
                                                  [A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1380/1380 [01:51<00:00, 14.16it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 276/276 [00:02<00:00, 105.44it/s][A
                                                  [A[INFO|trainer.py:1963] 2023-11-15 22:29:13,995 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                   100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1380/1380 [01:51<00:00, 14.16it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1380/1380 [01:51<00:00, 12.40it/s]
[INFO|trainer.py:2855] 2023-11-15 22:29:13,998 >> Saving model checkpoint to ./result/acl_roberta-base_seed4_lora
[INFO|tokenization_utils_base.py:2235] 2023-11-15 22:29:14,105 >> tokenizer config file saved in ./result/acl_roberta-base_seed4_lora/tokenizer_config.json
[INFO|tokenization_utils_base.py:2242] 2023-11-15 22:29:14,107 >> Special tokens file saved in ./result/acl_roberta-base_seed4_lora/special_tokens_map.json
{'eval_loss': 0.4258780777454376, 'eval_accuracy': 0.8452813067150635, 'eval_micro_f1': 0.8452813067150635, 'eval_macro_f1': 0.8359076901465498, 'eval_runtime': 2.6563, 'eval_samples_per_second': 829.715, 'eval_steps_per_second': 103.903, 'epoch': 5.0}
{'train_runtime': 111.2739, 'train_samples_per_second': 396.14, 'train_steps_per_second': 12.402, 'train_loss': 0.3538003949151523, 'epoch': 5.0}
***** train metrics *****
  epoch                    =        5.0
  train_loss               =     0.3538
  train_runtime            = 0:01:51.27
  train_samples            =       8816
  train_samples_per_second =     396.14
  train_steps_per_second   =     12.402
11/15/2023 22:29:14 - INFO - __main__ - *** Evaluate ***
[INFO|trainer.py:755] 2023-11-15 22:29:14,198 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModelForTokenClassification.forward` and have been ignored: text. If text are not expected by `PeftModelForTokenClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 22:29:14,199 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 22:29:14,200 >>   Num examples = 2204
[INFO|trainer.py:3134] 2023-11-15 22:29:14,200 >>   Batch size = 8
  0%|          | 0/276 [00:00<?, ?it/s]  4%|â–         | 12/276 [00:00<00:02, 117.03it/s]  9%|â–Š         | 24/276 [00:00<00:02, 110.86it/s] 13%|â–ˆâ–Ž        | 36/276 [00:00<00:02, 108.94it/s] 17%|â–ˆâ–‹        | 47/276 [00:00<00:02, 107.79it/s] 21%|â–ˆâ–ˆ        | 58/276 [00:00<00:02, 107.49it/s] 25%|â–ˆâ–ˆâ–Œ       | 69/276 [00:00<00:01, 107.09it/s] 29%|â–ˆâ–ˆâ–‰       | 80/276 [00:00<00:01, 106.66it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 91/276 [00:00<00:01, 106.70it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 102/276 [00:00<00:01, 106.73it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 113/276 [00:01<00:01, 106.61it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 124/276 [00:01<00:01, 106.82it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 135/276 [00:01<00:01, 106.81it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 146/276 [00:01<00:01, 106.70it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 157/276 [00:01<00:01, 106.63it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 168/276 [00:01<00:01, 106.55it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 179/276 [00:01<00:00, 106.53it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 190/276 [00:01<00:00, 106.53it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 201/276 [00:01<00:00, 106.62it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 212/276 [00:01<00:00, 106.72it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 223/276 [00:02<00:00, 106.68it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 234/276 [00:02<00:00, 106.75it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 245/276 [00:02<00:00, 106.69it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 256/276 [00:02<00:00, 106.63it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 267/276 [00:02<00:00, 106.66it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 276/276 [00:02<00:00, 105.49it/s]
***** eval metrics *****
  epoch                   =        5.0
  eval_accuracy           =     0.8453
  eval_loss               =     0.4259
  eval_macro_f1           =     0.8359
  eval_micro_f1           =     0.8453
  eval_runtime            = 0:00:02.62
  eval_samples            =       2204
  eval_samples_per_second =    838.148
  eval_steps_per_second   =    104.959
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                  eval/accuracy â–â–†â–ˆâ–‡â–†â–†
wandb:                      eval/loss â–ˆâ–â–â–‚â–„â–„
wandb:                  eval/macro_f1 â–â–‡â–ˆâ–‡â–†â–†
wandb:                  eval/micro_f1 â–â–†â–ˆâ–‡â–†â–†
wandb:                   eval/runtime â–â–‡â–‡â–ˆâ–ˆâ–ƒ
wandb:        eval/samples_per_second â–ˆâ–‚â–‚â–â–â–†
wandb:          eval/steps_per_second â–ˆâ–‚â–‚â–â–â–†
wandb:                    train/epoch â–â–â–ƒâ–ƒâ–…â–…â–†â–†â–ˆâ–ˆâ–ˆâ–ˆ
wandb:              train/global_step â–â–â–ƒâ–ƒâ–…â–…â–†â–†â–ˆâ–ˆâ–ˆâ–ˆ
wandb:            train/learning_rate â–ˆâ–†â–…â–ƒâ–
wandb:                     train/loss â–ˆâ–„â–ƒâ–‚â–
wandb:               train/total_flos â–
wandb:               train/train_loss â–
wandb:            train/train_runtime â–
wandb: train/train_samples_per_second â–
wandb:   train/train_steps_per_second â–
wandb: 
wandb: Run summary:
wandb:                  eval/accuracy 0.84528
wandb:                      eval/loss 0.42588
wandb:                  eval/macro_f1 0.83591
wandb:                  eval/micro_f1 0.84528
wandb:                   eval/runtime 2.6296
wandb:        eval/samples_per_second 838.148
wandb:          eval/steps_per_second 104.959
wandb:                    train/epoch 5.0
wandb:              train/global_step 1380
wandb:            train/learning_rate 0.0
wandb:                     train/loss 0.2791
wandb:               train/total_flos 1469774552739840.0
wandb:               train/train_loss 0.3538
wandb:            train/train_runtime 111.2739
wandb: train/train_samples_per_second 396.14
wandb:   train/train_steps_per_second 12.402
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /ceph/home/wangyifei/a3/wandb/offline-run-20231115_222605-2s3lgrno
wandb: Find logs at: ./wandb/offline-run-20231115_222605-2s3lgrno/logs
(ModelArguments(model_name_or_path='roberta-base', cache_dir='./cache', use_adapter=False, use_lora=True), DataTrainingArguments(dataset_name='agnews_sup', max_seq_length=64), TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.0005,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=./result/agnews_sup_roberta-base_seed4_lora/runs/Nov15_22-29-25_node010,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.EPOCH,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=5.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
output_dir=./result/agnews_sup_roberta-base_seed4_lora,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
resume_from_checkpoint=None,
run_name=./result/agnews_sup_roberta-base_seed4_lora,
save_on_each_node=False,
save_safetensors=False,
save_steps=500,
save_strategy=IntervalStrategy.NO,
save_total_limit=None,
seed=555,
sharded_ddp=[],
skip_memory_metrics=True,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
))
11/15/2023 22:29:25 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False
11/15/2023 22:29:25 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.0005,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=./result/agnews_sup_roberta-base_seed4_lora/runs/Nov15_22-29-25_node010,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.EPOCH,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=5.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
output_dir=./result/agnews_sup_roberta-base_seed4_lora,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
resume_from_checkpoint=None,
run_name=./result/agnews_sup_roberta-base_seed4_lora,
save_on_each_node=False,
save_safetensors=False,
save_steps=500,
save_strategy=IntervalStrategy.NO,
save_total_limit=None,
seed=555,
sharded_ddp=[],
skip_memory_metrics=True,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
wandb: Tracking run with wandb version 0.15.12
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[INFO|configuration_utils.py:715] 2023-11-15 22:29:41,004 >> loading configuration file config.json from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/config.json
[INFO|configuration_utils.py:775] 2023-11-15 22:29:41,013 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.33.3",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_auto.py:535] 2023-11-15 22:29:51,028 >> Could not locate the tokenizer configuration file, will try to use the model config instead.
[INFO|configuration_utils.py:715] 2023-11-15 22:30:01,039 >> loading configuration file config.json from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/config.json
[INFO|configuration_utils.py:775] 2023-11-15 22:30:01,040 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.33.3",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:1852] 2023-11-15 22:30:21,086 >> loading file vocab.json from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/vocab.json
[INFO|tokenization_utils_base.py:1852] 2023-11-15 22:30:21,086 >> loading file merges.txt from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/merges.txt
[INFO|tokenization_utils_base.py:1852] 2023-11-15 22:30:21,087 >> loading file tokenizer.json from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/tokenizer.json
[INFO|tokenization_utils_base.py:1852] 2023-11-15 22:30:21,087 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1852] 2023-11-15 22:30:21,087 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1852] 2023-11-15 22:30:21,087 >> loading file tokenizer_config.json from cache at None
[INFO|configuration_utils.py:715] 2023-11-15 22:30:21,089 >> loading configuration file config.json from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/config.json
[INFO|configuration_utils.py:775] 2023-11-15 22:30:21,089 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.33.3",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:2862] 2023-11-15 22:30:41,251 >> loading weights file model.safetensors from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/model.safetensors
[INFO|modeling_utils.py:3638] 2023-11-15 22:30:41,942 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3650] 2023-11-15 22:30:41,943 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
trainable params: 1,285,636 || all params: 125,832,200 || trainable%: 1.0217066855701482
model_max_length: 512
Running tokenizer on dataset:   0%|          | 0/6840 [00:00<?, ? examples/s]Running tokenizer on dataset:  29%|â–ˆâ–ˆâ–‰       | 2000/6840 [00:00<00:00, 16076.01 examples/s]Running tokenizer on dataset:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 4000/6840 [00:00<00:00, 17339.84 examples/s]Running tokenizer on dataset:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 6000/6840 [00:00<00:00, 18028.59 examples/s]Running tokenizer on dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6840/6840 [00:00<00:00, 17710.10 examples/s]
Running tokenizer on dataset:   0%|          | 0/760 [00:00<?, ? examples/s]Running tokenizer on dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 760/760 [00:00<00:00, 19614.26 examples/s]
11/15/2023 22:30:42 - INFO - __main__ - Sample 1583 of the training set: {'text': 'SA-India Test: South Africa declare at 510 for 9 : Sports India: Cricket  gt; Kanpur, Nov 22 : South Africa declared their first innings at 510 for nine on the third day of the first cricket Test against India here today.', 'label': 0, 'input_ids': [0, 3603, 12, 11015, 4500, 35, 391, 1327, 10152, 23, 30703, 13, 361, 4832, 1847, 666, 35, 10424, 1437, 821, 90, 131, 7542, 7748, 6, 1442, 820, 4832, 391, 1327, 2998, 49, 78, 2699, 23, 30703, 13, 1117, 15, 5, 371, 183, 9, 5, 78, 5630, 4500, 136, 666, 259, 452, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
11/15/2023 22:30:42 - INFO - __main__ - Sample 2249 of the training set: {'text': 'FTSE 100 lifted by bright Dow The FTSE 100 has climbed as a surge by US shares gives a boost to European markets. Shire Pharmaceuticals SHP.L jumped after winning approval for a key drug and consumer goods giant Unilever ULVR.', 'label': 1, 'input_ids': [0, 597, 2685, 717, 727, 4639, 30, 4520, 4614, 20, 274, 2685, 717, 727, 34, 7334, 25, 10, 6564, 30, 382, 327, 2029, 10, 2501, 7, 796, 1048, 4, 840, 1885, 7937, 29, 4584, 510, 4, 574, 4262, 71, 1298, 2846, 13, 10, 762, 1262, 8, 2267, 3057, 3065, 1890, 1848, 2802, 33987, 13055, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]}.
11/15/2023 22:30:42 - INFO - __main__ - Sample 1319 of the training set: {'text': 'Language of goals what counts for tongue-tied Ronnie and Michael England striker Michael Owen said his lack of Spanish and Ronaldo #39;s lack of English did not hinder celebrations of the Brazilian #39;s matchwinner for Real Madrid in Sunday #39;s 1-0 win at Mallorca.', 'label': 0, 'input_ids': [0, 46969, 9, 1175, 99, 3948, 13, 15686, 12, 90, 2550, 21127, 8, 988, 1156, 5955, 988, 12212, 26, 39, 1762, 9, 3453, 8, 7991, 849, 3416, 131, 29, 1762, 9, 2370, 222, 45, 26679, 9570, 9, 5, 6606, 849, 3416, 131, 29, 914, 20547, 13, 2822, 3622, 11, 395, 849, 3416, 131, 29, 112, 12, 288, 339, 23, 6633, 368, 3245, 4, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.
11/15/2023 22:30:42 - WARNING - accelerate.utils.other - Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[INFO|trainer.py:755] 2023-11-15 22:30:43,632 >> The following columns in the training set don't have a corresponding argument in `PeftModelForTokenClassification.forward` and have been ignored: text. If text are not expected by `PeftModelForTokenClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:1715] 2023-11-15 22:30:43,643 >> ***** Running training *****
[INFO|trainer.py:1716] 2023-11-15 22:30:43,643 >>   Num examples = 6,840
[INFO|trainer.py:1717] 2023-11-15 22:30:43,644 >>   Num Epochs = 5
[INFO|trainer.py:1718] 2023-11-15 22:30:43,644 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1721] 2023-11-15 22:30:43,644 >>   Total train batch size (w. parallel, distributed & accumulation) = 32
[INFO|trainer.py:1722] 2023-11-15 22:30:43,645 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:1723] 2023-11-15 22:30:43,645 >>   Total optimization steps = 1,070
[INFO|trainer.py:1724] 2023-11-15 22:30:43,646 >>   Number of trainable parameters = 1,285,636
[INFO|integration_utils.py:716] 2023-11-15 22:30:43,647 >> Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"
  0%|          | 0/1070 [00:00<?, ?it/s]  0%|          | 1/1070 [00:01<19:04,  1.07s/it]  0%|          | 3/1070 [00:01<05:52,  3.03it/s]  0%|          | 5/1070 [00:01<03:28,  5.10it/s]  1%|          | 7/1070 [00:01<02:31,  7.02it/s]  1%|          | 9/1070 [00:01<02:02,  8.69it/s]  1%|          | 11/1070 [00:01<01:45, 10.08it/s]  1%|          | 13/1070 [00:01<01:34, 11.19it/s]  1%|â–         | 15/1070 [00:02<01:27, 12.06it/s]  2%|â–         | 17/1070 [00:02<01:22, 12.71it/s]  2%|â–         | 19/1070 [00:02<01:19, 13.21it/s]  2%|â–         | 21/1070 [00:02<01:17, 13.55it/s]  2%|â–         | 23/1070 [00:02<01:15, 13.78it/s]  2%|â–         | 25/1070 [00:02<01:14, 13.97it/s]  3%|â–Ž         | 27/1070 [00:02<01:13, 14.13it/s]  3%|â–Ž         | 29/1070 [00:03<01:13, 14.20it/s]  3%|â–Ž         | 31/1070 [00:03<01:12, 14.27it/s]  3%|â–Ž         | 33/1070 [00:03<01:12, 14.30it/s]  3%|â–Ž         | 35/1070 [00:03<01:12, 14.33it/s]  3%|â–Ž         | 37/1070 [00:03<01:12, 14.33it/s]  4%|â–Ž         | 39/1070 [00:03<01:11, 14.36it/s]  4%|â–         | 41/1070 [00:03<01:11, 14.39it/s]  4%|â–         | 43/1070 [00:03<01:11, 14.40it/s]  4%|â–         | 45/1070 [00:04<01:11, 14.40it/s]  4%|â–         | 47/1070 [00:04<01:11, 14.40it/s]  5%|â–         | 49/1070 [00:04<01:10, 14.39it/s]  5%|â–         | 51/1070 [00:04<01:10, 14.39it/s]  5%|â–         | 53/1070 [00:04<01:10, 14.39it/s]  5%|â–Œ         | 55/1070 [00:04<01:10, 14.40it/s]  5%|â–Œ         | 57/1070 [00:04<01:10, 14.41it/s]  6%|â–Œ         | 59/1070 [00:05<01:10, 14.41it/s]  6%|â–Œ         | 61/1070 [00:05<01:09, 14.43it/s]  6%|â–Œ         | 63/1070 [00:05<01:09, 14.41it/s]  6%|â–Œ         | 65/1070 [00:05<01:09, 14.39it/s]  6%|â–‹         | 67/1070 [00:05<01:09, 14.35it/s]  6%|â–‹         | 69/1070 [00:05<01:09, 14.34it/s]  7%|â–‹         | 71/1070 [00:05<01:09, 14.33it/s]  7%|â–‹         | 73/1070 [00:06<01:09, 14.33it/s]  7%|â–‹         | 75/1070 [00:06<01:09, 14.33it/s]  7%|â–‹         | 77/1070 [00:06<01:09, 14.34it/s]  7%|â–‹         | 79/1070 [00:06<01:09, 14.29it/s]  8%|â–Š         | 81/1070 [00:06<01:09, 14.30it/s]  8%|â–Š         | 83/1070 [00:06<01:09, 14.28it/s]  8%|â–Š         | 85/1070 [00:06<01:08, 14.30it/s]  8%|â–Š         | 87/1070 [00:07<01:08, 14.32it/s]  8%|â–Š         | 89/1070 [00:07<01:08, 14.31it/s]  9%|â–Š         | 91/1070 [00:07<01:08, 14.31it/s]  9%|â–Š         | 93/1070 [00:07<01:08, 14.31it/s]  9%|â–‰         | 95/1070 [00:07<01:08, 14.32it/s]  9%|â–‰         | 97/1070 [00:07<01:07, 14.34it/s]  9%|â–‰         | 99/1070 [00:07<01:07, 14.33it/s]  9%|â–‰         | 101/1070 [00:08<01:07, 14.32it/s] 10%|â–‰         | 103/1070 [00:08<01:07, 14.34it/s] 10%|â–‰         | 105/1070 [00:08<01:07, 14.30it/s] 10%|â–ˆ         | 107/1070 [00:08<01:07, 14.32it/s] 10%|â–ˆ         | 109/1070 [00:08<01:06, 14.35it/s] 10%|â–ˆ         | 111/1070 [00:08<01:06, 14.35it/s] 11%|â–ˆ         | 113/1070 [00:08<01:06, 14.37it/s] 11%|â–ˆ         | 115/1070 [00:09<01:06, 14.36it/s] 11%|â–ˆ         | 117/1070 [00:09<01:06, 14.36it/s] 11%|â–ˆ         | 119/1070 [00:09<01:06, 14.35it/s] 11%|â–ˆâ–        | 121/1070 [00:09<01:06, 14.32it/s] 11%|â–ˆâ–        | 123/1070 [00:09<01:06, 14.30it/s] 12%|â–ˆâ–        | 125/1070 [00:09<01:06, 14.32it/s] 12%|â–ˆâ–        | 127/1070 [00:09<01:05, 14.31it/s] 12%|â–ˆâ–        | 129/1070 [00:09<01:05, 14.32it/s] 12%|â–ˆâ–        | 131/1070 [00:10<01:05, 14.32it/s] 12%|â–ˆâ–        | 133/1070 [00:10<01:05, 14.32it/s] 13%|â–ˆâ–Ž        | 135/1070 [00:10<01:05, 14.30it/s] 13%|â–ˆâ–Ž        | 137/1070 [00:10<01:05, 14.28it/s] 13%|â–ˆâ–Ž        | 139/1070 [00:10<01:05, 14.28it/s] 13%|â–ˆâ–Ž        | 141/1070 [00:10<01:05, 14.29it/s] 13%|â–ˆâ–Ž        | 143/1070 [00:10<01:04, 14.29it/s] 14%|â–ˆâ–Ž        | 145/1070 [00:11<01:04, 14.30it/s] 14%|â–ˆâ–Ž        | 147/1070 [00:11<01:04, 14.30it/s] 14%|â–ˆâ–        | 149/1070 [00:11<01:04, 14.28it/s] 14%|â–ˆâ–        | 151/1070 [00:11<01:04, 14.29it/s] 14%|â–ˆâ–        | 153/1070 [00:11<01:04, 14.29it/s] 14%|â–ˆâ–        | 155/1070 [00:11<01:04, 14.26it/s] 15%|â–ˆâ–        | 157/1070 [00:11<01:03, 14.27it/s] 15%|â–ˆâ–        | 159/1070 [00:12<01:03, 14.29it/s] 15%|â–ˆâ–Œ        | 161/1070 [00:12<01:03, 14.30it/s] 15%|â–ˆâ–Œ        | 163/1070 [00:12<01:03, 14.30it/s] 15%|â–ˆâ–Œ        | 165/1070 [00:12<01:03, 14.30it/s] 16%|â–ˆâ–Œ        | 167/1070 [00:12<01:03, 14.29it/s] 16%|â–ˆâ–Œ        | 169/1070 [00:12<01:03, 14.29it/s] 16%|â–ˆâ–Œ        | 171/1070 [00:12<01:02, 14.29it/s] 16%|â–ˆâ–Œ        | 173/1070 [00:13<01:02, 14.29it/s] 16%|â–ˆâ–‹        | 175/1070 [00:13<01:02, 14.29it/s] 17%|â–ˆâ–‹        | 177/1070 [00:13<01:02, 14.28it/s] 17%|â–ˆâ–‹        | 179/1070 [00:13<01:02, 14.29it/s] 17%|â–ˆâ–‹        | 181/1070 [00:13<01:02, 14.28it/s] 17%|â–ˆâ–‹        | 183/1070 [00:13<01:02, 14.29it/s] 17%|â–ˆâ–‹        | 185/1070 [00:13<01:01, 14.29it/s] 17%|â–ˆâ–‹        | 187/1070 [00:14<01:01, 14.28it/s] 18%|â–ˆâ–Š        | 189/1070 [00:14<01:01, 14.29it/s] 18%|â–ˆâ–Š        | 191/1070 [00:14<01:01, 14.29it/s] 18%|â–ˆâ–Š        | 193/1070 [00:14<01:01, 14.31it/s] 18%|â–ˆâ–Š        | 195/1070 [00:14<01:01, 14.33it/s] 18%|â–ˆâ–Š        | 197/1070 [00:14<01:00, 14.34it/s] 19%|â–ˆâ–Š        | 199/1070 [00:14<01:00, 14.33it/s] 19%|â–ˆâ–‰        | 201/1070 [00:15<01:00, 14.35it/s] 19%|â–ˆâ–‰        | 203/1070 [00:15<01:00, 14.35it/s] 19%|â–ˆâ–‰        | 205/1070 [00:15<01:00, 14.35it/s] 19%|â–ˆâ–‰        | 207/1070 [00:15<01:00, 14.36it/s] 20%|â–ˆâ–‰        | 209/1070 [00:15<00:59, 14.37it/s] 20%|â–ˆâ–‰        | 211/1070 [00:15<00:59, 14.40it/s] 20%|â–ˆâ–‰        | 213/1070 [00:15<00:59, 14.44it/s]                                                   20%|â–ˆâ–ˆ        | 214/1070 [00:15<00:59, 14.44it/s][INFO|trainer.py:755] 2023-11-15 22:30:59,560 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModelForTokenClassification.forward` and have been ignored: text. If text are not expected by `PeftModelForTokenClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 22:30:59,562 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 22:30:59,562 >>   Num examples = 760
[INFO|trainer.py:3134] 2023-11-15 22:30:59,562 >>   Batch size = 8
{'loss': 0.4352, 'learning_rate': 0.0004, 'epoch': 1.0}

  0%|          | 0/95 [00:00<?, ?it/s][A
 13%|â–ˆâ–Ž        | 12/95 [00:00<00:00, 117.03it/s][A
 25%|â–ˆâ–ˆâ–Œ       | 24/95 [00:00<00:00, 111.50it/s][A
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 36/95 [00:00<00:00, 109.56it/s][A
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 47/95 [00:00<00:00, 108.84it/s][A
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 58/95 [00:00<00:00, 108.51it/s][A
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 69/95 [00:00<00:00, 108.27it/s][A
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 80/95 [00:00<00:00, 108.13it/s][A
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 91/95 [00:00<00:00, 107.64it/s][A                                                  
                                                [A 20%|â–ˆâ–ˆ        | 214/1070 [00:16<00:59, 14.44it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 95/95 [00:00<00:00, 107.64it/s][A
                                                [A 20%|â–ˆâ–ˆ        | 215/1070 [00:16<02:55,  4.86it/s] 20%|â–ˆâ–ˆ        | 217/1070 [00:17<02:20,  6.07it/s] 20%|â–ˆâ–ˆ        | 219/1070 [00:17<01:55,  7.34it/s] 21%|â–ˆâ–ˆ        | 221/1070 [00:17<01:38,  8.60it/s] 21%|â–ˆâ–ˆ        | 223/1070 [00:17<01:26,  9.78it/s] 21%|â–ˆâ–ˆ        | 225/1070 [00:17<01:18, 10.83it/s] 21%|â–ˆâ–ˆ        | 227/1070 [00:17<01:11, 11.71it/s] 21%|â–ˆâ–ˆâ–       | 229/1070 [00:17<01:07, 12.42it/s] 22%|â–ˆâ–ˆâ–       | 231/1070 [00:18<01:04, 12.94it/s] 22%|â–ˆâ–ˆâ–       | 233/1070 [00:18<01:02, 13.34it/s] 22%|â–ˆâ–ˆâ–       | 235/1070 [00:18<01:01, 13.63it/s] 22%|â–ˆâ–ˆâ–       | 237/1070 [00:18<01:00, 13.85it/s] 22%|â–ˆâ–ˆâ–       | 239/1070 [00:18<00:59, 14.01it/s] 23%|â–ˆâ–ˆâ–Ž       | 241/1070 [00:18<00:58, 14.13it/s] 23%|â–ˆâ–ˆâ–Ž       | 243/1070 [00:18<00:58, 14.20it/s] 23%|â–ˆâ–ˆâ–Ž       | 245/1070 [00:18<00:57, 14.28it/s] 23%|â–ˆâ–ˆâ–Ž       | 247/1070 [00:19<00:57, 14.32it/s] 23%|â–ˆâ–ˆâ–Ž       | 249/1070 [00:19<00:57, 14.34it/s] 23%|â–ˆâ–ˆâ–Ž       | 251/1070 [00:19<00:57, 14.34it/s] 24%|â–ˆâ–ˆâ–Ž       | 253/1070 [00:19<00:56, 14.34it/s] 24%|â–ˆâ–ˆâ–       | 255/1070 [00:19<00:56, 14.35it/s] 24%|â–ˆâ–ˆâ–       | 257/1070 [00:19<00:56, 14.37it/s] 24%|â–ˆâ–ˆâ–       | 259/1070 [00:19<00:56, 14.38it/s] 24%|â–ˆâ–ˆâ–       | 261/1070 [00:20<00:56, 14.39it/s] 25%|â–ˆâ–ˆâ–       | 263/1070 [00:20<00:56, 14.41it/s] 25%|â–ˆâ–ˆâ–       | 265/1070 [00:20<00:55, 14.43it/s] 25%|â–ˆâ–ˆâ–       | 267/1070 [00:20<00:55, 14.44it/s] 25%|â–ˆâ–ˆâ–Œ       | 269/1070 [00:20<00:55, 14.42it/s] 25%|â–ˆâ–ˆâ–Œ       | 271/1070 [00:20<00:55, 14.42it/s] 26%|â–ˆâ–ˆâ–Œ       | 273/1070 [00:20<00:55, 14.41it/s] 26%|â–ˆâ–ˆâ–Œ       | 275/1070 [00:21<00:55, 14.40it/s] 26%|â–ˆâ–ˆâ–Œ       | 277/1070 [00:21<00:55, 14.41it/s] 26%|â–ˆâ–ˆâ–Œ       | 279/1070 [00:21<00:54, 14.41it/s] 26%|â–ˆâ–ˆâ–‹       | 281/1070 [00:21<00:54, 14.42it/s] 26%|â–ˆâ–ˆâ–‹       | 283/1070 [00:21<00:54, 14.42it/s] 27%|â–ˆâ–ˆâ–‹       | 285/1070 [00:21<00:54, 14.40it/s] 27%|â–ˆâ–ˆâ–‹       | 287/1070 [00:21<00:54, 14.37it/s] 27%|â–ˆâ–ˆâ–‹       | 289/1070 [00:22<00:54, 14.34it/s] 27%|â–ˆâ–ˆâ–‹       | 291/1070 [00:22<00:54, 14.31it/s] 27%|â–ˆâ–ˆâ–‹       | 293/1070 [00:22<00:54, 14.32it/s] 28%|â–ˆâ–ˆâ–Š       | 295/1070 [00:22<00:54, 14.34it/s] 28%|â–ˆâ–ˆâ–Š       | 297/1070 [00:22<00:53, 14.34it/s] 28%|â–ˆâ–ˆâ–Š       | 299/1070 [00:22<00:53, 14.35it/s] 28%|â–ˆâ–ˆâ–Š       | 301/1070 [00:22<00:53, 14.33it/s] 28%|â–ˆâ–ˆâ–Š       | 303/1070 [00:23<00:53, 14.35it/s] 29%|â–ˆâ–ˆâ–Š       | 305/1070 [00:23<00:53, 14.35it/s] 29%|â–ˆâ–ˆâ–Š       | 307/1070 [00:23<00:53, 14.34it/s] 29%|â–ˆâ–ˆâ–‰       | 309/1070 [00:23<00:52, 14.36it/s] 29%|â–ˆâ–ˆâ–‰       | 311/1070 [00:23<00:52, 14.37it/s] 29%|â–ˆâ–ˆâ–‰       | 313/1070 [00:23<00:52, 14.39it/s] 29%|â–ˆâ–ˆâ–‰       | 315/1070 [00:23<00:52, 14.39it/s] 30%|â–ˆâ–ˆâ–‰       | 317/1070 [00:23<00:52, 14.41it/s] 30%|â–ˆâ–ˆâ–‰       | 319/1070 [00:24<00:52, 14.39it/s] 30%|â–ˆâ–ˆâ–ˆ       | 321/1070 [00:24<00:52, 14.36it/s] 30%|â–ˆâ–ˆâ–ˆ       | 323/1070 [00:24<00:51, 14.37it/s] 30%|â–ˆâ–ˆâ–ˆ       | 325/1070 [00:24<00:51, 14.37it/s] 31%|â–ˆâ–ˆâ–ˆ       | 327/1070 [00:24<00:51, 14.37it/s] 31%|â–ˆâ–ˆâ–ˆ       | 329/1070 [00:24<00:51, 14.37it/s] 31%|â–ˆâ–ˆâ–ˆ       | 331/1070 [00:24<00:51, 14.38it/s] 31%|â–ˆâ–ˆâ–ˆ       | 333/1070 [00:25<00:51, 14.39it/s] 31%|â–ˆâ–ˆâ–ˆâ–      | 335/1070 [00:25<00:51, 14.40it/s] 31%|â–ˆâ–ˆâ–ˆâ–      | 337/1070 [00:25<00:50, 14.38it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 339/1070 [00:25<00:50, 14.39it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 341/1070 [00:25<00:50, 14.38it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 343/1070 [00:25<00:50, 14.37it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 345/1070 [00:25<00:50, 14.35it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 347/1070 [00:26<00:50, 14.31it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 349/1070 [00:26<00:50, 14.31it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 351/1070 [00:26<00:50, 14.32it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 353/1070 [00:26<00:50, 14.33it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 355/1070 [00:26<00:49, 14.33it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 357/1070 [00:26<00:49, 14.34it/s] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 359/1070 [00:26<00:49, 14.35it/s] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 361/1070 [00:27<00:49, 14.35it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 363/1070 [00:27<00:49, 14.35it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 365/1070 [00:27<00:49, 14.36it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 367/1070 [00:27<00:48, 14.36it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 369/1070 [00:27<00:48, 14.37it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 371/1070 [00:27<00:48, 14.39it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 373/1070 [00:27<00:48, 14.40it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 375/1070 [00:28<00:48, 14.39it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 377/1070 [00:28<00:48, 14.37it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 379/1070 [00:28<00:48, 14.36it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 381/1070 [00:28<00:48, 14.35it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 383/1070 [00:28<00:47, 14.35it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 385/1070 [00:28<00:47, 14.35it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 387/1070 [00:28<00:47, 14.35it/s] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 389/1070 [00:29<00:47, 14.35it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 391/1070 [00:29<00:47, 14.32it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 393/1070 [00:29<00:47, 14.33it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 395/1070 [00:29<00:47, 14.34it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 397/1070 [00:29<00:46, 14.34it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 399/1070 [00:29<00:46, 14.35it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 401/1070 [00:29<00:46, 14.35it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 403/1070 [00:29<00:46, 14.37it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 405/1070 [00:30<00:46, 14.38it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 407/1070 [00:30<00:46, 14.37it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 409/1070 [00:30<00:46, 14.36it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 411/1070 [00:30<00:45, 14.34it/s] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 413/1070 [00:30<00:45, 14.33it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 415/1070 [00:30<00:45, 14.32it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 417/1070 [00:30<00:45, 14.30it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 419/1070 [00:31<00:45, 14.30it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 421/1070 [00:31<00:45, 14.30it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 423/1070 [00:31<00:45, 14.29it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 425/1070 [00:31<00:45, 14.30it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 427/1070 [00:31<00:44, 14.31it/s]                                                   40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 428/1070 [00:31<00:44, 14.31it/s][INFO|trainer.py:755] 2023-11-15 22:31:15,365 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModelForTokenClassification.forward` and have been ignored: text. If text are not expected by `PeftModelForTokenClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 22:31:15,366 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 22:31:15,367 >>   Num examples = 760
[INFO|trainer.py:3134] 2023-11-15 22:31:15,367 >>   Batch size = 8
{'eval_loss': 0.31278756260871887, 'eval_accuracy': 0.8947368421052632, 'eval_micro_f1': 0.8947368421052632, 'eval_macro_f1': 0.8918304441969049, 'eval_runtime': 0.9157, 'eval_samples_per_second': 830.008, 'eval_steps_per_second': 103.751, 'epoch': 1.0}
{'loss': 0.2693, 'learning_rate': 0.0003, 'epoch': 2.0}

  0%|          | 0/95 [00:00<?, ?it/s][A
 13%|â–ˆâ–Ž        | 12/95 [00:00<00:00, 117.30it/s][A
 25%|â–ˆâ–ˆâ–Œ       | 24/95 [00:00<00:00, 110.53it/s][A
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 36/95 [00:00<00:00, 108.88it/s][A
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 47/95 [00:00<00:00, 108.02it/s][A
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 58/95 [00:00<00:00, 107.38it/s][A
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 69/95 [00:00<00:00, 107.25it/s][A
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 80/95 [00:00<00:00, 107.16it/s][A
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 91/95 [00:00<00:00, 107.02it/s][A                                                  
                                                [A 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 428/1070 [00:32<00:44, 14.31it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 95/95 [00:00<00:00, 107.02it/s][A
                                                [A 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 429/1070 [00:32<02:12,  4.84it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 431/1070 [00:32<01:45,  6.04it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 433/1070 [00:32<01:27,  7.31it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 435/1070 [00:33<01:14,  8.57it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 437/1070 [00:33<01:05,  9.74it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 439/1070 [00:33<00:58, 10.77it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 441/1070 [00:33<00:54, 11.62it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 443/1070 [00:33<00:50, 12.32it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 445/1070 [00:33<00:48, 12.85it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 447/1070 [00:33<00:47, 13.25it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 449/1070 [00:34<00:45, 13.54it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 451/1070 [00:34<00:45, 13.76it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 453/1070 [00:34<00:44, 13.92it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 455/1070 [00:34<00:43, 14.02it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 457/1070 [00:34<00:43, 14.09it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 459/1070 [00:34<00:43, 14.16it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 461/1070 [00:34<00:42, 14.20it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 463/1070 [00:35<00:42, 14.23it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 465/1070 [00:35<00:42, 14.25it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 467/1070 [00:35<00:42, 14.25it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 469/1070 [00:35<00:42, 14.27it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 471/1070 [00:35<00:41, 14.28it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 473/1070 [00:35<00:41, 14.29it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 475/1070 [00:35<00:41, 14.28it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 477/1070 [00:36<00:41, 14.25it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 479/1070 [00:36<00:41, 14.29it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 481/1070 [00:36<00:41, 14.29it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 483/1070 [00:36<00:41, 14.30it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 485/1070 [00:36<00:40, 14.29it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 487/1070 [00:36<00:40, 14.29it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 489/1070 [00:36<00:40, 14.30it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 491/1070 [00:37<00:40, 14.30it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 493/1070 [00:37<00:40, 14.29it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 495/1070 [00:37<00:40, 14.29it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 497/1070 [00:37<00:40, 14.29it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 499/1070 [00:37<00:39, 14.30it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 501/1070 [00:37<00:39, 14.29it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 503/1070 [00:37<00:39, 14.28it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 505/1070 [00:38<00:39, 14.28it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 507/1070 [00:38<00:39, 14.29it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 509/1070 [00:38<00:39, 14.28it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 511/1070 [00:38<00:39, 14.28it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 513/1070 [00:38<00:38, 14.30it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 515/1070 [00:38<00:38, 14.29it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 517/1070 [00:38<00:38, 14.29it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 519/1070 [00:39<00:38, 14.30it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 521/1070 [00:39<00:38, 14.29it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 523/1070 [00:39<00:38, 14.28it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 525/1070 [00:39<00:38, 14.29it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 527/1070 [00:39<00:38, 14.29it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 529/1070 [00:39<00:37, 14.29it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 531/1070 [00:39<00:37, 14.30it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 533/1070 [00:39<00:37, 14.29it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 535/1070 [00:40<00:37, 14.29it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 537/1070 [00:40<00:37, 14.29it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 539/1070 [00:40<00:37, 14.27it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 541/1070 [00:40<00:37, 14.29it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 543/1070 [00:40<00:36, 14.30it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 545/1070 [00:40<00:36, 14.30it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 547/1070 [00:40<00:36, 14.30it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 549/1070 [00:41<00:36, 14.29it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 551/1070 [00:41<00:36, 14.28it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 553/1070 [00:41<00:36, 14.26it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 555/1070 [00:41<00:36, 14.27it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 557/1070 [00:41<00:35, 14.28it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 559/1070 [00:41<00:35, 14.30it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 561/1070 [00:41<00:35, 14.31it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 563/1070 [00:42<00:35, 14.30it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 565/1070 [00:42<00:35, 14.30it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 567/1070 [00:42<00:35, 14.29it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 569/1070 [00:42<00:35, 14.29it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 571/1070 [00:42<00:34, 14.28it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 573/1070 [00:42<00:34, 14.27it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 575/1070 [00:42<00:34, 14.29it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 577/1070 [00:43<00:34, 14.30it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 579/1070 [00:43<00:34, 14.30it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 581/1070 [00:43<00:34, 14.30it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 583/1070 [00:43<00:34, 14.29it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 585/1070 [00:43<00:33, 14.29it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 587/1070 [00:43<00:33, 14.30it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 589/1070 [00:43<00:33, 14.30it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 591/1070 [00:44<00:33, 14.29it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 593/1070 [00:44<00:33, 14.29it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 595/1070 [00:44<00:33, 14.27it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 597/1070 [00:44<00:33, 14.28it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 599/1070 [00:44<00:32, 14.29it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 601/1070 [00:44<00:32, 14.28it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 603/1070 [00:44<00:32, 14.29it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 605/1070 [00:45<00:32, 14.28it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 607/1070 [00:45<00:32, 14.29it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 609/1070 [00:45<00:32, 14.30it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 611/1070 [00:45<00:32, 14.30it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 613/1070 [00:45<00:31, 14.30it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 615/1070 [00:45<00:31, 14.30it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 617/1070 [00:45<00:31, 14.30it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 619/1070 [00:46<00:31, 14.30it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 621/1070 [00:46<00:31, 14.29it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 623/1070 [00:46<00:31, 14.30it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 625/1070 [00:46<00:31, 14.25it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 627/1070 [00:46<00:31, 14.25it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 629/1070 [00:46<00:30, 14.28it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 631/1070 [00:46<00:30, 14.30it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 633/1070 [00:46<00:30, 14.30it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 635/1070 [00:47<00:30, 14.28it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 637/1070 [00:47<00:30, 14.29it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 639/1070 [00:47<00:30, 14.28it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 641/1070 [00:47<00:29, 14.31it/s]                                                   60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 642/1070 [00:47<00:29, 14.31it/s][INFO|trainer.py:755] 2023-11-15 22:31:31,251 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModelForTokenClassification.forward` and have been ignored: text. If text are not expected by `PeftModelForTokenClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 22:31:31,253 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 22:31:31,253 >>   Num examples = 760
[INFO|trainer.py:3134] 2023-11-15 22:31:31,253 >>   Batch size = 8
{'eval_loss': 0.33255288004875183, 'eval_accuracy': 0.8947368421052632, 'eval_micro_f1': 0.8947368421052632, 'eval_macro_f1': 0.8926216934263341, 'eval_runtime': 0.9204, 'eval_samples_per_second': 825.751, 'eval_steps_per_second': 103.219, 'epoch': 2.0}
{'loss': 0.2229, 'learning_rate': 0.0002, 'epoch': 3.0}

  0%|          | 0/95 [00:00<?, ?it/s][A
 13%|â–ˆâ–Ž        | 12/95 [00:00<00:00, 116.27it/s][A
 25%|â–ˆâ–ˆâ–Œ       | 24/95 [00:00<00:00, 110.41it/s][A
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 36/95 [00:00<00:00, 108.51it/s][A
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 47/95 [00:00<00:00, 107.66it/s][A
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 58/95 [00:00<00:00, 107.36it/s][A
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 69/95 [00:00<00:00, 106.85it/s][A
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 80/95 [00:00<00:00, 106.84it/s][A
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 91/95 [00:00<00:00, 106.71it/s][A                                                  
                                                [A 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 642/1070 [00:48<00:29, 14.31it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 95/95 [00:00<00:00, 106.71it/s][A
                                                [A 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 643/1070 [00:48<01:28,  4.82it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 645/1070 [00:48<01:10,  6.02it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 647/1070 [00:48<00:58,  7.28it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 649/1070 [00:49<00:49,  8.53it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 651/1070 [00:49<00:43,  9.70it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 653/1070 [00:49<00:38, 10.73it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 655/1070 [00:49<00:35, 11.58it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 657/1070 [00:49<00:33, 12.27it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 659/1070 [00:49<00:32, 12.80it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 661/1070 [00:49<00:30, 13.22it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 663/1070 [00:50<00:30, 13.51it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 665/1070 [00:50<00:29, 13.74it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 667/1070 [00:50<00:28, 13.90it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 669/1070 [00:50<00:28, 14.02it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 671/1070 [00:50<00:28, 14.10it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 673/1070 [00:50<00:28, 14.16it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 675/1070 [00:50<00:27, 14.18it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 677/1070 [00:50<00:27, 14.22it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 679/1070 [00:51<00:27, 14.25it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 681/1070 [00:51<00:27, 14.27it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 683/1070 [00:51<00:27, 14.27it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 685/1070 [00:51<00:27, 14.25it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 687/1070 [00:51<00:26, 14.26it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 689/1070 [00:51<00:26, 14.28it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 691/1070 [00:51<00:26, 14.28it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 693/1070 [00:52<00:26, 14.29it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 695/1070 [00:52<00:26, 14.28it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 697/1070 [00:52<00:26, 14.29it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 699/1070 [00:52<00:25, 14.29it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 701/1070 [00:52<00:25, 14.30it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 703/1070 [00:52<00:25, 14.30it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 705/1070 [00:52<00:25, 14.29it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 707/1070 [00:53<00:25, 14.28it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 709/1070 [00:53<00:25, 14.28it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 711/1070 [00:53<00:25, 14.28it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 713/1070 [00:53<00:24, 14.30it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 715/1070 [00:53<00:24, 14.30it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 717/1070 [00:53<00:24, 14.29it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 719/1070 [00:53<00:24, 14.29it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 721/1070 [00:54<00:24, 14.29it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 723/1070 [00:54<00:24, 14.29it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 725/1070 [00:54<00:24, 14.28it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 727/1070 [00:54<00:24, 14.29it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 729/1070 [00:54<00:23, 14.27it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 731/1070 [00:54<00:23, 14.29it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 733/1070 [00:54<00:23, 14.28it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 735/1070 [00:55<00:23, 14.30it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 737/1070 [00:55<00:23, 14.28it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 739/1070 [00:55<00:23, 14.29it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 741/1070 [00:55<00:23, 14.25it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 743/1070 [00:55<00:22, 14.28it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 745/1070 [00:55<00:22, 14.29it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 747/1070 [00:55<00:22, 14.31it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 749/1070 [00:56<00:22, 14.29it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 751/1070 [00:56<00:22, 14.29it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 753/1070 [00:56<00:22, 14.30it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 755/1070 [00:56<00:22, 14.29it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 757/1070 [00:56<00:21, 14.30it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 759/1070 [00:56<00:21, 14.29it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 761/1070 [00:56<00:21, 14.29it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 763/1070 [00:57<00:21, 14.30it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 765/1070 [00:57<00:21, 14.31it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 767/1070 [00:57<00:21, 14.30it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 769/1070 [00:57<00:21, 14.29it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 771/1070 [00:57<00:20, 14.29it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 773/1070 [00:57<00:20, 14.28it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 775/1070 [00:57<00:20, 14.29it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 777/1070 [00:57<00:20, 14.29it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 779/1070 [00:58<00:20, 14.29it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 781/1070 [00:58<00:20, 14.29it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 783/1070 [00:58<00:20, 14.28it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 785/1070 [00:58<00:19, 14.29it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 787/1070 [00:58<00:19, 14.27it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 789/1070 [00:58<00:19, 14.27it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 791/1070 [00:58<00:19, 14.27it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 793/1070 [00:59<00:19, 14.29it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 795/1070 [00:59<00:19, 14.28it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 797/1070 [00:59<00:19, 14.25it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 799/1070 [00:59<00:18, 14.28it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 801/1070 [00:59<00:18, 14.29it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 803/1070 [00:59<00:18, 14.30it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 805/1070 [00:59<00:18, 14.31it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 807/1070 [01:00<00:18, 14.30it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 809/1070 [01:00<00:18, 14.31it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 811/1070 [01:00<00:18, 14.31it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 813/1070 [01:00<00:17, 14.31it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 815/1070 [01:00<00:17, 14.31it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 817/1070 [01:00<00:17, 14.31it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 819/1070 [01:00<00:17, 14.31it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 821/1070 [01:01<00:17, 14.30it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 823/1070 [01:01<00:17, 14.29it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 825/1070 [01:01<00:17, 14.29it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 827/1070 [01:01<00:17, 14.29it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 829/1070 [01:01<00:16, 14.29it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 831/1070 [01:01<00:16, 14.29it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 833/1070 [01:01<00:16, 14.30it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 835/1070 [01:02<00:16, 14.30it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 837/1070 [01:02<00:16, 14.28it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 839/1070 [01:02<00:16, 14.29it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 841/1070 [01:02<00:16, 14.29it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 843/1070 [01:02<00:15, 14.30it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 845/1070 [01:02<00:15, 14.29it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 847/1070 [01:02<00:15, 14.24it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 849/1070 [01:03<00:15, 14.28it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 851/1070 [01:03<00:15, 14.28it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 853/1070 [01:03<00:15, 14.28it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 855/1070 [01:03<00:15, 14.31it/s]                                                   80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 856/1070 [01:03<00:14, 14.31it/s][INFO|trainer.py:755] 2023-11-15 22:31:47,146 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModelForTokenClassification.forward` and have been ignored: text. If text are not expected by `PeftModelForTokenClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 22:31:47,147 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 22:31:47,147 >>   Num examples = 760
[INFO|trainer.py:3134] 2023-11-15 22:31:47,148 >>   Batch size = 8
{'eval_loss': 0.2832222580909729, 'eval_accuracy': 0.9131578947368421, 'eval_micro_f1': 0.9131578947368421, 'eval_macro_f1': 0.9102371477314333, 'eval_runtime': 0.9248, 'eval_samples_per_second': 821.829, 'eval_steps_per_second': 102.729, 'epoch': 3.0}
{'loss': 0.1905, 'learning_rate': 0.0001, 'epoch': 4.0}

  0%|          | 0/95 [00:00<?, ?it/s][A
 13%|â–ˆâ–Ž        | 12/95 [00:00<00:00, 116.85it/s][A
 25%|â–ˆâ–ˆâ–Œ       | 24/95 [00:00<00:00, 110.65it/s][A
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 36/95 [00:00<00:00, 108.43it/s][A
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 47/95 [00:00<00:00, 107.59it/s][A
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 58/95 [00:00<00:00, 107.12it/s][A
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 69/95 [00:00<00:00, 106.91it/s][A
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 80/95 [00:00<00:00, 106.53it/s][A
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 91/95 [00:00<00:00, 106.47it/s][A                                                  
                                                [A 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 856/1070 [01:04<00:14, 14.31it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 95/95 [00:00<00:00, 106.47it/s][A
                                                [A 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 857/1070 [01:04<00:44,  4.82it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 859/1070 [01:04<00:35,  6.02it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 861/1070 [01:04<00:28,  7.28it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 863/1070 [01:04<00:24,  8.53it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 865/1070 [01:05<00:21,  9.69it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 867/1070 [01:05<00:18, 10.72it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 869/1070 [01:05<00:17, 11.58it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 871/1070 [01:05<00:16, 12.27it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 873/1070 [01:05<00:15, 12.80it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 875/1070 [01:05<00:14, 13.21it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 877/1070 [01:05<00:14, 13.52it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 879/1070 [01:06<00:13, 13.75it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 881/1070 [01:06<00:13, 13.91it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 883/1070 [01:06<00:13, 14.03it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 885/1070 [01:06<00:13, 14.09it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 887/1070 [01:06<00:12, 14.15it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 889/1070 [01:06<00:12, 14.19it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 891/1070 [01:06<00:12, 14.22it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 893/1070 [01:07<00:12, 14.24it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 895/1070 [01:07<00:12, 14.25it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 897/1070 [01:07<00:12, 14.27it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 899/1070 [01:07<00:11, 14.28it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 901/1070 [01:07<00:11, 14.28it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 903/1070 [01:07<00:11, 14.29it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 905/1070 [01:07<00:11, 14.29it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 907/1070 [01:08<00:11, 14.28it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 909/1070 [01:08<00:11, 14.24it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 911/1070 [01:08<00:11, 14.24it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 913/1070 [01:08<00:11, 14.25it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 915/1070 [01:08<00:10, 14.28it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 917/1070 [01:08<00:10, 14.29it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 919/1070 [01:08<00:10, 14.30it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 921/1070 [01:08<00:10, 14.31it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 923/1070 [01:09<00:10, 14.29it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 925/1070 [01:09<00:10, 14.30it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 927/1070 [01:09<00:09, 14.31it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 929/1070 [01:09<00:09, 14.31it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 931/1070 [01:09<00:09, 14.30it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 933/1070 [01:09<00:09, 14.29it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 935/1070 [01:09<00:09, 14.29it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 937/1070 [01:10<00:09, 14.30it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 939/1070 [01:10<00:09, 14.30it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 941/1070 [01:10<00:09, 14.30it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 943/1070 [01:10<00:08, 14.29it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 945/1070 [01:10<00:08, 14.28it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 947/1070 [01:10<00:08, 14.29it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 949/1070 [01:10<00:08, 14.29it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 951/1070 [01:11<00:08, 14.28it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 953/1070 [01:11<00:08, 14.29it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 955/1070 [01:11<00:08, 14.30it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 957/1070 [01:11<00:07, 14.30it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 959/1070 [01:11<00:07, 14.30it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 961/1070 [01:11<00:07, 14.30it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 963/1070 [01:11<00:07, 14.31it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 965/1070 [01:12<00:07, 14.26it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 967/1070 [01:12<00:07, 14.27it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 969/1070 [01:12<00:07, 14.29it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 971/1070 [01:12<00:06, 14.29it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 973/1070 [01:12<00:06, 14.30it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 975/1070 [01:12<00:06, 14.30it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 977/1070 [01:12<00:06, 14.30it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 979/1070 [01:13<00:06, 14.30it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 981/1070 [01:13<00:06, 14.30it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 983/1070 [01:13<00:06, 14.25it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 985/1070 [01:13<00:05, 14.27it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 987/1070 [01:13<00:05, 14.29it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 989/1070 [01:13<00:05, 14.30it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 991/1070 [01:13<00:05, 14.27it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 993/1070 [01:14<00:05, 14.28it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 995/1070 [01:14<00:05, 14.29it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 997/1070 [01:14<00:05, 14.30it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 999/1070 [01:14<00:04, 14.30it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1001/1070 [01:14<00:04, 14.29it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1003/1070 [01:14<00:04, 14.29it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1005/1070 [01:14<00:04, 14.30it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1007/1070 [01:14<00:04, 14.28it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1009/1070 [01:15<00:04, 14.27it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1011/1070 [01:15<00:04, 14.29it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1013/1070 [01:15<00:03, 14.29it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1015/1070 [01:15<00:03, 14.30it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1017/1070 [01:15<00:03, 14.28it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1019/1070 [01:15<00:03, 14.29it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1021/1070 [01:15<00:03, 14.29it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1023/1070 [01:16<00:03, 14.29it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1025/1070 [01:16<00:03, 14.29it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1027/1070 [01:16<00:03, 14.23it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1029/1070 [01:16<00:02, 14.05it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1031/1070 [01:16<00:02, 13.97it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1033/1070 [01:16<00:02, 13.90it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1035/1070 [01:16<00:02, 14.00it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1037/1070 [01:17<00:02, 14.08it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1039/1070 [01:17<00:02, 14.15it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1041/1070 [01:17<00:02, 14.19it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1043/1070 [01:17<00:01, 14.21it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1045/1070 [01:17<00:01, 14.24it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1047/1070 [01:17<00:01, 14.26it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1049/1070 [01:17<00:01, 14.27it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1051/1070 [01:18<00:01, 14.28it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1053/1070 [01:18<00:01, 14.28it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1055/1070 [01:18<00:01, 14.28it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1057/1070 [01:18<00:00, 14.29it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1059/1070 [01:18<00:00, 14.29it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1061/1070 [01:18<00:00, 14.31it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1063/1070 [01:18<00:00, 14.30it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1065/1070 [01:19<00:00, 14.30it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1067/1070 [01:19<00:00, 14.29it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1069/1070 [01:19<00:00, 14.30it/s]                                                   100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1070/1070 [01:19<00:00, 14.30it/s][INFO|trainer.py:755] 2023-11-15 22:32:03,061 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModelForTokenClassification.forward` and have been ignored: text. If text are not expected by `PeftModelForTokenClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 22:32:03,062 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 22:32:03,063 >>   Num examples = 760
[INFO|trainer.py:3134] 2023-11-15 22:32:03,063 >>   Batch size = 8
{'eval_loss': 0.30256906151771545, 'eval_accuracy': 0.9118421052631579, 'eval_micro_f1': 0.9118421052631579, 'eval_macro_f1': 0.9092192223490165, 'eval_runtime': 0.9245, 'eval_samples_per_second': 822.034, 'eval_steps_per_second': 102.754, 'epoch': 4.0}
{'loss': 0.1647, 'learning_rate': 0.0, 'epoch': 5.0}

  0%|          | 0/95 [00:00<?, ?it/s][A
 13%|â–ˆâ–Ž        | 12/95 [00:00<00:00, 116.05it/s][A
 25%|â–ˆâ–ˆâ–Œ       | 24/95 [00:00<00:00, 110.36it/s][A
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 36/95 [00:00<00:00, 108.43it/s][A
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 47/95 [00:00<00:00, 107.67it/s][A
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 58/95 [00:00<00:00, 107.24it/s][A
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 69/95 [00:00<00:00, 106.74it/s][A
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 80/95 [00:00<00:00, 106.49it/s][A
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 91/95 [00:00<00:00, 106.40it/s][A                                                   
                                                [A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1070/1070 [01:20<00:00, 14.30it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 95/95 [00:00<00:00, 106.40it/s][A
                                                [A[INFO|trainer.py:1963] 2023-11-15 22:32:03,991 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                   100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1070/1070 [01:20<00:00, 14.30it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1070/1070 [01:20<00:00, 13.32it/s]
[INFO|trainer.py:2855] 2023-11-15 22:32:03,994 >> Saving model checkpoint to ./result/agnews_sup_roberta-base_seed4_lora
[INFO|tokenization_utils_base.py:2235] 2023-11-15 22:32:04,101 >> tokenizer config file saved in ./result/agnews_sup_roberta-base_seed4_lora/tokenizer_config.json
[INFO|tokenization_utils_base.py:2242] 2023-11-15 22:32:04,102 >> Special tokens file saved in ./result/agnews_sup_roberta-base_seed4_lora/special_tokens_map.json
{'eval_loss': 0.28378114104270935, 'eval_accuracy': 0.9118421052631579, 'eval_micro_f1': 0.9118421052631579, 'eval_macro_f1': 0.9093237830908731, 'eval_runtime': 0.925, 'eval_samples_per_second': 821.627, 'eval_steps_per_second': 102.703, 'epoch': 5.0}
{'train_runtime': 80.3456, 'train_samples_per_second': 425.661, 'train_steps_per_second': 13.317, 'train_loss': 0.256504422481929, 'epoch': 5.0}
***** train metrics *****
  epoch                    =        5.0
  train_loss               =     0.2565
  train_runtime            = 0:01:20.34
  train_samples            =       6840
  train_samples_per_second =    425.661
  train_steps_per_second   =     13.317
11/15/2023 22:32:04 - INFO - __main__ - *** Evaluate ***
[INFO|trainer.py:755] 2023-11-15 22:32:04,205 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModelForTokenClassification.forward` and have been ignored: text. If text are not expected by `PeftModelForTokenClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 22:32:04,207 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 22:32:04,207 >>   Num examples = 760
[INFO|trainer.py:3134] 2023-11-15 22:32:04,208 >>   Batch size = 8
  0%|          | 0/95 [00:00<?, ?it/s] 13%|â–ˆâ–Ž        | 12/95 [00:00<00:00, 117.91it/s] 25%|â–ˆâ–ˆâ–Œ       | 24/95 [00:00<00:00, 110.85it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 36/95 [00:00<00:00, 108.91it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 47/95 [00:00<00:00, 107.76it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 58/95 [00:00<00:00, 107.52it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 69/95 [00:00<00:00, 107.15it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 80/95 [00:00<00:00, 106.83it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 91/95 [00:00<00:00, 106.61it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 95/95 [00:00<00:00, 104.84it/s]
***** eval metrics *****
  epoch                   =        5.0
  eval_accuracy           =     0.9118
  eval_loss               =     0.2838
  eval_macro_f1           =     0.9093
  eval_micro_f1           =     0.9118
  eval_runtime            = 0:00:00.91
  eval_samples            =        760
  eval_samples_per_second =    827.158
  eval_steps_per_second   =    103.395
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                  eval/accuracy â–â–â–ˆâ–ˆâ–ˆâ–ˆ
wandb:                      eval/loss â–…â–ˆâ–â–„â–â–
wandb:                  eval/macro_f1 â–â–â–ˆâ–ˆâ–ˆâ–ˆ
wandb:                  eval/micro_f1 â–â–â–ˆâ–ˆâ–ˆâ–ˆ
wandb:                   eval/runtime â–â–…â–ˆâ–ˆâ–ˆâ–ƒ
wandb:        eval/samples_per_second â–ˆâ–„â–â–â–â–†
wandb:          eval/steps_per_second â–ˆâ–„â–â–â–â–†
wandb:                    train/epoch â–â–â–ƒâ–ƒâ–…â–…â–†â–†â–ˆâ–ˆâ–ˆâ–ˆ
wandb:              train/global_step â–â–â–ƒâ–ƒâ–„â–„â–†â–†â–ˆâ–ˆâ–ˆâ–ˆ
wandb:            train/learning_rate â–ˆâ–†â–…â–ƒâ–
wandb:                     train/loss â–ˆâ–„â–ƒâ–‚â–
wandb:               train/total_flos â–
wandb:               train/train_loss â–
wandb:            train/train_runtime â–
wandb: train/train_samples_per_second â–
wandb:   train/train_steps_per_second â–
wandb: 
wandb: Run summary:
wandb:                  eval/accuracy 0.91184
wandb:                      eval/loss 0.28378
wandb:                  eval/macro_f1 0.90932
wandb:                  eval/micro_f1 0.91184
wandb:                   eval/runtime 0.9188
wandb:        eval/samples_per_second 827.158
wandb:          eval/steps_per_second 103.395
wandb:                    train/epoch 5.0
wandb:              train/global_step 1070
wandb:            train/learning_rate 0.0
wandb:                     train/loss 0.1647
wandb:               train/total_flos 1140362523648000.0
wandb:               train/train_loss 0.2565
wandb:            train/train_runtime 80.3456
wandb: train/train_samples_per_second 425.661
wandb:   train/train_steps_per_second 13.317
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /ceph/home/wangyifei/a3/wandb/offline-run-20231115_222926-2e0044xl
wandb: Find logs at: ./wandb/offline-run-20231115_222926-2e0044xl/logs
