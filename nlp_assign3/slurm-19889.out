(ModelArguments(model_name_or_path='roberta-base', cache_dir='./cache', use_adapter=True, use_lora=False), DataTrainingArguments(dataset_name='restaurant', max_seq_length=64), TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.0005,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=./result/restaurant_roberta-base_seed0_adapter/runs/Nov15_23-09-21_node013,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.EPOCH,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=5.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
output_dir=./result/restaurant_roberta-base_seed0_adapter,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
resume_from_checkpoint=None,
run_name=./result/restaurant_roberta-base_seed0_adapter,
save_on_each_node=False,
save_safetensors=False,
save_steps=500,
save_strategy=IntervalStrategy.NO,
save_total_limit=None,
seed=111,
sharded_ddp=[],
skip_memory_metrics=True,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
))
11/15/2023 23:09:21 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False
11/15/2023 23:09:21 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.0005,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=./result/restaurant_roberta-base_seed0_adapter/runs/Nov15_23-09-21_node013,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.EPOCH,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=5.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
output_dir=./result/restaurant_roberta-base_seed0_adapter,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
resume_from_checkpoint=None,
run_name=./result/restaurant_roberta-base_seed0_adapter,
save_on_each_node=False,
save_safetensors=False,
save_steps=500,
save_strategy=IntervalStrategy.NO,
save_total_limit=None,
seed=111,
sharded_ddp=[],
skip_memory_metrics=True,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
wandb: Tracking run with wandb version 0.15.12
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
Map:   0%|          | 0/4722 [00:00<?, ? examples/s]Map:  88%|████████▊ | 4137/4722 [00:00<00:00, 39526.90 examples/s]Map: 100%|██████████| 4722/4722 [00:00<00:00, 39120.67 examples/s]
[INFO|configuration_utils.py:715] 2023-11-15 23:09:37,573 >> loading configuration file config.json from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/config.json
[INFO|configuration_utils.py:775] 2023-11-15 23:09:37,582 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.33.3",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_auto.py:535] 2023-11-15 23:09:47,598 >> Could not locate the tokenizer configuration file, will try to use the model config instead.
[INFO|configuration_utils.py:715] 2023-11-15 23:09:57,614 >> loading configuration file config.json from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/config.json
[INFO|configuration_utils.py:775] 2023-11-15 23:09:57,615 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.33.3",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:1852] 2023-11-15 23:10:17,664 >> loading file vocab.json from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/vocab.json
[INFO|tokenization_utils_base.py:1852] 2023-11-15 23:10:17,664 >> loading file merges.txt from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/merges.txt
[INFO|tokenization_utils_base.py:1852] 2023-11-15 23:10:17,664 >> loading file tokenizer.json from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/tokenizer.json
[INFO|tokenization_utils_base.py:1852] 2023-11-15 23:10:17,665 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1852] 2023-11-15 23:10:17,665 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1852] 2023-11-15 23:10:17,665 >> loading file tokenizer_config.json from cache at None
[INFO|configuration_utils.py:715] 2023-11-15 23:10:17,667 >> loading configuration file config.json from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/config.json
[INFO|configuration_utils.py:775] 2023-11-15 23:10:17,668 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.33.3",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:2862] 2023-11-15 23:10:37,876 >> loading weights file model.safetensors from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/model.safetensors
[INFO|modeling_utils.py:3638] 2023-11-15 23:10:38,565 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3650] 2023-11-15 23:10:38,565 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
parameters 1487427
model_max_length: 512
Running tokenizer on dataset:   0%|          | 0/3777 [00:00<?, ? examples/s]Running tokenizer on dataset: 100%|██████████| 3777/3777 [00:00<00:00, 26821.61 examples/s]Running tokenizer on dataset: 100%|██████████| 3777/3777 [00:00<00:00, 26462.68 examples/s]
Running tokenizer on dataset:   0%|          | 0/945 [00:00<?, ? examples/s]Running tokenizer on dataset: 100%|██████████| 945/945 [00:00<00:00, 30717.62 examples/s]
11/15/2023 23:10:39 - INFO - __main__ - Sample 3388 of the training set: {'text': 'food <SEP> Had dinner here on a Friday and the food was great.', 'label': 0, 'input_ids': [0, 13193, 28696, 3388, 510, 15698, 7301, 3630, 259, 15, 10, 273, 8, 5, 689, 21, 372, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
11/15/2023 23:10:39 - INFO - __main__ - Sample 871 of the training set: {'text': 'drinks <SEP> You get what you pay for and with that logic in mind, Spice is a great place to grab some cheap eats and drinks in a beautiful setting.', 'label': 0, 'input_ids': [0, 10232, 12935, 28696, 3388, 510, 15698, 370, 120, 99, 47, 582, 13, 8, 19, 14, 14578, 11, 1508, 6, 21665, 16, 10, 372, 317, 7, 6895, 103, 6162, 24923, 8, 6696, 11, 10, 2721, 2749, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
11/15/2023 23:10:39 - INFO - __main__ - Sample 3773 of the training set: {'text': 'food <SEP> Good service, great food, good value, and never have to wait in line!', 'label': 0, 'input_ids': [0, 13193, 28696, 3388, 510, 15698, 2497, 544, 6, 372, 689, 6, 205, 923, 6, 8, 393, 33, 7, 2067, 11, 516, 328, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
11/15/2023 23:10:39 - WARNING - accelerate.utils.other - Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[INFO|trainer.py:755] 2023-11-15 23:10:40,654 >> The following columns in the training set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:1715] 2023-11-15 23:10:40,666 >> ***** Running training *****
[INFO|trainer.py:1716] 2023-11-15 23:10:40,666 >>   Num examples = 3,777
[INFO|trainer.py:1717] 2023-11-15 23:10:40,666 >>   Num Epochs = 5
[INFO|trainer.py:1718] 2023-11-15 23:10:40,667 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1721] 2023-11-15 23:10:40,667 >>   Total train batch size (w. parallel, distributed & accumulation) = 32
[INFO|trainer.py:1722] 2023-11-15 23:10:40,667 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:1723] 2023-11-15 23:10:40,668 >>   Total optimization steps = 595
[INFO|trainer.py:1724] 2023-11-15 23:10:40,669 >>   Number of trainable parameters = 1,487,427
[INFO|integration_utils.py:716] 2023-11-15 23:10:40,670 >> Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"
  0%|          | 0/595 [00:00<?, ?it/s]  0%|          | 1/595 [00:01<10:20,  1.04s/it]  1%|          | 3/595 [00:01<03:07,  3.15it/s]  1%|          | 5/595 [00:01<01:49,  5.38it/s]  1%|          | 7/595 [00:01<01:18,  7.49it/s]  2%|▏         | 9/595 [00:01<01:02,  9.35it/s]  2%|▏         | 11/595 [00:01<00:53, 10.99it/s]  2%|▏         | 13/595 [00:01<00:47, 12.30it/s]  3%|▎         | 15/595 [00:01<00:43, 13.29it/s]  3%|▎         | 17/595 [00:02<00:41, 13.87it/s]  3%|▎         | 19/595 [00:02<00:40, 14.27it/s]  4%|▎         | 21/595 [00:02<00:39, 14.64it/s]  4%|▍         | 23/595 [00:02<00:38, 15.05it/s]  4%|▍         | 25/595 [00:02<00:37, 15.34it/s]  5%|▍         | 27/595 [00:02<00:36, 15.59it/s]  5%|▍         | 29/595 [00:02<00:35, 15.78it/s]  5%|▌         | 31/595 [00:02<00:35, 15.94it/s]  6%|▌         | 33/595 [00:03<00:34, 16.10it/s]  6%|▌         | 35/595 [00:03<00:34, 16.21it/s]  6%|▌         | 37/595 [00:03<00:34, 16.28it/s]  7%|▋         | 39/595 [00:03<00:34, 16.30it/s]  7%|▋         | 41/595 [00:03<00:34, 16.27it/s]  7%|▋         | 43/595 [00:03<00:33, 16.28it/s]  8%|▊         | 45/595 [00:03<00:33, 16.27it/s]  8%|▊         | 47/595 [00:03<00:33, 16.27it/s]  8%|▊         | 49/595 [00:04<00:33, 16.32it/s]  9%|▊         | 51/595 [00:04<00:33, 16.36it/s]  9%|▉         | 53/595 [00:04<00:33, 16.38it/s]  9%|▉         | 55/595 [00:04<00:32, 16.37it/s] 10%|▉         | 57/595 [00:04<00:32, 16.37it/s] 10%|▉         | 59/595 [00:04<00:32, 16.32it/s] 10%|█         | 61/595 [00:04<00:32, 16.30it/s] 11%|█         | 63/595 [00:04<00:32, 16.27it/s] 11%|█         | 65/595 [00:05<00:32, 16.25it/s] 11%|█▏        | 67/595 [00:05<00:32, 16.19it/s] 12%|█▏        | 69/595 [00:05<00:32, 16.12it/s] 12%|█▏        | 71/595 [00:05<00:32, 16.05it/s] 12%|█▏        | 73/595 [00:05<00:32, 16.05it/s] 13%|█▎        | 75/595 [00:05<00:32, 16.08it/s] 13%|█▎        | 77/595 [00:05<00:32, 15.98it/s] 13%|█▎        | 79/595 [00:05<00:32, 16.00it/s] 14%|█▎        | 81/595 [00:06<00:32, 16.02it/s] 14%|█▍        | 83/595 [00:06<00:32, 15.90it/s] 14%|█▍        | 85/595 [00:06<00:31, 15.97it/s] 15%|█▍        | 87/595 [00:06<00:31, 15.98it/s] 15%|█▍        | 89/595 [00:06<00:31, 16.08it/s] 15%|█▌        | 91/595 [00:06<00:31, 15.99it/s] 16%|█▌        | 93/595 [00:06<00:31, 15.93it/s] 16%|█▌        | 95/595 [00:06<00:31, 15.91it/s] 16%|█▋        | 97/595 [00:07<00:31, 15.94it/s] 17%|█▋        | 99/595 [00:07<00:31, 15.99it/s] 17%|█▋        | 101/595 [00:07<00:31, 15.87it/s] 17%|█▋        | 103/595 [00:07<00:31, 15.79it/s] 18%|█▊        | 105/595 [00:07<00:31, 15.53it/s] 18%|█▊        | 107/595 [00:07<00:31, 15.53it/s] 18%|█▊        | 109/595 [00:07<00:31, 15.48it/s] 19%|█▊        | 111/595 [00:07<00:31, 15.34it/s] 19%|█▉        | 113/595 [00:08<00:31, 15.21it/s] 19%|█▉        | 115/595 [00:08<00:31, 15.08it/s] 20%|█▉        | 117/595 [00:08<00:31, 15.06it/s] 20%|██        | 119/595 [00:08<00:29, 16.12it/s]                                                  20%|██        | 119/595 [00:08<00:29, 16.12it/s][INFO|trainer.py:755] 2023-11-15 23:10:49,100 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 23:10:49,102 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 23:10:49,102 >>   Num examples = 945
[INFO|trainer.py:3134] 2023-11-15 23:10:49,102 >>   Batch size = 8
{'loss': 0.6769, 'learning_rate': 0.0004, 'epoch': 1.0}

  0%|          | 0/119 [00:00<?, ?it/s][A
  8%|▊         | 9/119 [00:00<00:01, 79.40it/s][A
 14%|█▍        | 17/119 [00:00<00:01, 79.44it/s][A
 21%|██        | 25/119 [00:00<00:01, 79.68it/s][A
 28%|██▊       | 33/119 [00:00<00:01, 78.88it/s][A
 34%|███▍      | 41/119 [00:00<00:00, 78.17it/s][A
 42%|████▏     | 50/119 [00:00<00:00, 79.83it/s][A
 50%|████▉     | 59/119 [00:00<00:00, 81.00it/s][A
 57%|█████▋    | 68/119 [00:00<00:00, 82.94it/s][A
 65%|██████▍   | 77/119 [00:00<00:00, 83.90it/s][A
 72%|███████▏  | 86/119 [00:01<00:00, 84.07it/s][A
 80%|███████▉  | 95/119 [00:01<00:00, 84.38it/s][A
 87%|████████▋ | 104/119 [00:01<00:00, 84.93it/s][A
 95%|█████████▍| 113/119 [00:01<00:00, 85.40it/s][A                                                 
                                                 [A 20%|██        | 119/595 [00:09<00:29, 16.12it/s]
100%|██████████| 119/119 [00:01<00:00, 85.40it/s][A
                                                 [A 20%|██        | 121/595 [00:10<02:15,  3.49it/s] 21%|██        | 123/595 [00:10<01:43,  4.56it/s] 21%|██        | 125/595 [00:10<01:21,  5.79it/s] 21%|██▏       | 127/595 [00:10<01:05,  7.15it/s] 22%|██▏       | 129/595 [00:10<00:54,  8.57it/s] 22%|██▏       | 131/595 [00:10<00:46,  9.98it/s] 22%|██▏       | 133/595 [00:10<00:41, 11.27it/s] 23%|██▎       | 135/595 [00:10<00:37, 12.38it/s] 23%|██▎       | 137/595 [00:11<00:34, 13.30it/s] 23%|██▎       | 139/595 [00:11<00:32, 14.00it/s] 24%|██▎       | 141/595 [00:11<00:31, 14.50it/s] 24%|██▍       | 143/595 [00:11<00:30, 14.87it/s] 24%|██▍       | 145/595 [00:11<00:29, 15.09it/s] 25%|██▍       | 147/595 [00:11<00:29, 15.22it/s] 25%|██▌       | 149/595 [00:11<00:28, 15.39it/s] 25%|██▌       | 151/595 [00:11<00:28, 15.55it/s] 26%|██▌       | 153/595 [00:12<00:28, 15.66it/s] 26%|██▌       | 155/595 [00:12<00:27, 15.76it/s] 26%|██▋       | 157/595 [00:12<00:27, 15.86it/s] 27%|██▋       | 159/595 [00:12<00:27, 15.92it/s] 27%|██▋       | 161/595 [00:12<00:27, 15.97it/s] 27%|██▋       | 163/595 [00:12<00:27, 15.92it/s] 28%|██▊       | 165/595 [00:12<00:26, 15.93it/s] 28%|██▊       | 167/595 [00:12<00:26, 15.91it/s] 28%|██▊       | 169/595 [00:13<00:26, 15.93it/s] 29%|██▊       | 171/595 [00:13<00:26, 15.96it/s] 29%|██▉       | 173/595 [00:13<00:26, 15.97it/s] 29%|██▉       | 175/595 [00:13<00:26, 15.83it/s] 30%|██▉       | 177/595 [00:13<00:26, 15.82it/s] 30%|███       | 179/595 [00:13<00:26, 15.63it/s] 30%|███       | 181/595 [00:13<00:26, 15.46it/s] 31%|███       | 183/595 [00:13<00:26, 15.40it/s] 31%|███       | 185/595 [00:14<00:26, 15.40it/s] 31%|███▏      | 187/595 [00:14<00:26, 15.55it/s] 32%|███▏      | 189/595 [00:14<00:25, 15.72it/s] 32%|███▏      | 191/595 [00:14<00:25, 15.86it/s] 32%|███▏      | 193/595 [00:14<00:25, 15.98it/s] 33%|███▎      | 195/595 [00:14<00:24, 16.07it/s] 33%|███▎      | 197/595 [00:14<00:24, 16.11it/s] 33%|███▎      | 199/595 [00:14<00:24, 16.09it/s] 34%|███▍      | 201/595 [00:15<00:24, 16.01it/s] 34%|███▍      | 203/595 [00:15<00:24, 16.00it/s] 34%|███▍      | 205/595 [00:15<00:24, 15.94it/s] 35%|███▍      | 207/595 [00:15<00:24, 15.97it/s] 35%|███▌      | 209/595 [00:15<00:24, 16.02it/s] 35%|███▌      | 211/595 [00:15<00:23, 16.05it/s] 36%|███▌      | 213/595 [00:15<00:23, 16.09it/s] 36%|███▌      | 215/595 [00:15<00:23, 16.08it/s] 36%|███▋      | 217/595 [00:16<00:23, 16.05it/s] 37%|███▋      | 219/595 [00:16<00:23, 16.11it/s] 37%|███▋      | 221/595 [00:16<00:23, 16.13it/s] 37%|███▋      | 223/595 [00:16<00:22, 16.20it/s] 38%|███▊      | 225/595 [00:16<00:22, 16.21it/s] 38%|███▊      | 227/595 [00:16<00:22, 16.11it/s] 38%|███▊      | 229/595 [00:16<00:22, 16.08it/s] 39%|███▉      | 231/595 [00:16<00:22, 16.08it/s] 39%|███▉      | 233/595 [00:17<00:22, 16.01it/s] 39%|███▉      | 235/595 [00:17<00:22, 15.86it/s] 40%|███▉      | 237/595 [00:17<00:22, 16.00it/s]                                                  40%|████      | 238/595 [00:17<00:22, 16.00it/s][INFO|trainer.py:755] 2023-11-15 23:10:58,030 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 23:10:58,031 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 23:10:58,032 >>   Num examples = 945
[INFO|trainer.py:3134] 2023-11-15 23:10:58,032 >>   Batch size = 8
{'eval_loss': 0.5299867987632751, 'eval_accuracy': 0.7767195767195767, 'eval_micro_f1': 0.7767195767195768, 'eval_macro_f1': 0.6789054118808485, 'eval_runtime': 1.4871, 'eval_samples_per_second': 635.454, 'eval_steps_per_second': 80.02, 'epoch': 1.0}
{'loss': 0.5139, 'learning_rate': 0.0003, 'epoch': 2.0}

  0%|          | 0/119 [00:00<?, ?it/s][A
  8%|▊         | 10/119 [00:00<00:01, 93.54it/s][A
 17%|█▋        | 20/119 [00:00<00:01, 87.74it/s][A
 24%|██▍       | 29/119 [00:00<00:01, 85.72it/s][A
 32%|███▏      | 38/119 [00:00<00:00, 84.74it/s][A
 39%|███▉      | 47/119 [00:00<00:00, 77.11it/s][A
 46%|████▌     | 55/119 [00:00<00:00, 73.05it/s][A
 53%|█████▎    | 63/119 [00:00<00:00, 71.11it/s][A
 60%|█████▉    | 71/119 [00:00<00:00, 65.67it/s][A
 66%|██████▌   | 78/119 [00:01<00:00, 65.99it/s][A
 71%|███████▏  | 85/119 [00:01<00:00, 66.59it/s][A
 77%|███████▋  | 92/119 [00:01<00:00, 67.09it/s][A
 83%|████████▎ | 99/119 [00:01<00:00, 67.17it/s][A
 89%|████████▉ | 106/119 [00:01<00:00, 67.30it/s][A
 95%|█████████▍| 113/119 [00:01<00:00, 67.79it/s][A                                                 
                                                 [A 40%|████      | 238/595 [00:19<00:22, 16.00it/s]
100%|██████████| 119/119 [00:01<00:00, 67.79it/s][A
                                                 [A 40%|████      | 239/595 [00:19<01:52,  3.15it/s] 41%|████      | 241/595 [00:19<01:25,  4.15it/s] 41%|████      | 243/595 [00:19<01:06,  5.32it/s] 41%|████      | 245/595 [00:19<00:52,  6.64it/s] 42%|████▏     | 247/595 [00:19<00:43,  8.01it/s] 42%|████▏     | 249/595 [00:19<00:36,  9.41it/s] 42%|████▏     | 251/595 [00:19<00:32, 10.71it/s] 43%|████▎     | 253/595 [00:20<00:28, 11.92it/s] 43%|████▎     | 255/595 [00:20<00:26, 12.94it/s] 43%|████▎     | 257/595 [00:20<00:24, 13.83it/s] 44%|████▎     | 259/595 [00:20<00:23, 14.52it/s] 44%|████▍     | 261/595 [00:20<00:22, 15.02it/s] 44%|████▍     | 263/595 [00:20<00:21, 15.40it/s] 45%|████▍     | 265/595 [00:20<00:21, 15.65it/s] 45%|████▍     | 267/595 [00:20<00:20, 15.82it/s] 45%|████▌     | 269/595 [00:21<00:20, 15.95it/s] 46%|████▌     | 271/595 [00:21<00:20, 15.93it/s] 46%|████▌     | 273/595 [00:21<00:20, 16.02it/s] 46%|████▌     | 275/595 [00:21<00:19, 16.07it/s] 47%|████▋     | 277/595 [00:21<00:19, 16.12it/s] 47%|████▋     | 279/595 [00:21<00:19, 16.14it/s] 47%|████▋     | 281/595 [00:21<00:19, 16.16it/s] 48%|████▊     | 283/595 [00:21<00:19, 16.24it/s] 48%|████▊     | 285/595 [00:22<00:19, 16.29it/s] 48%|████▊     | 287/595 [00:22<00:18, 16.30it/s] 49%|████▊     | 289/595 [00:22<00:18, 16.30it/s] 49%|████▉     | 291/595 [00:22<00:18, 16.29it/s] 49%|████▉     | 293/595 [00:22<00:18, 16.25it/s] 50%|████▉     | 295/595 [00:22<00:18, 16.14it/s] 50%|████▉     | 297/595 [00:22<00:18, 16.06it/s] 50%|█████     | 299/595 [00:22<00:18, 16.00it/s] 51%|█████     | 301/595 [00:22<00:18, 15.93it/s] 51%|█████     | 303/595 [00:23<00:18, 15.93it/s] 51%|█████▏    | 305/595 [00:23<00:18, 15.94it/s] 52%|█████▏    | 307/595 [00:23<00:18, 15.99it/s] 52%|█████▏    | 309/595 [00:23<00:17, 15.97it/s] 52%|█████▏    | 311/595 [00:23<00:17, 16.04it/s] 53%|█████▎    | 313/595 [00:23<00:17, 16.03it/s] 53%|█████▎    | 315/595 [00:23<00:17, 16.06it/s] 53%|█████▎    | 317/595 [00:23<00:17, 16.03it/s] 54%|█████▎    | 319/595 [00:24<00:17, 16.10it/s] 54%|█████▍    | 321/595 [00:24<00:16, 16.14it/s] 54%|█████▍    | 323/595 [00:24<00:16, 16.09it/s] 55%|█████▍    | 325/595 [00:24<00:16, 16.09it/s] 55%|█████▍    | 327/595 [00:24<00:16, 16.06it/s] 55%|█████▌    | 329/595 [00:24<00:16, 16.12it/s] 56%|█████▌    | 331/595 [00:24<00:16, 16.16it/s] 56%|█████▌    | 333/595 [00:24<00:16, 16.10it/s] 56%|█████▋    | 335/595 [00:25<00:16, 16.00it/s] 57%|█████▋    | 337/595 [00:25<00:16, 15.98it/s] 57%|█████▋    | 339/595 [00:25<00:16, 15.96it/s] 57%|█████▋    | 341/595 [00:25<00:15, 15.94it/s] 58%|█████▊    | 343/595 [00:25<00:15, 15.98it/s] 58%|█████▊    | 345/595 [00:25<00:15, 16.11it/s] 58%|█████▊    | 347/595 [00:25<00:15, 16.19it/s] 59%|█████▊    | 349/595 [00:25<00:15, 16.22it/s] 59%|█████▉    | 351/595 [00:26<00:15, 16.20it/s] 59%|█████▉    | 353/595 [00:26<00:14, 16.18it/s] 60%|█████▉    | 355/595 [00:26<00:14, 16.17it/s]                                                  60%|██████    | 357/595 [00:26<00:14, 16.17it/s][INFO|trainer.py:755] 2023-11-15 23:11:07,118 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 23:11:07,119 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 23:11:07,120 >>   Num examples = 945
[INFO|trainer.py:3134] 2023-11-15 23:11:07,120 >>   Batch size = 8
{'eval_loss': 0.4457710385322571, 'eval_accuracy': 0.8296296296296296, 'eval_micro_f1': 0.8296296296296296, 'eval_macro_f1': 0.7342941970310392, 'eval_runtime': 1.7188, 'eval_samples_per_second': 549.809, 'eval_steps_per_second': 69.235, 'epoch': 2.0}
{'loss': 0.3922, 'learning_rate': 0.0002, 'epoch': 3.0}

  0%|          | 0/119 [00:00<?, ?it/s][A
  8%|▊         | 10/119 [00:00<00:01, 94.95it/s][A
 17%|█▋        | 20/119 [00:00<00:01, 89.44it/s][A
 24%|██▍       | 29/119 [00:00<00:01, 87.01it/s][A
 32%|███▏      | 38/119 [00:00<00:00, 86.45it/s][A
 39%|███▉      | 47/119 [00:00<00:00, 85.20it/s][A
 47%|████▋     | 56/119 [00:00<00:00, 84.81it/s][A
 55%|█████▍    | 65/119 [00:00<00:00, 85.22it/s][A
 62%|██████▏   | 74/119 [00:00<00:00, 84.81it/s][A
 70%|██████▉   | 83/119 [00:00<00:00, 84.93it/s][A
 77%|███████▋  | 92/119 [00:01<00:00, 84.72it/s][A
 85%|████████▍ | 101/119 [00:01<00:00, 84.72it/s][A
 92%|█████████▏| 110/119 [00:01<00:00, 85.00it/s][A
100%|██████████| 119/119 [00:01<00:00, 85.57it/s][A                                                 
                                                 [A 60%|██████    | 357/595 [00:27<00:14, 16.17it/s]
100%|██████████| 119/119 [00:01<00:00, 85.57it/s][A
                                                 [A 60%|██████    | 358/595 [00:27<00:58,  4.07it/s] 61%|██████    | 360/595 [00:28<00:46,  5.11it/s] 61%|██████    | 362/595 [00:28<00:36,  6.30it/s] 61%|██████    | 364/595 [00:28<00:30,  7.60it/s] 62%|██████▏   | 366/595 [00:28<00:25,  8.95it/s] 62%|██████▏   | 368/595 [00:28<00:22, 10.25it/s] 62%|██████▏   | 370/595 [00:28<00:19, 11.43it/s] 63%|██████▎   | 372/595 [00:28<00:17, 12.45it/s] 63%|██████▎   | 374/595 [00:28<00:16, 13.32it/s] 63%|██████▎   | 376/595 [00:29<00:15, 13.99it/s] 64%|██████▎   | 378/595 [00:29<00:14, 14.50it/s] 64%|██████▍   | 380/595 [00:29<00:14, 14.91it/s] 64%|██████▍   | 382/595 [00:29<00:13, 15.24it/s] 65%|██████▍   | 384/595 [00:29<00:13, 15.45it/s] 65%|██████▍   | 386/595 [00:29<00:13, 15.59it/s] 65%|██████▌   | 388/595 [00:29<00:13, 15.67it/s] 66%|██████▌   | 390/595 [00:29<00:12, 15.77it/s] 66%|██████▌   | 392/595 [00:30<00:12, 15.81it/s] 66%|██████▌   | 394/595 [00:30<00:12, 15.84it/s] 67%|██████▋   | 396/595 [00:30<00:12, 15.91it/s] 67%|██████▋   | 398/595 [00:30<00:12, 15.95it/s] 67%|██████▋   | 400/595 [00:30<00:12, 15.92it/s] 68%|██████▊   | 402/595 [00:30<00:12, 15.85it/s] 68%|██████▊   | 404/595 [00:30<00:12, 15.86it/s] 68%|██████▊   | 406/595 [00:30<00:11, 15.82it/s] 69%|██████▊   | 408/595 [00:31<00:11, 15.84it/s] 69%|██████▉   | 410/595 [00:31<00:11, 15.95it/s] 69%|██████▉   | 412/595 [00:31<00:11, 16.00it/s] 70%|██████▉   | 414/595 [00:31<00:11, 15.93it/s] 70%|██████▉   | 416/595 [00:31<00:11, 15.96it/s] 70%|███████   | 418/595 [00:31<00:11, 16.03it/s] 71%|███████   | 420/595 [00:31<00:10, 16.09it/s] 71%|███████   | 422/595 [00:31<00:10, 16.11it/s] 71%|███████▏  | 424/595 [00:32<00:10, 16.06it/s] 72%|███████▏  | 426/595 [00:32<00:10, 16.04it/s] 72%|███████▏  | 428/595 [00:32<00:10, 16.02it/s] 72%|███████▏  | 430/595 [00:32<00:10, 16.05it/s] 73%|███████▎  | 432/595 [00:32<00:10, 16.08it/s] 73%|███████▎  | 434/595 [00:32<00:10, 16.10it/s] 73%|███████▎  | 436/595 [00:32<00:09, 16.12it/s] 74%|███████▎  | 438/595 [00:32<00:09, 16.18it/s] 74%|███████▍  | 440/595 [00:33<00:09, 16.20it/s] 74%|███████▍  | 442/595 [00:33<00:09, 16.21it/s] 75%|███████▍  | 444/595 [00:33<00:09, 16.19it/s] 75%|███████▍  | 446/595 [00:33<00:09, 16.17it/s] 75%|███████▌  | 448/595 [00:33<00:09, 16.18it/s] 76%|███████▌  | 450/595 [00:33<00:09, 16.11it/s] 76%|███████▌  | 452/595 [00:33<00:09, 15.66it/s] 76%|███████▋  | 454/595 [00:33<00:08, 15.67it/s] 77%|███████▋  | 456/595 [00:34<00:08, 15.67it/s] 77%|███████▋  | 458/595 [00:34<00:08, 15.71it/s] 77%|███████▋  | 460/595 [00:34<00:08, 15.78it/s] 78%|███████▊  | 462/595 [00:34<00:08, 15.83it/s] 78%|███████▊  | 464/595 [00:34<00:08, 15.85it/s] 78%|███████▊  | 466/595 [00:34<00:08, 15.88it/s] 79%|███████▊  | 468/595 [00:34<00:07, 15.91it/s] 79%|███████▉  | 470/595 [00:34<00:07, 15.93it/s] 79%|███████▉  | 472/595 [00:35<00:07, 15.97it/s] 80%|███████▉  | 474/595 [00:35<00:07, 15.99it/s]                                                  80%|████████  | 476/595 [00:35<00:07, 15.99it/s][INFO|trainer.py:755] 2023-11-15 23:11:15,985 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 23:11:15,987 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 23:11:15,987 >>   Num examples = 945
[INFO|trainer.py:3134] 2023-11-15 23:11:15,987 >>   Batch size = 8
{'eval_loss': 0.3679794669151306, 'eval_accuracy': 0.8582010582010582, 'eval_micro_f1': 0.8582010582010582, 'eval_macro_f1': 0.8058814719675755, 'eval_runtime': 1.4363, 'eval_samples_per_second': 657.926, 'eval_steps_per_second': 82.85, 'epoch': 3.0}
{'loss': 0.3102, 'learning_rate': 0.0001, 'epoch': 4.0}

  0%|          | 0/119 [00:00<?, ?it/s][A
  8%|▊         | 10/119 [00:00<00:01, 94.35it/s][A
 17%|█▋        | 20/119 [00:00<00:01, 88.87it/s][A
 24%|██▍       | 29/119 [00:00<00:01, 87.16it/s][A
 32%|███▏      | 38/119 [00:00<00:00, 86.18it/s][A
 39%|███▉      | 47/119 [00:00<00:00, 85.67it/s][A
 47%|████▋     | 56/119 [00:00<00:00, 85.65it/s][A
 55%|█████▍    | 65/119 [00:00<00:00, 84.64it/s][A
 62%|██████▏   | 74/119 [00:00<00:00, 80.71it/s][A
 70%|██████▉   | 83/119 [00:00<00:00, 79.64it/s][A
 76%|███████▋  | 91/119 [00:01<00:00, 78.13it/s][A
 83%|████████▎ | 99/119 [00:01<00:00, 77.09it/s][A
 91%|█████████ | 108/119 [00:01<00:00, 79.68it/s][A
 98%|█████████▊| 117/119 [00:01<00:00, 81.87it/s][A                                                 
                                                 [A 80%|████████  | 476/595 [00:36<00:07, 15.99it/s]
100%|██████████| 119/119 [00:01<00:00, 81.87it/s][A
                                                 [A 80%|████████  | 477/595 [00:36<00:29,  3.97it/s] 81%|████████  | 479/595 [00:36<00:23,  4.99it/s] 81%|████████  | 481/595 [00:37<00:18,  6.18it/s] 81%|████████  | 483/595 [00:37<00:14,  7.50it/s] 82%|████████▏ | 485/595 [00:37<00:12,  8.90it/s] 82%|████████▏ | 487/595 [00:37<00:10, 10.26it/s] 82%|████████▏ | 489/595 [00:37<00:09, 11.50it/s] 83%|████████▎ | 491/595 [00:37<00:08, 12.56it/s] 83%|████████▎ | 493/595 [00:37<00:07, 13.42it/s] 83%|████████▎ | 495/595 [00:37<00:07, 14.15it/s] 84%|████████▎ | 497/595 [00:38<00:06, 14.74it/s] 84%|████████▍ | 499/595 [00:38<00:06, 15.15it/s] 84%|████████▍ | 501/595 [00:38<00:06, 15.45it/s] 85%|████████▍ | 503/595 [00:38<00:05, 15.65it/s] 85%|████████▍ | 505/595 [00:38<00:05, 15.78it/s] 85%|████████▌ | 507/595 [00:38<00:05, 15.89it/s] 86%|████████▌ | 509/595 [00:38<00:05, 15.91it/s] 86%|████████▌ | 511/595 [00:38<00:05, 15.89it/s] 86%|████████▌ | 513/595 [00:39<00:05, 15.92it/s] 87%|████████▋ | 515/595 [00:39<00:05, 15.90it/s] 87%|████████▋ | 517/595 [00:39<00:04, 15.88it/s] 87%|████████▋ | 519/595 [00:39<00:04, 15.81it/s] 88%|████████▊ | 521/595 [00:39<00:04, 15.84it/s] 88%|████████▊ | 523/595 [00:39<00:04, 15.83it/s] 88%|████████▊ | 525/595 [00:39<00:04, 15.82it/s] 89%|████████▊ | 527/595 [00:39<00:04, 15.88it/s] 89%|████████▉ | 529/595 [00:40<00:04, 15.87it/s] 89%|████████▉ | 531/595 [00:40<00:04, 15.85it/s] 90%|████████▉ | 533/595 [00:40<00:03, 15.89it/s] 90%|████████▉ | 535/595 [00:40<00:03, 15.95it/s] 90%|█████████ | 537/595 [00:40<00:03, 15.96it/s] 91%|█████████ | 539/595 [00:40<00:03, 15.94it/s] 91%|█████████ | 541/595 [00:40<00:03, 15.91it/s] 91%|█████████▏| 543/595 [00:40<00:03, 15.89it/s] 92%|█████████▏| 545/595 [00:41<00:03, 15.90it/s] 92%|█████████▏| 547/595 [00:41<00:03, 15.90it/s] 92%|█████████▏| 549/595 [00:41<00:02, 15.97it/s] 93%|█████████▎| 551/595 [00:41<00:02, 15.99it/s] 93%|█████████▎| 553/595 [00:41<00:02, 15.93it/s] 93%|█████████▎| 555/595 [00:41<00:02, 15.88it/s] 94%|█████████▎| 557/595 [00:41<00:02, 15.84it/s] 94%|█████████▍| 559/595 [00:41<00:02, 15.79it/s] 94%|█████████▍| 561/595 [00:42<00:02, 15.85it/s] 95%|█████████▍| 563/595 [00:42<00:02, 15.96it/s] 95%|█████████▍| 565/595 [00:42<00:01, 16.00it/s] 95%|█████████▌| 567/595 [00:42<00:01, 15.95it/s] 96%|█████████▌| 569/595 [00:42<00:01, 16.00it/s] 96%|█████████▌| 571/595 [00:42<00:01, 16.08it/s] 96%|█████████▋| 573/595 [00:42<00:01, 16.10it/s] 97%|█████████▋| 575/595 [00:42<00:01, 16.09it/s] 97%|█████████▋| 577/595 [00:43<00:01, 16.05it/s] 97%|█████████▋| 579/595 [00:43<00:00, 16.09it/s] 98%|█████████▊| 581/595 [00:43<00:00, 16.12it/s] 98%|█████████▊| 583/595 [00:43<00:00, 16.12it/s] 98%|█████████▊| 585/595 [00:43<00:00, 16.10it/s] 99%|█████████▊| 587/595 [00:43<00:00, 16.08it/s] 99%|█████████▉| 589/595 [00:43<00:00, 16.12it/s] 99%|█████████▉| 591/595 [00:43<00:00, 16.10it/s]100%|█████████▉| 593/595 [00:44<00:00, 16.11it/s]                                                 100%|██████████| 595/595 [00:44<00:00, 16.11it/s][INFO|trainer.py:755] 2023-11-15 23:11:24,865 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 23:11:24,866 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 23:11:24,867 >>   Num examples = 945
[INFO|trainer.py:3134] 2023-11-15 23:11:24,867 >>   Batch size = 8
{'eval_loss': 0.37011778354644775, 'eval_accuracy': 0.8772486772486773, 'eval_micro_f1': 0.8772486772486773, 'eval_macro_f1': 0.8236312172878325, 'eval_runtime': 1.4829, 'eval_samples_per_second': 637.268, 'eval_steps_per_second': 80.249, 'epoch': 4.0}
{'loss': 0.2587, 'learning_rate': 0.0, 'epoch': 5.0}

  0%|          | 0/119 [00:00<?, ?it/s][A
  8%|▊         | 10/119 [00:00<00:01, 96.97it/s][A
 17%|█▋        | 20/119 [00:00<00:01, 89.90it/s][A
 25%|██▌       | 30/119 [00:00<00:01, 87.75it/s][A
 33%|███▎      | 39/119 [00:00<00:00, 85.99it/s][A
 40%|████      | 48/119 [00:00<00:00, 84.60it/s][A
 48%|████▊     | 57/119 [00:00<00:00, 83.41it/s][A
 55%|█████▌    | 66/119 [00:00<00:00, 83.40it/s][A
 63%|██████▎   | 75/119 [00:00<00:00, 83.58it/s][A
 71%|███████   | 84/119 [00:00<00:00, 83.43it/s][A
 78%|███████▊  | 93/119 [00:01<00:00, 81.99it/s][A
 86%|████████▌ | 102/119 [00:01<00:00, 81.34it/s][A
 93%|█████████▎| 111/119 [00:01<00:00, 81.35it/s][A                                                 
                                                 [A100%|██████████| 595/595 [00:45<00:00, 16.11it/s]
100%|██████████| 119/119 [00:01<00:00, 81.35it/s][A
                                                 [A[INFO|trainer.py:1963] 2023-11-15 23:11:26,338 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                 100%|██████████| 595/595 [00:45<00:00, 16.11it/s]100%|██████████| 595/595 [00:45<00:00, 13.03it/s]
[INFO|trainer.py:2855] 2023-11-15 23:11:26,341 >> Saving model checkpoint to ./result/restaurant_roberta-base_seed0_adapter
[INFO|configuration_utils.py:460] 2023-11-15 23:11:26,344 >> Configuration saved in ./result/restaurant_roberta-base_seed0_adapter/config.json
[INFO|modeling_utils.py:1997] 2023-11-15 23:11:27,416 >> Model weights saved in ./result/restaurant_roberta-base_seed0_adapter/pytorch_model.bin
[INFO|tokenization_utils_base.py:2235] 2023-11-15 23:11:27,418 >> tokenizer config file saved in ./result/restaurant_roberta-base_seed0_adapter/tokenizer_config.json
[INFO|tokenization_utils_base.py:2242] 2023-11-15 23:11:27,421 >> Special tokens file saved in ./result/restaurant_roberta-base_seed0_adapter/special_tokens_map.json
{'eval_loss': 0.37290865182876587, 'eval_accuracy': 0.870899470899471, 'eval_micro_f1': 0.870899470899471, 'eval_macro_f1': 0.8202846525711888, 'eval_runtime': 1.467, 'eval_samples_per_second': 644.167, 'eval_steps_per_second': 81.117, 'epoch': 5.0}
{'train_runtime': 45.6689, 'train_samples_per_second': 413.52, 'train_steps_per_second': 13.029, 'train_loss': 0.4303771587980895, 'epoch': 5.0}
***** train metrics *****
  epoch                    =        5.0
  train_loss               =     0.4304
  train_runtime            = 0:00:45.66
  train_samples            =       3777
  train_samples_per_second =     413.52
  train_steps_per_second   =     13.029
11/15/2023 23:11:27 - INFO - __main__ - *** Evaluate ***
[INFO|trainer.py:755] 2023-11-15 23:11:27,554 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 23:11:27,555 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 23:11:27,556 >>   Num examples = 945
[INFO|trainer.py:3134] 2023-11-15 23:11:27,556 >>   Batch size = 8
  0%|          | 0/119 [00:00<?, ?it/s]  8%|▊         | 10/119 [00:00<00:01, 94.62it/s] 17%|█▋        | 20/119 [00:00<00:01, 90.04it/s] 25%|██▌       | 30/119 [00:00<00:01, 88.32it/s] 33%|███▎      | 39/119 [00:00<00:00, 84.05it/s] 40%|████      | 48/119 [00:00<00:00, 81.74it/s] 48%|████▊     | 57/119 [00:00<00:00, 80.65it/s] 55%|█████▌    | 66/119 [00:00<00:00, 79.17it/s] 62%|██████▏   | 74/119 [00:00<00:00, 77.78it/s] 70%|██████▉   | 83/119 [00:01<00:00, 80.51it/s] 77%|███████▋  | 92/119 [00:01<00:00, 82.93it/s] 85%|████████▍ | 101/119 [00:01<00:00, 84.51it/s] 93%|█████████▎| 111/119 [00:01<00:00, 86.25it/s]100%|██████████| 119/119 [00:01<00:00, 82.39it/s]
***** eval metrics *****
  epoch                   =        5.0
  eval_accuracy           =     0.8709
  eval_loss               =     0.3729
  eval_macro_f1           =     0.8203
  eval_micro_f1           =     0.8709
  eval_runtime            = 0:00:01.45
  eval_samples            =        945
  eval_samples_per_second =    647.657
  eval_steps_per_second   =     81.557
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                  eval/accuracy ▁▅▇███
wandb:                      eval/loss █▄▁▁▁▁
wandb:                  eval/macro_f1 ▁▄▇███
wandb:                  eval/micro_f1 ▁▅▇███
wandb:                   eval/runtime ▂█▁▂▂▂
wandb:        eval/samples_per_second ▇▁█▇▇▇
wandb:          eval/steps_per_second ▇▁█▇▇▇
wandb:                    train/epoch ▁▁▃▃▅▅▆▆████
wandb:              train/global_step ▁▁▃▃▅▅▆▆████
wandb:            train/learning_rate █▆▅▃▁
wandb:                     train/loss █▅▃▂▁
wandb:               train/total_flos ▁
wandb:               train/train_loss ▁
wandb:            train/train_runtime ▁
wandb: train/train_samples_per_second ▁
wandb:   train/train_steps_per_second ▁
wandb: 
wandb: Run summary:
wandb:                  eval/accuracy 0.8709
wandb:                      eval/loss 0.37291
wandb:                  eval/macro_f1 0.82028
wandb:                  eval/micro_f1 0.8709
wandb:                   eval/runtime 1.4591
wandb:        eval/samples_per_second 647.657
wandb:          eval/steps_per_second 81.557
wandb:                    train/epoch 5.0
wandb:              train/global_step 595
wandb:            train/learning_rate 0.0
wandb:                     train/loss 0.2587
wandb:               train/total_flos 627599085655680.0
wandb:               train/train_loss 0.43038
wandb:            train/train_runtime 45.6689
wandb: train/train_samples_per_second 413.52
wandb:   train/train_steps_per_second 13.029
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /ceph/home/wangyifei/a3/wandb/offline-run-20231115_230923-01fhs3pm
wandb: Find logs at: ./wandb/offline-run-20231115_230923-01fhs3pm/logs
(ModelArguments(model_name_or_path='roberta-base', cache_dir='./cache', use_adapter=True, use_lora=False), DataTrainingArguments(dataset_name='acl', max_seq_length=64), TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.0005,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=./result/acl_roberta-base_seed0_adapter/runs/Nov15_23-11-39_node013,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.EPOCH,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=5.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
output_dir=./result/acl_roberta-base_seed0_adapter,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
resume_from_checkpoint=None,
run_name=./result/acl_roberta-base_seed0_adapter,
save_on_each_node=False,
save_safetensors=False,
save_steps=500,
save_strategy=IntervalStrategy.NO,
save_total_limit=None,
seed=111,
sharded_ddp=[],
skip_memory_metrics=True,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
))
11/15/2023 23:11:39 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False
11/15/2023 23:11:39 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.0005,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=./result/acl_roberta-base_seed0_adapter/runs/Nov15_23-11-38_node013,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.EPOCH,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=5.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
output_dir=./result/acl_roberta-base_seed0_adapter,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
resume_from_checkpoint=None,
run_name=./result/acl_roberta-base_seed0_adapter,
save_on_each_node=False,
save_safetensors=False,
save_steps=500,
save_strategy=IntervalStrategy.NO,
save_total_limit=None,
seed=111,
sharded_ddp=[],
skip_memory_metrics=True,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
wandb: Tracking run with wandb version 0.15.12
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
Map:   0%|          | 0/11020 [00:00<?, ? examples/s]Map:  36%|███▋      | 4000/11020 [00:00<00:00, 39741.37 examples/s]Map:  75%|███████▍  | 8227/11020 [00:00<00:00, 41221.33 examples/s]Map: 100%|██████████| 11020/11020 [00:00<00:00, 40190.66 examples/s]
[INFO|configuration_utils.py:715] 2023-11-15 23:11:55,395 >> loading configuration file config.json from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/config.json
[INFO|configuration_utils.py:775] 2023-11-15 23:11:55,404 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.33.3",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_auto.py:535] 2023-11-15 23:12:05,420 >> Could not locate the tokenizer configuration file, will try to use the model config instead.
[INFO|configuration_utils.py:715] 2023-11-15 23:12:15,436 >> loading configuration file config.json from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/config.json
[INFO|configuration_utils.py:775] 2023-11-15 23:12:15,436 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.33.3",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:1852] 2023-11-15 23:12:35,483 >> loading file vocab.json from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/vocab.json
[INFO|tokenization_utils_base.py:1852] 2023-11-15 23:12:35,483 >> loading file merges.txt from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/merges.txt
[INFO|tokenization_utils_base.py:1852] 2023-11-15 23:12:35,483 >> loading file tokenizer.json from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/tokenizer.json
[INFO|tokenization_utils_base.py:1852] 2023-11-15 23:12:35,484 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1852] 2023-11-15 23:12:35,484 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1852] 2023-11-15 23:12:35,484 >> loading file tokenizer_config.json from cache at None
[INFO|configuration_utils.py:715] 2023-11-15 23:12:35,485 >> loading configuration file config.json from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/config.json
[INFO|configuration_utils.py:775] 2023-11-15 23:12:35,486 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.33.3",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:2862] 2023-11-15 23:12:55,649 >> loading weights file model.safetensors from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/model.safetensors
[INFO|modeling_utils.py:3638] 2023-11-15 23:12:56,340 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.bias']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3650] 2023-11-15 23:12:56,341 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
parameters 1487427
model_max_length: 512
Running tokenizer on dataset:   0%|          | 0/8816 [00:00<?, ? examples/s]Running tokenizer on dataset:  45%|████▌     | 4000/8816 [00:00<00:00, 20889.66 examples/s]Running tokenizer on dataset:  91%|█████████ | 8000/8816 [00:00<00:00, 21177.21 examples/s]Running tokenizer on dataset: 100%|██████████| 8816/8816 [00:00<00:00, 20899.17 examples/s]
Running tokenizer on dataset:   0%|          | 0/2204 [00:00<?, ? examples/s]Running tokenizer on dataset: 100%|██████████| 2204/2204 [00:00<00:00, 22283.83 examples/s]
11/15/2023 23:12:57 - INFO - __main__ - Sample 3485 of the training set: {'text': 'ERGMs are particularly useful for testing hypotheses about network relations, and they have started to be applied more widely in public health [27].', 'label': 0, 'input_ids': [0, 39042, 13123, 32, 1605, 5616, 13, 3044, 44850, 59, 1546, 3115, 6, 8, 51, 33, 554, 7, 28, 5049, 55, 3924, 11, 285, 474, 646, 2518, 8174, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
11/15/2023 23:12:57 - INFO - __main__ - Sample 5176 of the training set: {'text': 'Consistent with previous results (Koroch et al. 2002; Liu et al. 2002; Staniszewska et al. 2003; Washida et al. 2004), the addition of IBA at optimum levels enhanced the growth of HRC of Echinacea but had no effect on the production of secondary metabolites.', 'label': 2, 'input_ids': [0, 24514, 21464, 19, 986, 775, 36, 530, 368, 4306, 4400, 1076, 4, 5241, 131, 13768, 4400, 1076, 4, 5241, 131, 8995, 354, 329, 10269, 2348, 4400, 1076, 4, 4999, 131, 13852, 4347, 4400, 1076, 4, 4482, 238, 5, 1285, 9, 38, 3813, 23, 33771, 1389, 9094, 5, 434, 9, 43204, 9, 381, 16682, 38937, 53, 56, 117, 1683, 15, 5, 931, 9, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.
11/15/2023 23:12:57 - INFO - __main__ - Sample 8092 of the training set: {'text': 'Syllables with a voiced onset developed a low tone, and those with a voiceless initial induced a high tone, resulting in a six-way tonal contrast (2 pitch heights x 3 contours).1\n(Kang 2014, Kim 2000, Oh 2011, Silva 2006, Wright 2007).', 'label': 0, 'input_ids': [0, 35615, 890, 6058, 19, 10, 12559, 23808, 2226, 10, 614, 6328, 6, 8, 167, 19, 10, 30118, 13802, 2557, 26914, 10, 239, 6328, 6, 5203, 11, 10, 411, 12, 1970, 4866, 337, 5709, 36, 176, 3242, 16889, 3023, 155, 8541, 5634, 322, 134, 50118, 1640, 530, 1097, 777, 6, 1636, 3788, 6, 5534, 1466, 6, 9392, 3503, 6, 5825, 3010, 322, 2, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]}.
11/15/2023 23:12:57 - WARNING - accelerate.utils.other - Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[INFO|trainer.py:755] 2023-11-15 23:12:58,812 >> The following columns in the training set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:1715] 2023-11-15 23:12:58,823 >> ***** Running training *****
[INFO|trainer.py:1716] 2023-11-15 23:12:58,823 >>   Num examples = 8,816
[INFO|trainer.py:1717] 2023-11-15 23:12:58,823 >>   Num Epochs = 5
[INFO|trainer.py:1718] 2023-11-15 23:12:58,824 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1721] 2023-11-15 23:12:58,824 >>   Total train batch size (w. parallel, distributed & accumulation) = 32
[INFO|trainer.py:1722] 2023-11-15 23:12:58,824 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:1723] 2023-11-15 23:12:58,825 >>   Total optimization steps = 1,380
[INFO|trainer.py:1724] 2023-11-15 23:12:58,826 >>   Number of trainable parameters = 1,487,427
[INFO|integration_utils.py:716] 2023-11-15 23:12:58,827 >> Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"
  0%|          | 0/1380 [00:00<?, ?it/s]  0%|          | 1/1380 [00:01<24:52,  1.08s/it]  0%|          | 3/1380 [00:01<07:31,  3.05it/s]  0%|          | 5/1380 [00:01<04:22,  5.23it/s]  1%|          | 7/1380 [00:01<03:07,  7.33it/s]  1%|          | 9/1380 [00:01<02:28,  9.21it/s]  1%|          | 11/1380 [00:01<02:06, 10.81it/s]  1%|          | 13/1380 [00:01<01:52, 12.17it/s]  1%|          | 15/1380 [00:01<01:43, 13.21it/s]  1%|          | 17/1380 [00:02<01:37, 14.00it/s]  1%|▏         | 19/1380 [00:02<01:33, 14.60it/s]  2%|▏         | 21/1380 [00:02<01:30, 15.04it/s]  2%|▏         | 23/1380 [00:02<01:30, 15.02it/s]  2%|▏         | 25/1380 [00:02<01:28, 15.31it/s]  2%|▏         | 27/1380 [00:02<01:26, 15.57it/s]  2%|▏         | 29/1380 [00:02<01:25, 15.74it/s]  2%|▏         | 31/1380 [00:02<01:25, 15.85it/s]  2%|▏         | 33/1380 [00:03<01:24, 15.92it/s]  3%|▎         | 35/1380 [00:03<01:24, 15.97it/s]  3%|▎         | 37/1380 [00:03<01:23, 16.04it/s]  3%|▎         | 39/1380 [00:03<01:23, 16.02it/s]  3%|▎         | 41/1380 [00:03<01:23, 16.05it/s]  3%|▎         | 43/1380 [00:03<01:23, 16.05it/s]  3%|▎         | 45/1380 [00:03<01:23, 15.98it/s]  3%|▎         | 47/1380 [00:03<01:23, 15.93it/s]  4%|▎         | 49/1380 [00:04<01:24, 15.84it/s]  4%|▎         | 51/1380 [00:04<01:24, 15.82it/s]  4%|▍         | 53/1380 [00:04<01:23, 15.81it/s]  4%|▍         | 55/1380 [00:04<01:23, 15.79it/s]  4%|▍         | 57/1380 [00:04<01:23, 15.80it/s]  4%|▍         | 59/1380 [00:04<01:23, 15.78it/s]  4%|▍         | 61/1380 [00:04<01:23, 15.79it/s]  5%|▍         | 63/1380 [00:04<01:23, 15.87it/s]  5%|▍         | 65/1380 [00:05<01:22, 15.90it/s]  5%|▍         | 67/1380 [00:05<01:22, 15.91it/s]  5%|▌         | 69/1380 [00:05<01:22, 15.87it/s]  5%|▌         | 71/1380 [00:05<01:22, 15.87it/s]  5%|▌         | 73/1380 [00:05<01:22, 15.89it/s]  5%|▌         | 75/1380 [00:05<01:21, 15.92it/s]  6%|▌         | 77/1380 [00:05<01:21, 15.93it/s]  6%|▌         | 79/1380 [00:05<01:21, 15.94it/s]  6%|▌         | 81/1380 [00:06<01:21, 15.89it/s]  6%|▌         | 83/1380 [00:06<01:21, 15.85it/s]  6%|▌         | 85/1380 [00:06<01:21, 15.81it/s]  6%|▋         | 87/1380 [00:06<01:22, 15.73it/s]  6%|▋         | 89/1380 [00:06<01:21, 15.76it/s]  7%|▋         | 91/1380 [00:06<01:22, 15.68it/s]  7%|▋         | 93/1380 [00:06<01:21, 15.71it/s]  7%|▋         | 95/1380 [00:06<01:20, 15.86it/s]  7%|▋         | 97/1380 [00:07<01:20, 15.96it/s]  7%|▋         | 99/1380 [00:07<01:19, 16.03it/s]  7%|▋         | 101/1380 [00:07<01:19, 16.03it/s]  7%|▋         | 103/1380 [00:07<01:19, 16.05it/s]  8%|▊         | 105/1380 [00:07<01:19, 16.10it/s]  8%|▊         | 107/1380 [00:07<01:19, 16.11it/s]  8%|▊         | 109/1380 [00:07<01:19, 16.06it/s]  8%|▊         | 111/1380 [00:07<01:19, 16.02it/s]  8%|▊         | 113/1380 [00:08<01:19, 16.01it/s]  8%|▊         | 115/1380 [00:08<01:18, 16.05it/s]  8%|▊         | 117/1380 [00:08<01:18, 16.04it/s]  9%|▊         | 119/1380 [00:08<01:18, 16.05it/s]  9%|▉         | 121/1380 [00:08<01:17, 16.15it/s]  9%|▉         | 123/1380 [00:08<01:17, 16.18it/s]  9%|▉         | 125/1380 [00:08<01:17, 16.18it/s]  9%|▉         | 127/1380 [00:08<01:17, 16.19it/s]  9%|▉         | 129/1380 [00:09<01:17, 16.18it/s]  9%|▉         | 131/1380 [00:09<01:17, 16.16it/s] 10%|▉         | 133/1380 [00:09<01:17, 16.11it/s] 10%|▉         | 135/1380 [00:09<01:17, 16.05it/s] 10%|▉         | 137/1380 [00:09<01:17, 15.99it/s] 10%|█         | 139/1380 [00:09<01:17, 15.95it/s] 10%|█         | 141/1380 [00:09<01:17, 15.94it/s] 10%|█         | 143/1380 [00:09<01:18, 15.84it/s] 11%|█         | 145/1380 [00:10<01:17, 15.84it/s] 11%|█         | 147/1380 [00:10<01:17, 15.87it/s] 11%|█         | 149/1380 [00:10<01:17, 15.91it/s] 11%|█         | 151/1380 [00:10<01:17, 15.94it/s] 11%|█         | 153/1380 [00:10<01:16, 15.97it/s] 11%|█         | 155/1380 [00:10<01:16, 16.02it/s] 11%|█▏        | 157/1380 [00:10<01:16, 16.04it/s] 12%|█▏        | 159/1380 [00:10<01:16, 15.96it/s] 12%|█▏        | 161/1380 [00:11<01:16, 15.97it/s] 12%|█▏        | 163/1380 [00:11<01:16, 16.01it/s] 12%|█▏        | 165/1380 [00:11<01:16, 15.95it/s] 12%|█▏        | 167/1380 [00:11<01:16, 15.92it/s] 12%|█▏        | 169/1380 [00:11<01:15, 15.96it/s] 12%|█▏        | 171/1380 [00:11<01:15, 15.96it/s] 13%|█▎        | 173/1380 [00:11<01:16, 15.76it/s] 13%|█▎        | 175/1380 [00:11<01:16, 15.77it/s] 13%|█▎        | 177/1380 [00:12<01:16, 15.73it/s] 13%|█▎        | 179/1380 [00:12<01:16, 15.71it/s] 13%|█▎        | 181/1380 [00:12<01:16, 15.76it/s] 13%|█▎        | 183/1380 [00:12<01:15, 15.83it/s] 13%|█▎        | 185/1380 [00:12<01:15, 15.90it/s] 14%|█▎        | 187/1380 [00:12<01:14, 15.98it/s] 14%|█▎        | 189/1380 [00:12<01:14, 16.06it/s] 14%|█▍        | 191/1380 [00:12<01:13, 16.11it/s] 14%|█▍        | 193/1380 [00:13<01:13, 16.13it/s] 14%|█▍        | 195/1380 [00:13<01:14, 15.93it/s] 14%|█▍        | 197/1380 [00:13<01:15, 15.64it/s] 14%|█▍        | 199/1380 [00:13<01:17, 15.16it/s] 15%|█▍        | 201/1380 [00:13<01:19, 14.90it/s] 15%|█▍        | 203/1380 [00:13<01:18, 15.02it/s] 15%|█▍        | 205/1380 [00:13<01:17, 15.15it/s] 15%|█▌        | 207/1380 [00:14<01:16, 15.25it/s] 15%|█▌        | 209/1380 [00:14<01:16, 15.32it/s] 15%|█▌        | 211/1380 [00:14<01:15, 15.43it/s] 15%|█▌        | 213/1380 [00:14<01:14, 15.59it/s] 16%|█▌        | 215/1380 [00:14<01:14, 15.67it/s] 16%|█▌        | 217/1380 [00:14<01:13, 15.75it/s] 16%|█▌        | 219/1380 [00:14<01:13, 15.80it/s] 16%|█▌        | 221/1380 [00:14<01:12, 15.88it/s] 16%|█▌        | 223/1380 [00:15<01:12, 15.88it/s] 16%|█▋        | 225/1380 [00:15<01:13, 15.81it/s] 16%|█▋        | 227/1380 [00:15<01:13, 15.79it/s] 17%|█▋        | 229/1380 [00:15<01:13, 15.76it/s] 17%|█▋        | 231/1380 [00:15<01:12, 15.75it/s] 17%|█▋        | 233/1380 [00:15<01:12, 15.80it/s] 17%|█▋        | 235/1380 [00:15<01:14, 15.46it/s] 17%|█▋        | 237/1380 [00:15<01:13, 15.46it/s] 17%|█▋        | 239/1380 [00:16<01:13, 15.60it/s] 17%|█▋        | 241/1380 [00:16<01:12, 15.75it/s] 18%|█▊        | 243/1380 [00:16<01:11, 15.81it/s] 18%|█▊        | 245/1380 [00:16<01:11, 15.84it/s] 18%|█▊        | 247/1380 [00:16<01:11, 15.87it/s] 18%|█▊        | 249/1380 [00:16<01:11, 15.89it/s] 18%|█▊        | 251/1380 [00:16<01:11, 15.89it/s] 18%|█▊        | 253/1380 [00:16<01:11, 15.86it/s] 18%|█▊        | 255/1380 [00:17<01:11, 15.82it/s] 19%|█▊        | 257/1380 [00:17<01:11, 15.70it/s] 19%|█▉        | 259/1380 [00:17<01:11, 15.75it/s] 19%|█▉        | 261/1380 [00:17<01:11, 15.70it/s] 19%|█▉        | 263/1380 [00:17<01:11, 15.68it/s] 19%|█▉        | 265/1380 [00:17<01:11, 15.49it/s] 19%|█▉        | 267/1380 [00:17<01:12, 15.35it/s] 19%|█▉        | 269/1380 [00:17<01:12, 15.31it/s] 20%|█▉        | 271/1380 [00:18<01:12, 15.28it/s] 20%|█▉        | 273/1380 [00:18<01:11, 15.48it/s] 20%|█▉        | 275/1380 [00:18<01:10, 15.62it/s]                                                   20%|██        | 276/1380 [00:18<01:10, 15.62it/s][INFO|trainer.py:755] 2023-11-15 23:13:17,248 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 23:13:17,249 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 23:13:17,250 >>   Num examples = 2204
[INFO|trainer.py:3134] 2023-11-15 23:13:17,250 >>   Batch size = 8
{'loss': 0.5268, 'learning_rate': 0.0004, 'epoch': 1.0}

  0%|          | 0/276 [00:00<?, ?it/s][A
  4%|▎         | 10/276 [00:00<00:02, 99.68it/s][A
  7%|▋         | 20/276 [00:00<00:02, 92.98it/s][A
 11%|█         | 30/276 [00:00<00:02, 90.52it/s][A
 14%|█▍        | 40/276 [00:00<00:02, 90.00it/s][A
 18%|█▊        | 50/276 [00:00<00:02, 89.53it/s][A
 21%|██▏       | 59/276 [00:00<00:02, 88.56it/s][A
 25%|██▍       | 68/276 [00:00<00:02, 88.38it/s][A
 28%|██▊       | 77/276 [00:00<00:02, 88.34it/s][A
 31%|███       | 86/276 [00:00<00:02, 87.69it/s][A
 34%|███▍      | 95/276 [00:01<00:02, 87.58it/s][A
 38%|███▊      | 104/276 [00:01<00:01, 87.21it/s][A
 41%|████      | 113/276 [00:01<00:01, 87.30it/s][A
 44%|████▍     | 122/276 [00:01<00:01, 87.69it/s][A
 47%|████▋     | 131/276 [00:01<00:01, 87.54it/s][A
 51%|█████     | 140/276 [00:01<00:01, 88.02it/s][A
 54%|█████▍    | 149/276 [00:01<00:01, 87.98it/s][A
 57%|█████▋    | 158/276 [00:01<00:01, 87.36it/s][A
 61%|██████    | 167/276 [00:01<00:01, 87.85it/s][A
 64%|██████▍   | 176/276 [00:01<00:01, 87.85it/s][A
 67%|██████▋   | 185/276 [00:02<00:01, 87.53it/s][A
 70%|███████   | 194/276 [00:02<00:00, 87.43it/s][A
 74%|███████▎  | 203/276 [00:02<00:00, 86.63it/s][A
 77%|███████▋  | 212/276 [00:02<00:00, 86.12it/s][A
 80%|████████  | 221/276 [00:02<00:00, 86.34it/s][A
 83%|████████▎ | 230/276 [00:02<00:00, 85.33it/s][A
 87%|████████▋ | 239/276 [00:02<00:00, 84.18it/s][A
 90%|████████▉ | 248/276 [00:02<00:00, 83.49it/s][A
 93%|█████████▎| 257/276 [00:02<00:00, 83.54it/s][A
 96%|█████████▋| 266/276 [00:03<00:00, 84.10it/s][A
100%|█████████▉| 275/276 [00:03<00:00, 84.43it/s][A                                                  
                                                 [A 20%|██        | 276/1380 [00:21<01:10, 15.62it/s]
100%|██████████| 276/276 [00:03<00:00, 84.43it/s][A
                                                 [A 20%|██        | 277/1380 [00:21<10:01,  1.83it/s] 20%|██        | 279/1380 [00:21<07:21,  2.50it/s] 20%|██        | 281/1380 [00:21<05:28,  3.34it/s] 21%|██        | 283/1380 [00:22<04:10,  4.38it/s] 21%|██        | 285/1380 [00:22<03:15,  5.60it/s] 21%|██        | 287/1380 [00:22<02:37,  6.94it/s] 21%|██        | 289/1380 [00:22<02:10,  8.36it/s] 21%|██        | 291/1380 [00:22<01:51,  9.75it/s] 21%|██        | 293/1380 [00:22<01:38, 10.98it/s] 21%|██▏       | 295/1380 [00:22<01:29, 12.08it/s] 22%|██▏       | 297/1380 [00:22<01:23, 12.95it/s] 22%|██▏       | 299/1380 [00:23<01:18, 13.70it/s] 22%|██▏       | 301/1380 [00:23<01:16, 14.11it/s] 22%|██▏       | 303/1380 [00:23<01:15, 14.32it/s] 22%|██▏       | 305/1380 [00:23<01:13, 14.61it/s] 22%|██▏       | 307/1380 [00:23<01:11, 14.93it/s] 22%|██▏       | 309/1380 [00:23<01:10, 15.11it/s] 23%|██▎       | 311/1380 [00:23<01:09, 15.35it/s] 23%|██▎       | 313/1380 [00:24<01:08, 15.58it/s] 23%|██▎       | 315/1380 [00:24<01:07, 15.72it/s] 23%|██▎       | 317/1380 [00:24<01:07, 15.81it/s] 23%|██▎       | 319/1380 [00:24<01:07, 15.69it/s] 23%|██▎       | 321/1380 [00:24<01:08, 15.47it/s] 23%|██▎       | 323/1380 [00:24<01:09, 15.27it/s] 24%|██▎       | 325/1380 [00:24<01:10, 14.99it/s] 24%|██▎       | 327/1380 [00:24<01:10, 15.03it/s] 24%|██▍       | 329/1380 [00:25<01:09, 15.10it/s] 24%|██▍       | 331/1380 [00:25<01:09, 15.18it/s] 24%|██▍       | 333/1380 [00:25<01:08, 15.22it/s] 24%|██▍       | 335/1380 [00:25<01:08, 15.30it/s] 24%|██▍       | 337/1380 [00:25<01:07, 15.39it/s] 25%|██▍       | 339/1380 [00:25<01:07, 15.44it/s] 25%|██▍       | 341/1380 [00:25<01:06, 15.52it/s] 25%|██▍       | 343/1380 [00:25<01:06, 15.55it/s] 25%|██▌       | 345/1380 [00:26<01:05, 15.69it/s] 25%|██▌       | 347/1380 [00:26<01:05, 15.78it/s] 25%|██▌       | 349/1380 [00:26<01:05, 15.83it/s] 25%|██▌       | 351/1380 [00:26<01:04, 15.90it/s] 26%|██▌       | 353/1380 [00:26<01:04, 15.94it/s] 26%|██▌       | 355/1380 [00:26<01:04, 15.88it/s] 26%|██▌       | 357/1380 [00:26<01:04, 15.83it/s] 26%|██▌       | 359/1380 [00:26<01:04, 15.83it/s] 26%|██▌       | 361/1380 [00:27<01:04, 15.74it/s] 26%|██▋       | 363/1380 [00:27<01:04, 15.70it/s] 26%|██▋       | 365/1380 [00:27<01:04, 15.72it/s] 27%|██▋       | 367/1380 [00:27<01:04, 15.72it/s] 27%|██▋       | 369/1380 [00:27<01:04, 15.72it/s] 27%|██▋       | 371/1380 [00:27<01:03, 15.79it/s] 27%|██▋       | 373/1380 [00:27<01:03, 15.82it/s] 27%|██▋       | 375/1380 [00:27<01:03, 15.84it/s] 27%|██▋       | 377/1380 [00:28<01:03, 15.90it/s] 27%|██▋       | 379/1380 [00:28<01:03, 15.73it/s] 28%|██▊       | 381/1380 [00:28<01:03, 15.79it/s] 28%|██▊       | 383/1380 [00:28<01:02, 15.88it/s] 28%|██▊       | 385/1380 [00:28<01:02, 15.90it/s] 28%|██▊       | 387/1380 [00:28<01:02, 15.88it/s] 28%|██▊       | 389/1380 [00:28<01:02, 15.92it/s] 28%|██▊       | 391/1380 [00:28<01:02, 15.87it/s] 28%|██▊       | 393/1380 [00:29<01:02, 15.85it/s] 29%|██▊       | 395/1380 [00:29<01:02, 15.79it/s] 29%|██▉       | 397/1380 [00:29<01:02, 15.79it/s] 29%|██▉       | 399/1380 [00:29<01:02, 15.73it/s] 29%|██▉       | 401/1380 [00:29<01:02, 15.73it/s] 29%|██▉       | 403/1380 [00:29<01:01, 15.82it/s] 29%|██▉       | 405/1380 [00:29<01:01, 15.87it/s] 29%|██▉       | 407/1380 [00:29<01:01, 15.93it/s] 30%|██▉       | 409/1380 [00:30<01:00, 15.99it/s] 30%|██▉       | 411/1380 [00:30<01:00, 16.03it/s] 30%|██▉       | 413/1380 [00:30<01:00, 16.00it/s] 30%|███       | 415/1380 [00:30<01:00, 16.00it/s] 30%|███       | 417/1380 [00:30<01:00, 16.03it/s] 30%|███       | 419/1380 [00:30<00:59, 16.06it/s] 31%|███       | 421/1380 [00:30<00:59, 16.06it/s] 31%|███       | 423/1380 [00:30<00:59, 16.04it/s] 31%|███       | 425/1380 [00:31<00:59, 16.07it/s] 31%|███       | 427/1380 [00:31<00:59, 16.09it/s] 31%|███       | 429/1380 [00:31<00:59, 16.08it/s] 31%|███       | 431/1380 [00:31<00:59, 16.01it/s] 31%|███▏      | 433/1380 [00:31<00:59, 16.02it/s] 32%|███▏      | 435/1380 [00:31<00:58, 16.03it/s] 32%|███▏      | 437/1380 [00:31<00:58, 16.01it/s] 32%|███▏      | 439/1380 [00:31<00:58, 16.01it/s] 32%|███▏      | 441/1380 [00:32<00:58, 15.99it/s] 32%|███▏      | 443/1380 [00:32<00:58, 15.92it/s] 32%|███▏      | 445/1380 [00:32<00:58, 15.85it/s] 32%|███▏      | 447/1380 [00:32<00:59, 15.81it/s] 33%|███▎      | 449/1380 [00:32<00:59, 15.72it/s] 33%|███▎      | 451/1380 [00:32<01:00, 15.42it/s] 33%|███▎      | 453/1380 [00:32<00:59, 15.49it/s] 33%|███▎      | 455/1380 [00:33<00:59, 15.53it/s] 33%|███▎      | 457/1380 [00:33<00:59, 15.63it/s] 33%|███▎      | 459/1380 [00:33<00:58, 15.68it/s] 33%|███▎      | 461/1380 [00:33<00:58, 15.74it/s] 34%|███▎      | 463/1380 [00:33<00:58, 15.81it/s] 34%|███▎      | 465/1380 [00:33<00:57, 15.86it/s] 34%|███▍      | 467/1380 [00:33<00:57, 15.81it/s] 34%|███▍      | 469/1380 [00:33<00:57, 15.81it/s] 34%|███▍      | 471/1380 [00:34<00:57, 15.83it/s] 34%|███▍      | 473/1380 [00:34<00:57, 15.82it/s] 34%|███▍      | 475/1380 [00:34<00:57, 15.87it/s] 35%|███▍      | 477/1380 [00:34<00:56, 15.90it/s] 35%|███▍      | 479/1380 [00:34<00:56, 15.81it/s] 35%|███▍      | 481/1380 [00:34<00:57, 15.77it/s] 35%|███▌      | 483/1380 [00:34<00:57, 15.71it/s] 35%|███▌      | 485/1380 [00:34<00:57, 15.70it/s] 35%|███▌      | 487/1380 [00:35<00:56, 15.73it/s] 35%|███▌      | 489/1380 [00:35<00:56, 15.72it/s] 36%|███▌      | 491/1380 [00:35<00:56, 15.71it/s] 36%|███▌      | 493/1380 [00:35<00:56, 15.80it/s] 36%|███▌      | 495/1380 [00:35<00:55, 15.87it/s] 36%|███▌      | 497/1380 [00:35<00:55, 15.91it/s] 36%|███▌      | 499/1380 [00:35<00:55, 15.95it/s] 36%|███▋      | 501/1380 [00:35<00:55, 15.96it/s] 36%|███▋      | 503/1380 [00:36<00:54, 15.99it/s] 37%|███▋      | 505/1380 [00:36<00:54, 15.97it/s] 37%|███▋      | 507/1380 [00:36<00:54, 15.98it/s] 37%|███▋      | 509/1380 [00:36<00:54, 16.01it/s] 37%|███▋      | 511/1380 [00:36<00:54, 16.00it/s] 37%|███▋      | 513/1380 [00:36<00:54, 15.99it/s] 37%|███▋      | 515/1380 [00:36<00:54, 16.02it/s] 37%|███▋      | 517/1380 [00:36<00:53, 16.05it/s] 38%|███▊      | 519/1380 [00:37<00:53, 16.05it/s] 38%|███▊      | 521/1380 [00:37<00:53, 16.02it/s] 38%|███▊      | 523/1380 [00:37<00:53, 16.02it/s] 38%|███▊      | 525/1380 [00:37<00:53, 16.02it/s] 38%|███▊      | 527/1380 [00:37<00:53, 15.93it/s] 38%|███▊      | 529/1380 [00:37<00:53, 15.89it/s] 38%|███▊      | 531/1380 [00:37<00:53, 15.88it/s] 39%|███▊      | 533/1380 [00:37<00:53, 15.75it/s] 39%|███▉      | 535/1380 [00:38<00:53, 15.72it/s] 39%|███▉      | 537/1380 [00:38<00:53, 15.68it/s] 39%|███▉      | 539/1380 [00:38<00:53, 15.65it/s] 39%|███▉      | 541/1380 [00:38<00:53, 15.72it/s] 39%|███▉      | 543/1380 [00:38<00:53, 15.69it/s] 39%|███▉      | 545/1380 [00:38<00:53, 15.69it/s] 40%|███▉      | 547/1380 [00:38<00:52, 15.72it/s] 40%|███▉      | 549/1380 [00:38<00:52, 15.78it/s] 40%|███▉      | 551/1380 [00:39<00:52, 15.83it/s]                                                   40%|████      | 552/1380 [00:39<00:52, 15.83it/s][INFO|trainer.py:755] 2023-11-15 23:13:37,942 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 23:13:37,944 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 23:13:37,944 >>   Num examples = 2204
[INFO|trainer.py:3134] 2023-11-15 23:13:37,944 >>   Batch size = 8
{'eval_loss': 0.5106453895568848, 'eval_accuracy': 0.8189655172413793, 'eval_micro_f1': 0.8189655172413793, 'eval_macro_f1': 0.8003893548610369, 'eval_runtime': 3.2287, 'eval_samples_per_second': 682.633, 'eval_steps_per_second': 85.484, 'epoch': 1.0}
{'loss': 0.3806, 'learning_rate': 0.0003, 'epoch': 2.0}

  0%|          | 0/276 [00:00<?, ?it/s][A
  4%|▎         | 10/276 [00:00<00:02, 95.93it/s][A
  7%|▋         | 20/276 [00:00<00:02, 90.28it/s][A
 11%|█         | 30/276 [00:00<00:02, 88.19it/s][A
 14%|█▍        | 39/276 [00:00<00:02, 87.00it/s][A
 17%|█▋        | 48/276 [00:00<00:02, 86.52it/s][A
 21%|██        | 57/276 [00:00<00:02, 85.94it/s][A
 24%|██▍       | 66/276 [00:00<00:02, 85.40it/s][A
 27%|██▋       | 75/276 [00:00<00:02, 84.88it/s][A
 30%|███       | 84/276 [00:00<00:02, 84.88it/s][A
 34%|███▎      | 93/276 [00:01<00:02, 85.28it/s][A
 37%|███▋      | 102/276 [00:01<00:02, 85.20it/s][A
 40%|████      | 111/276 [00:01<00:02, 81.95it/s][A
 43%|████▎     | 120/276 [00:01<00:01, 80.68it/s][A
 47%|████▋     | 129/276 [00:01<00:01, 78.71it/s][A
 50%|████▉     | 137/276 [00:01<00:01, 77.51it/s][A
 53%|█████▎    | 146/276 [00:01<00:01, 78.86it/s][A
 56%|█████▌    | 155/276 [00:01<00:01, 81.15it/s][A
 59%|█████▉    | 164/276 [00:01<00:01, 82.49it/s][A
 63%|██████▎   | 173/276 [00:02<00:01, 83.95it/s][A
 66%|██████▌   | 182/276 [00:02<00:01, 84.68it/s][A
 69%|██████▉   | 191/276 [00:02<00:00, 85.66it/s][A
 72%|███████▏  | 200/276 [00:02<00:00, 86.43it/s][A
 76%|███████▌  | 209/276 [00:02<00:00, 86.53it/s][A
 79%|███████▉  | 218/276 [00:02<00:00, 86.21it/s][A
 82%|████████▏ | 227/276 [00:02<00:00, 86.59it/s][A
 86%|████████▌ | 236/276 [00:02<00:00, 86.93it/s][A
 89%|████████▉ | 245/276 [00:02<00:00, 86.79it/s][A
 92%|█████████▏| 254/276 [00:03<00:00, 86.08it/s][A
 95%|█████████▌| 263/276 [00:03<00:00, 86.12it/s][A
 99%|█████████▊| 272/276 [00:03<00:00, 86.26it/s][A                                                  
                                                 [A 40%|████      | 552/1380 [00:42<00:52, 15.83it/s]
100%|██████████| 276/276 [00:03<00:00, 86.26it/s][A
                                                 [A 40%|████      | 553/1380 [00:42<07:41,  1.79it/s] 40%|████      | 555/1380 [00:42<05:37,  2.44it/s] 40%|████      | 557/1380 [00:42<04:11,  3.28it/s] 41%|████      | 559/1380 [00:42<03:10,  4.31it/s] 41%|████      | 561/1380 [00:42<02:28,  5.52it/s] 41%|████      | 563/1380 [00:43<01:58,  6.87it/s] 41%|████      | 565/1380 [00:43<01:38,  8.29it/s] 41%|████      | 567/1380 [00:43<01:24,  9.65it/s] 41%|████      | 569/1380 [00:43<01:14, 10.91it/s] 41%|████▏     | 571/1380 [00:43<01:07, 11.97it/s] 42%|████▏     | 573/1380 [00:43<01:02, 12.84it/s] 42%|████▏     | 575/1380 [00:43<00:59, 13.57it/s] 42%|████▏     | 577/1380 [00:44<00:56, 14.15it/s] 42%|████▏     | 579/1380 [00:44<00:54, 14.58it/s] 42%|████▏     | 581/1380 [00:44<00:53, 14.87it/s] 42%|████▏     | 583/1380 [00:44<00:52, 15.15it/s] 42%|████▏     | 585/1380 [00:44<00:51, 15.38it/s] 43%|████▎     | 587/1380 [00:44<00:51, 15.50it/s] 43%|████▎     | 589/1380 [00:44<00:50, 15.59it/s] 43%|████▎     | 591/1380 [00:44<00:50, 15.71it/s] 43%|████▎     | 593/1380 [00:45<00:50, 15.71it/s] 43%|████▎     | 595/1380 [00:45<00:49, 15.71it/s] 43%|████▎     | 597/1380 [00:45<00:49, 15.74it/s] 43%|████▎     | 599/1380 [00:45<00:49, 15.79it/s] 44%|████▎     | 601/1380 [00:45<00:49, 15.81it/s] 44%|████▎     | 603/1380 [00:45<00:48, 15.88it/s] 44%|████▍     | 605/1380 [00:45<00:49, 15.72it/s] 44%|████▍     | 607/1380 [00:45<00:49, 15.56it/s] 44%|████▍     | 609/1380 [00:46<00:50, 15.27it/s] 44%|████▍     | 611/1380 [00:46<00:51, 15.05it/s] 44%|████▍     | 613/1380 [00:46<00:51, 15.03it/s] 45%|████▍     | 615/1380 [00:46<00:50, 15.01it/s] 45%|████▍     | 617/1380 [00:46<00:51, 14.96it/s] 45%|████▍     | 619/1380 [00:46<00:50, 15.00it/s] 45%|████▌     | 621/1380 [00:46<00:50, 15.10it/s] 45%|████▌     | 623/1380 [00:46<00:49, 15.18it/s] 45%|████▌     | 625/1380 [00:47<00:49, 15.26it/s] 45%|████▌     | 627/1380 [00:47<00:48, 15.37it/s] 46%|████▌     | 629/1380 [00:47<00:48, 15.40it/s] 46%|████▌     | 631/1380 [00:47<00:48, 15.46it/s] 46%|████▌     | 633/1380 [00:47<00:47, 15.57it/s] 46%|████▌     | 635/1380 [00:47<00:47, 15.67it/s] 46%|████▌     | 637/1380 [00:47<00:47, 15.77it/s] 46%|████▋     | 639/1380 [00:48<00:46, 15.82it/s] 46%|████▋     | 641/1380 [00:48<00:46, 15.86it/s] 47%|████▋     | 643/1380 [00:48<00:46, 15.93it/s] 47%|████▋     | 645/1380 [00:48<00:46, 15.97it/s] 47%|████▋     | 647/1380 [00:48<00:45, 15.99it/s] 47%|████▋     | 649/1380 [00:48<00:45, 15.96it/s] 47%|████▋     | 651/1380 [00:48<00:45, 15.98it/s] 47%|████▋     | 653/1380 [00:48<00:45, 15.99it/s] 47%|████▋     | 655/1380 [00:49<00:45, 15.93it/s] 48%|████▊     | 657/1380 [00:49<00:45, 15.89it/s] 48%|████▊     | 659/1380 [00:49<00:45, 15.81it/s] 48%|████▊     | 661/1380 [00:49<00:45, 15.71it/s] 48%|████▊     | 663/1380 [00:49<00:45, 15.70it/s] 48%|████▊     | 665/1380 [00:49<00:45, 15.66it/s] 48%|████▊     | 667/1380 [00:49<00:45, 15.62it/s] 48%|████▊     | 669/1380 [00:49<00:45, 15.62it/s] 49%|████▊     | 671/1380 [00:50<00:45, 15.63it/s] 49%|████▉     | 673/1380 [00:50<00:45, 15.67it/s] 49%|████▉     | 675/1380 [00:50<00:44, 15.68it/s] 49%|████▉     | 677/1380 [00:50<00:44, 15.63it/s] 49%|████▉     | 679/1380 [00:50<00:44, 15.62it/s] 49%|████▉     | 681/1380 [00:50<00:44, 15.54it/s] 49%|████▉     | 683/1380 [00:50<00:44, 15.57it/s] 50%|████▉     | 685/1380 [00:50<00:44, 15.55it/s] 50%|████▉     | 687/1380 [00:51<00:44, 15.59it/s] 50%|████▉     | 689/1380 [00:51<00:44, 15.64it/s] 50%|█████     | 691/1380 [00:51<00:43, 15.68it/s] 50%|█████     | 693/1380 [00:51<00:43, 15.69it/s] 50%|█████     | 695/1380 [00:51<00:43, 15.67it/s] 51%|█████     | 697/1380 [00:51<00:43, 15.63it/s] 51%|█████     | 699/1380 [00:51<00:44, 15.41it/s] 51%|█████     | 701/1380 [00:51<00:43, 15.44it/s] 51%|█████     | 703/1380 [00:52<00:43, 15.47it/s] 51%|█████     | 705/1380 [00:52<00:43, 15.37it/s] 51%|█████     | 707/1380 [00:52<00:43, 15.54it/s] 51%|█████▏    | 709/1380 [00:52<00:42, 15.65it/s] 52%|█████▏    | 711/1380 [00:52<00:42, 15.71it/s] 52%|█████▏    | 713/1380 [00:52<00:43, 15.42it/s] 52%|█████▏    | 715/1380 [00:52<00:43, 15.25it/s] 52%|█████▏    | 717/1380 [00:52<00:43, 15.23it/s] 52%|█████▏    | 719/1380 [00:53<00:43, 15.22it/s] 52%|█████▏    | 721/1380 [00:53<00:43, 15.23it/s] 52%|█████▏    | 723/1380 [00:53<00:43, 15.18it/s] 53%|█████▎    | 725/1380 [00:53<00:43, 15.21it/s] 53%|█████▎    | 727/1380 [00:53<00:42, 15.23it/s] 53%|█████▎    | 729/1380 [00:53<00:42, 15.24it/s] 53%|█████▎    | 731/1380 [00:53<00:42, 15.27it/s] 53%|█████▎    | 733/1380 [00:54<00:42, 15.34it/s] 53%|█████▎    | 735/1380 [00:54<00:41, 15.39it/s] 53%|█████▎    | 737/1380 [00:54<00:41, 15.45it/s] 54%|█████▎    | 739/1380 [00:54<00:41, 15.55it/s] 54%|█████▎    | 741/1380 [00:54<00:40, 15.68it/s] 54%|█████▍    | 743/1380 [00:54<00:40, 15.68it/s] 54%|█████▍    | 745/1380 [00:54<00:40, 15.72it/s] 54%|█████▍    | 747/1380 [00:54<00:40, 15.82it/s] 54%|█████▍    | 749/1380 [00:55<00:39, 15.90it/s] 54%|█████▍    | 751/1380 [00:55<00:39, 15.89it/s] 55%|█████▍    | 753/1380 [00:55<00:39, 15.78it/s] 55%|█████▍    | 755/1380 [00:55<00:39, 15.76it/s] 55%|█████▍    | 757/1380 [00:55<00:39, 15.72it/s] 55%|█████▌    | 759/1380 [00:55<00:39, 15.66it/s] 55%|█████▌    | 761/1380 [00:55<00:39, 15.62it/s] 55%|█████▌    | 763/1380 [00:55<00:39, 15.66it/s] 55%|█████▌    | 765/1380 [00:56<00:39, 15.70it/s] 56%|█████▌    | 767/1380 [00:56<00:39, 15.69it/s] 56%|█████▌    | 769/1380 [00:56<00:39, 15.42it/s] 56%|█████▌    | 771/1380 [00:56<00:39, 15.45it/s] 56%|█████▌    | 773/1380 [00:56<00:38, 15.59it/s] 56%|█████▌    | 775/1380 [00:56<00:38, 15.71it/s] 56%|█████▋    | 777/1380 [00:56<00:38, 15.75it/s] 56%|█████▋    | 779/1380 [00:56<00:38, 15.74it/s] 57%|█████▋    | 781/1380 [00:57<00:38, 15.73it/s] 57%|█████▋    | 783/1380 [00:57<00:37, 15.74it/s] 57%|█████▋    | 785/1380 [00:57<00:37, 15.78it/s] 57%|█████▋    | 787/1380 [00:57<00:37, 15.83it/s] 57%|█████▋    | 789/1380 [00:57<00:37, 15.83it/s] 57%|█████▋    | 791/1380 [00:57<00:37, 15.76it/s] 57%|█████▋    | 793/1380 [00:57<00:37, 15.71it/s] 58%|█████▊    | 795/1380 [00:57<00:37, 15.66it/s] 58%|█████▊    | 797/1380 [00:58<00:37, 15.57it/s] 58%|█████▊    | 799/1380 [00:58<00:37, 15.61it/s] 58%|█████▊    | 801/1380 [00:58<00:36, 15.72it/s] 58%|█████▊    | 803/1380 [00:58<00:36, 15.82it/s] 58%|█████▊    | 805/1380 [00:58<00:36, 15.86it/s] 58%|█████▊    | 807/1380 [00:58<00:36, 15.88it/s] 59%|█████▊    | 809/1380 [00:58<00:35, 15.94it/s] 59%|█████▉    | 811/1380 [00:58<00:35, 15.95it/s] 59%|█████▉    | 813/1380 [00:59<00:35, 15.95it/s] 59%|█████▉    | 815/1380 [00:59<00:35, 15.94it/s] 59%|█████▉    | 817/1380 [00:59<00:35, 15.97it/s] 59%|█████▉    | 819/1380 [00:59<00:35, 15.98it/s] 59%|█████▉    | 821/1380 [00:59<00:35, 15.95it/s] 60%|█████▉    | 823/1380 [00:59<00:34, 15.93it/s] 60%|█████▉    | 825/1380 [00:59<00:34, 15.87it/s] 60%|█████▉    | 827/1380 [01:00<00:34, 15.86it/s]                                                   60%|██████    | 828/1380 [01:00<00:34, 15.86it/s][INFO|trainer.py:755] 2023-11-15 23:13:58,872 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 23:13:58,873 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 23:13:58,874 >>   Num examples = 2204
[INFO|trainer.py:3134] 2023-11-15 23:13:58,874 >>   Batch size = 8
{'eval_loss': 0.36459076404571533, 'eval_accuracy': 0.8634301270417423, 'eval_micro_f1': 0.8634301270417423, 'eval_macro_f1': 0.8431040812436906, 'eval_runtime': 3.3159, 'eval_samples_per_second': 664.673, 'eval_steps_per_second': 83.235, 'epoch': 2.0}
{'loss': 0.3323, 'learning_rate': 0.0002, 'epoch': 3.0}

  0%|          | 0/276 [00:00<?, ?it/s][A
  4%|▎         | 10/276 [00:00<00:02, 94.81it/s][A
  7%|▋         | 20/276 [00:00<00:02, 89.98it/s][A
 11%|█         | 30/276 [00:00<00:02, 89.17it/s][A
 14%|█▍        | 39/276 [00:00<00:02, 88.19it/s][A
 17%|█▋        | 48/276 [00:00<00:02, 87.92it/s][A
 21%|██        | 57/276 [00:00<00:02, 87.65it/s][A
 24%|██▍       | 66/276 [00:00<00:02, 87.01it/s][A
 27%|██▋       | 75/276 [00:00<00:02, 85.98it/s][A
 30%|███       | 84/276 [00:00<00:02, 85.16it/s][A
 34%|███▎      | 93/276 [00:01<00:02, 84.32it/s][A
 37%|███▋      | 102/276 [00:01<00:02, 83.23it/s][A
 40%|████      | 111/276 [00:01<00:02, 80.47it/s][A
 43%|████▎     | 120/276 [00:01<00:01, 81.07it/s][A
 47%|████▋     | 129/276 [00:01<00:01, 81.08it/s][A
 50%|█████     | 138/276 [00:01<00:01, 81.58it/s][A
 53%|█████▎    | 147/276 [00:01<00:01, 80.64it/s][A
 57%|█████▋    | 156/276 [00:01<00:01, 76.01it/s][A
 60%|█████▉    | 165/276 [00:01<00:01, 77.76it/s][A
 63%|██████▎   | 174/276 [00:02<00:01, 78.58it/s][A
 66%|██████▌   | 182/276 [00:02<00:01, 78.81it/s][A
 69%|██████▉   | 191/276 [00:02<00:01, 80.95it/s][A
 72%|███████▏  | 200/276 [00:02<00:00, 81.77it/s][A
 76%|███████▌  | 209/276 [00:02<00:00, 82.10it/s][A
 79%|███████▉  | 218/276 [00:02<00:00, 81.70it/s][A
 82%|████████▏ | 227/276 [00:02<00:00, 81.04it/s][A
 86%|████████▌ | 236/276 [00:02<00:00, 81.07it/s][A
 89%|████████▉ | 245/276 [00:02<00:00, 82.31it/s][A
 92%|█████████▏| 254/276 [00:03<00:00, 82.84it/s][A
 95%|█████████▌| 263/276 [00:03<00:00, 81.99it/s][A
 99%|█████████▊| 272/276 [00:03<00:00, 79.25it/s][A                                                  
                                                 [A 60%|██████    | 828/1380 [01:03<00:34, 15.86it/s]
100%|██████████| 276/276 [00:03<00:00, 79.25it/s][A
                                                 [A 60%|██████    | 829/1380 [01:03<05:16,  1.74it/s] 60%|██████    | 831/1380 [01:03<03:51,  2.37it/s] 60%|██████    | 833/1380 [01:03<02:52,  3.18it/s] 61%|██████    | 835/1380 [01:03<02:10,  4.18it/s] 61%|██████    | 837/1380 [01:04<01:41,  5.35it/s] 61%|██████    | 839/1380 [01:04<01:21,  6.66it/s] 61%|██████    | 841/1380 [01:04<01:06,  8.06it/s] 61%|██████    | 843/1380 [01:04<00:56,  9.47it/s] 61%|██████    | 845/1380 [01:04<00:49, 10.78it/s] 61%|██████▏   | 847/1380 [01:04<00:44, 11.94it/s] 62%|██████▏   | 849/1380 [01:04<00:41, 12.95it/s] 62%|██████▏   | 851/1380 [01:04<00:38, 13.71it/s] 62%|██████▏   | 853/1380 [01:05<00:36, 14.28it/s] 62%|██████▏   | 855/1380 [01:05<00:35, 14.76it/s] 62%|██████▏   | 857/1380 [01:05<00:34, 15.11it/s] 62%|██████▏   | 859/1380 [01:05<00:33, 15.35it/s] 62%|██████▏   | 861/1380 [01:05<00:33, 15.53it/s] 63%|██████▎   | 863/1380 [01:05<00:32, 15.68it/s] 63%|██████▎   | 865/1380 [01:05<00:32, 15.78it/s] 63%|██████▎   | 867/1380 [01:05<00:32, 15.84it/s] 63%|██████▎   | 869/1380 [01:06<00:32, 15.88it/s] 63%|██████▎   | 871/1380 [01:06<00:31, 15.93it/s] 63%|██████▎   | 873/1380 [01:06<00:31, 15.95it/s] 63%|██████▎   | 875/1380 [01:06<00:31, 15.89it/s] 64%|██████▎   | 877/1380 [01:06<00:31, 15.82it/s] 64%|██████▎   | 879/1380 [01:06<00:31, 15.78it/s] 64%|██████▍   | 881/1380 [01:06<00:31, 15.72it/s] 64%|██████▍   | 883/1380 [01:06<00:31, 15.71it/s] 64%|██████▍   | 885/1380 [01:07<00:31, 15.68it/s] 64%|██████▍   | 887/1380 [01:07<00:31, 15.69it/s] 64%|██████▍   | 889/1380 [01:07<00:31, 15.67it/s] 65%|██████▍   | 891/1380 [01:07<00:31, 15.66it/s] 65%|██████▍   | 893/1380 [01:07<00:30, 15.74it/s] 65%|██████▍   | 895/1380 [01:07<00:30, 15.80it/s] 65%|██████▌   | 897/1380 [01:07<00:30, 15.82it/s] 65%|██████▌   | 899/1380 [01:07<00:30, 15.84it/s] 65%|██████▌   | 901/1380 [01:08<00:30, 15.83it/s] 65%|██████▌   | 903/1380 [01:08<00:30, 15.78it/s] 66%|██████▌   | 905/1380 [01:08<00:30, 15.78it/s] 66%|██████▌   | 907/1380 [01:08<00:29, 15.82it/s] 66%|██████▌   | 909/1380 [01:08<00:29, 15.84it/s] 66%|██████▌   | 911/1380 [01:08<00:29, 15.83it/s] 66%|██████▌   | 913/1380 [01:08<00:29, 15.79it/s] 66%|██████▋   | 915/1380 [01:08<00:29, 15.70it/s] 66%|██████▋   | 917/1380 [01:09<00:29, 15.69it/s] 67%|██████▋   | 919/1380 [01:09<00:29, 15.67it/s] 67%|██████▋   | 921/1380 [01:09<00:29, 15.69it/s] 67%|██████▋   | 923/1380 [01:09<00:28, 15.80it/s] 67%|██████▋   | 925/1380 [01:09<00:28, 15.88it/s] 67%|██████▋   | 927/1380 [01:09<00:28, 15.91it/s] 67%|██████▋   | 929/1380 [01:09<00:28, 15.92it/s] 67%|██████▋   | 931/1380 [01:09<00:28, 15.96it/s] 68%|██████▊   | 933/1380 [01:10<00:28, 15.94it/s] 68%|██████▊   | 935/1380 [01:10<00:27, 15.93it/s] 68%|██████▊   | 937/1380 [01:10<00:27, 15.94it/s] 68%|██████▊   | 939/1380 [01:10<00:27, 15.99it/s] 68%|██████▊   | 941/1380 [01:10<00:27, 16.00it/s] 68%|██████▊   | 943/1380 [01:10<00:27, 15.98it/s] 68%|██████▊   | 945/1380 [01:10<00:27, 15.96it/s] 69%|██████▊   | 947/1380 [01:10<00:27, 16.00it/s] 69%|██████▉   | 949/1380 [01:11<00:26, 16.01it/s] 69%|██████▉   | 951/1380 [01:11<00:26, 16.00it/s] 69%|██████▉   | 953/1380 [01:11<00:26, 15.98it/s] 69%|██████▉   | 955/1380 [01:11<00:26, 16.01it/s] 69%|██████▉   | 957/1380 [01:11<00:26, 16.02it/s] 69%|██████▉   | 959/1380 [01:11<00:26, 15.95it/s] 70%|██████▉   | 961/1380 [01:11<00:26, 15.91it/s] 70%|██████▉   | 963/1380 [01:11<00:26, 15.87it/s] 70%|██████▉   | 965/1380 [01:12<00:26, 15.48it/s] 70%|███████   | 967/1380 [01:12<00:26, 15.54it/s] 70%|███████   | 969/1380 [01:12<00:26, 15.57it/s] 70%|███████   | 971/1380 [01:12<00:26, 15.59it/s] 71%|███████   | 973/1380 [01:12<00:26, 15.62it/s] 71%|███████   | 975/1380 [01:12<00:25, 15.62it/s] 71%|███████   | 977/1380 [01:12<00:25, 15.63it/s] 71%|███████   | 979/1380 [01:13<00:25, 15.67it/s] 71%|███████   | 981/1380 [01:13<00:25, 15.73it/s] 71%|███████   | 983/1380 [01:13<00:25, 15.75it/s] 71%|███████▏  | 985/1380 [01:13<00:25, 15.78it/s] 72%|███████▏  | 987/1380 [01:13<00:24, 15.83it/s] 72%|███████▏  | 989/1380 [01:13<00:24, 15.77it/s] 72%|███████▏  | 991/1380 [01:13<00:24, 15.78it/s] 72%|███████▏  | 993/1380 [01:13<00:24, 15.84it/s] 72%|███████▏  | 995/1380 [01:14<00:24, 15.86it/s] 72%|███████▏  | 997/1380 [01:14<00:24, 15.84it/s] 72%|███████▏  | 999/1380 [01:14<00:24, 15.86it/s] 73%|███████▎  | 1001/1380 [01:14<00:24, 15.77it/s] 73%|███████▎  | 1003/1380 [01:14<00:24, 15.71it/s] 73%|███████▎  | 1005/1380 [01:14<00:23, 15.70it/s] 73%|███████▎  | 1007/1380 [01:14<00:23, 15.56it/s] 73%|███████▎  | 1009/1380 [01:14<00:24, 15.46it/s] 73%|███████▎  | 1011/1380 [01:15<00:23, 15.56it/s] 73%|███████▎  | 1013/1380 [01:15<00:23, 15.66it/s] 74%|███████▎  | 1015/1380 [01:15<00:23, 15.75it/s] 74%|███████▎  | 1017/1380 [01:15<00:22, 15.82it/s] 74%|███████▍  | 1019/1380 [01:15<00:22, 15.79it/s] 74%|███████▍  | 1021/1380 [01:15<00:22, 15.79it/s] 74%|███████▍  | 1023/1380 [01:15<00:22, 15.84it/s] 74%|███████▍  | 1025/1380 [01:15<00:22, 15.87it/s] 74%|███████▍  | 1027/1380 [01:16<00:22, 15.92it/s] 75%|███████▍  | 1029/1380 [01:16<00:22, 15.95it/s] 75%|███████▍  | 1031/1380 [01:16<00:21, 15.95it/s] 75%|███████▍  | 1033/1380 [01:16<00:21, 15.91it/s] 75%|███████▌  | 1035/1380 [01:16<00:21, 15.97it/s] 75%|███████▌  | 1037/1380 [01:16<00:21, 15.99it/s] 75%|███████▌  | 1039/1380 [01:16<00:21, 15.96it/s] 75%|███████▌  | 1041/1380 [01:16<00:21, 15.94it/s] 76%|███████▌  | 1043/1380 [01:17<00:21, 15.97it/s] 76%|███████▌  | 1045/1380 [01:17<00:20, 15.97it/s] 76%|███████▌  | 1047/1380 [01:17<00:20, 15.92it/s] 76%|███████▌  | 1049/1380 [01:17<00:20, 15.82it/s] 76%|███████▌  | 1051/1380 [01:17<00:20, 15.78it/s] 76%|███████▋  | 1053/1380 [01:17<00:20, 15.73it/s] 76%|███████▋  | 1055/1380 [01:17<00:20, 15.72it/s] 77%|███████▋  | 1057/1380 [01:17<00:20, 15.68it/s] 77%|███████▋  | 1059/1380 [01:18<00:20, 15.69it/s] 77%|███████▋  | 1061/1380 [01:18<00:20, 15.62it/s] 77%|███████▋  | 1063/1380 [01:18<00:20, 15.63it/s] 77%|███████▋  | 1065/1380 [01:18<00:20, 15.65it/s] 77%|███████▋  | 1067/1380 [01:18<00:19, 15.66it/s] 77%|███████▋  | 1069/1380 [01:18<00:19, 15.59it/s] 78%|███████▊  | 1071/1380 [01:18<00:19, 15.61it/s] 78%|███████▊  | 1073/1380 [01:18<00:19, 15.63it/s] 78%|███████▊  | 1075/1380 [01:19<00:19, 15.70it/s] 78%|███████▊  | 1077/1380 [01:19<00:19, 15.67it/s] 78%|███████▊  | 1079/1380 [01:19<00:19, 15.68it/s] 78%|███████▊  | 1081/1380 [01:19<00:19, 15.72it/s] 78%|███████▊  | 1083/1380 [01:19<00:18, 15.67it/s] 79%|███████▊  | 1085/1380 [01:19<00:18, 15.71it/s] 79%|███████▉  | 1087/1380 [01:19<00:18, 15.64it/s] 79%|███████▉  | 1089/1380 [01:20<00:18, 15.42it/s] 79%|███████▉  | 1091/1380 [01:20<00:18, 15.45it/s] 79%|███████▉  | 1093/1380 [01:20<00:18, 15.48it/s] 79%|███████▉  | 1095/1380 [01:20<00:18, 15.54it/s] 79%|███████▉  | 1097/1380 [01:20<00:18, 15.64it/s] 80%|███████▉  | 1099/1380 [01:20<00:17, 15.76it/s] 80%|███████▉  | 1101/1380 [01:20<00:17, 15.82it/s] 80%|███████▉  | 1103/1380 [01:20<00:17, 15.88it/s]                                                    80%|████████  | 1104/1380 [01:20<00:17, 15.88it/s][INFO|trainer.py:755] 2023-11-15 23:14:19,762 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 23:14:19,763 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 23:14:19,764 >>   Num examples = 2204
[INFO|trainer.py:3134] 2023-11-15 23:14:19,764 >>   Batch size = 8
{'eval_loss': 0.38141292333602905, 'eval_accuracy': 0.8756805807622504, 'eval_micro_f1': 0.8756805807622504, 'eval_macro_f1': 0.8566406821342629, 'eval_runtime': 3.425, 'eval_samples_per_second': 643.5, 'eval_steps_per_second': 80.583, 'epoch': 3.0}
{'loss': 0.2964, 'learning_rate': 0.0001, 'epoch': 4.0}

  0%|          | 0/276 [00:00<?, ?it/s][A
  4%|▎         | 10/276 [00:00<00:02, 97.88it/s][A
  7%|▋         | 20/276 [00:00<00:02, 90.36it/s][A
 11%|█         | 30/276 [00:00<00:02, 88.61it/s][A
 14%|█▍        | 39/276 [00:00<00:02, 87.05it/s][A
 17%|█▋        | 48/276 [00:00<00:02, 87.04it/s][A
 21%|██        | 57/276 [00:00<00:02, 87.12it/s][A
 24%|██▍       | 66/276 [00:00<00:02, 86.67it/s][A
 27%|██▋       | 75/276 [00:00<00:02, 86.21it/s][A
 30%|███       | 84/276 [00:00<00:02, 86.24it/s][A
 34%|███▎      | 93/276 [00:01<00:02, 86.60it/s][A
 37%|███▋      | 102/276 [00:01<00:02, 85.40it/s][A
 40%|████      | 111/276 [00:01<00:01, 84.84it/s][A
 43%|████▎     | 120/276 [00:01<00:01, 85.33it/s][A
 47%|████▋     | 129/276 [00:01<00:01, 85.52it/s][A
 50%|█████     | 138/276 [00:01<00:01, 85.89it/s][A
 53%|█████▎    | 147/276 [00:01<00:01, 86.06it/s][A
 57%|█████▋    | 156/276 [00:01<00:01, 85.84it/s][A
 60%|█████▉    | 165/276 [00:01<00:01, 85.56it/s][A
 63%|██████▎   | 174/276 [00:02<00:01, 84.44it/s][A
 66%|██████▋   | 183/276 [00:02<00:01, 83.84it/s][A
 70%|██████▉   | 192/276 [00:02<00:00, 84.11it/s][A
 73%|███████▎  | 201/276 [00:02<00:00, 83.50it/s][A
 76%|███████▌  | 210/276 [00:02<00:00, 82.73it/s][A
 79%|███████▉  | 219/276 [00:02<00:00, 83.15it/s][A
 83%|████████▎ | 228/276 [00:02<00:00, 83.20it/s][A
 86%|████████▌ | 237/276 [00:02<00:00, 83.74it/s][A
 89%|████████▉ | 246/276 [00:02<00:00, 83.87it/s][A
 92%|█████████▏| 255/276 [00:02<00:00, 84.09it/s][A
 96%|█████████▌| 264/276 [00:03<00:00, 84.44it/s][A
 99%|█████████▉| 273/276 [00:03<00:00, 84.64it/s][A                                                   
                                                 [A 80%|████████  | 1104/1380 [01:24<00:17, 15.88it/s]
100%|██████████| 276/276 [00:03<00:00, 84.64it/s][A
                                                 [A 80%|████████  | 1105/1380 [01:24<02:32,  1.80it/s] 80%|████████  | 1107/1380 [01:24<01:51,  2.46it/s] 80%|████████  | 1109/1380 [01:24<01:22,  3.29it/s] 81%|████████  | 1111/1380 [01:24<01:02,  4.31it/s] 81%|████████  | 1113/1380 [01:24<00:48,  5.50it/s] 81%|████████  | 1115/1380 [01:24<00:38,  6.84it/s] 81%|████████  | 1117/1380 [01:25<00:31,  8.23it/s] 81%|████████  | 1119/1380 [01:25<00:27,  9.60it/s] 81%|████████  | 1121/1380 [01:25<00:23, 10.85it/s] 81%|████████▏ | 1123/1380 [01:25<00:21, 11.92it/s] 82%|████████▏ | 1125/1380 [01:25<00:20, 12.71it/s] 82%|████████▏ | 1127/1380 [01:25<00:18, 13.44it/s] 82%|████████▏ | 1129/1380 [01:25<00:17, 13.98it/s] 82%|████████▏ | 1131/1380 [01:25<00:17, 14.47it/s] 82%|████████▏ | 1133/1380 [01:26<00:16, 14.84it/s] 82%|████████▏ | 1135/1380 [01:26<00:16, 15.10it/s] 82%|████████▏ | 1137/1380 [01:26<00:15, 15.34it/s] 83%|████████▎ | 1139/1380 [01:26<00:15, 15.50it/s] 83%|████████▎ | 1141/1380 [01:26<00:15, 15.60it/s] 83%|████████▎ | 1143/1380 [01:26<00:15, 15.63it/s] 83%|████████▎ | 1145/1380 [01:26<00:14, 15.69it/s] 83%|████████▎ | 1147/1380 [01:26<00:14, 15.73it/s] 83%|████████▎ | 1149/1380 [01:27<00:14, 15.77it/s] 83%|████████▎ | 1151/1380 [01:27<00:14, 15.80it/s] 84%|████████▎ | 1153/1380 [01:27<00:14, 15.80it/s] 84%|████████▎ | 1155/1380 [01:27<00:14, 15.82it/s] 84%|████████▍ | 1157/1380 [01:27<00:14, 15.84it/s] 84%|████████▍ | 1159/1380 [01:27<00:13, 15.84it/s] 84%|████████▍ | 1161/1380 [01:27<00:13, 15.82it/s] 84%|████████▍ | 1163/1380 [01:27<00:13, 15.82it/s] 84%|████████▍ | 1165/1380 [01:28<00:13, 15.80it/s] 85%|████████▍ | 1167/1380 [01:28<00:13, 15.80it/s] 85%|████████▍ | 1169/1380 [01:28<00:13, 15.80it/s] 85%|████████▍ | 1171/1380 [01:28<00:13, 15.69it/s] 85%|████████▌ | 1173/1380 [01:28<00:13, 15.65it/s] 85%|████████▌ | 1175/1380 [01:28<00:13, 15.63it/s] 85%|████████▌ | 1177/1380 [01:28<00:13, 15.59it/s] 85%|████████▌ | 1179/1380 [01:29<00:12, 15.64it/s] 86%|████████▌ | 1181/1380 [01:29<00:12, 15.63it/s] 86%|████████▌ | 1183/1380 [01:29<00:12, 15.66it/s] 86%|████████▌ | 1185/1380 [01:29<00:12, 15.68it/s] 86%|████████▌ | 1187/1380 [01:29<00:12, 15.64it/s] 86%|████████▌ | 1189/1380 [01:29<00:12, 15.68it/s] 86%|████████▋ | 1191/1380 [01:29<00:12, 15.67it/s] 86%|████████▋ | 1193/1380 [01:29<00:11, 15.69it/s] 87%|████████▋ | 1195/1380 [01:30<00:11, 15.72it/s] 87%|████████▋ | 1197/1380 [01:30<00:11, 15.74it/s] 87%|████████▋ | 1199/1380 [01:30<00:11, 15.76it/s] 87%|████████▋ | 1201/1380 [01:30<00:11, 15.73it/s] 87%|████████▋ | 1203/1380 [01:30<00:11, 15.72it/s] 87%|████████▋ | 1205/1380 [01:30<00:11, 15.72it/s] 87%|████████▋ | 1207/1380 [01:30<00:10, 15.76it/s] 88%|████████▊ | 1209/1380 [01:30<00:10, 15.71it/s] 88%|████████▊ | 1211/1380 [01:31<00:10, 15.66it/s] 88%|████████▊ | 1213/1380 [01:31<00:10, 15.58it/s] 88%|████████▊ | 1215/1380 [01:31<00:10, 15.61it/s] 88%|████████▊ | 1217/1380 [01:31<00:10, 15.59it/s] 88%|████████▊ | 1219/1380 [01:31<00:10, 15.63it/s] 88%|████████▊ | 1221/1380 [01:31<00:10, 15.72it/s] 89%|████████▊ | 1223/1380 [01:31<00:09, 15.77it/s] 89%|████████▉ | 1225/1380 [01:31<00:09, 15.78it/s] 89%|████████▉ | 1227/1380 [01:32<00:09, 15.85it/s] 89%|████████▉ | 1229/1380 [01:32<00:09, 15.89it/s] 89%|████████▉ | 1231/1380 [01:32<00:09, 15.86it/s] 89%|████████▉ | 1233/1380 [01:32<00:09, 15.90it/s] 89%|████████▉ | 1235/1380 [01:32<00:09, 15.94it/s] 90%|████████▉ | 1237/1380 [01:32<00:08, 15.94it/s] 90%|████████▉ | 1239/1380 [01:32<00:08, 15.88it/s] 90%|████████▉ | 1241/1380 [01:32<00:08, 15.93it/s] 90%|█████████ | 1243/1380 [01:33<00:08, 15.95it/s] 90%|█████████ | 1245/1380 [01:33<00:08, 15.94it/s] 90%|█████████ | 1247/1380 [01:33<00:08, 15.94it/s] 91%|█████████ | 1249/1380 [01:33<00:08, 15.96it/s] 91%|█████████ | 1251/1380 [01:33<00:08, 15.97it/s] 91%|█████████ | 1253/1380 [01:33<00:07, 15.94it/s] 91%|█████████ | 1255/1380 [01:33<00:08, 15.56it/s] 91%|█████████ | 1257/1380 [01:33<00:07, 15.60it/s] 91%|█████████ | 1259/1380 [01:34<00:07, 15.61it/s] 91%|█████████▏| 1261/1380 [01:34<00:07, 15.57it/s] 92%|█████████▏| 1263/1380 [01:34<00:07, 15.61it/s] 92%|█████████▏| 1265/1380 [01:34<00:07, 15.60it/s] 92%|█████████▏| 1267/1380 [01:34<00:07, 15.63it/s] 92%|█████████▏| 1269/1380 [01:34<00:07, 15.60it/s] 92%|█████████▏| 1271/1380 [01:34<00:06, 15.67it/s] 92%|█████████▏| 1273/1380 [01:34<00:06, 15.71it/s] 92%|█████████▏| 1275/1380 [01:35<00:06, 15.74it/s] 93%|█████████▎| 1277/1380 [01:35<00:06, 15.73it/s] 93%|█████████▎| 1279/1380 [01:35<00:06, 15.77it/s] 93%|█████████▎| 1281/1380 [01:35<00:06, 15.81it/s] 93%|█████████▎| 1283/1380 [01:35<00:06, 15.82it/s] 93%|█████████▎| 1285/1380 [01:35<00:06, 15.79it/s] 93%|█████████▎| 1287/1380 [01:35<00:05, 15.75it/s] 93%|█████████▎| 1289/1380 [01:35<00:05, 15.72it/s] 94%|█████████▎| 1291/1380 [01:36<00:05, 15.77it/s] 94%|█████████▎| 1293/1380 [01:36<00:05, 15.81it/s] 94%|█████████▍| 1295/1380 [01:36<00:05, 15.83it/s] 94%|█████████▍| 1297/1380 [01:36<00:05, 15.83it/s] 94%|█████████▍| 1299/1380 [01:36<00:05, 15.75it/s] 94%|█████████▍| 1301/1380 [01:36<00:05, 15.65it/s] 94%|█████████▍| 1303/1380 [01:36<00:04, 15.65it/s] 95%|█████████▍| 1305/1380 [01:37<00:04, 15.62it/s] 95%|█████████▍| 1307/1380 [01:37<00:04, 15.64it/s] 95%|█████████▍| 1309/1380 [01:37<00:04, 15.71it/s] 95%|█████████▌| 1311/1380 [01:37<00:04, 15.75it/s] 95%|█████████▌| 1313/1380 [01:37<00:04, 15.79it/s] 95%|█████████▌| 1315/1380 [01:37<00:04, 15.85it/s] 95%|█████████▌| 1317/1380 [01:37<00:03, 15.84it/s] 96%|█████████▌| 1319/1380 [01:37<00:03, 15.87it/s] 96%|█████████▌| 1321/1380 [01:38<00:03, 15.92it/s] 96%|█████████▌| 1323/1380 [01:38<00:03, 15.92it/s] 96%|█████████▌| 1325/1380 [01:38<00:03, 15.92it/s] 96%|█████████▌| 1327/1380 [01:38<00:03, 15.93it/s] 96%|█████████▋| 1329/1380 [01:38<00:03, 15.97it/s] 96%|█████████▋| 1331/1380 [01:38<00:03, 15.97it/s] 97%|█████████▋| 1333/1380 [01:38<00:02, 15.95it/s] 97%|█████████▋| 1335/1380 [01:38<00:02, 15.94it/s] 97%|█████████▋| 1337/1380 [01:39<00:02, 15.97it/s] 97%|█████████▋| 1339/1380 [01:39<00:02, 15.97it/s] 97%|█████████▋| 1341/1380 [01:39<00:02, 15.95it/s] 97%|█████████▋| 1343/1380 [01:39<00:02, 15.97it/s] 97%|█████████▋| 1345/1380 [01:39<00:02, 15.97it/s] 98%|█████████▊| 1347/1380 [01:39<00:02, 15.87it/s] 98%|█████████▊| 1349/1380 [01:39<00:01, 15.78it/s] 98%|█████████▊| 1351/1380 [01:39<00:01, 15.74it/s] 98%|█████████▊| 1353/1380 [01:40<00:01, 15.67it/s] 98%|█████████▊| 1355/1380 [01:40<00:01, 15.70it/s] 98%|█████████▊| 1357/1380 [01:40<00:01, 15.68it/s] 98%|█████████▊| 1359/1380 [01:40<00:01, 15.72it/s] 99%|█████████▊| 1361/1380 [01:40<00:01, 15.76it/s] 99%|█████████▉| 1363/1380 [01:40<00:01, 15.72it/s] 99%|█████████▉| 1365/1380 [01:40<00:00, 15.69it/s] 99%|█████████▉| 1367/1380 [01:40<00:00, 15.75it/s] 99%|█████████▉| 1369/1380 [01:41<00:00, 15.76it/s] 99%|█████████▉| 1371/1380 [01:41<00:00, 15.80it/s] 99%|█████████▉| 1373/1380 [01:41<00:00, 15.81it/s]100%|█████████▉| 1375/1380 [01:41<00:00, 15.77it/s]100%|█████████▉| 1377/1380 [01:41<00:00, 15.76it/s]100%|█████████▉| 1379/1380 [01:41<00:00, 15.81it/s]                                                   100%|██████████| 1380/1380 [01:41<00:00, 15.81it/s][INFO|trainer.py:755] 2023-11-15 23:14:40,553 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 23:14:40,554 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 23:14:40,554 >>   Num examples = 2204
[INFO|trainer.py:3134] 2023-11-15 23:14:40,555 >>   Batch size = 8
{'eval_loss': 0.3831818699836731, 'eval_accuracy': 0.8688747731397459, 'eval_micro_f1': 0.8688747731397459, 'eval_macro_f1': 0.8499255724359459, 'eval_runtime': 3.2922, 'eval_samples_per_second': 669.46, 'eval_steps_per_second': 83.834, 'epoch': 4.0}
{'loss': 0.2645, 'learning_rate': 0.0, 'epoch': 5.0}

  0%|          | 0/276 [00:00<?, ?it/s][A
  4%|▎         | 10/276 [00:00<00:02, 92.69it/s][A
  7%|▋         | 20/276 [00:00<00:02, 87.45it/s][A
 11%|█         | 29/276 [00:00<00:02, 84.39it/s][A
 14%|█▍        | 38/276 [00:00<00:03, 78.76it/s][A
 17%|█▋        | 46/276 [00:00<00:03, 74.83it/s][A
 20%|█▉        | 54/276 [00:00<00:03, 73.66it/s][A
 22%|██▏       | 62/276 [00:00<00:02, 72.31it/s][A
 25%|██▌       | 70/276 [00:00<00:02, 71.58it/s][A
 29%|██▊       | 79/276 [00:01<00:02, 74.84it/s][A
 32%|███▏      | 88/276 [00:01<00:02, 77.78it/s][A
 35%|███▌      | 97/276 [00:01<00:02, 79.99it/s][A
 38%|███▊      | 106/276 [00:01<00:02, 81.69it/s][A
 42%|████▏     | 115/276 [00:01<00:01, 82.43it/s][A
 45%|████▍     | 124/276 [00:01<00:01, 83.46it/s][A
 48%|████▊     | 133/276 [00:01<00:01, 84.35it/s][A
 51%|█████▏    | 142/276 [00:01<00:01, 85.12it/s][A
 55%|█████▍    | 151/276 [00:01<00:01, 85.87it/s][A
 58%|█████▊    | 160/276 [00:01<00:01, 86.19it/s][A
 61%|██████    | 169/276 [00:02<00:01, 85.55it/s][A
 64%|██████▍   | 178/276 [00:02<00:01, 85.49it/s][A
 68%|██████▊   | 187/276 [00:02<00:01, 85.38it/s][A
 71%|███████   | 196/276 [00:02<00:00, 85.67it/s][A
 74%|███████▍  | 205/276 [00:02<00:00, 85.56it/s][A
 78%|███████▊  | 214/276 [00:02<00:00, 85.35it/s][A
 81%|████████  | 223/276 [00:02<00:00, 86.04it/s][A
 84%|████████▍ | 232/276 [00:02<00:00, 86.28it/s][A
 87%|████████▋ | 241/276 [00:02<00:00, 86.54it/s][A
 91%|█████████ | 250/276 [00:03<00:00, 86.58it/s][A
 94%|█████████▍| 259/276 [00:03<00:00, 86.53it/s][A
 97%|█████████▋| 268/276 [00:03<00:00, 86.39it/s][A                                                   
                                                 [A100%|██████████| 1380/1380 [01:45<00:00, 15.81it/s]
100%|██████████| 276/276 [00:03<00:00, 86.39it/s][A
                                                 [A[INFO|trainer.py:1963] 2023-11-15 23:14:43,936 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                   100%|██████████| 1380/1380 [01:45<00:00, 15.81it/s]100%|██████████| 1380/1380 [01:45<00:00, 13.13it/s]
[INFO|trainer.py:2855] 2023-11-15 23:14:43,939 >> Saving model checkpoint to ./result/acl_roberta-base_seed0_adapter
[INFO|configuration_utils.py:460] 2023-11-15 23:14:43,942 >> Configuration saved in ./result/acl_roberta-base_seed0_adapter/config.json
[INFO|modeling_utils.py:1997] 2023-11-15 23:14:45,071 >> Model weights saved in ./result/acl_roberta-base_seed0_adapter/pytorch_model.bin
[INFO|tokenization_utils_base.py:2235] 2023-11-15 23:14:45,073 >> tokenizer config file saved in ./result/acl_roberta-base_seed0_adapter/tokenizer_config.json
[INFO|tokenization_utils_base.py:2242] 2023-11-15 23:14:45,075 >> Special tokens file saved in ./result/acl_roberta-base_seed0_adapter/special_tokens_map.json
{'eval_loss': 0.39289575815200806, 'eval_accuracy': 0.8693284936479129, 'eval_micro_f1': 0.8693284936479129, 'eval_macro_f1': 0.8524150701296742, 'eval_runtime': 3.3788, 'eval_samples_per_second': 652.293, 'eval_steps_per_second': 81.685, 'epoch': 5.0}
{'train_runtime': 105.1108, 'train_samples_per_second': 419.367, 'train_steps_per_second': 13.129, 'train_loss': 0.36012782221255096, 'epoch': 5.0}
***** train metrics *****
  epoch                    =        5.0
  train_loss               =     0.3601
  train_runtime            = 0:01:45.11
  train_samples            =       8816
  train_samples_per_second =    419.367
  train_steps_per_second   =     13.129
11/15/2023 23:14:45 - INFO - __main__ - *** Evaluate ***
[INFO|trainer.py:755] 2023-11-15 23:14:45,211 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 23:14:45,212 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 23:14:45,212 >>   Num examples = 2204
[INFO|trainer.py:3134] 2023-11-15 23:14:45,213 >>   Batch size = 8
  0%|          | 0/276 [00:00<?, ?it/s]  4%|▎         | 10/276 [00:00<00:02, 93.02it/s]  7%|▋         | 20/276 [00:00<00:02, 87.76it/s] 11%|█         | 29/276 [00:00<00:02, 86.86it/s] 14%|█▍        | 38/276 [00:00<00:02, 86.29it/s] 17%|█▋        | 47/276 [00:00<00:02, 85.70it/s] 20%|██        | 56/276 [00:00<00:02, 85.13it/s] 24%|██▎       | 65/276 [00:00<00:02, 85.68it/s] 27%|██▋       | 74/276 [00:00<00:02, 85.19it/s] 30%|███       | 83/276 [00:00<00:02, 84.82it/s] 33%|███▎      | 92/276 [00:01<00:02, 85.22it/s] 37%|███▋      | 101/276 [00:01<00:02, 84.44it/s] 40%|███▉      | 110/276 [00:01<00:01, 85.05it/s] 43%|████▎     | 119/276 [00:01<00:01, 84.71it/s] 46%|████▋     | 128/276 [00:01<00:01, 82.73it/s] 50%|████▉     | 137/276 [00:01<00:01, 80.16it/s] 53%|█████▎    | 146/276 [00:01<00:01, 79.41it/s] 56%|█████▌    | 154/276 [00:01<00:01, 78.02it/s] 59%|█████▊    | 162/276 [00:01<00:01, 77.42it/s] 62%|██████▏   | 171/276 [00:02<00:01, 80.25it/s] 65%|██████▌   | 180/276 [00:02<00:01, 82.64it/s] 68%|██████▊   | 189/276 [00:02<00:01, 84.07it/s] 72%|███████▏  | 198/276 [00:02<00:00, 85.27it/s] 75%|███████▌  | 207/276 [00:02<00:00, 85.54it/s] 78%|███████▊  | 216/276 [00:02<00:00, 86.30it/s] 82%|████████▏ | 225/276 [00:02<00:00, 86.68it/s] 85%|████████▍ | 234/276 [00:02<00:00, 85.97it/s] 88%|████████▊ | 243/276 [00:02<00:00, 86.14it/s] 91%|█████████▏| 252/276 [00:02<00:00, 86.38it/s] 95%|█████████▍| 261/276 [00:03<00:00, 86.82it/s] 98%|█████████▊| 270/276 [00:03<00:00, 86.64it/s]100%|██████████| 276/276 [00:03<00:00, 83.50it/s]
***** eval metrics *****
  epoch                   =        5.0
  eval_accuracy           =     0.8693
  eval_loss               =     0.3929
  eval_macro_f1           =     0.8524
  eval_micro_f1           =     0.8693
  eval_runtime            = 0:00:03.32
  eval_samples            =       2204
  eval_samples_per_second =    663.638
  eval_steps_per_second   =     83.105
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                  eval/accuracy ▁▆█▇▇▇
wandb:                      eval/loss █▁▂▂▂▂
wandb:                  eval/macro_f1 ▁▆█▇▇▇
wandb:                  eval/micro_f1 ▁▆█▇▇▇
wandb:                   eval/runtime ▁▄█▃▆▄
wandb:        eval/samples_per_second █▅▁▆▃▅
wandb:          eval/steps_per_second █▅▁▆▃▅
wandb:                    train/epoch ▁▁▃▃▅▅▆▆████
wandb:              train/global_step ▁▁▃▃▅▅▆▆████
wandb:            train/learning_rate █▆▅▃▁
wandb:                     train/loss █▄▃▂▁
wandb:               train/total_flos ▁
wandb:               train/train_loss ▁
wandb:            train/train_runtime ▁
wandb: train/train_samples_per_second ▁
wandb:   train/train_steps_per_second ▁
wandb: 
wandb: Run summary:
wandb:                  eval/accuracy 0.86933
wandb:                      eval/loss 0.3929
wandb:                  eval/macro_f1 0.85242
wandb:                  eval/micro_f1 0.86933
wandb:                   eval/runtime 3.3211
wandb:        eval/samples_per_second 663.638
wandb:          eval/steps_per_second 83.105
wandb:                    train/epoch 5.0
wandb:              train/global_step 1380
wandb:            train/learning_rate 0.0
wandb:                     train/loss 0.2645
wandb:               train/total_flos 1464896356669440.0
wandb:               train/train_loss 0.36013
wandb:            train/train_runtime 105.1108
wandb: train/train_samples_per_second 419.367
wandb:   train/train_steps_per_second 13.129
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /ceph/home/wangyifei/a3/wandb/offline-run-20231115_231140-v6zv9oom
wandb: Find logs at: ./wandb/offline-run-20231115_231140-v6zv9oom/logs
(ModelArguments(model_name_or_path='roberta-base', cache_dir='./cache', use_adapter=True, use_lora=False), DataTrainingArguments(dataset_name='agnews_sup', max_seq_length=64), TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.0005,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=./result/agnews_sup_roberta-base_seed0_adapter/runs/Nov15_23-14-58_node013,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.EPOCH,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=5.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
output_dir=./result/agnews_sup_roberta-base_seed0_adapter,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
resume_from_checkpoint=None,
run_name=./result/agnews_sup_roberta-base_seed0_adapter,
save_on_each_node=False,
save_safetensors=False,
save_steps=500,
save_strategy=IntervalStrategy.NO,
save_total_limit=None,
seed=111,
sharded_ddp=[],
skip_memory_metrics=True,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
))
11/15/2023 23:14:58 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False
11/15/2023 23:14:58 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.0005,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=./result/agnews_sup_roberta-base_seed0_adapter/runs/Nov15_23-14-57_node013,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.EPOCH,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=5.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
output_dir=./result/agnews_sup_roberta-base_seed0_adapter,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
resume_from_checkpoint=None,
run_name=./result/agnews_sup_roberta-base_seed0_adapter,
save_on_each_node=False,
save_safetensors=False,
save_steps=500,
save_strategy=IntervalStrategy.NO,
save_total_limit=None,
seed=111,
sharded_ddp=[],
skip_memory_metrics=True,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
wandb: Tracking run with wandb version 0.15.12
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[INFO|configuration_utils.py:715] 2023-11-15 23:15:14,217 >> loading configuration file config.json from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/config.json
[INFO|configuration_utils.py:775] 2023-11-15 23:15:14,226 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.33.3",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_auto.py:535] 2023-11-15 23:15:24,242 >> Could not locate the tokenizer configuration file, will try to use the model config instead.
[INFO|configuration_utils.py:715] 2023-11-15 23:15:34,258 >> loading configuration file config.json from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/config.json
[INFO|configuration_utils.py:775] 2023-11-15 23:15:34,259 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.33.3",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:1852] 2023-11-15 23:15:54,304 >> loading file vocab.json from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/vocab.json
[INFO|tokenization_utils_base.py:1852] 2023-11-15 23:15:54,305 >> loading file merges.txt from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/merges.txt
[INFO|tokenization_utils_base.py:1852] 2023-11-15 23:15:54,305 >> loading file tokenizer.json from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/tokenizer.json
[INFO|tokenization_utils_base.py:1852] 2023-11-15 23:15:54,305 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1852] 2023-11-15 23:15:54,306 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1852] 2023-11-15 23:15:54,306 >> loading file tokenizer_config.json from cache at None
[INFO|configuration_utils.py:715] 2023-11-15 23:15:54,307 >> loading configuration file config.json from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/config.json
[INFO|configuration_utils.py:775] 2023-11-15 23:15:54,308 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.33.3",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:2862] 2023-11-15 23:16:14,491 >> loading weights file model.safetensors from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/model.safetensors
[INFO|modeling_utils.py:3638] 2023-11-15 23:16:15,223 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3650] 2023-11-15 23:16:15,224 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
parameters 1488196
model_max_length: 512
Running tokenizer on dataset:   0%|          | 0/6840 [00:00<?, ? examples/s]Running tokenizer on dataset:  44%|████▍     | 3000/6840 [00:00<00:00, 19693.11 examples/s]Running tokenizer on dataset:  88%|████████▊ | 6000/6840 [00:00<00:00, 20754.65 examples/s]Running tokenizer on dataset: 100%|██████████| 6840/6840 [00:00<00:00, 20172.01 examples/s]
Running tokenizer on dataset:   0%|          | 0/760 [00:00<?, ? examples/s]Running tokenizer on dataset: 100%|██████████| 760/760 [00:00<00:00, 23542.10 examples/s]
11/15/2023 23:16:16 - INFO - __main__ - Sample 6776 of the training set: {'text': 'Cup chase lands in Dover When the green flag drops for today #39;s MBNA America 400 at Dover International Speedway, 43 drivers will be lined up to cross the start/finish line.', 'label': 0, 'input_ids': [0, 347, 658, 7859, 8952, 11, 21860, 520, 5, 2272, 3794, 9305, 13, 452, 849, 3416, 131, 29, 17025, 4444, 730, 3675, 23, 21860, 1016, 13243, 6, 3557, 2377, 40, 28, 9321, 62, 7, 2116, 5, 386, 73, 13597, 1173, 516, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
11/15/2023 23:16:16 - INFO - __main__ - Sample 1742 of the training set: {'text': 'Louisiana Tech Bulldogs RUSTON, Louisiana (Ticker) -- No. 17 Fresno State could not overcome a dominant performance by Ryan Moats or a poor one by Paul Pinegar.', 'label': 0, 'input_ids': [0, 29923, 8878, 4569, 10135, 248, 11120, 2191, 6, 5993, 36, 565, 13917, 43, 480, 440, 4, 601, 18084, 331, 115, 45, 6647, 10, 7353, 819, 30, 1774, 3713, 2923, 50, 10, 2129, 65, 30, 1206, 11542, 6276, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
11/15/2023 23:16:16 - INFO - __main__ - Sample 2588 of the training set: {'text': 'Rossi:  #39;I #39;m fairly happy #39; Valentino Rossi, who on Thursday pledged his future to Yamaha, entered the final qualifying session with the fastest time to date, but with the morning rain having washed the circuit clean, the Italian was unable to challenge Makoto Tamada for the pole.', 'label': 0, 'input_ids': [0, 35978, 118, 35, 1437, 849, 3416, 131, 100, 849, 3416, 131, 119, 5342, 1372, 849, 3416, 131, 18352, 1696, 21873, 6, 54, 15, 296, 7114, 39, 499, 7, 25297, 6, 2867, 5, 507, 7310, 1852, 19, 5, 6273, 86, 7, 1248, 6, 53, 19, 5, 662, 1895, 519, 15158, 5, 9326, 2382, 6, 5, 3108, 21, 3276, 7, 1539, 46383, 7736, 2095, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.
11/15/2023 23:16:16 - WARNING - accelerate.utils.other - Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[INFO|trainer.py:755] 2023-11-15 23:16:17,586 >> The following columns in the training set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:1715] 2023-11-15 23:16:17,597 >> ***** Running training *****
[INFO|trainer.py:1716] 2023-11-15 23:16:17,597 >>   Num examples = 6,840
[INFO|trainer.py:1717] 2023-11-15 23:16:17,597 >>   Num Epochs = 5
[INFO|trainer.py:1718] 2023-11-15 23:16:17,597 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1721] 2023-11-15 23:16:17,598 >>   Total train batch size (w. parallel, distributed & accumulation) = 32
[INFO|trainer.py:1722] 2023-11-15 23:16:17,598 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:1723] 2023-11-15 23:16:17,598 >>   Total optimization steps = 1,070
[INFO|trainer.py:1724] 2023-11-15 23:16:17,599 >>   Number of trainable parameters = 1,488,196
[INFO|integration_utils.py:716] 2023-11-15 23:16:17,600 >> Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"
  0%|          | 0/1070 [00:00<?, ?it/s]  0%|          | 1/1070 [00:00<17:41,  1.01it/s]  0%|          | 3/1070 [00:01<05:24,  3.29it/s]  0%|          | 5/1070 [00:01<03:11,  5.57it/s]  1%|          | 7/1070 [00:01<02:17,  7.71it/s]  1%|          | 9/1070 [00:01<01:50,  9.59it/s]  1%|          | 11/1070 [00:01<01:34, 11.19it/s]  1%|          | 13/1070 [00:01<01:24, 12.47it/s]  1%|▏         | 15/1070 [00:01<01:18, 13.47it/s]  2%|▏         | 17/1070 [00:01<01:14, 14.20it/s]  2%|▏         | 19/1070 [00:02<01:11, 14.74it/s]  2%|▏         | 21/1070 [00:02<01:09, 15.09it/s]  2%|▏         | 23/1070 [00:02<01:08, 15.29it/s]  2%|▏         | 25/1070 [00:02<01:07, 15.37it/s]  3%|▎         | 27/1070 [00:02<01:07, 15.50it/s]  3%|▎         | 29/1070 [00:02<01:06, 15.55it/s]  3%|▎         | 31/1070 [00:02<01:07, 15.45it/s]  3%|▎         | 33/1070 [00:03<01:06, 15.52it/s]  3%|▎         | 35/1070 [00:03<01:06, 15.53it/s]  3%|▎         | 37/1070 [00:03<01:06, 15.57it/s]  4%|▎         | 39/1070 [00:03<01:05, 15.64it/s]  4%|▍         | 41/1070 [00:03<01:05, 15.74it/s]  4%|▍         | 43/1070 [00:03<01:05, 15.75it/s]  4%|▍         | 45/1070 [00:03<01:05, 15.77it/s]  4%|▍         | 47/1070 [00:03<01:05, 15.72it/s]  5%|▍         | 49/1070 [00:04<01:04, 15.73it/s]  5%|▍         | 51/1070 [00:04<01:04, 15.73it/s]  5%|▍         | 53/1070 [00:04<01:04, 15.76it/s]  5%|▌         | 55/1070 [00:04<01:04, 15.86it/s]  5%|▌         | 57/1070 [00:04<01:03, 15.87it/s]  6%|▌         | 59/1070 [00:04<01:03, 15.90it/s]  6%|▌         | 61/1070 [00:04<01:03, 15.88it/s]  6%|▌         | 63/1070 [00:04<01:03, 15.80it/s]  6%|▌         | 65/1070 [00:05<01:04, 15.59it/s]  6%|▋         | 67/1070 [00:05<01:04, 15.54it/s]  6%|▋         | 69/1070 [00:05<01:04, 15.40it/s]  7%|▋         | 71/1070 [00:05<01:04, 15.53it/s]  7%|▋         | 73/1070 [00:05<01:03, 15.67it/s]  7%|▋         | 75/1070 [00:05<01:02, 15.82it/s]  7%|▋         | 77/1070 [00:05<01:02, 15.94it/s]  7%|▋         | 79/1070 [00:05<01:01, 16.05it/s]  8%|▊         | 81/1070 [00:06<01:01, 16.08it/s]  8%|▊         | 83/1070 [00:06<01:01, 16.09it/s]  8%|▊         | 85/1070 [00:06<01:01, 16.11it/s]  8%|▊         | 87/1070 [00:06<01:00, 16.18it/s]  8%|▊         | 89/1070 [00:06<01:00, 16.22it/s]  9%|▊         | 91/1070 [00:06<01:00, 16.23it/s]  9%|▊         | 93/1070 [00:06<01:00, 16.22it/s]  9%|▉         | 95/1070 [00:06<01:00, 16.21it/s]  9%|▉         | 97/1070 [00:07<01:00, 16.21it/s]  9%|▉         | 99/1070 [00:07<00:59, 16.22it/s]  9%|▉         | 101/1070 [00:07<00:59, 16.25it/s] 10%|▉         | 103/1070 [00:07<00:59, 16.26it/s] 10%|▉         | 105/1070 [00:07<00:59, 16.24it/s] 10%|█         | 107/1070 [00:07<00:59, 16.23it/s] 10%|█         | 109/1070 [00:07<00:59, 16.20it/s] 10%|█         | 111/1070 [00:07<00:59, 16.13it/s] 11%|█         | 113/1070 [00:08<00:59, 16.05it/s] 11%|█         | 115/1070 [00:08<00:59, 16.00it/s] 11%|█         | 117/1070 [00:08<01:00, 15.64it/s] 11%|█         | 119/1070 [00:08<01:01, 15.39it/s] 11%|█▏        | 121/1070 [00:08<01:01, 15.51it/s] 11%|█▏        | 123/1070 [00:08<01:00, 15.64it/s] 12%|█▏        | 125/1070 [00:08<01:00, 15.71it/s] 12%|█▏        | 127/1070 [00:08<00:59, 15.81it/s] 12%|█▏        | 129/1070 [00:09<01:00, 15.67it/s] 12%|█▏        | 131/1070 [00:09<01:00, 15.54it/s] 12%|█▏        | 133/1070 [00:09<00:59, 15.70it/s] 13%|█▎        | 135/1070 [00:09<00:59, 15.82it/s] 13%|█▎        | 137/1070 [00:09<00:58, 15.91it/s] 13%|█▎        | 139/1070 [00:09<00:58, 15.88it/s] 13%|█▎        | 141/1070 [00:09<00:58, 15.89it/s] 13%|█▎        | 143/1070 [00:09<00:58, 15.91it/s] 14%|█▎        | 145/1070 [00:10<00:57, 15.99it/s] 14%|█▎        | 147/1070 [00:10<00:57, 16.03it/s] 14%|█▍        | 149/1070 [00:10<00:57, 16.02it/s] 14%|█▍        | 151/1070 [00:10<00:57, 15.96it/s] 14%|█▍        | 153/1070 [00:10<00:57, 15.93it/s] 14%|█▍        | 155/1070 [00:10<00:57, 15.85it/s] 15%|█▍        | 157/1070 [00:10<00:57, 15.82it/s] 15%|█▍        | 159/1070 [00:10<00:57, 15.83it/s] 15%|█▌        | 161/1070 [00:11<00:57, 15.86it/s] 15%|█▌        | 163/1070 [00:11<00:57, 15.84it/s] 15%|█▌        | 165/1070 [00:11<00:56, 15.93it/s] 16%|█▌        | 167/1070 [00:11<00:56, 16.01it/s] 16%|█▌        | 169/1070 [00:11<00:56, 16.07it/s] 16%|█▌        | 171/1070 [00:11<00:55, 16.11it/s] 16%|█▌        | 173/1070 [00:11<00:55, 16.12it/s] 16%|█▋        | 175/1070 [00:11<00:55, 16.11it/s] 17%|█▋        | 177/1070 [00:12<00:55, 16.14it/s] 17%|█▋        | 179/1070 [00:12<00:55, 16.17it/s] 17%|█▋        | 181/1070 [00:12<00:54, 16.18it/s] 17%|█▋        | 183/1070 [00:12<00:54, 16.13it/s] 17%|█▋        | 185/1070 [00:12<00:54, 16.12it/s] 17%|█▋        | 187/1070 [00:12<00:54, 16.17it/s] 18%|█▊        | 189/1070 [00:12<00:54, 16.13it/s] 18%|█▊        | 191/1070 [00:12<00:54, 16.07it/s] 18%|█▊        | 193/1070 [00:13<00:54, 16.07it/s] 18%|█▊        | 195/1070 [00:13<00:54, 16.08it/s] 18%|█▊        | 197/1070 [00:13<00:54, 16.07it/s] 19%|█▊        | 199/1070 [00:13<00:54, 16.09it/s] 19%|█▉        | 201/1070 [00:13<00:54, 15.96it/s] 19%|█▉        | 203/1070 [00:13<00:54, 15.92it/s] 19%|█▉        | 205/1070 [00:13<00:54, 15.88it/s] 19%|█▉        | 207/1070 [00:13<00:54, 15.84it/s] 20%|█▉        | 209/1070 [00:14<00:54, 15.82it/s] 20%|█▉        | 211/1070 [00:14<00:54, 15.84it/s] 20%|█▉        | 213/1070 [00:14<00:54, 15.83it/s]                                                   20%|██        | 214/1070 [00:14<00:54, 15.83it/s][INFO|trainer.py:755] 2023-11-15 23:16:31,959 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 23:16:31,961 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 23:16:31,961 >>   Num examples = 760
[INFO|trainer.py:3134] 2023-11-15 23:16:31,961 >>   Batch size = 8
{'loss': 0.4476, 'learning_rate': 0.0004, 'epoch': 1.0}

  0%|          | 0/95 [00:00<?, ?it/s][A
 11%|█         | 10/95 [00:00<00:00, 91.10it/s][A
 21%|██        | 20/95 [00:00<00:00, 86.08it/s][A
 31%|███       | 29/95 [00:00<00:00, 83.61it/s][A
 40%|████      | 38/95 [00:00<00:00, 82.22it/s][A
 49%|████▉     | 47/95 [00:00<00:00, 81.73it/s][A
 59%|█████▉    | 56/95 [00:00<00:00, 82.18it/s][A
 68%|██████▊   | 65/95 [00:00<00:00, 81.66it/s][A
 78%|███████▊  | 74/95 [00:00<00:00, 81.73it/s][A
 87%|████████▋ | 83/95 [00:01<00:00, 81.47it/s][A
 97%|█████████▋| 92/95 [00:01<00:00, 81.52it/s][A                                                  
                                               [A 20%|██        | 214/1070 [00:15<00:54, 15.83it/s]
100%|██████████| 95/95 [00:01<00:00, 81.52it/s][A
                                               [A 20%|██        | 215/1070 [00:15<03:27,  4.12it/s] 20%|██        | 217/1070 [00:15<02:40,  5.31it/s] 20%|██        | 219/1070 [00:15<02:08,  6.63it/s] 21%|██        | 221/1070 [00:16<01:46,  8.00it/s] 21%|██        | 223/1070 [00:16<01:30,  9.38it/s] 21%|██        | 225/1070 [00:16<01:19, 10.64it/s] 21%|██        | 227/1070 [00:16<01:11, 11.80it/s] 21%|██▏       | 229/1070 [00:16<01:05, 12.76it/s] 22%|██▏       | 231/1070 [00:16<01:01, 13.56it/s] 22%|██▏       | 233/1070 [00:16<00:58, 14.22it/s] 22%|██▏       | 235/1070 [00:16<00:56, 14.75it/s] 22%|██▏       | 237/1070 [00:17<00:55, 15.07it/s] 22%|██▏       | 239/1070 [00:17<00:54, 15.32it/s] 23%|██▎       | 241/1070 [00:17<00:53, 15.49it/s] 23%|██▎       | 243/1070 [00:17<00:52, 15.67it/s] 23%|██▎       | 245/1070 [00:17<00:52, 15.81it/s] 23%|██▎       | 247/1070 [00:17<00:51, 15.88it/s] 23%|██▎       | 249/1070 [00:17<00:51, 15.93it/s] 23%|██▎       | 251/1070 [00:17<00:51, 15.97it/s] 24%|██▎       | 253/1070 [00:18<00:51, 16.02it/s] 24%|██▍       | 255/1070 [00:18<00:50, 16.01it/s] 24%|██▍       | 257/1070 [00:18<00:50, 16.02it/s] 24%|██▍       | 259/1070 [00:18<00:50, 16.03it/s] 24%|██▍       | 261/1070 [00:18<00:50, 16.07it/s] 25%|██▍       | 263/1070 [00:18<00:50, 16.08it/s] 25%|██▍       | 265/1070 [00:18<00:50, 16.07it/s] 25%|██▍       | 267/1070 [00:18<00:49, 16.07it/s] 25%|██▌       | 269/1070 [00:19<00:49, 16.10it/s] 25%|██▌       | 271/1070 [00:19<00:49, 16.05it/s] 26%|██▌       | 273/1070 [00:19<00:49, 15.94it/s] 26%|██▌       | 275/1070 [00:19<00:50, 15.88it/s] 26%|██▌       | 277/1070 [00:19<00:50, 15.80it/s] 26%|██▌       | 279/1070 [00:19<00:50, 15.71it/s] 26%|██▋       | 281/1070 [00:19<00:50, 15.75it/s] 26%|██▋       | 283/1070 [00:19<00:49, 15.75it/s] 27%|██▋       | 285/1070 [00:20<00:49, 15.74it/s] 27%|██▋       | 287/1070 [00:20<00:49, 15.78it/s] 27%|██▋       | 289/1070 [00:20<00:49, 15.79it/s] 27%|██▋       | 291/1070 [00:20<00:49, 15.78it/s] 27%|██▋       | 293/1070 [00:20<00:49, 15.85it/s] 28%|██▊       | 295/1070 [00:20<00:48, 15.85it/s] 28%|██▊       | 297/1070 [00:20<00:49, 15.77it/s] 28%|██▊       | 299/1070 [00:20<00:48, 15.81it/s] 28%|██▊       | 301/1070 [00:21<00:48, 15.83it/s] 28%|██▊       | 303/1070 [00:21<00:48, 15.81it/s] 29%|██▊       | 305/1070 [00:21<00:48, 15.84it/s] 29%|██▊       | 307/1070 [00:21<00:48, 15.81it/s] 29%|██▉       | 309/1070 [00:21<00:50, 15.15it/s] 29%|██▉       | 311/1070 [00:21<00:51, 14.75it/s] 29%|██▉       | 313/1070 [00:21<00:52, 14.49it/s] 29%|██▉       | 315/1070 [00:21<00:51, 14.54it/s] 30%|██▉       | 317/1070 [00:22<00:51, 14.72it/s] 30%|██▉       | 319/1070 [00:22<00:51, 14.65it/s] 30%|███       | 321/1070 [00:22<00:50, 14.74it/s] 30%|███       | 323/1070 [00:22<00:50, 14.92it/s] 30%|███       | 325/1070 [00:22<00:49, 15.06it/s] 31%|███       | 327/1070 [00:22<00:48, 15.19it/s] 31%|███       | 329/1070 [00:22<00:48, 15.32it/s] 31%|███       | 331/1070 [00:23<00:47, 15.46it/s] 31%|███       | 333/1070 [00:23<00:47, 15.49it/s] 31%|███▏      | 335/1070 [00:23<00:47, 15.61it/s] 31%|███▏      | 337/1070 [00:23<00:46, 15.68it/s] 32%|███▏      | 339/1070 [00:23<00:46, 15.75it/s] 32%|███▏      | 341/1070 [00:23<00:46, 15.84it/s] 32%|███▏      | 343/1070 [00:23<00:45, 15.91it/s] 32%|███▏      | 345/1070 [00:23<00:45, 15.98it/s] 32%|███▏      | 347/1070 [00:24<00:45, 16.01it/s] 33%|███▎      | 349/1070 [00:24<00:44, 16.03it/s] 33%|███▎      | 351/1070 [00:24<00:44, 16.06it/s] 33%|███▎      | 353/1070 [00:24<00:44, 16.11it/s] 33%|███▎      | 355/1070 [00:24<00:44, 16.12it/s] 33%|███▎      | 357/1070 [00:24<00:44, 16.07it/s] 34%|███▎      | 359/1070 [00:24<00:44, 16.05it/s] 34%|███▎      | 361/1070 [00:24<00:44, 16.08it/s] 34%|███▍      | 363/1070 [00:25<00:44, 16.03it/s] 34%|███▍      | 365/1070 [00:25<00:44, 15.97it/s] 34%|███▍      | 367/1070 [00:25<00:48, 14.42it/s] 34%|███▍      | 369/1070 [00:25<00:50, 13.94it/s] 35%|███▍      | 371/1070 [00:25<00:48, 14.35it/s] 35%|███▍      | 373/1070 [00:25<00:47, 14.62it/s] 35%|███▌      | 375/1070 [00:25<00:46, 14.85it/s] 35%|███▌      | 377/1070 [00:26<00:46, 14.98it/s] 35%|███▌      | 379/1070 [00:26<00:45, 15.12it/s] 36%|███▌      | 381/1070 [00:26<00:45, 15.11it/s] 36%|███▌      | 383/1070 [00:26<00:45, 15.15it/s] 36%|███▌      | 385/1070 [00:26<00:45, 15.18it/s] 36%|███▌      | 387/1070 [00:26<00:44, 15.20it/s] 36%|███▋      | 389/1070 [00:26<00:44, 15.24it/s] 37%|███▋      | 391/1070 [00:26<00:44, 15.27it/s] 37%|███▋      | 393/1070 [00:27<00:44, 15.32it/s] 37%|███▋      | 395/1070 [00:27<00:43, 15.37it/s] 37%|███▋      | 397/1070 [00:27<00:43, 15.39it/s] 37%|███▋      | 399/1070 [00:27<00:43, 15.36it/s] 37%|███▋      | 401/1070 [00:27<00:43, 15.36it/s] 38%|███▊      | 403/1070 [00:27<00:43, 15.36it/s] 38%|███▊      | 405/1070 [00:27<00:43, 15.39it/s] 38%|███▊      | 407/1070 [00:27<00:43, 15.40it/s] 38%|███▊      | 409/1070 [00:28<00:42, 15.44it/s] 38%|███▊      | 411/1070 [00:28<00:42, 15.50it/s] 39%|███▊      | 413/1070 [00:28<00:42, 15.49it/s] 39%|███▉      | 415/1070 [00:28<00:42, 15.52it/s] 39%|███▉      | 417/1070 [00:28<00:42, 15.51it/s] 39%|███▉      | 419/1070 [00:28<00:41, 15.54it/s] 39%|███▉      | 421/1070 [00:28<00:41, 15.53it/s] 40%|███▉      | 423/1070 [00:28<00:41, 15.57it/s] 40%|███▉      | 425/1070 [00:29<00:41, 15.57it/s] 40%|███▉      | 427/1070 [00:29<00:41, 15.58it/s]                                                   40%|████      | 428/1070 [00:29<00:41, 15.58it/s][INFO|trainer.py:755] 2023-11-15 23:16:46,898 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 23:16:46,900 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 23:16:46,900 >>   Num examples = 760
[INFO|trainer.py:3134] 2023-11-15 23:16:46,901 >>   Batch size = 8
{'eval_loss': 0.3093966841697693, 'eval_accuracy': 0.8881578947368421, 'eval_micro_f1': 0.8881578947368421, 'eval_macro_f1': 0.886262306228385, 'eval_runtime': 1.2006, 'eval_samples_per_second': 633.004, 'eval_steps_per_second': 79.126, 'epoch': 1.0}
{'loss': 0.2632, 'learning_rate': 0.0003, 'epoch': 2.0}

  0%|          | 0/95 [00:00<?, ?it/s][A
  9%|▉         | 9/95 [00:00<00:00, 88.85it/s][A
 19%|█▉        | 18/95 [00:00<00:00, 83.69it/s][A
 28%|██▊       | 27/95 [00:00<00:00, 81.79it/s][A
 38%|███▊      | 36/95 [00:00<00:00, 81.73it/s][A
 47%|████▋     | 45/95 [00:00<00:00, 80.25it/s][A
 57%|█████▋    | 54/95 [00:00<00:00, 81.18it/s][A
 66%|██████▋   | 63/95 [00:00<00:00, 80.36it/s][A
 76%|███████▌  | 72/95 [00:00<00:00, 81.11it/s][A
 85%|████████▌ | 81/95 [00:00<00:00, 80.69it/s][A
 95%|█████████▍| 90/95 [00:01<00:00, 80.33it/s][A                                                  
                                               [A 40%|████      | 428/1070 [00:30<00:41, 15.58it/s]
100%|██████████| 95/95 [00:01<00:00, 80.33it/s][A
                                               [A 40%|████      | 429/1070 [00:30<02:37,  4.06it/s] 40%|████      | 431/1070 [00:30<02:02,  5.22it/s] 40%|████      | 433/1070 [00:30<01:37,  6.51it/s] 41%|████      | 435/1070 [00:30<01:20,  7.89it/s] 41%|████      | 437/1070 [00:31<01:08,  9.25it/s] 41%|████      | 439/1070 [00:31<00:59, 10.56it/s] 41%|████      | 441/1070 [00:31<00:53, 11.67it/s] 41%|████▏     | 443/1070 [00:31<00:49, 12.63it/s] 42%|████▏     | 445/1070 [00:31<00:46, 13.41it/s] 42%|████▏     | 447/1070 [00:31<00:44, 13.99it/s] 42%|████▏     | 449/1070 [00:31<00:42, 14.45it/s] 42%|████▏     | 451/1070 [00:31<00:41, 14.75it/s] 42%|████▏     | 453/1070 [00:32<00:41, 15.00it/s] 43%|████▎     | 455/1070 [00:32<00:40, 15.19it/s] 43%|████▎     | 457/1070 [00:32<00:40, 15.28it/s] 43%|████▎     | 459/1070 [00:32<00:39, 15.39it/s] 43%|████▎     | 461/1070 [00:32<00:39, 15.41it/s] 43%|████▎     | 463/1070 [00:32<00:39, 15.50it/s] 43%|████▎     | 465/1070 [00:32<00:38, 15.52it/s] 44%|████▎     | 467/1070 [00:33<00:38, 15.55it/s] 44%|████▍     | 469/1070 [00:33<00:38, 15.54it/s] 44%|████▍     | 471/1070 [00:33<00:38, 15.53it/s] 44%|████▍     | 473/1070 [00:33<00:38, 15.57it/s] 44%|████▍     | 475/1070 [00:33<00:38, 15.60it/s] 45%|████▍     | 477/1070 [00:33<00:37, 15.64it/s] 45%|████▍     | 479/1070 [00:33<00:37, 15.67it/s] 45%|████▍     | 481/1070 [00:33<00:37, 15.63it/s] 45%|████▌     | 483/1070 [00:34<00:37, 15.66it/s] 45%|████▌     | 485/1070 [00:34<00:37, 15.65it/s] 46%|████▌     | 487/1070 [00:34<00:37, 15.61it/s] 46%|████▌     | 489/1070 [00:34<00:37, 15.63it/s] 46%|████▌     | 491/1070 [00:34<00:37, 15.60it/s] 46%|████▌     | 493/1070 [00:34<00:36, 15.60it/s] 46%|████▋     | 495/1070 [00:34<00:36, 15.60it/s] 46%|████▋     | 497/1070 [00:34<00:36, 15.64it/s] 47%|████▋     | 499/1070 [00:35<00:36, 15.63it/s] 47%|████▋     | 501/1070 [00:35<00:36, 15.62it/s] 47%|████▋     | 503/1070 [00:35<00:36, 15.59it/s] 47%|████▋     | 505/1070 [00:35<00:36, 15.55it/s] 47%|████▋     | 507/1070 [00:35<00:36, 15.61it/s] 48%|████▊     | 509/1070 [00:35<00:35, 15.60it/s] 48%|████▊     | 511/1070 [00:35<00:35, 15.60it/s] 48%|████▊     | 513/1070 [00:35<00:35, 15.63it/s] 48%|████▊     | 515/1070 [00:36<00:35, 15.62it/s] 48%|████▊     | 517/1070 [00:36<00:35, 15.66it/s] 49%|████▊     | 519/1070 [00:36<00:35, 15.67it/s] 49%|████▊     | 521/1070 [00:36<00:35, 15.64it/s] 49%|████▉     | 523/1070 [00:36<00:34, 15.67it/s] 49%|████▉     | 525/1070 [00:36<00:34, 15.62it/s] 49%|████▉     | 527/1070 [00:36<00:34, 15.61it/s] 49%|████▉     | 529/1070 [00:36<00:34, 15.60it/s] 50%|████▉     | 531/1070 [00:37<00:34, 15.62it/s] 50%|████▉     | 533/1070 [00:37<00:34, 15.64it/s] 50%|█████     | 535/1070 [00:37<00:34, 15.58it/s] 50%|█████     | 537/1070 [00:37<00:34, 15.59it/s] 50%|█████     | 539/1070 [00:37<00:34, 15.55it/s] 51%|█████     | 541/1070 [00:37<00:33, 15.59it/s] 51%|█████     | 543/1070 [00:37<00:33, 15.59it/s] 51%|█████     | 545/1070 [00:38<00:33, 15.59it/s] 51%|█████     | 547/1070 [00:38<00:33, 15.59it/s] 51%|█████▏    | 549/1070 [00:38<00:33, 15.57it/s] 51%|█████▏    | 551/1070 [00:38<00:33, 15.61it/s] 52%|█████▏    | 553/1070 [00:38<00:33, 15.60it/s] 52%|█████▏    | 555/1070 [00:38<00:33, 15.57it/s] 52%|█████▏    | 557/1070 [00:38<00:32, 15.61it/s] 52%|█████▏    | 559/1070 [00:38<00:32, 15.58it/s] 52%|█████▏    | 561/1070 [00:39<00:32, 15.65it/s] 53%|█████▎    | 563/1070 [00:39<00:32, 15.60it/s] 53%|█████▎    | 565/1070 [00:39<00:32, 15.60it/s] 53%|█████▎    | 567/1070 [00:39<00:32, 15.63it/s] 53%|█████▎    | 569/1070 [00:39<00:32, 15.62it/s] 53%|█████▎    | 571/1070 [00:39<00:31, 15.64it/s] 54%|█████▎    | 573/1070 [00:39<00:31, 15.64it/s] 54%|█████▎    | 575/1070 [00:39<00:31, 15.63it/s] 54%|█████▍    | 577/1070 [00:40<00:31, 15.64it/s] 54%|█████▍    | 579/1070 [00:40<00:31, 15.61it/s] 54%|█████▍    | 581/1070 [00:40<00:31, 15.65it/s] 54%|█████▍    | 583/1070 [00:40<00:31, 15.62it/s] 55%|█████▍    | 585/1070 [00:40<00:31, 15.63it/s] 55%|█████▍    | 587/1070 [00:40<00:30, 15.64it/s] 55%|█████▌    | 589/1070 [00:40<00:30, 15.62it/s] 55%|█████▌    | 591/1070 [00:40<00:30, 15.66it/s] 55%|█████▌    | 593/1070 [00:41<00:30, 15.61it/s] 56%|█████▌    | 595/1070 [00:41<00:30, 15.61it/s] 56%|█████▌    | 597/1070 [00:41<00:30, 15.64it/s] 56%|█████▌    | 599/1070 [00:41<00:30, 15.59it/s] 56%|█████▌    | 601/1070 [00:41<00:29, 15.64it/s] 56%|█████▋    | 603/1070 [00:41<00:29, 15.63it/s] 57%|█████▋    | 605/1070 [00:41<00:29, 15.60it/s] 57%|█████▋    | 607/1070 [00:41<00:29, 15.61it/s] 57%|█████▋    | 609/1070 [00:42<00:29, 15.58it/s] 57%|█████▋    | 611/1070 [00:42<00:29, 15.57it/s] 57%|█████▋    | 613/1070 [00:42<00:29, 15.55it/s] 57%|█████▋    | 615/1070 [00:42<00:29, 15.58it/s] 58%|█████▊    | 617/1070 [00:42<00:29, 15.54it/s] 58%|█████▊    | 619/1070 [00:42<00:29, 15.52it/s] 58%|█████▊    | 621/1070 [00:42<00:28, 15.53it/s] 58%|█████▊    | 623/1070 [00:43<00:28, 15.55it/s] 58%|█████▊    | 625/1070 [00:43<00:28, 15.57it/s] 59%|█████▊    | 627/1070 [00:43<00:28, 15.52it/s] 59%|█████▉    | 629/1070 [00:43<00:28, 15.55it/s] 59%|█████▉    | 631/1070 [00:43<00:28, 15.50it/s] 59%|█████▉    | 633/1070 [00:43<00:28, 15.55it/s] 59%|█████▉    | 635/1070 [00:43<00:27, 15.60it/s] 60%|█████▉    | 637/1070 [00:43<00:27, 15.59it/s] 60%|█████▉    | 639/1070 [00:44<00:27, 15.61it/s] 60%|█████▉    | 641/1070 [00:44<00:27, 15.62it/s]                                                   60%|██████    | 642/1070 [00:44<00:27, 15.62it/s][INFO|trainer.py:755] 2023-11-15 23:17:01,829 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 23:17:01,830 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 23:17:01,830 >>   Num examples = 760
[INFO|trainer.py:3134] 2023-11-15 23:17:01,831 >>   Batch size = 8
{'eval_loss': 0.26687273383140564, 'eval_accuracy': 0.9078947368421053, 'eval_micro_f1': 0.9078947368421053, 'eval_macro_f1': 0.9056270267744817, 'eval_runtime': 1.2203, 'eval_samples_per_second': 622.794, 'eval_steps_per_second': 77.849, 'epoch': 2.0}
{'loss': 0.22, 'learning_rate': 0.0002, 'epoch': 3.0}

  0%|          | 0/95 [00:00<?, ?it/s][A
  9%|▉         | 9/95 [00:00<00:00, 88.54it/s][A
 19%|█▉        | 18/95 [00:00<00:00, 83.50it/s][A
 28%|██▊       | 27/95 [00:00<00:00, 79.66it/s][A
 38%|███▊      | 36/95 [00:00<00:00, 80.29it/s][A
 47%|████▋     | 45/95 [00:00<00:00, 80.24it/s][A
 57%|█████▋    | 54/95 [00:00<00:00, 79.49it/s][A
 66%|██████▋   | 63/95 [00:00<00:00, 80.21it/s][A
 76%|███████▌  | 72/95 [00:00<00:00, 79.81it/s][A
 85%|████████▌ | 81/95 [00:01<00:00, 80.19it/s][A
 95%|█████████▍| 90/95 [00:01<00:00, 79.22it/s][A                                                  
                                               [A 60%|██████    | 642/1070 [00:45<00:27, 15.62it/s]
100%|██████████| 95/95 [00:01<00:00, 79.22it/s][A
                                               [A 60%|██████    | 643/1070 [00:45<01:46,  4.03it/s] 60%|██████    | 645/1070 [00:45<01:22,  5.18it/s] 60%|██████    | 647/1070 [00:45<01:05,  6.47it/s] 61%|██████    | 649/1070 [00:45<00:53,  7.84it/s] 61%|██████    | 651/1070 [00:46<00:45,  9.21it/s] 61%|██████    | 653/1070 [00:46<00:39, 10.48it/s] 61%|██████    | 655/1070 [00:46<00:35, 11.64it/s] 61%|██████▏   | 657/1070 [00:46<00:32, 12.56it/s] 62%|██████▏   | 659/1070 [00:46<00:30, 13.33it/s] 62%|██████▏   | 661/1070 [00:46<00:29, 13.91it/s] 62%|██████▏   | 663/1070 [00:46<00:28, 14.35it/s] 62%|██████▏   | 665/1070 [00:46<00:27, 14.67it/s] 62%|██████▏   | 667/1070 [00:47<00:27, 14.92it/s] 63%|██████▎   | 669/1070 [00:47<00:26, 15.04it/s] 63%|██████▎   | 671/1070 [00:47<00:26, 15.16it/s] 63%|██████▎   | 673/1070 [00:47<00:26, 15.26it/s] 63%|██████▎   | 675/1070 [00:47<00:25, 15.34it/s] 63%|██████▎   | 677/1070 [00:47<00:25, 15.41it/s] 63%|██████▎   | 679/1070 [00:47<00:25, 15.44it/s] 64%|██████▎   | 681/1070 [00:47<00:25, 15.46it/s] 64%|██████▍   | 683/1070 [00:48<00:25, 15.46it/s] 64%|██████▍   | 685/1070 [00:48<00:24, 15.47it/s] 64%|██████▍   | 687/1070 [00:48<00:24, 15.48it/s] 64%|██████▍   | 689/1070 [00:48<00:24, 15.43it/s] 65%|██████▍   | 691/1070 [00:48<00:24, 15.45it/s] 65%|██████▍   | 693/1070 [00:48<00:24, 15.41it/s] 65%|██████▍   | 695/1070 [00:48<00:24, 15.40it/s] 65%|██████▌   | 697/1070 [00:49<00:24, 15.43it/s] 65%|██████▌   | 699/1070 [00:49<00:23, 15.49it/s] 66%|██████▌   | 701/1070 [00:49<00:23, 15.45it/s] 66%|██████▌   | 703/1070 [00:49<00:23, 15.50it/s] 66%|██████▌   | 705/1070 [00:49<00:23, 15.50it/s] 66%|██████▌   | 707/1070 [00:49<00:23, 15.51it/s] 66%|██████▋   | 709/1070 [00:49<00:23, 15.48it/s] 66%|██████▋   | 711/1070 [00:49<00:23, 15.50it/s] 67%|██████▋   | 713/1070 [00:50<00:23, 15.47it/s] 67%|██████▋   | 715/1070 [00:50<00:22, 15.46it/s] 67%|██████▋   | 717/1070 [00:50<00:22, 15.47it/s] 67%|██████▋   | 719/1070 [00:50<00:22, 15.48it/s] 67%|██████▋   | 721/1070 [00:50<00:22, 15.51it/s] 68%|██████▊   | 723/1070 [00:50<00:22, 15.49it/s] 68%|██████▊   | 725/1070 [00:50<00:22, 15.50it/s] 68%|██████▊   | 727/1070 [00:50<00:22, 15.49it/s] 68%|██████▊   | 729/1070 [00:51<00:21, 15.52it/s] 68%|██████▊   | 731/1070 [00:51<00:21, 15.52it/s] 69%|██████▊   | 733/1070 [00:51<00:21, 15.50it/s] 69%|██████▊   | 735/1070 [00:51<00:21, 15.48it/s] 69%|██████▉   | 737/1070 [00:51<00:21, 15.50it/s] 69%|██████▉   | 739/1070 [00:51<00:21, 15.52it/s] 69%|██████▉   | 741/1070 [00:51<00:21, 15.51it/s] 69%|██████▉   | 743/1070 [00:51<00:21, 15.50it/s] 70%|██████▉   | 745/1070 [00:52<00:20, 15.50it/s] 70%|██████▉   | 747/1070 [00:52<00:20, 15.53it/s] 70%|███████   | 749/1070 [00:52<00:20, 15.50it/s] 70%|███████   | 751/1070 [00:52<00:20, 15.48it/s] 70%|███████   | 753/1070 [00:52<00:20, 15.44it/s] 71%|███████   | 755/1070 [00:52<00:20, 15.49it/s] 71%|███████   | 757/1070 [00:52<00:20, 15.48it/s] 71%|███████   | 759/1070 [00:53<00:20, 15.49it/s] 71%|███████   | 761/1070 [00:53<00:19, 15.51it/s] 71%|███████▏  | 763/1070 [00:53<00:19, 15.49it/s] 71%|███████▏  | 765/1070 [00:53<00:19, 15.50it/s] 72%|███████▏  | 767/1070 [00:53<00:19, 15.49it/s] 72%|███████▏  | 769/1070 [00:53<00:19, 15.50it/s] 72%|███████▏  | 771/1070 [00:53<00:19, 15.50it/s] 72%|███████▏  | 773/1070 [00:53<00:19, 15.54it/s] 72%|███████▏  | 775/1070 [00:54<00:19, 15.52it/s] 73%|███████▎  | 777/1070 [00:54<00:18, 15.53it/s] 73%|███████▎  | 779/1070 [00:54<00:18, 15.57it/s] 73%|███████▎  | 781/1070 [00:54<00:18, 15.51it/s] 73%|███████▎  | 783/1070 [00:54<00:18, 15.52it/s] 73%|███████▎  | 785/1070 [00:54<00:18, 15.49it/s] 74%|███████▎  | 787/1070 [00:54<00:18, 15.48it/s] 74%|███████▎  | 789/1070 [00:54<00:18, 15.44it/s] 74%|███████▍  | 791/1070 [00:55<00:18, 15.45it/s] 74%|███████▍  | 793/1070 [00:55<00:17, 15.43it/s] 74%|███████▍  | 795/1070 [00:55<00:17, 15.46it/s] 74%|███████▍  | 797/1070 [00:55<00:17, 15.41it/s] 75%|███████▍  | 799/1070 [00:55<00:17, 15.44it/s] 75%|███████▍  | 801/1070 [00:55<00:17, 15.41it/s] 75%|███████▌  | 803/1070 [00:55<00:17, 15.44it/s] 75%|███████▌  | 805/1070 [00:55<00:17, 15.40it/s] 75%|███████▌  | 807/1070 [00:56<00:17, 15.40it/s] 76%|███████▌  | 809/1070 [00:56<00:16, 15.42it/s] 76%|███████▌  | 811/1070 [00:56<00:16, 15.47it/s] 76%|███████▌  | 813/1070 [00:56<00:16, 15.48it/s] 76%|███████▌  | 815/1070 [00:56<00:16, 15.49it/s] 76%|███████▋  | 817/1070 [00:56<00:16, 15.51it/s] 77%|███████▋  | 819/1070 [00:56<00:16, 15.48it/s] 77%|███████▋  | 821/1070 [00:57<00:16, 15.47it/s] 77%|███████▋  | 823/1070 [00:57<00:15, 15.47it/s] 77%|███████▋  | 825/1070 [00:57<00:15, 15.48it/s] 77%|███████▋  | 827/1070 [00:57<00:15, 15.46it/s] 77%|███████▋  | 829/1070 [00:57<00:15, 15.47it/s] 78%|███████▊  | 831/1070 [00:57<00:15, 15.47it/s] 78%|███████▊  | 833/1070 [00:57<00:15, 15.50it/s] 78%|███████▊  | 835/1070 [00:57<00:15, 15.49it/s] 78%|███████▊  | 837/1070 [00:58<00:15, 15.51it/s] 78%|███████▊  | 839/1070 [00:58<00:14, 15.50it/s] 79%|███████▊  | 841/1070 [00:58<00:14, 15.48it/s] 79%|███████▉  | 843/1070 [00:58<00:14, 15.49it/s] 79%|███████▉  | 845/1070 [00:58<00:14, 15.49it/s] 79%|███████▉  | 847/1070 [00:58<00:14, 15.50it/s] 79%|███████▉  | 849/1070 [00:58<00:14, 15.49it/s] 80%|███████▉  | 851/1070 [00:58<00:14, 15.51it/s] 80%|███████▉  | 853/1070 [00:59<00:13, 15.52it/s] 80%|███████▉  | 855/1070 [00:59<00:13, 15.58it/s]                                                   80%|████████  | 856/1070 [00:59<00:13, 15.58it/s][INFO|trainer.py:755] 2023-11-15 23:17:16,874 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 23:17:16,876 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 23:17:16,877 >>   Num examples = 760
[INFO|trainer.py:3134] 2023-11-15 23:17:16,877 >>   Batch size = 8
{'eval_loss': 0.2893165051937103, 'eval_accuracy': 0.9118421052631579, 'eval_micro_f1': 0.9118421052631579, 'eval_macro_f1': 0.9087818724327675, 'eval_runtime': 1.2348, 'eval_samples_per_second': 615.484, 'eval_steps_per_second': 76.936, 'epoch': 3.0}
{'loss': 0.1685, 'learning_rate': 0.0001, 'epoch': 4.0}

  0%|          | 0/95 [00:00<?, ?it/s][A
  9%|▉         | 9/95 [00:00<00:00, 88.53it/s][A
 19%|█▉        | 18/95 [00:00<00:00, 81.70it/s][A
 28%|██▊       | 27/95 [00:00<00:00, 80.62it/s][A
 38%|███▊      | 36/95 [00:00<00:00, 79.78it/s][A
 47%|████▋     | 45/95 [00:00<00:00, 80.33it/s][A
 57%|█████▋    | 54/95 [00:00<00:00, 79.69it/s][A
 66%|██████▋   | 63/95 [00:00<00:00, 80.21it/s][A
 76%|███████▌  | 72/95 [00:00<00:00, 80.26it/s][A
 85%|████████▌ | 81/95 [00:01<00:00, 79.87it/s][A
 95%|█████████▍| 90/95 [00:01<00:00, 80.02it/s][A                                                  
                                               [A 80%|████████  | 856/1070 [01:00<00:13, 15.58it/s]
100%|██████████| 95/95 [00:01<00:00, 80.02it/s][A
                                               [A 80%|████████  | 857/1070 [01:00<00:52,  4.03it/s] 80%|████████  | 859/1070 [01:00<00:40,  5.18it/s] 80%|████████  | 861/1070 [01:00<00:32,  6.46it/s] 81%|████████  | 863/1070 [01:00<00:26,  7.84it/s] 81%|████████  | 865/1070 [01:01<00:22,  9.19it/s] 81%|████████  | 867/1070 [01:01<00:19, 10.47it/s] 81%|████████  | 869/1070 [01:01<00:17, 11.60it/s] 81%|████████▏ | 871/1070 [01:01<00:15, 12.57it/s] 82%|████████▏ | 873/1070 [01:01<00:14, 13.34it/s] 82%|████████▏ | 875/1070 [01:01<00:14, 13.91it/s] 82%|████████▏ | 877/1070 [01:01<00:13, 14.35it/s] 82%|████████▏ | 879/1070 [01:01<00:13, 14.68it/s] 82%|████████▏ | 881/1070 [01:02<00:12, 14.93it/s] 83%|████████▎ | 883/1070 [01:02<00:12, 15.09it/s] 83%|████████▎ | 885/1070 [01:02<00:12, 15.21it/s] 83%|████████▎ | 887/1070 [01:02<00:11, 15.25it/s] 83%|████████▎ | 889/1070 [01:02<00:11, 15.33it/s] 83%|████████▎ | 891/1070 [01:02<00:11, 15.32it/s] 83%|████████▎ | 893/1070 [01:02<00:11, 15.34it/s] 84%|████████▎ | 895/1070 [01:03<00:11, 15.35it/s] 84%|████████▍ | 897/1070 [01:03<00:11, 15.39it/s] 84%|████████▍ | 899/1070 [01:03<00:11, 15.35it/s] 84%|████████▍ | 901/1070 [01:03<00:10, 15.40it/s] 84%|████████▍ | 903/1070 [01:03<00:10, 15.42it/s] 85%|████████▍ | 905/1070 [01:03<00:10, 15.48it/s] 85%|████████▍ | 907/1070 [01:03<00:10, 15.48it/s] 85%|████████▍ | 909/1070 [01:03<00:10, 15.49it/s] 85%|████████▌ | 911/1070 [01:04<00:10, 15.50it/s] 85%|████████▌ | 913/1070 [01:04<00:10, 15.51it/s] 86%|████████▌ | 915/1070 [01:04<00:09, 15.53it/s] 86%|████████▌ | 917/1070 [01:04<00:09, 15.50it/s] 86%|████████▌ | 919/1070 [01:04<00:09, 15.51it/s] 86%|████████▌ | 921/1070 [01:04<00:09, 15.48it/s] 86%|████████▋ | 923/1070 [01:04<00:09, 15.53it/s] 86%|████████▋ | 925/1070 [01:04<00:09, 15.49it/s] 87%|████████▋ | 927/1070 [01:05<00:09, 15.45it/s] 87%|████████▋ | 929/1070 [01:05<00:09, 15.45it/s] 87%|████████▋ | 931/1070 [01:05<00:08, 15.48it/s] 87%|████████▋ | 933/1070 [01:05<00:08, 15.44it/s] 87%|████████▋ | 935/1070 [01:05<00:08, 15.45it/s] 88%|████████▊ | 937/1070 [01:05<00:08, 15.45it/s] 88%|████████▊ | 939/1070 [01:05<00:08, 15.44it/s] 88%|████████▊ | 941/1070 [01:06<00:08, 15.44it/s] 88%|████████▊ | 943/1070 [01:06<00:08, 15.42it/s] 88%|████████▊ | 945/1070 [01:06<00:08, 15.45it/s] 89%|████████▊ | 947/1070 [01:06<00:07, 15.45it/s] 89%|████████▊ | 949/1070 [01:06<00:07, 15.49it/s] 89%|████████▉ | 951/1070 [01:06<00:07, 15.50it/s] 89%|████████▉ | 953/1070 [01:06<00:07, 15.51it/s] 89%|████████▉ | 955/1070 [01:06<00:07, 15.51it/s] 89%|████████▉ | 957/1070 [01:07<00:07, 15.51it/s] 90%|████████▉ | 959/1070 [01:07<00:07, 15.52it/s] 90%|████████▉ | 961/1070 [01:07<00:07, 15.48it/s] 90%|█████████ | 963/1070 [01:07<00:06, 15.51it/s] 90%|█████████ | 965/1070 [01:07<00:06, 15.47it/s] 90%|█████████ | 967/1070 [01:07<00:06, 15.49it/s] 91%|█████████ | 969/1070 [01:07<00:06, 15.47it/s] 91%|█████████ | 971/1070 [01:07<00:06, 15.50it/s] 91%|█████████ | 973/1070 [01:08<00:06, 15.49it/s] 91%|█████████ | 975/1070 [01:08<00:06, 15.53it/s] 91%|█████████▏| 977/1070 [01:08<00:06, 15.50it/s] 91%|█████████▏| 979/1070 [01:08<00:05, 15.50it/s] 92%|█████████▏| 981/1070 [01:08<00:05, 15.50it/s] 92%|█████████▏| 983/1070 [01:08<00:05, 15.51it/s] 92%|█████████▏| 985/1070 [01:08<00:05, 15.52it/s] 92%|█████████▏| 987/1070 [01:08<00:05, 15.50it/s] 92%|█████████▏| 989/1070 [01:09<00:05, 15.51it/s] 93%|█████████▎| 991/1070 [01:09<00:05, 15.48it/s] 93%|█████████▎| 993/1070 [01:09<00:04, 15.48it/s] 93%|█████████▎| 995/1070 [01:09<00:04, 15.44it/s] 93%|█████████▎| 997/1070 [01:09<00:04, 15.45it/s] 93%|█████████▎| 999/1070 [01:09<00:04, 15.42it/s] 94%|█████████▎| 1001/1070 [01:09<00:04, 15.40it/s] 94%|█████████▎| 1003/1070 [01:10<00:04, 15.36it/s] 94%|█████████▍| 1005/1070 [01:10<00:04, 15.38it/s] 94%|█████████▍| 1007/1070 [01:10<00:04, 15.37it/s] 94%|█████████▍| 1009/1070 [01:10<00:03, 15.44it/s] 94%|█████████▍| 1011/1070 [01:10<00:03, 15.42it/s] 95%|█████████▍| 1013/1070 [01:10<00:03, 15.47it/s] 95%|█████████▍| 1015/1070 [01:10<00:03, 15.44it/s] 95%|█████████▌| 1017/1070 [01:10<00:03, 15.46it/s] 95%|█████████▌| 1019/1070 [01:11<00:03, 15.43it/s] 95%|█████████▌| 1021/1070 [01:11<00:03, 15.42it/s] 96%|█████████▌| 1023/1070 [01:11<00:03, 15.42it/s] 96%|█████████▌| 1025/1070 [01:11<00:02, 15.43it/s] 96%|█████████▌| 1027/1070 [01:11<00:02, 15.45it/s] 96%|█████████▌| 1029/1070 [01:11<00:02, 15.45it/s] 96%|█████████▋| 1031/1070 [01:11<00:02, 15.37it/s] 97%|█████████▋| 1033/1070 [01:11<00:02, 15.08it/s] 97%|█████████▋| 1035/1070 [01:12<00:02, 14.86it/s] 97%|█████████▋| 1037/1070 [01:12<00:02, 14.88it/s] 97%|█████████▋| 1039/1070 [01:12<00:02, 14.75it/s] 97%|█████████▋| 1041/1070 [01:12<00:01, 14.82it/s] 97%|█████████▋| 1043/1070 [01:12<00:01, 14.91it/s] 98%|█████████▊| 1045/1070 [01:12<00:01, 14.97it/s] 98%|█████████▊| 1047/1070 [01:12<00:01, 15.03it/s] 98%|█████████▊| 1049/1070 [01:13<00:01, 15.08it/s] 98%|█████████▊| 1051/1070 [01:13<00:01, 15.17it/s] 98%|█████████▊| 1053/1070 [01:13<00:01, 15.26it/s] 99%|█████████▊| 1055/1070 [01:13<00:00, 15.29it/s] 99%|█████████▉| 1057/1070 [01:13<00:00, 15.35it/s] 99%|█████████▉| 1059/1070 [01:13<00:00, 15.35it/s] 99%|█████████▉| 1061/1070 [01:13<00:00, 15.40it/s] 99%|█████████▉| 1063/1070 [01:13<00:00, 15.40it/s]100%|█████████▉| 1065/1070 [01:14<00:00, 15.43it/s]100%|█████████▉| 1067/1070 [01:14<00:00, 15.47it/s]100%|█████████▉| 1069/1070 [01:14<00:00, 15.50it/s]                                                   100%|██████████| 1070/1070 [01:14<00:00, 15.50it/s][INFO|trainer.py:755] 2023-11-15 23:17:31,986 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 23:17:31,987 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 23:17:31,987 >>   Num examples = 760
[INFO|trainer.py:3134] 2023-11-15 23:17:31,987 >>   Batch size = 8
{'eval_loss': 0.28559115529060364, 'eval_accuracy': 0.9157894736842105, 'eval_micro_f1': 0.9157894736842105, 'eval_macro_f1': 0.9135346983477257, 'eval_runtime': 1.232, 'eval_samples_per_second': 616.86, 'eval_steps_per_second': 77.108, 'epoch': 4.0}
{'loss': 0.1368, 'learning_rate': 0.0, 'epoch': 5.0}

  0%|          | 0/95 [00:00<?, ?it/s][A
 11%|█         | 10/95 [00:00<00:00, 91.55it/s][A
 21%|██        | 20/95 [00:00<00:00, 85.12it/s][A
 31%|███       | 29/95 [00:00<00:00, 82.67it/s][A
 40%|████      | 38/95 [00:00<00:00, 81.54it/s][A
 49%|████▉     | 47/95 [00:00<00:00, 80.67it/s][A
 59%|█████▉    | 56/95 [00:00<00:00, 79.89it/s][A
 68%|██████▊   | 65/95 [00:00<00:00, 80.25it/s][A
 78%|███████▊  | 74/95 [00:00<00:00, 79.90it/s][A
 86%|████████▋ | 82/95 [00:01<00:00, 79.91it/s][A
 95%|█████████▍| 90/95 [00:01<00:00, 78.92it/s][A                                                   
                                               [A100%|██████████| 1070/1070 [01:15<00:00, 15.50it/s]
100%|██████████| 95/95 [00:01<00:00, 78.92it/s][A
                                               [A[INFO|trainer.py:1963] 2023-11-15 23:17:33,229 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                   100%|██████████| 1070/1070 [01:15<00:00, 15.50it/s]100%|██████████| 1070/1070 [01:15<00:00, 14.15it/s]
[INFO|trainer.py:2855] 2023-11-15 23:17:33,233 >> Saving model checkpoint to ./result/agnews_sup_roberta-base_seed0_adapter
[INFO|configuration_utils.py:460] 2023-11-15 23:17:33,235 >> Configuration saved in ./result/agnews_sup_roberta-base_seed0_adapter/config.json
[INFO|modeling_utils.py:1997] 2023-11-15 23:17:34,331 >> Model weights saved in ./result/agnews_sup_roberta-base_seed0_adapter/pytorch_model.bin
[INFO|tokenization_utils_base.py:2235] 2023-11-15 23:17:34,334 >> tokenizer config file saved in ./result/agnews_sup_roberta-base_seed0_adapter/tokenizer_config.json
[INFO|tokenization_utils_base.py:2242] 2023-11-15 23:17:34,336 >> Special tokens file saved in ./result/agnews_sup_roberta-base_seed0_adapter/special_tokens_map.json
{'eval_loss': 0.29811158776283264, 'eval_accuracy': 0.9171052631578948, 'eval_micro_f1': 0.9171052631578948, 'eval_macro_f1': 0.9148591683551444, 'eval_runtime': 1.2225, 'eval_samples_per_second': 621.686, 'eval_steps_per_second': 77.711, 'epoch': 5.0}
{'train_runtime': 75.6302, 'train_samples_per_second': 452.2, 'train_steps_per_second': 14.148, 'train_loss': 0.24722969019524405, 'epoch': 5.0}
***** train metrics *****
  epoch                    =        5.0
  train_loss               =     0.2472
  train_runtime            = 0:01:15.63
  train_samples            =       6840
  train_samples_per_second =      452.2
  train_steps_per_second   =     14.148
11/15/2023 23:17:34 - INFO - __main__ - *** Evaluate ***
[INFO|trainer.py:755] 2023-11-15 23:17:34,476 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 23:17:34,477 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 23:17:34,477 >>   Num examples = 760
[INFO|trainer.py:3134] 2023-11-15 23:17:34,477 >>   Batch size = 8
  0%|          | 0/95 [00:00<?, ?it/s] 11%|█         | 10/95 [00:00<00:00, 93.22it/s] 21%|██        | 20/95 [00:00<00:00, 85.74it/s] 31%|███       | 29/95 [00:00<00:00, 83.00it/s] 40%|████      | 38/95 [00:00<00:00, 81.89it/s] 49%|████▉     | 47/95 [00:00<00:00, 80.33it/s] 59%|█████▉    | 56/95 [00:00<00:00, 79.48it/s] 68%|██████▊   | 65/95 [00:00<00:00, 80.90it/s] 78%|███████▊  | 74/95 [00:00<00:00, 80.33it/s] 87%|████████▋ | 83/95 [00:01<00:00, 81.34it/s] 97%|█████████▋| 92/95 [00:01<00:00, 81.29it/s]100%|██████████| 95/95 [00:01<00:00, 79.58it/s]
***** eval metrics *****
  epoch                   =        5.0
  eval_accuracy           =     0.9171
  eval_loss               =     0.2981
  eval_macro_f1           =     0.9149
  eval_micro_f1           =     0.9171
  eval_runtime            = 0:00:01.20
  eval_samples            =        760
  eval_samples_per_second =     628.32
  eval_steps_per_second   =      78.54
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                  eval/accuracy ▁▆▇███
wandb:                      eval/loss █▁▅▄▆▆
wandb:                  eval/macro_f1 ▁▆▇███
wandb:                  eval/micro_f1 ▁▆▇███
wandb:                   eval/runtime ▁▅█▇▅▃
wandb:        eval/samples_per_second █▄▁▂▃▆
wandb:          eval/steps_per_second █▄▁▂▃▆
wandb:                    train/epoch ▁▁▃▃▅▅▆▆████
wandb:              train/global_step ▁▁▃▃▄▄▆▆████
wandb:            train/learning_rate █▆▅▃▁
wandb:                     train/loss █▄▃▂▁
wandb:               train/total_flos ▁
wandb:               train/train_loss ▁
wandb:            train/train_runtime ▁
wandb: train/train_samples_per_second ▁
wandb:   train/train_steps_per_second ▁
wandb: 
wandb: Run summary:
wandb:                  eval/accuracy 0.91711
wandb:                      eval/loss 0.29811
wandb:                  eval/macro_f1 0.91486
wandb:                  eval/micro_f1 0.91711
wandb:                   eval/runtime 1.2096
wandb:        eval/samples_per_second 628.32
wandb:          eval/steps_per_second 78.54
wandb:                    train/epoch 5.0
wandb:              train/global_step 1070
wandb:            train/learning_rate 0.0
wandb:                     train/loss 0.1368
wandb:               train/total_flos 1136567617228800.0
wandb:               train/train_loss 0.24723
wandb:            train/train_runtime 75.6302
wandb: train/train_samples_per_second 452.2
wandb:   train/train_steps_per_second 14.148
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /ceph/home/wangyifei/a3/wandb/offline-run-20231115_231459-9qki9wn4
wandb: Find logs at: ./wandb/offline-run-20231115_231459-9qki9wn4/logs
(ModelArguments(model_name_or_path='roberta-base', cache_dir='./cache', use_adapter=True, use_lora=False), DataTrainingArguments(dataset_name='restaurant', max_seq_length=64), TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.0005,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=./result/restaurant_roberta-base_seed1_adapter/runs/Nov15_23-17-46_node013,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.EPOCH,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=5.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
output_dir=./result/restaurant_roberta-base_seed1_adapter,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
resume_from_checkpoint=None,
run_name=./result/restaurant_roberta-base_seed1_adapter,
save_on_each_node=False,
save_safetensors=False,
save_steps=500,
save_strategy=IntervalStrategy.NO,
save_total_limit=None,
seed=222,
sharded_ddp=[],
skip_memory_metrics=True,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
))
11/15/2023 23:17:46 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False
11/15/2023 23:17:46 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.0005,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=./result/restaurant_roberta-base_seed1_adapter/runs/Nov15_23-17-45_node013,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.EPOCH,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=5.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
output_dir=./result/restaurant_roberta-base_seed1_adapter,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
resume_from_checkpoint=None,
run_name=./result/restaurant_roberta-base_seed1_adapter,
save_on_each_node=False,
save_safetensors=False,
save_steps=500,
save_strategy=IntervalStrategy.NO,
save_total_limit=None,
seed=222,
sharded_ddp=[],
skip_memory_metrics=True,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
wandb: Tracking run with wandb version 0.15.12
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
Map:   0%|          | 0/4722 [00:00<?, ? examples/s]Map:  86%|████████▌ | 4067/4722 [00:00<00:00, 40320.60 examples/s]Map: 100%|██████████| 4722/4722 [00:00<00:00, 39296.63 examples/s]
[INFO|configuration_utils.py:715] 2023-11-15 23:18:02,329 >> loading configuration file config.json from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/config.json
[INFO|configuration_utils.py:775] 2023-11-15 23:18:02,337 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.33.3",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_auto.py:535] 2023-11-15 23:18:12,353 >> Could not locate the tokenizer configuration file, will try to use the model config instead.
[INFO|configuration_utils.py:715] 2023-11-15 23:18:22,370 >> loading configuration file config.json from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/config.json
[INFO|configuration_utils.py:775] 2023-11-15 23:18:22,371 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.33.3",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:1852] 2023-11-15 23:18:42,418 >> loading file vocab.json from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/vocab.json
[INFO|tokenization_utils_base.py:1852] 2023-11-15 23:18:42,418 >> loading file merges.txt from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/merges.txt
[INFO|tokenization_utils_base.py:1852] 2023-11-15 23:18:42,418 >> loading file tokenizer.json from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/tokenizer.json
[INFO|tokenization_utils_base.py:1852] 2023-11-15 23:18:42,419 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1852] 2023-11-15 23:18:42,419 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1852] 2023-11-15 23:18:42,419 >> loading file tokenizer_config.json from cache at None
[INFO|configuration_utils.py:715] 2023-11-15 23:18:42,420 >> loading configuration file config.json from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/config.json
[INFO|configuration_utils.py:775] 2023-11-15 23:18:42,421 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.33.3",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:2862] 2023-11-15 23:19:02,641 >> loading weights file model.safetensors from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/model.safetensors
[INFO|modeling_utils.py:3638] 2023-11-15 23:19:03,369 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.bias']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3650] 2023-11-15 23:19:03,369 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
parameters 1487427
model_max_length: 512
Running tokenizer on dataset:   0%|          | 0/3777 [00:00<?, ? examples/s]Running tokenizer on dataset: 100%|██████████| 3777/3777 [00:00<00:00, 23477.74 examples/s]Running tokenizer on dataset: 100%|██████████| 3777/3777 [00:00<00:00, 23121.68 examples/s]
Running tokenizer on dataset:   0%|          | 0/945 [00:00<?, ? examples/s]Running tokenizer on dataset: 100%|██████████| 945/945 [00:00<00:00, 25470.83 examples/s]
11/15/2023 23:19:04 - INFO - __main__ - Sample 3190 of the training set: {'text': 'priced <SEP> The food is great and reasonably priced.', 'label': 0, 'input_ids': [0, 18288, 28696, 3388, 510, 15698, 20, 689, 16, 372, 8, 15646, 7663, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
11/15/2023 23:19:04 - INFO - __main__ - Sample 441 of the training set: {'text': 'dhosas <SEP> I like the somosas, chai, and the chole, but the dhosas and dhal were kinda dissapointing.', 'label': 2, 'input_ids': [0, 16593, 366, 281, 28696, 3388, 510, 15698, 38, 101, 5, 16487, 366, 281, 6, 1855, 1439, 6, 8, 5, 14310, 459, 6, 53, 5, 19480, 366, 281, 8, 385, 11124, 58, 24282, 14863, 1115, 15494, 154, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
11/15/2023 23:19:04 - INFO - __main__ - Sample 963 of the training set: {'text': 'beef carpaachio <SEP> Service was warm and attentive, beef carpaachio was exellent (huge portion) and pasta was fresh and well-prepared.', 'label': 0, 'input_ids': [0, 1610, 4550, 512, 6709, 1488, 1020, 28696, 3388, 510, 15698, 1841, 21, 3279, 8, 36670, 6, 6829, 512, 6709, 1488, 1020, 21, 1931, 1641, 1342, 36, 30214, 4745, 43, 8, 18236, 21, 2310, 8, 157, 12, 5234, 33160, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
11/15/2023 23:19:04 - WARNING - accelerate.utils.other - Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[INFO|trainer.py:755] 2023-11-15 23:19:05,700 >> The following columns in the training set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:1715] 2023-11-15 23:19:05,711 >> ***** Running training *****
[INFO|trainer.py:1716] 2023-11-15 23:19:05,712 >>   Num examples = 3,777
[INFO|trainer.py:1717] 2023-11-15 23:19:05,712 >>   Num Epochs = 5
[INFO|trainer.py:1718] 2023-11-15 23:19:05,712 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1721] 2023-11-15 23:19:05,712 >>   Total train batch size (w. parallel, distributed & accumulation) = 32
[INFO|trainer.py:1722] 2023-11-15 23:19:05,713 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:1723] 2023-11-15 23:19:05,713 >>   Total optimization steps = 595
[INFO|trainer.py:1724] 2023-11-15 23:19:05,714 >>   Number of trainable parameters = 1,487,427
[INFO|integration_utils.py:716] 2023-11-15 23:19:05,715 >> Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"
  0%|          | 0/595 [00:00<?, ?it/s]  0%|          | 1/595 [00:01<10:24,  1.05s/it]  1%|          | 3/595 [00:01<03:09,  3.13it/s]  1%|          | 5/595 [00:01<01:50,  5.33it/s]  1%|          | 7/595 [00:01<01:19,  7.41it/s]  2%|▏         | 9/595 [00:01<01:03,  9.26it/s]  2%|▏         | 11/595 [00:01<00:53, 10.82it/s]  2%|▏         | 13/595 [00:01<00:48, 12.05it/s]  3%|▎         | 15/595 [00:01<00:44, 13.02it/s]  3%|▎         | 17/595 [00:02<00:41, 13.78it/s]  3%|▎         | 19/595 [00:02<00:40, 14.36it/s]  4%|▎         | 21/595 [00:02<00:38, 14.74it/s]  4%|▍         | 23/595 [00:02<00:37, 15.07it/s]  4%|▍         | 25/595 [00:02<00:37, 15.30it/s]  5%|▍         | 27/595 [00:02<00:36, 15.42it/s]  5%|▍         | 29/595 [00:02<00:36, 15.51it/s]  5%|▌         | 31/595 [00:02<00:36, 15.61it/s]  6%|▌         | 33/595 [00:03<00:36, 15.61it/s]  6%|▌         | 35/595 [00:03<00:35, 15.64it/s]  6%|▌         | 37/595 [00:03<00:35, 15.68it/s]  7%|▋         | 39/595 [00:03<00:35, 15.69it/s]  7%|▋         | 41/595 [00:03<00:35, 15.73it/s]  7%|▋         | 43/595 [00:03<00:35, 15.70it/s]  8%|▊         | 45/595 [00:03<00:34, 15.72it/s]  8%|▊         | 47/595 [00:03<00:34, 15.73it/s]  8%|▊         | 49/595 [00:04<00:34, 15.63it/s]  9%|▊         | 51/595 [00:04<00:34, 15.64it/s]  9%|▉         | 53/595 [00:04<00:34, 15.61it/s]  9%|▉         | 55/595 [00:04<00:34, 15.58it/s] 10%|▉         | 57/595 [00:04<00:34, 15.67it/s] 10%|▉         | 59/595 [00:04<00:34, 15.65it/s] 10%|█         | 61/595 [00:04<00:33, 15.71it/s] 11%|█         | 63/595 [00:04<00:33, 15.73it/s] 11%|█         | 65/595 [00:05<00:33, 15.71it/s] 11%|█▏        | 67/595 [00:05<00:33, 15.71it/s] 12%|█▏        | 69/595 [00:05<00:33, 15.71it/s] 12%|█▏        | 71/595 [00:05<00:33, 15.68it/s] 12%|█▏        | 73/595 [00:05<00:33, 15.73it/s] 13%|█▎        | 75/595 [00:05<00:33, 15.64it/s] 13%|█▎        | 77/595 [00:05<00:33, 15.67it/s] 13%|█▎        | 79/595 [00:06<00:32, 15.65it/s] 14%|█▎        | 81/595 [00:06<00:32, 15.63it/s] 14%|█▍        | 83/595 [00:06<00:32, 15.63it/s] 14%|█▍        | 85/595 [00:06<00:32, 15.62it/s] 15%|█▍        | 87/595 [00:06<00:32, 15.64it/s] 15%|█▍        | 89/595 [00:06<00:32, 15.65it/s] 15%|█▌        | 91/595 [00:06<00:32, 15.63it/s] 16%|█▌        | 93/595 [00:06<00:32, 15.67it/s] 16%|█▌        | 95/595 [00:07<00:32, 15.62it/s] 16%|█▋        | 97/595 [00:07<00:31, 15.67it/s] 17%|█▋        | 99/595 [00:07<00:31, 15.63it/s] 17%|█▋        | 101/595 [00:07<00:31, 15.62it/s] 17%|█▋        | 103/595 [00:07<00:31, 15.68it/s] 18%|█▊        | 105/595 [00:07<00:31, 15.66it/s] 18%|█▊        | 107/595 [00:07<00:31, 15.66it/s] 18%|█▊        | 109/595 [00:07<00:30, 15.69it/s] 19%|█▊        | 111/595 [00:08<00:30, 15.69it/s] 19%|█▉        | 113/595 [00:08<00:30, 15.72it/s] 19%|█▉        | 115/595 [00:08<00:30, 15.72it/s] 20%|█▉        | 117/595 [00:08<00:30, 15.73it/s]                                                  20%|██        | 119/595 [00:08<00:30, 15.73it/s][INFO|trainer.py:755] 2023-11-15 23:19:14,256 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 23:19:14,258 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 23:19:14,258 >>   Num examples = 945
[INFO|trainer.py:3134] 2023-11-15 23:19:14,258 >>   Batch size = 8
{'loss': 0.7127, 'learning_rate': 0.0004, 'epoch': 1.0}

  0%|          | 0/119 [00:00<?, ?it/s][A
  8%|▊         | 10/119 [00:00<00:01, 90.98it/s][A
 17%|█▋        | 20/119 [00:00<00:01, 84.35it/s][A
 24%|██▍       | 29/119 [00:00<00:01, 81.75it/s][A
 32%|███▏      | 38/119 [00:00<00:01, 80.79it/s][A
 39%|███▉      | 47/119 [00:00<00:00, 80.11it/s][A
 47%|████▋     | 56/119 [00:00<00:00, 80.03it/s][A
 55%|█████▍    | 65/119 [00:00<00:00, 79.46it/s][A
 62%|██████▏   | 74/119 [00:00<00:00, 79.86it/s][A
 70%|██████▉   | 83/119 [00:01<00:00, 80.27it/s][A
 77%|███████▋  | 92/119 [00:01<00:00, 80.39it/s][A
 85%|████████▍ | 101/119 [00:01<00:00, 80.23it/s][A
 92%|█████████▏| 110/119 [00:01<00:00, 81.22it/s][A
100%|██████████| 119/119 [00:01<00:00, 80.79it/s][A                                                 
                                                 [A 20%|██        | 119/595 [00:10<00:30, 15.73it/s]
100%|██████████| 119/119 [00:01<00:00, 80.79it/s][A
                                                 [A 20%|██        | 120/595 [00:10<02:03,  3.86it/s] 21%|██        | 122/595 [00:10<01:37,  4.85it/s] 21%|██        | 124/595 [00:10<01:18,  6.02it/s] 21%|██        | 126/595 [00:10<01:04,  7.32it/s] 22%|██▏       | 128/595 [00:10<00:53,  8.69it/s] 22%|██▏       | 130/595 [00:10<00:46, 10.02it/s] 22%|██▏       | 132/595 [00:10<00:41, 11.29it/s] 23%|██▎       | 134/595 [00:11<00:37, 12.38it/s] 23%|██▎       | 136/595 [00:11<00:34, 13.27it/s] 23%|██▎       | 138/595 [00:11<00:32, 13.95it/s] 24%|██▎       | 140/595 [00:11<00:31, 14.54it/s] 24%|██▍       | 142/595 [00:11<00:30, 14.96it/s] 24%|██▍       | 144/595 [00:11<00:29, 15.28it/s] 25%|██▍       | 146/595 [00:11<00:29, 15.45it/s] 25%|██▍       | 148/595 [00:11<00:28, 15.58it/s] 25%|██▌       | 150/595 [00:12<00:28, 15.68it/s] 26%|██▌       | 152/595 [00:12<00:28, 15.75it/s] 26%|██▌       | 154/595 [00:12<00:27, 15.80it/s] 26%|██▌       | 156/595 [00:12<00:27, 15.87it/s] 27%|██▋       | 158/595 [00:12<00:27, 15.89it/s] 27%|██▋       | 160/595 [00:12<00:27, 15.89it/s] 27%|██▋       | 162/595 [00:12<00:27, 15.90it/s] 28%|██▊       | 164/595 [00:12<00:26, 15.97it/s] 28%|██▊       | 166/595 [00:13<00:26, 15.96it/s] 28%|██▊       | 168/595 [00:13<00:26, 15.95it/s] 29%|██▊       | 170/595 [00:13<00:26, 15.91it/s] 29%|██▉       | 172/595 [00:13<00:26, 15.92it/s] 29%|██▉       | 174/595 [00:13<00:26, 15.95it/s] 30%|██▉       | 176/595 [00:13<00:26, 15.96it/s] 30%|██▉       | 178/595 [00:13<00:26, 15.97it/s] 30%|███       | 180/595 [00:13<00:26, 15.92it/s] 31%|███       | 182/595 [00:14<00:28, 14.24it/s] 31%|███       | 184/595 [00:14<00:29, 13.81it/s] 31%|███▏      | 186/595 [00:14<00:28, 14.25it/s] 32%|███▏      | 188/595 [00:14<00:28, 14.54it/s] 32%|███▏      | 190/595 [00:14<00:27, 14.80it/s] 32%|███▏      | 192/595 [00:14<00:26, 14.98it/s] 33%|███▎      | 194/595 [00:14<00:26, 15.14it/s] 33%|███▎      | 196/595 [00:15<00:26, 15.20it/s] 33%|███▎      | 198/595 [00:15<00:25, 15.28it/s] 34%|███▎      | 200/595 [00:15<00:25, 15.30it/s] 34%|███▍      | 202/595 [00:15<00:25, 15.34it/s] 34%|███▍      | 204/595 [00:15<00:25, 15.34it/s] 35%|███▍      | 206/595 [00:15<00:25, 15.40it/s] 35%|███▍      | 208/595 [00:15<00:25, 15.37it/s] 35%|███▌      | 210/595 [00:15<00:24, 15.49it/s] 36%|███▌      | 212/595 [00:16<00:24, 15.48it/s] 36%|███▌      | 214/595 [00:16<00:24, 15.54it/s] 36%|███▋      | 216/595 [00:16<00:24, 15.57it/s] 37%|███▋      | 218/595 [00:16<00:24, 15.55it/s] 37%|███▋      | 220/595 [00:16<00:23, 15.63it/s] 37%|███▋      | 222/595 [00:16<00:23, 15.65it/s] 38%|███▊      | 224/595 [00:16<00:23, 15.69it/s] 38%|███▊      | 226/595 [00:16<00:23, 15.68it/s] 38%|███▊      | 228/595 [00:17<00:23, 15.69it/s] 39%|███▊      | 230/595 [00:17<00:23, 15.71it/s] 39%|███▉      | 232/595 [00:17<00:23, 15.70it/s] 39%|███▉      | 234/595 [00:17<00:22, 15.70it/s] 40%|███▉      | 236/595 [00:17<00:22, 15.71it/s]                                                  40%|████      | 238/595 [00:17<00:22, 15.71it/s][INFO|trainer.py:755] 2023-11-15 23:19:23,376 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 23:19:23,377 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 23:19:23,377 >>   Num examples = 945
[INFO|trainer.py:3134] 2023-11-15 23:19:23,378 >>   Batch size = 8
{'eval_loss': 0.48747533559799194, 'eval_accuracy': 0.7968253968253968, 'eval_micro_f1': 0.7968253968253968, 'eval_macro_f1': 0.6763891742385582, 'eval_runtime': 1.5243, 'eval_samples_per_second': 619.977, 'eval_steps_per_second': 78.071, 'epoch': 1.0}
{'loss': 0.5169, 'learning_rate': 0.0003, 'epoch': 2.0}

  0%|          | 0/119 [00:00<?, ?it/s][A
  8%|▊         | 9/119 [00:00<00:01, 87.64it/s][A
 15%|█▌        | 18/119 [00:00<00:01, 81.54it/s][A
 23%|██▎       | 27/119 [00:00<00:01, 79.15it/s][A
 29%|██▉       | 35/119 [00:00<00:01, 78.36it/s][A
 36%|███▌      | 43/119 [00:00<00:00, 77.85it/s][A
 43%|████▎     | 51/119 [00:00<00:00, 76.62it/s][A
 50%|████▉     | 59/119 [00:00<00:00, 77.01it/s][A
 56%|█████▋    | 67/119 [00:00<00:00, 77.60it/s][A
 63%|██████▎   | 75/119 [00:00<00:00, 76.70it/s][A
 70%|██████▉   | 83/119 [00:01<00:00, 76.82it/s][A
 76%|███████▋  | 91/119 [00:01<00:00, 76.30it/s][A
 83%|████████▎ | 99/119 [00:01<00:00, 77.14it/s][A
 90%|████████▉ | 107/119 [00:01<00:00, 77.28it/s][A
 97%|█████████▋| 115/119 [00:01<00:00, 77.57it/s][A                                                 
                                                 [A 40%|████      | 238/595 [00:19<00:22, 15.71it/s]
100%|██████████| 119/119 [00:01<00:00, 77.57it/s][A
                                                 [A 40%|████      | 239/595 [00:19<01:34,  3.77it/s] 41%|████      | 241/595 [00:19<01:14,  4.74it/s] 41%|████      | 243/595 [00:19<00:59,  5.89it/s] 41%|████      | 245/595 [00:19<00:48,  7.18it/s] 42%|████▏     | 247/595 [00:19<00:41,  8.49it/s] 42%|████▏     | 249/595 [00:19<00:35,  9.82it/s] 42%|████▏     | 251/595 [00:20<00:31, 11.03it/s] 43%|████▎     | 253/595 [00:20<00:28, 11.87it/s] 43%|████▎     | 255/595 [00:20<00:27, 12.49it/s] 43%|████▎     | 257/595 [00:20<00:25, 13.03it/s] 44%|████▎     | 259/595 [00:20<00:25, 13.37it/s] 44%|████▍     | 261/595 [00:20<00:24, 13.81it/s] 44%|████▍     | 263/595 [00:20<00:23, 14.24it/s] 45%|████▍     | 265/595 [00:21<00:22, 14.54it/s] 45%|████▍     | 267/595 [00:21<00:22, 14.66it/s] 45%|████▌     | 269/595 [00:21<00:22, 14.58it/s] 46%|████▌     | 271/595 [00:21<00:21, 14.82it/s] 46%|████▌     | 273/595 [00:21<00:21, 14.98it/s] 46%|████▌     | 275/595 [00:21<00:21, 15.14it/s] 47%|████▋     | 277/595 [00:21<00:20, 15.28it/s] 47%|████▋     | 279/595 [00:21<00:20, 15.37it/s] 47%|████▋     | 281/595 [00:22<00:20, 15.48it/s] 48%|████▊     | 283/595 [00:22<00:20, 15.51it/s] 48%|████▊     | 285/595 [00:22<00:19, 15.57it/s] 48%|████▊     | 287/595 [00:22<00:19, 15.59it/s] 49%|████▊     | 289/595 [00:22<00:19, 15.58it/s] 49%|████▉     | 291/595 [00:22<00:19, 15.64it/s] 49%|████▉     | 293/595 [00:22<00:19, 15.61it/s] 50%|████▉     | 295/595 [00:22<00:19, 15.59it/s] 50%|████▉     | 297/595 [00:23<00:19, 15.57it/s] 50%|█████     | 299/595 [00:23<00:18, 15.58it/s] 51%|█████     | 301/595 [00:23<00:18, 15.65it/s] 51%|█████     | 303/595 [00:23<00:18, 15.62it/s] 51%|█████▏    | 305/595 [00:23<00:18, 15.66it/s] 52%|█████▏    | 307/595 [00:23<00:18, 15.66it/s] 52%|█████▏    | 309/595 [00:23<00:18, 15.65it/s] 52%|█████▏    | 311/595 [00:23<00:18, 15.67it/s] 53%|█████▎    | 313/595 [00:24<00:18, 15.66it/s] 53%|█████▎    | 315/595 [00:24<00:17, 15.65it/s] 53%|█████▎    | 317/595 [00:24<00:17, 15.66it/s] 54%|█████▎    | 319/595 [00:24<00:17, 15.63it/s] 54%|█████▍    | 321/595 [00:24<00:17, 15.68it/s] 54%|█████▍    | 323/595 [00:24<00:17, 15.67it/s] 55%|█████▍    | 325/595 [00:24<00:17, 15.67it/s] 55%|█████▍    | 327/595 [00:25<00:17, 15.68it/s] 55%|█████▌    | 329/595 [00:25<00:16, 15.65it/s] 56%|█████▌    | 331/595 [00:25<00:16, 15.66it/s] 56%|█████▌    | 333/595 [00:25<00:16, 15.65it/s] 56%|█████▋    | 335/595 [00:25<00:16, 15.64it/s] 57%|█████▋    | 337/595 [00:25<00:16, 15.67it/s] 57%|█████▋    | 339/595 [00:25<00:16, 15.63it/s] 57%|█████▋    | 341/595 [00:25<00:16, 15.63it/s] 58%|█████▊    | 343/595 [00:26<00:16, 15.60it/s] 58%|█████▊    | 345/595 [00:26<00:16, 15.61it/s] 58%|█████▊    | 347/595 [00:26<00:15, 15.64it/s] 59%|█████▊    | 349/595 [00:26<00:15, 15.61it/s] 59%|█████▉    | 351/595 [00:26<00:15, 15.64it/s] 59%|█████▉    | 353/595 [00:26<00:15, 15.62it/s] 60%|█████▉    | 355/595 [00:26<00:15, 15.62it/s]                                                  60%|██████    | 357/595 [00:26<00:15, 15.62it/s][INFO|trainer.py:755] 2023-11-15 23:19:32,610 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 23:19:32,612 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 23:19:32,613 >>   Num examples = 945
[INFO|trainer.py:3134] 2023-11-15 23:19:32,613 >>   Batch size = 8
{'eval_loss': 0.3956124484539032, 'eval_accuracy': 0.8391534391534392, 'eval_micro_f1': 0.8391534391534392, 'eval_macro_f1': 0.745085075352455, 'eval_runtime': 1.5783, 'eval_samples_per_second': 598.744, 'eval_steps_per_second': 75.397, 'epoch': 2.0}
{'loss': 0.389, 'learning_rate': 0.0002, 'epoch': 3.0}

  0%|          | 0/119 [00:00<?, ?it/s][A
  8%|▊         | 9/119 [00:00<00:01, 89.26it/s][A
 15%|█▌        | 18/119 [00:00<00:01, 82.97it/s][A
 23%|██▎       | 27/119 [00:00<00:01, 78.66it/s][A
 29%|██▉       | 35/119 [00:00<00:01, 78.26it/s][A
 36%|███▌      | 43/119 [00:00<00:00, 77.95it/s][A
 43%|████▎     | 51/119 [00:00<00:00, 77.95it/s][A
 50%|████▉     | 59/119 [00:00<00:00, 77.60it/s][A
 56%|█████▋    | 67/119 [00:00<00:00, 77.98it/s][A
 63%|██████▎   | 75/119 [00:00<00:00, 77.49it/s][A
 70%|██████▉   | 83/119 [00:01<00:00, 77.63it/s][A
 76%|███████▋  | 91/119 [00:01<00:00, 76.85it/s][A
 83%|████████▎ | 99/119 [00:01<00:00, 75.62it/s][A
 90%|████████▉ | 107/119 [00:01<00:00, 76.03it/s][A
 97%|█████████▋| 115/119 [00:01<00:00, 76.07it/s][A                                                 
                                                 [A 60%|██████    | 357/595 [00:28<00:15, 15.62it/s]
100%|██████████| 119/119 [00:01<00:00, 76.07it/s][A
                                                 [A 60%|██████    | 358/595 [00:28<01:03,  3.74it/s] 61%|██████    | 360/595 [00:28<00:49,  4.70it/s] 61%|██████    | 362/595 [00:28<00:39,  5.84it/s] 61%|██████    | 364/595 [00:28<00:32,  7.11it/s] 62%|██████▏   | 366/595 [00:29<00:27,  8.45it/s] 62%|██████▏   | 368/595 [00:29<00:23,  9.74it/s] 62%|██████▏   | 370/595 [00:29<00:20, 10.94it/s] 63%|██████▎   | 372/595 [00:29<00:18, 11.98it/s] 63%|██████▎   | 374/595 [00:29<00:17, 12.90it/s] 63%|██████▎   | 376/595 [00:29<00:16, 13.61it/s] 64%|██████▎   | 378/595 [00:29<00:15, 14.14it/s] 64%|██████▍   | 380/595 [00:29<00:14, 14.59it/s] 64%|██████▍   | 382/595 [00:30<00:14, 14.85it/s] 65%|██████▍   | 384/595 [00:30<00:13, 15.12it/s] 65%|██████▍   | 386/595 [00:30<00:13, 15.29it/s] 65%|██████▌   | 388/595 [00:30<00:13, 15.33it/s] 66%|██████▌   | 390/595 [00:30<00:13, 15.44it/s] 66%|██████▌   | 392/595 [00:30<00:13, 15.41it/s] 66%|██████▌   | 394/595 [00:30<00:12, 15.48it/s] 67%|██████▋   | 396/595 [00:30<00:12, 15.48it/s] 67%|██████▋   | 398/595 [00:31<00:12, 15.54it/s] 67%|██████▋   | 400/595 [00:31<00:12, 15.60it/s] 68%|██████▊   | 402/595 [00:31<00:12, 15.57it/s] 68%|██████▊   | 404/595 [00:31<00:12, 15.60it/s] 68%|██████▊   | 406/595 [00:31<00:12, 15.59it/s] 69%|██████▊   | 408/595 [00:31<00:11, 15.61it/s] 69%|██████▉   | 410/595 [00:31<00:11, 15.61it/s] 69%|██████▉   | 412/595 [00:32<00:11, 15.60it/s] 70%|██████▉   | 414/595 [00:32<00:11, 15.63it/s] 70%|██████▉   | 416/595 [00:32<00:11, 15.61it/s] 70%|███████   | 418/595 [00:32<00:11, 15.61it/s] 71%|███████   | 420/595 [00:32<00:11, 15.64it/s] 71%|███████   | 422/595 [00:32<00:11, 15.63it/s] 71%|███████▏  | 424/595 [00:32<00:10, 15.65it/s] 72%|███████▏  | 426/595 [00:32<00:10, 15.63it/s] 72%|███████▏  | 428/595 [00:33<00:10, 15.64it/s] 72%|███████▏  | 430/595 [00:33<00:10, 15.62it/s] 73%|███████▎  | 432/595 [00:33<00:10, 15.59it/s] 73%|███████▎  | 434/595 [00:33<00:10, 15.62it/s] 73%|███████▎  | 436/595 [00:33<00:10, 15.57it/s] 74%|███████▎  | 438/595 [00:33<00:10, 15.59it/s] 74%|███████▍  | 440/595 [00:33<00:09, 15.56it/s] 74%|███████▍  | 442/595 [00:33<00:09, 15.58it/s] 75%|███████▍  | 444/595 [00:34<00:09, 15.62it/s] 75%|███████▍  | 446/595 [00:34<00:09, 15.60it/s] 75%|███████▌  | 448/595 [00:34<00:09, 15.62it/s] 76%|███████▌  | 450/595 [00:34<00:09, 15.61it/s] 76%|███████▌  | 452/595 [00:34<00:09, 15.60it/s] 76%|███████▋  | 454/595 [00:34<00:09, 15.61it/s] 77%|███████▋  | 456/595 [00:34<00:08, 15.57it/s] 77%|███████▋  | 458/595 [00:34<00:08, 15.59it/s] 77%|███████▋  | 460/595 [00:35<00:08, 15.56it/s] 78%|███████▊  | 462/595 [00:35<00:08, 15.57it/s] 78%|███████▊  | 464/595 [00:35<00:08, 15.56it/s] 78%|███████▊  | 466/595 [00:35<00:08, 15.57it/s] 79%|███████▊  | 468/595 [00:35<00:08, 15.59it/s] 79%|███████▉  | 470/595 [00:35<00:08, 15.58it/s] 79%|███████▉  | 472/595 [00:35<00:07, 15.62it/s] 80%|███████▉  | 474/595 [00:35<00:07, 15.61it/s]                                                  80%|████████  | 476/595 [00:36<00:07, 15.61it/s][INFO|trainer.py:755] 2023-11-15 23:19:41,801 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 23:19:41,803 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 23:19:41,803 >>   Num examples = 945
[INFO|trainer.py:3134] 2023-11-15 23:19:41,803 >>   Batch size = 8
{'eval_loss': 0.3594565987586975, 'eval_accuracy': 0.8465608465608465, 'eval_micro_f1': 0.8465608465608465, 'eval_macro_f1': 0.7847825840707268, 'eval_runtime': 1.5882, 'eval_samples_per_second': 595.003, 'eval_steps_per_second': 74.926, 'epoch': 3.0}
{'loss': 0.3134, 'learning_rate': 0.0001, 'epoch': 4.0}

  0%|          | 0/119 [00:00<?, ?it/s][A
  8%|▊         | 9/119 [00:00<00:01, 88.71it/s][A
 15%|█▌        | 18/119 [00:00<00:01, 82.66it/s][A
 23%|██▎       | 27/119 [00:00<00:01, 80.20it/s][A
 30%|███       | 36/119 [00:00<00:01, 80.02it/s][A
 38%|███▊      | 45/119 [00:00<00:00, 78.62it/s][A
 45%|████▍     | 53/119 [00:00<00:00, 77.87it/s][A
 51%|█████▏    | 61/119 [00:00<00:00, 77.72it/s][A
 58%|█████▊    | 69/119 [00:00<00:00, 78.20it/s][A
 65%|██████▍   | 77/119 [00:00<00:00, 77.36it/s][A
 71%|███████▏  | 85/119 [00:01<00:00, 77.06it/s][A
 78%|███████▊  | 93/119 [00:01<00:00, 77.26it/s][A
 85%|████████▍ | 101/119 [00:01<00:00, 76.98it/s][A
 92%|█████████▏| 109/119 [00:01<00:00, 77.30it/s][A
 98%|█████████▊| 117/119 [00:01<00:00, 77.72it/s][A                                                 
                                                 [A 80%|████████  | 476/595 [00:37<00:07, 15.61it/s]
100%|██████████| 119/119 [00:01<00:00, 77.72it/s][A
                                                 [A 80%|████████  | 477/595 [00:37<00:31,  3.78it/s] 81%|████████  | 479/595 [00:37<00:24,  4.76it/s] 81%|████████  | 481/595 [00:37<00:19,  5.91it/s] 81%|████████  | 483/595 [00:38<00:15,  7.18it/s] 82%|████████▏ | 485/595 [00:38<00:12,  8.51it/s] 82%|████████▏ | 487/595 [00:38<00:10,  9.82it/s] 82%|████████▏ | 489/595 [00:38<00:09, 11.02it/s] 83%|████████▎ | 491/595 [00:38<00:08, 12.07it/s] 83%|████████▎ | 493/595 [00:38<00:07, 12.92it/s] 83%|████████▎ | 495/595 [00:38<00:07, 13.59it/s] 84%|████████▎ | 497/595 [00:39<00:06, 14.16it/s] 84%|████████▍ | 499/595 [00:39<00:06, 14.54it/s] 84%|████████▍ | 501/595 [00:39<00:06, 14.86it/s] 85%|████████▍ | 503/595 [00:39<00:06, 15.07it/s] 85%|████████▍ | 505/595 [00:39<00:05, 15.22it/s] 85%|████████▌ | 507/595 [00:39<00:05, 15.36it/s] 86%|████████▌ | 509/595 [00:39<00:05, 15.40it/s] 86%|████████▌ | 511/595 [00:39<00:05, 15.48it/s] 86%|████████▌ | 513/595 [00:40<00:05, 15.51it/s] 87%|████████▋ | 515/595 [00:40<00:05, 15.54it/s] 87%|████████▋ | 517/595 [00:40<00:05, 15.58it/s] 87%|████████▋ | 519/595 [00:40<00:04, 15.54it/s] 88%|████████▊ | 521/595 [00:40<00:04, 15.58it/s] 88%|████████▊ | 523/595 [00:40<00:04, 15.58it/s] 88%|████████▊ | 525/595 [00:40<00:04, 15.58it/s] 89%|████████▊ | 527/595 [00:40<00:04, 15.57it/s] 89%|████████▉ | 529/595 [00:41<00:04, 15.55it/s] 89%|████████▉ | 531/595 [00:41<00:04, 15.57it/s] 90%|████████▉ | 533/595 [00:41<00:03, 15.54it/s] 90%|████████▉ | 535/595 [00:41<00:03, 15.58it/s] 90%|█████████ | 537/595 [00:41<00:03, 15.55it/s] 91%|█████████ | 539/595 [00:41<00:03, 15.59it/s] 91%|█████████ | 541/595 [00:41<00:03, 15.60it/s] 91%|█████████▏| 543/595 [00:41<00:03, 15.57it/s] 92%|█████████▏| 545/595 [00:42<00:03, 15.57it/s] 92%|█████████▏| 547/595 [00:42<00:03, 15.52it/s] 92%|█████████▏| 549/595 [00:42<00:02, 15.53it/s] 93%|█████████▎| 551/595 [00:42<00:02, 15.53it/s] 93%|█████████▎| 553/595 [00:42<00:02, 15.56it/s] 93%|█████████▎| 555/595 [00:42<00:02, 15.55it/s] 94%|█████████▎| 557/595 [00:42<00:02, 15.56it/s] 94%|█████████▍| 559/595 [00:42<00:02, 15.63it/s] 94%|█████████▍| 561/595 [00:43<00:02, 15.56it/s] 95%|█████████▍| 563/595 [00:43<00:02, 15.62it/s] 95%|█████████▍| 565/595 [00:43<00:01, 15.59it/s] 95%|█████████▌| 567/595 [00:43<00:01, 15.61it/s] 96%|█████████▌| 569/595 [00:43<00:01, 15.61it/s] 96%|█████████▌| 571/595 [00:43<00:01, 15.58it/s] 96%|█████████▋| 573/595 [00:43<00:01, 15.57it/s] 97%|█████████▋| 575/595 [00:44<00:01, 15.56it/s] 97%|█████████▋| 577/595 [00:44<00:01, 15.57it/s] 97%|█████████▋| 579/595 [00:44<00:01, 15.57it/s] 98%|█████████▊| 581/595 [00:44<00:00, 15.55it/s] 98%|█████████▊| 583/595 [00:44<00:00, 15.57it/s] 98%|█████████▊| 585/595 [00:44<00:00, 15.55it/s] 99%|█████████▊| 587/595 [00:44<00:00, 15.57it/s] 99%|█████████▉| 589/595 [00:44<00:00, 15.56it/s] 99%|█████████▉| 591/595 [00:45<00:00, 15.59it/s]100%|█████████▉| 593/595 [00:45<00:00, 15.57it/s]                                                 100%|██████████| 595/595 [00:45<00:00, 15.57it/s][INFO|trainer.py:755] 2023-11-15 23:19:50,977 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 23:19:50,978 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 23:19:50,979 >>   Num examples = 945
[INFO|trainer.py:3134] 2023-11-15 23:19:50,979 >>   Batch size = 8
{'eval_loss': 0.33377620577812195, 'eval_accuracy': 0.8740740740740741, 'eval_micro_f1': 0.8740740740740742, 'eval_macro_f1': 0.8084874442950625, 'eval_runtime': 1.5664, 'eval_samples_per_second': 603.282, 'eval_steps_per_second': 75.969, 'epoch': 4.0}
{'loss': 0.2583, 'learning_rate': 0.0, 'epoch': 5.0}

  0%|          | 0/119 [00:00<?, ?it/s][A
  8%|▊         | 9/119 [00:00<00:01, 88.19it/s][A
 15%|█▌        | 18/119 [00:00<00:01, 80.55it/s][A
 23%|██▎       | 27/119 [00:00<00:01, 79.54it/s][A
 29%|██▉       | 35/119 [00:00<00:01, 78.83it/s][A
 37%|███▋      | 44/119 [00:00<00:00, 79.94it/s][A
 45%|████▍     | 53/119 [00:00<00:00, 79.96it/s][A
 52%|█████▏    | 62/119 [00:00<00:00, 79.60it/s][A
 59%|█████▉    | 70/119 [00:00<00:00, 78.60it/s][A
 66%|██████▌   | 78/119 [00:00<00:00, 78.74it/s][A
 72%|███████▏  | 86/119 [00:01<00:00, 78.96it/s][A
 79%|███████▉  | 94/119 [00:01<00:00, 78.95it/s][A
 87%|████████▋ | 103/119 [00:01<00:00, 79.18it/s][A
 93%|█████████▎| 111/119 [00:01<00:00, 79.17it/s][A
100%|██████████| 119/119 [00:01<00:00, 78.83it/s][A                                                 
                                                 [A100%|██████████| 595/595 [00:46<00:00, 15.57it/s]
100%|██████████| 119/119 [00:01<00:00, 78.83it/s][A
                                                 [A[INFO|trainer.py:1963] 2023-11-15 23:19:52,529 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                 100%|██████████| 595/595 [00:46<00:00, 15.57it/s]100%|██████████| 595/595 [00:46<00:00, 12.71it/s]
[INFO|trainer.py:2855] 2023-11-15 23:19:52,532 >> Saving model checkpoint to ./result/restaurant_roberta-base_seed1_adapter
[INFO|configuration_utils.py:460] 2023-11-15 23:19:52,535 >> Configuration saved in ./result/restaurant_roberta-base_seed1_adapter/config.json
[INFO|modeling_utils.py:1997] 2023-11-15 23:19:53,634 >> Model weights saved in ./result/restaurant_roberta-base_seed1_adapter/pytorch_model.bin
[INFO|tokenization_utils_base.py:2235] 2023-11-15 23:19:53,637 >> tokenizer config file saved in ./result/restaurant_roberta-base_seed1_adapter/tokenizer_config.json
[INFO|tokenization_utils_base.py:2242] 2023-11-15 23:19:53,639 >> Special tokens file saved in ./result/restaurant_roberta-base_seed1_adapter/special_tokens_map.json
{'eval_loss': 0.33885911107063293, 'eval_accuracy': 0.8814814814814815, 'eval_micro_f1': 0.8814814814814815, 'eval_macro_f1': 0.8212475997558877, 'eval_runtime': 1.5474, 'eval_samples_per_second': 610.714, 'eval_steps_per_second': 76.905, 'epoch': 5.0}
{'train_runtime': 46.8154, 'train_samples_per_second': 403.393, 'train_steps_per_second': 12.71, 'train_loss': 0.4380662870006401, 'epoch': 5.0}
***** train metrics *****
  epoch                    =        5.0
  train_loss               =     0.4381
  train_runtime            = 0:00:46.81
  train_samples            =       3777
  train_samples_per_second =    403.393
  train_steps_per_second   =      12.71
11/15/2023 23:19:53 - INFO - __main__ - *** Evaluate ***
[INFO|trainer.py:755] 2023-11-15 23:19:53,786 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 23:19:53,787 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 23:19:53,788 >>   Num examples = 945
[INFO|trainer.py:3134] 2023-11-15 23:19:53,788 >>   Batch size = 8
  0%|          | 0/119 [00:00<?, ?it/s]  8%|▊         | 9/119 [00:00<00:01, 88.89it/s] 15%|█▌        | 18/119 [00:00<00:01, 81.94it/s] 23%|██▎       | 27/119 [00:00<00:01, 81.36it/s] 30%|███       | 36/119 [00:00<00:01, 80.58it/s] 38%|███▊      | 45/119 [00:00<00:00, 79.73it/s] 45%|████▍     | 53/119 [00:00<00:00, 79.60it/s] 51%|█████▏    | 61/119 [00:00<00:00, 79.08it/s] 58%|█████▊    | 69/119 [00:00<00:00, 78.03it/s] 65%|██████▍   | 77/119 [00:00<00:00, 76.94it/s] 71%|███████▏  | 85/119 [00:01<00:00, 76.74it/s] 79%|███████▉  | 94/119 [00:01<00:00, 77.95it/s] 86%|████████▌ | 102/119 [00:01<00:00, 77.68it/s] 92%|█████████▏| 110/119 [00:01<00:00, 78.27it/s] 99%|█████████▉| 118/119 [00:01<00:00, 78.04it/s]100%|██████████| 119/119 [00:01<00:00, 77.27it/s]
***** eval metrics *****
  epoch                   =        5.0
  eval_accuracy           =     0.8815
  eval_loss               =     0.3389
  eval_macro_f1           =     0.8212
  eval_micro_f1           =     0.8815
  eval_runtime            = 0:00:01.55
  eval_samples            =        945
  eval_samples_per_second =    607.187
  eval_steps_per_second   =     76.461
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                  eval/accuracy ▁▅▅▇██
wandb:                      eval/loss █▄▂▁▁▁
wandb:                  eval/macro_f1 ▁▄▆▇██
wandb:                  eval/micro_f1 ▁▅▅▇██
wandb:                   eval/runtime ▁▇█▆▄▅
wandb:        eval/samples_per_second █▂▁▃▅▄
wandb:          eval/steps_per_second █▂▁▃▅▄
wandb:                    train/epoch ▁▁▃▃▅▅▆▆████
wandb:              train/global_step ▁▁▃▃▅▅▆▆████
wandb:            train/learning_rate █▆▅▃▁
wandb:                     train/loss █▅▃▂▁
wandb:               train/total_flos ▁
wandb:               train/train_loss ▁
wandb:            train/train_runtime ▁
wandb: train/train_samples_per_second ▁
wandb:   train/train_steps_per_second ▁
wandb: 
wandb: Run summary:
wandb:                  eval/accuracy 0.88148
wandb:                      eval/loss 0.33886
wandb:                  eval/macro_f1 0.82125
wandb:                  eval/micro_f1 0.88148
wandb:                   eval/runtime 1.5564
wandb:        eval/samples_per_second 607.187
wandb:          eval/steps_per_second 76.461
wandb:                    train/epoch 5.0
wandb:              train/global_step 595
wandb:            train/learning_rate 0.0
wandb:                     train/loss 0.2583
wandb:               train/total_flos 627599085655680.0
wandb:               train/train_loss 0.43807
wandb:            train/train_runtime 46.8154
wandb: train/train_samples_per_second 403.393
wandb:   train/train_steps_per_second 12.71
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /ceph/home/wangyifei/a3/wandb/offline-run-20231115_231747-89mpu0di
wandb: Find logs at: ./wandb/offline-run-20231115_231747-89mpu0di/logs
(ModelArguments(model_name_or_path='roberta-base', cache_dir='./cache', use_adapter=True, use_lora=False), DataTrainingArguments(dataset_name='acl', max_seq_length=64), TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.0005,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=./result/acl_roberta-base_seed1_adapter/runs/Nov15_23-20-05_node013,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.EPOCH,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=5.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
output_dir=./result/acl_roberta-base_seed1_adapter,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
resume_from_checkpoint=None,
run_name=./result/acl_roberta-base_seed1_adapter,
save_on_each_node=False,
save_safetensors=False,
save_steps=500,
save_strategy=IntervalStrategy.NO,
save_total_limit=None,
seed=222,
sharded_ddp=[],
skip_memory_metrics=True,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
))
11/15/2023 23:20:05 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False
11/15/2023 23:20:05 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.0005,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=./result/acl_roberta-base_seed1_adapter/runs/Nov15_23-20-04_node013,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.EPOCH,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=5.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
output_dir=./result/acl_roberta-base_seed1_adapter,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
resume_from_checkpoint=None,
run_name=./result/acl_roberta-base_seed1_adapter,
save_on_each_node=False,
save_safetensors=False,
save_steps=500,
save_strategy=IntervalStrategy.NO,
save_total_limit=None,
seed=222,
sharded_ddp=[],
skip_memory_metrics=True,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
wandb: Tracking run with wandb version 0.15.12
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
Map:   0%|          | 0/11020 [00:00<?, ? examples/s]Map:  35%|███▌      | 3891/11020 [00:00<00:00, 38689.58 examples/s]Map:  71%|███████▏  | 7866/11020 [00:00<00:00, 39308.18 examples/s]Map: 100%|██████████| 11020/11020 [00:00<00:00, 38423.75 examples/s]
[INFO|configuration_utils.py:715] 2023-11-15 23:20:21,168 >> loading configuration file config.json from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/config.json
[INFO|configuration_utils.py:775] 2023-11-15 23:20:21,177 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.33.3",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_auto.py:535] 2023-11-15 23:20:31,193 >> Could not locate the tokenizer configuration file, will try to use the model config instead.
[INFO|configuration_utils.py:715] 2023-11-15 23:20:41,210 >> loading configuration file config.json from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/config.json
[INFO|configuration_utils.py:775] 2023-11-15 23:20:41,210 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.33.3",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:1852] 2023-11-15 23:21:01,258 >> loading file vocab.json from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/vocab.json
[INFO|tokenization_utils_base.py:1852] 2023-11-15 23:21:01,259 >> loading file merges.txt from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/merges.txt
[INFO|tokenization_utils_base.py:1852] 2023-11-15 23:21:01,259 >> loading file tokenizer.json from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/tokenizer.json
[INFO|tokenization_utils_base.py:1852] 2023-11-15 23:21:01,259 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1852] 2023-11-15 23:21:01,260 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1852] 2023-11-15 23:21:01,260 >> loading file tokenizer_config.json from cache at None
[INFO|configuration_utils.py:715] 2023-11-15 23:21:01,261 >> loading configuration file config.json from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/config.json
[INFO|configuration_utils.py:775] 2023-11-15 23:21:01,262 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.33.3",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:2862] 2023-11-15 23:21:21,592 >> loading weights file model.safetensors from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/model.safetensors
[INFO|modeling_utils.py:3638] 2023-11-15 23:21:22,330 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3650] 2023-11-15 23:21:22,330 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
parameters 1487427
model_max_length: 512
Running tokenizer on dataset:   0%|          | 0/8816 [00:00<?, ? examples/s]Running tokenizer on dataset:  23%|██▎       | 2000/8816 [00:00<00:00, 19342.18 examples/s]Running tokenizer on dataset:  45%|████▌     | 4000/8816 [00:00<00:00, 19357.36 examples/s]Running tokenizer on dataset:  68%|██████▊   | 6000/8816 [00:00<00:00, 19381.58 examples/s]Running tokenizer on dataset: 100%|██████████| 8816/8816 [00:00<00:00, 19786.90 examples/s]Running tokenizer on dataset: 100%|██████████| 8816/8816 [00:00<00:00, 19542.19 examples/s]
Running tokenizer on dataset:   0%|          | 0/2204 [00:00<?, ? examples/s]Running tokenizer on dataset: 100%|██████████| 2204/2204 [00:00<00:00, 20563.79 examples/s]Running tokenizer on dataset: 100%|██████████| 2204/2204 [00:00<00:00, 20252.79 examples/s]
11/15/2023 23:21:23 - INFO - __main__ - Sample 1767 of the training set: {'text': 'Second, HASM cells in culture, when observed between 3 and 6 h after plating, were not spindle shaped or aligned in parallel, as they are at the tissue level (5); instead, they were irregularly shaped (Fig.', 'label': 0, 'input_ids': [0, 32703, 6, 31963, 448, 4590, 11, 2040, 6, 77, 6373, 227, 155, 8, 231, 1368, 71, 2968, 1295, 6, 58, 45, 2292, 38969, 14216, 50, 14485, 11, 12980, 6, 25, 51, 32, 23, 5, 11576, 672, 36, 245, 4397, 1386, 6, 51, 58, 22937, 352, 14216, 36, 44105, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
11/15/2023 23:21:23 - INFO - __main__ - Sample 3854 of the training set: {'text': 'Yet, allowing for interruptions might decrease classification accuracy [24] as well as making results vulnerable to variation in wear time if analyzed with different epoch lengths [37].', 'label': 0, 'input_ids': [0, 34995, 6, 2455, 13, 22749, 2485, 429, 7280, 20257, 8611, 646, 1978, 742, 25, 157, 25, 442, 775, 4478, 7, 21875, 11, 3568, 86, 114, 13773, 19, 430, 43660, 18915, 646, 3272, 8174, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
11/15/2023 23:21:23 - INFO - __main__ - Sample 4652 of the training set: {'text': 'Examination of plaque formation and growth curves were performed by standard methods as previously described (10, 35).', 'label': 1, 'input_ids': [0, 9089, 41121, 9, 22054, 9285, 8, 434, 23739, 58, 3744, 30, 2526, 6448, 25, 1433, 1602, 36, 698, 6, 1718, 322, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
11/15/2023 23:21:23 - WARNING - accelerate.utils.other - Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[INFO|trainer.py:755] 2023-11-15 23:21:24,884 >> The following columns in the training set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:1715] 2023-11-15 23:21:24,895 >> ***** Running training *****
[INFO|trainer.py:1716] 2023-11-15 23:21:24,896 >>   Num examples = 8,816
[INFO|trainer.py:1717] 2023-11-15 23:21:24,896 >>   Num Epochs = 5
[INFO|trainer.py:1718] 2023-11-15 23:21:24,896 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1721] 2023-11-15 23:21:24,897 >>   Total train batch size (w. parallel, distributed & accumulation) = 32
[INFO|trainer.py:1722] 2023-11-15 23:21:24,897 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:1723] 2023-11-15 23:21:24,897 >>   Total optimization steps = 1,380
[INFO|trainer.py:1724] 2023-11-15 23:21:24,898 >>   Number of trainable parameters = 1,487,427
[INFO|integration_utils.py:716] 2023-11-15 23:21:24,899 >> Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"
  0%|          | 0/1380 [00:00<?, ?it/s]  0%|          | 1/1380 [00:01<24:09,  1.05s/it]  0%|          | 3/1380 [00:01<07:20,  3.13it/s]  0%|          | 5/1380 [00:01<04:19,  5.31it/s]  1%|          | 7/1380 [00:01<03:06,  7.37it/s]  1%|          | 9/1380 [00:01<02:28,  9.22it/s]  1%|          | 11/1380 [00:01<02:07, 10.76it/s]  1%|          | 13/1380 [00:01<01:53, 12.03it/s]  1%|          | 15/1380 [00:01<01:44, 13.02it/s]  1%|          | 17/1380 [00:02<01:38, 13.77it/s]  1%|▏         | 19/1380 [00:02<01:35, 14.27it/s]  2%|▏         | 21/1380 [00:02<01:32, 14.69it/s]  2%|▏         | 23/1380 [00:02<01:30, 14.94it/s]  2%|▏         | 25/1380 [00:02<01:29, 15.15it/s]  2%|▏         | 27/1380 [00:02<01:28, 15.27it/s]  2%|▏         | 29/1380 [00:02<01:27, 15.41it/s]  2%|▏         | 31/1380 [00:02<01:26, 15.52it/s]  2%|▏         | 33/1380 [00:03<01:26, 15.58it/s]  3%|▎         | 35/1380 [00:03<01:26, 15.49it/s]  3%|▎         | 37/1380 [00:03<01:26, 15.54it/s]  3%|▎         | 39/1380 [00:03<01:25, 15.64it/s]  3%|▎         | 41/1380 [00:03<01:25, 15.68it/s]  3%|▎         | 43/1380 [00:03<01:25, 15.68it/s]  3%|▎         | 45/1380 [00:03<01:25, 15.68it/s]  3%|▎         | 47/1380 [00:03<01:24, 15.69it/s]  4%|▎         | 49/1380 [00:04<01:25, 15.63it/s]  4%|▎         | 51/1380 [00:04<01:25, 15.62it/s]  4%|▍         | 53/1380 [00:04<01:25, 15.58it/s]  4%|▍         | 55/1380 [00:04<01:24, 15.61it/s]  4%|▍         | 57/1380 [00:04<01:24, 15.57it/s]  4%|▍         | 59/1380 [00:04<01:24, 15.57it/s]  4%|▍         | 61/1380 [00:04<01:24, 15.60it/s]  5%|▍         | 63/1380 [00:05<01:24, 15.60it/s]  5%|▍         | 65/1380 [00:05<01:23, 15.66it/s]  5%|▍         | 67/1380 [00:05<01:24, 15.58it/s]  5%|▌         | 69/1380 [00:05<01:24, 15.59it/s]  5%|▌         | 71/1380 [00:05<01:23, 15.64it/s]  5%|▌         | 73/1380 [00:05<01:23, 15.59it/s]  5%|▌         | 75/1380 [00:05<01:23, 15.62it/s]  6%|▌         | 77/1380 [00:05<01:23, 15.61it/s]  6%|▌         | 79/1380 [00:06<01:23, 15.63it/s]  6%|▌         | 81/1380 [00:06<01:23, 15.62it/s]  6%|▌         | 83/1380 [00:06<01:23, 15.59it/s]  6%|▌         | 85/1380 [00:06<01:22, 15.61it/s]  6%|▋         | 87/1380 [00:06<01:22, 15.62it/s]  6%|▋         | 89/1380 [00:06<01:22, 15.61it/s]  7%|▋         | 91/1380 [00:06<01:22, 15.62it/s]  7%|▋         | 93/1380 [00:06<01:22, 15.63it/s]  7%|▋         | 95/1380 [00:07<01:21, 15.69it/s]  7%|▋         | 97/1380 [00:07<01:21, 15.66it/s]  7%|▋         | 99/1380 [00:07<01:21, 15.64it/s]  7%|▋         | 101/1380 [00:07<01:21, 15.62it/s]  7%|▋         | 103/1380 [00:07<01:21, 15.58it/s]  8%|▊         | 105/1380 [00:07<01:21, 15.59it/s]  8%|▊         | 107/1380 [00:07<01:21, 15.60it/s]  8%|▊         | 109/1380 [00:07<01:21, 15.61it/s]  8%|▊         | 111/1380 [00:08<01:21, 15.62it/s]  8%|▊         | 113/1380 [00:08<01:21, 15.63it/s]  8%|▊         | 115/1380 [00:08<01:20, 15.64it/s]  8%|▊         | 117/1380 [00:08<01:20, 15.61it/s]  9%|▊         | 119/1380 [00:08<01:20, 15.65it/s]  9%|▉         | 121/1380 [00:08<01:20, 15.64it/s]  9%|▉         | 123/1380 [00:08<01:20, 15.63it/s]  9%|▉         | 125/1380 [00:08<01:20, 15.67it/s]  9%|▉         | 127/1380 [00:09<01:20, 15.64it/s]  9%|▉         | 129/1380 [00:09<01:20, 15.58it/s]  9%|▉         | 131/1380 [00:09<01:20, 15.58it/s] 10%|▉         | 133/1380 [00:09<01:20, 15.59it/s] 10%|▉         | 135/1380 [00:09<01:19, 15.64it/s] 10%|▉         | 137/1380 [00:09<01:19, 15.63it/s] 10%|█         | 139/1380 [00:09<01:19, 15.64it/s] 10%|█         | 141/1380 [00:10<01:19, 15.65it/s] 10%|█         | 143/1380 [00:10<01:19, 15.61it/s] 11%|█         | 145/1380 [00:10<01:19, 15.63it/s] 11%|█         | 147/1380 [00:10<01:18, 15.62it/s] 11%|█         | 149/1380 [00:10<01:18, 15.65it/s] 11%|█         | 151/1380 [00:10<01:18, 15.66it/s] 11%|█         | 153/1380 [00:10<01:18, 15.63it/s] 11%|█         | 155/1380 [00:10<01:18, 15.66it/s] 11%|█▏        | 157/1380 [00:11<01:18, 15.67it/s] 12%|█▏        | 159/1380 [00:11<01:18, 15.65it/s] 12%|█▏        | 161/1380 [00:11<01:17, 15.67it/s] 12%|█▏        | 163/1380 [00:11<01:18, 15.58it/s] 12%|█▏        | 165/1380 [00:11<01:17, 15.60it/s] 12%|█▏        | 167/1380 [00:11<01:18, 15.50it/s] 12%|█▏        | 169/1380 [00:11<01:17, 15.56it/s] 12%|█▏        | 171/1380 [00:11<01:17, 15.52it/s] 13%|█▎        | 173/1380 [00:12<01:17, 15.55it/s] 13%|█▎        | 175/1380 [00:12<01:17, 15.52it/s] 13%|█▎        | 177/1380 [00:12<01:17, 15.54it/s] 13%|█▎        | 179/1380 [00:12<01:17, 15.52it/s] 13%|█▎        | 181/1380 [00:12<01:17, 15.52it/s] 13%|█▎        | 183/1380 [00:12<01:17, 15.49it/s] 13%|█▎        | 185/1380 [00:12<01:17, 15.47it/s] 14%|█▎        | 187/1380 [00:12<01:17, 15.49it/s] 14%|█▎        | 189/1380 [00:13<01:16, 15.47it/s] 14%|█▍        | 191/1380 [00:13<01:16, 15.53it/s] 14%|█▍        | 193/1380 [00:13<01:16, 15.56it/s] 14%|█▍        | 195/1380 [00:13<01:16, 15.56it/s] 14%|█▍        | 197/1380 [00:13<01:16, 15.52it/s] 14%|█▍        | 199/1380 [00:13<01:16, 15.47it/s] 15%|█▍        | 201/1380 [00:13<01:15, 15.52it/s] 15%|█▍        | 203/1380 [00:13<01:15, 15.51it/s] 15%|█▍        | 205/1380 [00:14<01:15, 15.57it/s] 15%|█▌        | 207/1380 [00:14<01:15, 15.51it/s] 15%|█▌        | 209/1380 [00:14<01:15, 15.52it/s] 15%|█▌        | 211/1380 [00:14<01:15, 15.51it/s] 15%|█▌        | 213/1380 [00:14<01:15, 15.53it/s] 16%|█▌        | 215/1380 [00:14<01:15, 15.52it/s] 16%|█▌        | 217/1380 [00:14<01:14, 15.53it/s] 16%|█▌        | 219/1380 [00:15<01:14, 15.55it/s] 16%|█▌        | 221/1380 [00:15<01:14, 15.56it/s] 16%|█▌        | 223/1380 [00:15<01:14, 15.62it/s] 16%|█▋        | 225/1380 [00:15<01:13, 15.63it/s] 16%|█▋        | 227/1380 [00:15<01:13, 15.64it/s] 17%|█▋        | 229/1380 [00:15<01:13, 15.66it/s] 17%|█▋        | 231/1380 [00:15<01:13, 15.66it/s] 17%|█▋        | 233/1380 [00:15<01:13, 15.68it/s] 17%|█▋        | 235/1380 [00:16<01:13, 15.65it/s] 17%|█▋        | 237/1380 [00:16<01:13, 15.60it/s] 17%|█▋        | 239/1380 [00:16<01:13, 15.62it/s] 17%|█▋        | 241/1380 [00:16<01:12, 15.64it/s] 18%|█▊        | 243/1380 [00:16<01:12, 15.65it/s] 18%|█▊        | 245/1380 [00:16<01:12, 15.68it/s] 18%|█▊        | 247/1380 [00:16<01:12, 15.65it/s] 18%|█▊        | 249/1380 [00:16<01:12, 15.66it/s] 18%|█▊        | 251/1380 [00:17<01:12, 15.67it/s] 18%|█▊        | 253/1380 [00:17<01:11, 15.66it/s] 18%|█▊        | 255/1380 [00:17<01:11, 15.68it/s] 19%|█▊        | 257/1380 [00:17<01:11, 15.66it/s] 19%|█▉        | 259/1380 [00:17<01:11, 15.64it/s] 19%|█▉        | 261/1380 [00:17<01:11, 15.60it/s] 19%|█▉        | 263/1380 [00:17<01:11, 15.62it/s] 19%|█▉        | 265/1380 [00:17<01:11, 15.58it/s] 19%|█▉        | 267/1380 [00:18<01:11, 15.52it/s] 19%|█▉        | 269/1380 [00:18<01:11, 15.58it/s] 20%|█▉        | 271/1380 [00:18<01:11, 15.62it/s] 20%|█▉        | 273/1380 [00:18<01:10, 15.64it/s] 20%|█▉        | 275/1380 [00:18<01:10, 15.67it/s]                                                   20%|██        | 276/1380 [00:18<01:10, 15.67it/s][INFO|trainer.py:755] 2023-11-15 23:21:43,546 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 23:21:43,548 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 23:21:43,548 >>   Num examples = 2204
[INFO|trainer.py:3134] 2023-11-15 23:21:43,549 >>   Batch size = 8
{'loss': 0.5201, 'learning_rate': 0.0004, 'epoch': 1.0}

  0%|          | 0/276 [00:00<?, ?it/s][A
  4%|▎         | 10/276 [00:00<00:02, 91.41it/s][A
  7%|▋         | 20/276 [00:00<00:02, 85.84it/s][A
 11%|█         | 29/276 [00:00<00:02, 84.15it/s][A
 14%|█▍        | 38/276 [00:00<00:02, 83.32it/s][A
 17%|█▋        | 47/276 [00:00<00:02, 82.61it/s][A
 20%|██        | 56/276 [00:00<00:02, 82.52it/s][A
 24%|██▎       | 65/276 [00:00<00:02, 82.20it/s][A
 27%|██▋       | 74/276 [00:00<00:02, 82.34it/s][A
 30%|███       | 83/276 [00:00<00:02, 82.51it/s][A
 33%|███▎      | 92/276 [00:01<00:02, 82.35it/s][A
 37%|███▋      | 101/276 [00:01<00:02, 81.57it/s][A
 40%|███▉      | 110/276 [00:01<00:02, 81.59it/s][A
 43%|████▎     | 119/276 [00:01<00:01, 81.80it/s][A
 46%|████▋     | 128/276 [00:01<00:01, 81.59it/s][A
 50%|████▉     | 137/276 [00:01<00:01, 81.71it/s][A
 53%|█████▎    | 146/276 [00:01<00:01, 81.83it/s][A
 56%|█████▌    | 155/276 [00:01<00:01, 80.43it/s][A
 59%|█████▉    | 164/276 [00:01<00:01, 80.33it/s][A
 63%|██████▎   | 173/276 [00:02<00:01, 80.09it/s][A
 66%|██████▌   | 182/276 [00:02<00:01, 80.76it/s][A
 69%|██████▉   | 191/276 [00:02<00:01, 81.10it/s][A
 72%|███████▏  | 200/276 [00:02<00:00, 82.14it/s][A
 76%|███████▌  | 209/276 [00:02<00:00, 82.52it/s][A
 79%|███████▉  | 218/276 [00:02<00:00, 83.03it/s][A
 82%|████████▏ | 227/276 [00:02<00:00, 83.33it/s][A
 86%|████████▌ | 236/276 [00:02<00:00, 83.64it/s][A
 89%|████████▉ | 245/276 [00:02<00:00, 83.80it/s][A
 92%|█████████▏| 254/276 [00:03<00:00, 81.08it/s][A
 95%|█████████▌| 263/276 [00:03<00:00, 81.39it/s][A
 99%|█████████▊| 272/276 [00:03<00:00, 81.47it/s][A                                                  
                                                 [A 20%|██        | 276/1380 [00:22<01:10, 15.67it/s]
100%|██████████| 276/276 [00:03<00:00, 81.47it/s][A
                                                 [A 20%|██        | 277/1380 [00:22<10:34,  1.74it/s] 20%|██        | 279/1380 [00:22<07:43,  2.37it/s] 20%|██        | 281/1380 [00:22<05:44,  3.19it/s] 21%|██        | 283/1380 [00:22<04:21,  4.20it/s] 21%|██        | 285/1380 [00:22<03:23,  5.39it/s] 21%|██        | 287/1380 [00:22<02:42,  6.74it/s] 21%|██        | 289/1380 [00:22<02:13,  8.15it/s] 21%|██        | 291/1380 [00:23<01:54,  9.53it/s] 21%|██        | 293/1380 [00:23<01:41, 10.76it/s] 21%|██▏       | 295/1380 [00:23<01:31, 11.80it/s] 22%|██▏       | 297/1380 [00:23<01:24, 12.81it/s] 22%|██▏       | 299/1380 [00:23<01:19, 13.60it/s] 22%|██▏       | 301/1380 [00:23<01:16, 14.19it/s] 22%|██▏       | 303/1380 [00:23<01:13, 14.64it/s] 22%|██▏       | 305/1380 [00:23<01:11, 14.94it/s] 22%|██▏       | 307/1380 [00:24<01:10, 15.19it/s] 22%|██▏       | 309/1380 [00:24<01:09, 15.45it/s] 23%|██▎       | 311/1380 [00:24<01:08, 15.53it/s] 23%|██▎       | 313/1380 [00:24<01:08, 15.57it/s] 23%|██▎       | 315/1380 [00:24<01:07, 15.72it/s] 23%|██▎       | 317/1380 [00:24<01:08, 15.60it/s] 23%|██▎       | 319/1380 [00:24<01:07, 15.72it/s] 23%|██▎       | 321/1380 [00:24<01:06, 15.83it/s] 23%|██▎       | 323/1380 [00:25<01:06, 15.89it/s] 24%|██▎       | 325/1380 [00:25<01:06, 15.91it/s] 24%|██▎       | 327/1380 [00:25<01:06, 15.93it/s] 24%|██▍       | 329/1380 [00:25<01:05, 15.98it/s] 24%|██▍       | 331/1380 [00:25<01:05, 15.98it/s] 24%|██▍       | 333/1380 [00:25<01:05, 15.99it/s] 24%|██▍       | 335/1380 [00:25<01:05, 15.99it/s] 24%|██▍       | 337/1380 [00:25<01:05, 16.04it/s] 25%|██▍       | 339/1380 [00:26<01:04, 16.04it/s] 25%|██▍       | 341/1380 [00:26<01:04, 16.01it/s] 25%|██▍       | 343/1380 [00:26<01:04, 16.01it/s] 25%|██▌       | 345/1380 [00:26<01:04, 15.93it/s] 25%|██▌       | 347/1380 [00:26<01:04, 15.95it/s] 25%|██▌       | 349/1380 [00:26<01:04, 15.97it/s] 25%|██▌       | 351/1380 [00:26<01:04, 15.99it/s] 26%|██▌       | 353/1380 [00:26<01:04, 16.02it/s] 26%|██▌       | 355/1380 [00:27<01:03, 16.02it/s] 26%|██▌       | 357/1380 [00:27<01:03, 16.01it/s] 26%|██▌       | 359/1380 [00:27<01:03, 16.01it/s] 26%|██▌       | 361/1380 [00:27<01:03, 16.04it/s] 26%|██▋       | 363/1380 [00:27<01:03, 16.03it/s] 26%|██▋       | 365/1380 [00:27<01:03, 16.00it/s] 27%|██▋       | 367/1380 [00:27<01:03, 15.93it/s] 27%|██▋       | 369/1380 [00:27<01:03, 15.95it/s] 27%|██▋       | 371/1380 [00:28<01:03, 15.97it/s] 27%|██▋       | 373/1380 [00:28<01:03, 15.97it/s] 27%|██▋       | 375/1380 [00:28<01:02, 15.98it/s] 27%|██▋       | 377/1380 [00:28<01:02, 16.01it/s] 27%|██▋       | 379/1380 [00:28<01:02, 16.03it/s] 28%|██▊       | 381/1380 [00:28<01:02, 16.00it/s] 28%|██▊       | 383/1380 [00:28<01:02, 16.01it/s] 28%|██▊       | 385/1380 [00:28<01:02, 16.04it/s] 28%|██▊       | 387/1380 [00:29<01:01, 16.04it/s] 28%|██▊       | 389/1380 [00:29<01:01, 16.02it/s] 28%|██▊       | 391/1380 [00:29<01:01, 16.00it/s] 28%|██▊       | 393/1380 [00:29<01:01, 16.02it/s] 29%|██▊       | 395/1380 [00:29<01:01, 16.02it/s] 29%|██▉       | 397/1380 [00:29<01:01, 16.00it/s] 29%|██▉       | 399/1380 [00:29<01:01, 16.00it/s] 29%|██▉       | 401/1380 [00:29<01:01, 16.02it/s] 29%|██▉       | 403/1380 [00:30<01:00, 16.02it/s] 29%|██▉       | 405/1380 [00:30<01:00, 16.00it/s] 29%|██▉       | 407/1380 [00:30<01:00, 15.99it/s] 30%|██▉       | 409/1380 [00:30<01:00, 16.01it/s] 30%|██▉       | 411/1380 [00:30<01:00, 16.00it/s] 30%|██▉       | 413/1380 [00:30<01:00, 15.99it/s] 30%|███       | 415/1380 [00:30<01:00, 15.98it/s] 30%|███       | 417/1380 [00:30<01:00, 15.99it/s] 30%|███       | 419/1380 [00:31<01:00, 15.96it/s] 31%|███       | 421/1380 [00:31<01:00, 15.94it/s] 31%|███       | 423/1380 [00:31<00:59, 15.96it/s] 31%|███       | 425/1380 [00:31<00:59, 15.97it/s] 31%|███       | 427/1380 [00:31<00:59, 15.97it/s] 31%|███       | 429/1380 [00:31<00:59, 15.96it/s] 31%|███       | 431/1380 [00:31<00:59, 15.95it/s] 31%|███▏      | 433/1380 [00:31<00:59, 15.97it/s] 32%|███▏      | 435/1380 [00:32<00:59, 15.93it/s] 32%|███▏      | 437/1380 [00:32<00:59, 15.93it/s] 32%|███▏      | 439/1380 [00:32<00:58, 15.96it/s] 32%|███▏      | 441/1380 [00:32<00:58, 15.97it/s] 32%|███▏      | 443/1380 [00:32<00:58, 15.95it/s] 32%|███▏      | 445/1380 [00:32<00:58, 15.95it/s] 32%|███▏      | 447/1380 [00:32<00:58, 15.96it/s] 33%|███▎      | 449/1380 [00:32<00:58, 15.94it/s] 33%|███▎      | 451/1380 [00:33<00:58, 15.92it/s] 33%|███▎      | 453/1380 [00:33<00:58, 15.95it/s] 33%|███▎      | 455/1380 [00:33<00:57, 15.96it/s] 33%|███▎      | 457/1380 [00:33<00:57, 15.94it/s] 33%|███▎      | 459/1380 [00:33<00:57, 15.94it/s] 33%|███▎      | 461/1380 [00:33<00:57, 15.96it/s] 34%|███▎      | 463/1380 [00:33<00:57, 15.97it/s] 34%|███▎      | 465/1380 [00:33<00:57, 15.93it/s] 34%|███▍      | 467/1380 [00:34<00:57, 15.93it/s] 34%|███▍      | 469/1380 [00:34<00:57, 15.92it/s] 34%|███▍      | 471/1380 [00:34<00:57, 15.94it/s] 34%|███▍      | 473/1380 [00:34<00:57, 15.89it/s] 34%|███▍      | 475/1380 [00:34<00:56, 15.93it/s] 35%|███▍      | 477/1380 [00:34<00:56, 15.94it/s] 35%|███▍      | 479/1380 [00:34<00:56, 15.92it/s] 35%|███▍      | 481/1380 [00:34<00:56, 15.93it/s] 35%|███▌      | 483/1380 [00:35<00:56, 15.97it/s] 35%|███▌      | 485/1380 [00:35<00:56, 15.81it/s] 35%|███▌      | 487/1380 [00:35<00:57, 15.66it/s] 35%|███▌      | 489/1380 [00:35<00:57, 15.57it/s] 36%|███▌      | 491/1380 [00:35<00:57, 15.58it/s] 36%|███▌      | 493/1380 [00:35<00:56, 15.65it/s] 36%|███▌      | 495/1380 [00:35<00:56, 15.72it/s] 36%|███▌      | 497/1380 [00:35<00:55, 15.78it/s] 36%|███▌      | 499/1380 [00:36<00:55, 15.85it/s] 36%|███▋      | 501/1380 [00:36<00:55, 15.87it/s] 36%|███▋      | 503/1380 [00:36<00:55, 15.87it/s] 37%|███▋      | 505/1380 [00:36<00:55, 15.81it/s] 37%|███▋      | 507/1380 [00:36<00:56, 15.58it/s] 37%|███▋      | 509/1380 [00:36<00:55, 15.71it/s] 37%|███▋      | 511/1380 [00:36<00:55, 15.78it/s] 37%|███▋      | 513/1380 [00:36<00:54, 15.81it/s] 37%|███▋      | 515/1380 [00:37<00:54, 15.85it/s] 37%|███▋      | 517/1380 [00:37<00:54, 15.90it/s] 38%|███▊      | 519/1380 [00:37<00:54, 15.90it/s] 38%|███▊      | 521/1380 [00:37<00:54, 15.91it/s] 38%|███▊      | 523/1380 [00:37<00:53, 15.93it/s] 38%|███▊      | 525/1380 [00:37<00:53, 15.95it/s] 38%|███▊      | 527/1380 [00:37<00:53, 15.92it/s] 38%|███▊      | 529/1380 [00:37<00:53, 15.91it/s] 38%|███▊      | 531/1380 [00:38<00:53, 15.93it/s] 39%|███▊      | 533/1380 [00:38<00:53, 15.92it/s] 39%|███▉      | 535/1380 [00:38<00:53, 15.89it/s] 39%|███▉      | 537/1380 [00:38<00:52, 15.93it/s] 39%|███▉      | 539/1380 [00:38<00:52, 15.93it/s] 39%|███▉      | 541/1380 [00:38<00:52, 15.92it/s] 39%|███▉      | 543/1380 [00:38<00:52, 15.90it/s] 39%|███▉      | 545/1380 [00:38<00:52, 15.93it/s] 40%|███▉      | 547/1380 [00:39<00:52, 15.92it/s] 40%|███▉      | 549/1380 [00:39<00:52, 15.90it/s] 40%|███▉      | 551/1380 [00:39<00:51, 15.96it/s]                                                   40%|████      | 552/1380 [00:39<00:51, 15.96it/s][INFO|trainer.py:755] 2023-11-15 23:22:04,287 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 23:22:04,289 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 23:22:04,290 >>   Num examples = 2204
[INFO|trainer.py:3134] 2023-11-15 23:22:04,290 >>   Batch size = 8
{'eval_loss': 0.400596559047699, 'eval_accuracy': 0.8557168784029038, 'eval_micro_f1': 0.8557168784029038, 'eval_macro_f1': 0.836233444819093, 'eval_runtime': 3.4229, 'eval_samples_per_second': 643.906, 'eval_steps_per_second': 80.634, 'epoch': 1.0}
{'loss': 0.3719, 'learning_rate': 0.0003, 'epoch': 2.0}

  0%|          | 0/276 [00:00<?, ?it/s][A
  4%|▎         | 10/276 [00:00<00:02, 91.94it/s][A
  7%|▋         | 20/276 [00:00<00:02, 85.86it/s][A
 11%|█         | 29/276 [00:00<00:02, 83.50it/s][A
 14%|█▍        | 38/276 [00:00<00:02, 82.82it/s][A
 17%|█▋        | 47/276 [00:00<00:02, 82.81it/s][A
 20%|██        | 56/276 [00:00<00:02, 82.22it/s][A
 24%|██▎       | 65/276 [00:00<00:02, 82.28it/s][A
 27%|██▋       | 74/276 [00:00<00:02, 82.53it/s][A
 30%|███       | 83/276 [00:00<00:02, 82.86it/s][A
 33%|███▎      | 92/276 [00:01<00:02, 83.09it/s][A
 37%|███▋      | 101/276 [00:01<00:02, 83.21it/s][A
 40%|███▉      | 110/276 [00:01<00:01, 83.36it/s][A
 43%|████▎     | 119/276 [00:01<00:01, 82.86it/s][A
 46%|████▋     | 128/276 [00:01<00:01, 82.61it/s][A
 50%|████▉     | 137/276 [00:01<00:01, 82.64it/s][A
 53%|█████▎    | 146/276 [00:01<00:01, 82.62it/s][A
 56%|█████▌    | 155/276 [00:01<00:01, 82.77it/s][A
 59%|█████▉    | 164/276 [00:01<00:01, 82.70it/s][A
 63%|██████▎   | 173/276 [00:02<00:01, 82.52it/s][A
 66%|██████▌   | 182/276 [00:02<00:01, 82.66it/s][A
 69%|██████▉   | 191/276 [00:02<00:01, 82.78it/s][A
 72%|███████▏  | 200/276 [00:02<00:00, 82.78it/s][A
 76%|███████▌  | 209/276 [00:02<00:00, 81.70it/s][A
 79%|███████▉  | 218/276 [00:02<00:00, 81.86it/s][A
 82%|████████▏ | 227/276 [00:02<00:00, 81.35it/s][A
 86%|████████▌ | 236/276 [00:02<00:00, 81.51it/s][A
 89%|████████▉ | 245/276 [00:02<00:00, 81.77it/s][A
 92%|█████████▏| 254/276 [00:03<00:00, 82.27it/s][A
 95%|█████████▌| 263/276 [00:03<00:00, 82.40it/s][A
 99%|█████████▊| 272/276 [00:03<00:00, 82.26it/s][A                                                  
                                                 [A 40%|████      | 552/1380 [00:42<00:51, 15.96it/s]
100%|██████████| 276/276 [00:03<00:00, 82.26it/s][A
                                                 [A 40%|████      | 553/1380 [00:42<07:52,  1.75it/s] 40%|████      | 555/1380 [00:42<05:45,  2.39it/s] 40%|████      | 557/1380 [00:43<04:16,  3.20it/s] 41%|████      | 559/1380 [00:43<03:14,  4.21it/s] 41%|████      | 561/1380 [00:43<02:31,  5.41it/s] 41%|████      | 563/1380 [00:43<02:01,  6.73it/s] 41%|████      | 565/1380 [00:43<01:40,  8.13it/s] 41%|████      | 567/1380 [00:43<01:25,  9.52it/s] 41%|████      | 569/1380 [00:43<01:14, 10.83it/s] 41%|████▏     | 571/1380 [00:43<01:07, 11.97it/s] 42%|████▏     | 573/1380 [00:44<01:02, 12.92it/s] 42%|████▏     | 575/1380 [00:44<00:58, 13.71it/s] 42%|████▏     | 577/1380 [00:44<00:56, 14.30it/s] 42%|████▏     | 579/1380 [00:44<00:54, 14.73it/s] 42%|████▏     | 581/1380 [00:44<00:53, 14.95it/s] 42%|████▏     | 583/1380 [00:44<00:52, 15.09it/s] 42%|████▏     | 585/1380 [00:44<00:51, 15.32it/s] 43%|████▎     | 587/1380 [00:45<00:51, 15.52it/s] 43%|████▎     | 589/1380 [00:45<00:50, 15.66it/s] 43%|████▎     | 591/1380 [00:45<00:50, 15.75it/s] 43%|████▎     | 593/1380 [00:45<00:49, 15.85it/s] 43%|████▎     | 595/1380 [00:45<00:49, 15.92it/s] 43%|████▎     | 597/1380 [00:45<00:49, 15.94it/s] 43%|████▎     | 599/1380 [00:45<00:49, 15.87it/s] 44%|████▎     | 601/1380 [00:45<00:48, 15.94it/s] 44%|████▎     | 603/1380 [00:46<00:48, 15.95it/s] 44%|████▍     | 605/1380 [00:46<00:48, 15.95it/s] 44%|████▍     | 607/1380 [00:46<00:48, 15.95it/s] 44%|████▍     | 609/1380 [00:46<00:48, 15.99it/s] 44%|████▍     | 611/1380 [00:46<00:48, 15.99it/s] 44%|████▍     | 613/1380 [00:46<00:48, 15.97it/s] 45%|████▍     | 615/1380 [00:46<00:47, 15.97it/s] 45%|████▍     | 617/1380 [00:46<00:47, 16.01it/s] 45%|████▍     | 619/1380 [00:47<00:47, 15.97it/s] 45%|████▌     | 621/1380 [00:47<00:47, 15.96it/s] 45%|████▌     | 623/1380 [00:47<00:47, 15.96it/s] 45%|████▌     | 625/1380 [00:47<00:47, 16.00it/s] 45%|████▌     | 627/1380 [00:47<00:47, 15.98it/s] 46%|████▌     | 629/1380 [00:47<00:47, 15.93it/s] 46%|████▌     | 631/1380 [00:47<00:47, 15.91it/s] 46%|████▌     | 633/1380 [00:47<00:47, 15.81it/s] 46%|████▌     | 635/1380 [00:48<00:47, 15.81it/s] 46%|████▌     | 637/1380 [00:48<00:46, 15.86it/s] 46%|████▋     | 639/1380 [00:48<00:46, 15.91it/s] 46%|████▋     | 641/1380 [00:48<00:46, 15.90it/s] 47%|████▋     | 643/1380 [00:48<00:46, 15.92it/s] 47%|████▋     | 645/1380 [00:48<00:46, 15.97it/s] 47%|████▋     | 647/1380 [00:48<00:45, 15.98it/s] 47%|████▋     | 649/1380 [00:48<00:45, 15.97it/s] 47%|████▋     | 651/1380 [00:49<00:45, 15.97it/s] 47%|████▋     | 653/1380 [00:49<00:45, 16.01it/s] 47%|████▋     | 655/1380 [00:49<00:45, 16.01it/s] 48%|████▊     | 657/1380 [00:49<00:45, 15.98it/s] 48%|████▊     | 659/1380 [00:49<00:45, 15.98it/s] 48%|████▊     | 661/1380 [00:49<00:44, 16.03it/s] 48%|████▊     | 663/1380 [00:49<00:45, 15.90it/s] 48%|████▊     | 665/1380 [00:49<00:44, 15.92it/s] 48%|████▊     | 667/1380 [00:50<00:44, 15.96it/s] 48%|████▊     | 669/1380 [00:50<00:44, 15.99it/s] 49%|████▊     | 671/1380 [00:50<00:44, 15.99it/s] 49%|████▉     | 673/1380 [00:50<00:44, 15.98it/s] 49%|████▉     | 675/1380 [00:50<00:44, 16.00it/s] 49%|████▉     | 677/1380 [00:50<00:43, 16.02it/s] 49%|████▉     | 679/1380 [00:50<00:43, 16.01it/s] 49%|████▉     | 681/1380 [00:50<00:44, 15.87it/s] 49%|████▉     | 683/1380 [00:51<00:43, 15.93it/s] 50%|████▉     | 685/1380 [00:51<00:43, 15.96it/s] 50%|████▉     | 687/1380 [00:51<00:43, 15.95it/s] 50%|████▉     | 689/1380 [00:51<00:43, 15.78it/s] 50%|█████     | 691/1380 [00:51<00:43, 15.78it/s] 50%|█████     | 693/1380 [00:51<00:43, 15.72it/s] 50%|█████     | 695/1380 [00:51<00:43, 15.81it/s] 51%|█████     | 697/1380 [00:51<00:43, 15.86it/s] 51%|█████     | 699/1380 [00:52<00:42, 15.88it/s] 51%|█████     | 701/1380 [00:52<00:42, 15.91it/s] 51%|█████     | 703/1380 [00:52<00:42, 15.96it/s] 51%|█████     | 705/1380 [00:52<00:42, 15.97it/s] 51%|█████     | 707/1380 [00:52<00:42, 15.95it/s] 51%|█████▏    | 709/1380 [00:52<00:42, 15.92it/s] 52%|█████▏    | 711/1380 [00:52<00:41, 15.95it/s] 52%|█████▏    | 713/1380 [00:52<00:41, 15.95it/s] 52%|█████▏    | 715/1380 [00:53<00:42, 15.83it/s] 52%|█████▏    | 717/1380 [00:53<00:41, 15.86it/s] 52%|█████▏    | 719/1380 [00:53<00:41, 15.82it/s] 52%|█████▏    | 721/1380 [00:53<00:41, 15.71it/s] 52%|█████▏    | 723/1380 [00:53<00:42, 15.63it/s] 53%|█████▎    | 725/1380 [00:53<00:42, 15.52it/s] 53%|█████▎    | 727/1380 [00:53<00:42, 15.48it/s] 53%|█████▎    | 729/1380 [00:53<00:42, 15.46it/s] 53%|█████▎    | 731/1380 [00:54<00:41, 15.46it/s] 53%|█████▎    | 733/1380 [00:54<00:41, 15.42it/s] 53%|█████▎    | 735/1380 [00:54<00:41, 15.37it/s] 53%|█████▎    | 737/1380 [00:54<00:41, 15.34it/s] 54%|█████▎    | 739/1380 [00:54<00:41, 15.34it/s] 54%|█████▎    | 741/1380 [00:54<00:41, 15.35it/s] 54%|█████▍    | 743/1380 [00:54<00:41, 15.32it/s] 54%|█████▍    | 745/1380 [00:54<00:41, 15.32it/s] 54%|█████▍    | 747/1380 [00:55<00:41, 15.31it/s] 54%|█████▍    | 749/1380 [00:55<00:41, 15.33it/s] 54%|█████▍    | 751/1380 [00:55<00:41, 15.29it/s] 55%|█████▍    | 753/1380 [00:55<00:41, 15.28it/s] 55%|█████▍    | 755/1380 [00:55<00:40, 15.26it/s] 55%|█████▍    | 757/1380 [00:55<00:40, 15.25it/s] 55%|█████▌    | 759/1380 [00:55<00:40, 15.27it/s] 55%|█████▌    | 761/1380 [00:56<00:40, 15.26it/s] 55%|█████▌    | 763/1380 [00:56<00:40, 15.31it/s] 55%|█████▌    | 765/1380 [00:56<00:40, 15.31it/s] 56%|█████▌    | 767/1380 [00:56<00:39, 15.33it/s] 56%|█████▌    | 769/1380 [00:56<00:39, 15.33it/s] 56%|█████▌    | 771/1380 [00:56<00:39, 15.34it/s] 56%|█████▌    | 773/1380 [00:56<00:39, 15.32it/s] 56%|█████▌    | 775/1380 [00:56<00:39, 15.35it/s] 56%|█████▋    | 777/1380 [00:57<00:39, 15.30it/s] 56%|█████▋    | 779/1380 [00:57<00:39, 15.30it/s] 57%|█████▋    | 781/1380 [00:57<00:39, 15.28it/s] 57%|█████▋    | 783/1380 [00:57<00:39, 15.28it/s] 57%|█████▋    | 785/1380 [00:57<00:38, 15.27it/s] 57%|█████▋    | 787/1380 [00:57<00:38, 15.30it/s] 57%|█████▋    | 789/1380 [00:57<00:38, 15.27it/s] 57%|█████▋    | 791/1380 [00:57<00:38, 15.29it/s] 57%|█████▋    | 793/1380 [00:58<00:38, 15.31it/s] 58%|█████▊    | 795/1380 [00:58<00:38, 15.26it/s] 58%|█████▊    | 797/1380 [00:58<00:37, 15.36it/s] 58%|█████▊    | 799/1380 [00:58<00:37, 15.51it/s] 58%|█████▊    | 801/1380 [00:58<00:37, 15.63it/s] 58%|█████▊    | 803/1380 [00:58<00:36, 15.73it/s] 58%|█████▊    | 805/1380 [00:58<00:36, 15.81it/s] 58%|█████▊    | 807/1380 [00:59<00:36, 15.85it/s] 59%|█████▊    | 809/1380 [00:59<00:35, 15.88it/s] 59%|█████▉    | 811/1380 [00:59<00:35, 15.93it/s] 59%|█████▉    | 813/1380 [00:59<00:35, 15.95it/s] 59%|█████▉    | 815/1380 [00:59<00:35, 15.95it/s] 59%|█████▉    | 817/1380 [00:59<00:35, 15.97it/s] 59%|█████▉    | 819/1380 [00:59<00:35, 16.01it/s] 59%|█████▉    | 821/1380 [00:59<00:34, 16.01it/s] 60%|█████▉    | 823/1380 [01:00<00:34, 15.98it/s] 60%|█████▉    | 825/1380 [01:00<00:34, 15.98it/s] 60%|█████▉    | 827/1380 [01:00<00:34, 16.03it/s]                                                   60%|██████    | 828/1380 [01:00<00:34, 16.03it/s][INFO|trainer.py:755] 2023-11-15 23:22:25,204 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 23:22:25,205 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 23:22:25,206 >>   Num examples = 2204
[INFO|trainer.py:3134] 2023-11-15 23:22:25,206 >>   Batch size = 8
{'eval_loss': 0.410011887550354, 'eval_accuracy': 0.8507259528130672, 'eval_micro_f1': 0.8507259528130672, 'eval_macro_f1': 0.8357298259103806, 'eval_runtime': 3.4071, 'eval_samples_per_second': 646.877, 'eval_steps_per_second': 81.006, 'epoch': 2.0}
{'loss': 0.3336, 'learning_rate': 0.0002, 'epoch': 3.0}

  0%|          | 0/276 [00:00<?, ?it/s][A
  4%|▎         | 10/276 [00:00<00:02, 92.70it/s][A
  7%|▋         | 20/276 [00:00<00:02, 86.81it/s][A
 11%|█         | 29/276 [00:00<00:02, 84.99it/s][A
 14%|█▍        | 38/276 [00:00<00:02, 84.31it/s][A
 17%|█▋        | 47/276 [00:00<00:02, 84.33it/s][A
 20%|██        | 56/276 [00:00<00:02, 83.91it/s][A
 24%|██▎       | 65/276 [00:00<00:02, 83.43it/s][A
 27%|██▋       | 74/276 [00:00<00:02, 83.62it/s][A
 30%|███       | 83/276 [00:00<00:02, 83.40it/s][A
 33%|███▎      | 92/276 [00:01<00:02, 83.58it/s][A
 37%|███▋      | 101/276 [00:01<00:02, 76.11it/s][A
 40%|███▉      | 110/276 [00:01<00:02, 77.80it/s][A
 43%|████▎     | 119/276 [00:01<00:01, 78.89it/s][A
 46%|████▋     | 128/276 [00:01<00:01, 80.12it/s][A
 50%|████▉     | 137/276 [00:01<00:01, 80.68it/s][A
 53%|█████▎    | 146/276 [00:01<00:01, 81.55it/s][A
 56%|█████▌    | 155/276 [00:01<00:01, 81.63it/s][A
 59%|█████▉    | 164/276 [00:01<00:01, 81.99it/s][A
 63%|██████▎   | 173/276 [00:02<00:01, 81.68it/s][A
 66%|██████▌   | 182/276 [00:02<00:01, 81.38it/s][A
 69%|██████▉   | 191/276 [00:02<00:01, 81.96it/s][A
 72%|███████▏  | 200/276 [00:02<00:00, 82.02it/s][A
 76%|███████▌  | 209/276 [00:02<00:00, 82.38it/s][A
 79%|███████▉  | 218/276 [00:02<00:00, 82.45it/s][A
 82%|████████▏ | 227/276 [00:02<00:00, 82.35it/s][A
 86%|████████▌ | 236/276 [00:02<00:00, 82.42it/s][A
 89%|████████▉ | 245/276 [00:02<00:00, 81.38it/s][A
 92%|█████████▏| 254/276 [00:03<00:00, 81.85it/s][A
 95%|█████████▌| 263/276 [00:03<00:00, 82.22it/s][A
 99%|█████████▊| 272/276 [00:03<00:00, 82.47it/s][A                                                  
                                                 [A 60%|██████    | 828/1380 [01:03<00:34, 16.03it/s]
100%|██████████| 276/276 [00:03<00:00, 82.47it/s][A
                                                 [A 60%|██████    | 829/1380 [01:03<05:15,  1.74it/s] 60%|██████    | 831/1380 [01:03<03:50,  2.38it/s] 60%|██████    | 833/1380 [01:04<02:51,  3.19it/s] 61%|██████    | 835/1380 [01:04<02:09,  4.19it/s] 61%|██████    | 837/1380 [01:04<01:40,  5.39it/s] 61%|██████    | 839/1380 [01:04<01:20,  6.72it/s] 61%|██████    | 841/1380 [01:04<01:06,  8.12it/s] 61%|██████    | 843/1380 [01:04<00:56,  9.52it/s] 61%|██████    | 845/1380 [01:04<00:49, 10.82it/s] 61%|██████▏   | 847/1380 [01:04<00:44, 11.93it/s] 62%|██████▏   | 849/1380 [01:05<00:41, 12.89it/s] 62%|██████▏   | 851/1380 [01:05<00:38, 13.70it/s] 62%|██████▏   | 853/1380 [01:05<00:36, 14.31it/s] 62%|██████▏   | 855/1380 [01:05<00:35, 14.77it/s] 62%|██████▏   | 857/1380 [01:05<00:34, 15.10it/s] 62%|██████▏   | 859/1380 [01:05<00:33, 15.38it/s] 62%|██████▏   | 861/1380 [01:05<00:33, 15.57it/s] 63%|██████▎   | 863/1380 [01:05<00:32, 15.69it/s] 63%|██████▎   | 865/1380 [01:06<00:32, 15.74it/s] 63%|██████▎   | 867/1380 [01:06<00:32, 15.77it/s] 63%|██████▎   | 869/1380 [01:06<00:32, 15.70it/s] 63%|██████▎   | 871/1380 [01:06<00:32, 15.78it/s] 63%|██████▎   | 873/1380 [01:06<00:31, 15.85it/s] 63%|██████▎   | 875/1380 [01:06<00:31, 15.86it/s] 64%|██████▎   | 877/1380 [01:06<00:31, 15.86it/s] 64%|██████▎   | 879/1380 [01:06<00:31, 15.92it/s] 64%|██████▍   | 881/1380 [01:07<00:31, 15.94it/s] 64%|██████▍   | 883/1380 [01:07<00:31, 15.94it/s] 64%|██████▍   | 885/1380 [01:07<00:31, 15.93it/s] 64%|██████▍   | 887/1380 [01:07<00:30, 15.94it/s] 64%|██████▍   | 889/1380 [01:07<00:30, 15.92it/s] 65%|██████▍   | 891/1380 [01:07<00:30, 15.91it/s] 65%|██████▍   | 893/1380 [01:07<00:30, 15.94it/s] 65%|██████▍   | 895/1380 [01:07<00:30, 15.95it/s] 65%|██████▌   | 897/1380 [01:08<00:30, 15.94it/s] 65%|██████▌   | 899/1380 [01:08<00:30, 15.94it/s] 65%|██████▌   | 901/1380 [01:08<00:29, 15.97it/s] 65%|██████▌   | 903/1380 [01:08<00:29, 15.99it/s] 66%|██████▌   | 905/1380 [01:08<00:29, 15.98it/s] 66%|██████▌   | 907/1380 [01:08<00:29, 15.96it/s] 66%|██████▌   | 909/1380 [01:08<00:29, 15.95it/s] 66%|██████▌   | 911/1380 [01:08<00:29, 15.96it/s] 66%|██████▌   | 913/1380 [01:09<00:29, 15.93it/s] 66%|██████▋   | 915/1380 [01:09<00:29, 15.94it/s] 66%|██████▋   | 917/1380 [01:09<00:28, 15.97it/s] 67%|██████▋   | 919/1380 [01:09<00:28, 15.98it/s] 67%|██████▋   | 921/1380 [01:09<00:28, 15.96it/s] 67%|██████▋   | 923/1380 [01:09<00:28, 15.97it/s] 67%|██████▋   | 925/1380 [01:09<00:28, 15.73it/s] 67%|██████▋   | 927/1380 [01:09<00:29, 15.61it/s] 67%|██████▋   | 929/1380 [01:10<00:28, 15.74it/s] 67%|██████▋   | 931/1380 [01:10<00:28, 15.80it/s] 68%|██████▊   | 933/1380 [01:10<00:28, 15.83it/s] 68%|██████▊   | 935/1380 [01:10<00:27, 15.91it/s] 68%|██████▊   | 937/1380 [01:10<00:27, 15.94it/s] 68%|██████▊   | 939/1380 [01:10<00:27, 15.94it/s] 68%|██████▊   | 941/1380 [01:10<00:27, 15.94it/s] 68%|██████▊   | 943/1380 [01:10<00:27, 15.83it/s] 68%|██████▊   | 945/1380 [01:11<00:27, 15.85it/s] 69%|██████▊   | 947/1380 [01:11<00:27, 15.84it/s] 69%|██████▉   | 949/1380 [01:11<00:27, 15.89it/s] 69%|██████▉   | 951/1380 [01:11<00:26, 15.94it/s] 69%|██████▉   | 953/1380 [01:11<00:26, 15.92it/s] 69%|██████▉   | 955/1380 [01:11<00:26, 15.93it/s] 69%|██████▉   | 957/1380 [01:11<00:26, 15.94it/s] 69%|██████▉   | 959/1380 [01:11<00:27, 15.17it/s] 70%|██████▉   | 961/1380 [01:12<00:27, 15.38it/s] 70%|██████▉   | 963/1380 [01:12<00:26, 15.58it/s] 70%|██████▉   | 965/1380 [01:12<00:26, 15.55it/s] 70%|███████   | 967/1380 [01:12<00:26, 15.65it/s] 70%|███████   | 969/1380 [01:12<00:26, 15.78it/s] 70%|███████   | 971/1380 [01:12<00:25, 15.86it/s] 71%|███████   | 973/1380 [01:12<00:25, 15.86it/s] 71%|███████   | 975/1380 [01:12<00:25, 15.86it/s] 71%|███████   | 977/1380 [01:13<00:25, 15.90it/s] 71%|███████   | 979/1380 [01:13<00:25, 15.93it/s] 71%|███████   | 981/1380 [01:13<00:25, 15.92it/s] 71%|███████   | 983/1380 [01:13<00:24, 15.93it/s] 71%|███████▏  | 985/1380 [01:13<00:24, 15.97it/s] 72%|███████▏  | 987/1380 [01:13<00:24, 15.98it/s] 72%|███████▏  | 989/1380 [01:13<00:24, 15.96it/s] 72%|███████▏  | 991/1380 [01:13<00:24, 15.95it/s] 72%|███████▏  | 993/1380 [01:14<00:24, 15.92it/s] 72%|███████▏  | 995/1380 [01:14<00:24, 15.89it/s] 72%|███████▏  | 997/1380 [01:14<00:24, 15.91it/s] 72%|███████▏  | 999/1380 [01:14<00:23, 15.96it/s] 73%|███████▎  | 1001/1380 [01:14<00:23, 15.97it/s] 73%|███████▎  | 1003/1380 [01:14<00:23, 15.95it/s] 73%|███████▎  | 1005/1380 [01:14<00:23, 15.95it/s] 73%|███████▎  | 1007/1380 [01:15<00:23, 15.99it/s] 73%|███████▎  | 1009/1380 [01:15<00:23, 15.98it/s] 73%|███████▎  | 1011/1380 [01:15<00:23, 15.91it/s] 73%|███████▎  | 1013/1380 [01:15<00:23, 15.81it/s] 74%|███████▎  | 1015/1380 [01:15<00:23, 15.73it/s] 74%|███████▎  | 1017/1380 [01:15<00:22, 15.79it/s] 74%|███████▍  | 1019/1380 [01:15<00:22, 15.88it/s] 74%|███████▍  | 1021/1380 [01:15<00:22, 15.90it/s] 74%|███████▍  | 1023/1380 [01:16<00:22, 15.90it/s] 74%|███████▍  | 1025/1380 [01:16<00:22, 15.92it/s] 74%|███████▍  | 1027/1380 [01:16<00:22, 15.97it/s] 75%|███████▍  | 1029/1380 [01:16<00:21, 15.98it/s] 75%|███████▍  | 1031/1380 [01:16<00:21, 15.96it/s] 75%|███████▍  | 1033/1380 [01:16<00:21, 15.97it/s] 75%|███████▌  | 1035/1380 [01:16<00:21, 16.00it/s] 75%|███████▌  | 1037/1380 [01:16<00:21, 15.99it/s] 75%|███████▌  | 1039/1380 [01:17<00:21, 15.96it/s] 75%|███████▌  | 1041/1380 [01:17<00:21, 15.97it/s] 76%|███████▌  | 1043/1380 [01:17<00:21, 15.98it/s] 76%|███████▌  | 1045/1380 [01:17<00:21, 15.95it/s] 76%|███████▌  | 1047/1380 [01:17<00:20, 15.91it/s] 76%|███████▌  | 1049/1380 [01:17<00:20, 15.94it/s] 76%|███████▌  | 1051/1380 [01:17<00:20, 15.95it/s] 76%|███████▋  | 1053/1380 [01:17<00:20, 15.92it/s] 76%|███████▋  | 1055/1380 [01:18<00:20, 15.93it/s] 77%|███████▋  | 1057/1380 [01:18<00:20, 15.95it/s] 77%|███████▋  | 1059/1380 [01:18<00:20, 15.96it/s] 77%|███████▋  | 1061/1380 [01:18<00:19, 15.96it/s] 77%|███████▋  | 1063/1380 [01:18<00:19, 15.97it/s] 77%|███████▋  | 1065/1380 [01:18<00:19, 15.96it/s] 77%|███████▋  | 1067/1380 [01:18<00:19, 15.84it/s] 77%|███████▋  | 1069/1380 [01:18<00:19, 15.76it/s] 78%|███████▊  | 1071/1380 [01:19<00:19, 15.77it/s] 78%|███████▊  | 1073/1380 [01:19<00:19, 15.81it/s] 78%|███████▊  | 1075/1380 [01:19<00:19, 15.88it/s] 78%|███████▊  | 1077/1380 [01:19<00:19, 15.92it/s] 78%|███████▊  | 1079/1380 [01:19<00:18, 15.93it/s] 78%|███████▊  | 1081/1380 [01:19<00:18, 15.91it/s] 78%|███████▊  | 1083/1380 [01:19<00:19, 15.62it/s] 79%|███████▊  | 1085/1380 [01:19<00:18, 15.70it/s] 79%|███████▉  | 1087/1380 [01:20<00:18, 15.80it/s] 79%|███████▉  | 1089/1380 [01:20<00:18, 15.74it/s] 79%|███████▉  | 1091/1380 [01:20<00:18, 15.79it/s] 79%|███████▉  | 1093/1380 [01:20<00:18, 15.84it/s] 79%|███████▉  | 1095/1380 [01:20<00:17, 15.90it/s] 79%|███████▉  | 1097/1380 [01:20<00:17, 15.92it/s] 80%|███████▉  | 1099/1380 [01:20<00:17, 15.93it/s] 80%|███████▉  | 1101/1380 [01:20<00:17, 15.96it/s] 80%|███████▉  | 1103/1380 [01:21<00:17, 16.00it/s]                                                    80%|████████  | 1104/1380 [01:21<00:17, 16.00it/s][INFO|trainer.py:755] 2023-11-15 23:22:45,979 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 23:22:45,981 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 23:22:45,982 >>   Num examples = 2204
[INFO|trainer.py:3134] 2023-11-15 23:22:45,982 >>   Batch size = 8
{'eval_loss': 0.38457444310188293, 'eval_accuracy': 0.8570780399274047, 'eval_micro_f1': 0.8570780399274048, 'eval_macro_f1': 0.8411889062244033, 'eval_runtime': 3.4234, 'eval_samples_per_second': 643.811, 'eval_steps_per_second': 80.622, 'epoch': 3.0}
{'loss': 0.2999, 'learning_rate': 0.0001, 'epoch': 4.0}

  0%|          | 0/276 [00:00<?, ?it/s][A
  4%|▎         | 10/276 [00:00<00:02, 93.09it/s][A
  7%|▋         | 20/276 [00:00<00:02, 86.62it/s][A
 11%|█         | 29/276 [00:00<00:02, 85.05it/s][A
 14%|█▍        | 38/276 [00:00<00:02, 83.90it/s][A
 17%|█▋        | 47/276 [00:00<00:02, 83.57it/s][A
 20%|██        | 56/276 [00:00<00:02, 83.37it/s][A
 24%|██▎       | 65/276 [00:00<00:02, 83.16it/s][A
 27%|██▋       | 74/276 [00:00<00:02, 82.37it/s][A
 30%|███       | 83/276 [00:00<00:02, 82.20it/s][A
 33%|███▎      | 92/276 [00:01<00:02, 82.38it/s][A
 37%|███▋      | 101/276 [00:01<00:02, 82.52it/s][A
 40%|███▉      | 110/276 [00:01<00:02, 82.56it/s][A
 43%|████▎     | 119/276 [00:01<00:01, 82.67it/s][A
 46%|████▋     | 128/276 [00:01<00:01, 82.72it/s][A
 50%|████▉     | 137/276 [00:01<00:01, 82.75it/s][A
 53%|█████▎    | 146/276 [00:01<00:01, 82.50it/s][A
 56%|█████▌    | 155/276 [00:01<00:01, 82.63it/s][A
 59%|█████▉    | 164/276 [00:01<00:01, 82.74it/s][A
 63%|██████▎   | 173/276 [00:02<00:01, 82.75it/s][A
 66%|██████▌   | 182/276 [00:02<00:01, 82.19it/s][A
 69%|██████▉   | 191/276 [00:02<00:01, 82.49it/s][A
 72%|███████▏  | 200/276 [00:02<00:00, 82.69it/s][A
 76%|███████▌  | 209/276 [00:02<00:00, 82.30it/s][A
 79%|███████▉  | 218/276 [00:02<00:00, 82.15it/s][A
 82%|████████▏ | 227/276 [00:02<00:00, 81.82it/s][A
 86%|████████▌ | 236/276 [00:02<00:00, 81.88it/s][A
 89%|████████▉ | 245/276 [00:02<00:00, 81.34it/s][A
 92%|█████████▏| 254/276 [00:03<00:00, 81.74it/s][A
 95%|█████████▌| 263/276 [00:03<00:00, 79.10it/s][A
 99%|█████████▊| 272/276 [00:03<00:00, 80.47it/s][A                                                   
                                                 [A 80%|████████  | 1104/1380 [01:24<00:17, 16.00it/s]
100%|██████████| 276/276 [00:03<00:00, 80.47it/s][A
                                                 [A 80%|████████  | 1105/1380 [01:24<02:37,  1.75it/s] 80%|████████  | 1107/1380 [01:24<01:54,  2.39it/s] 80%|████████  | 1109/1380 [01:24<01:24,  3.20it/s] 81%|████████  | 1111/1380 [01:24<01:03,  4.21it/s] 81%|████████  | 1113/1380 [01:25<00:49,  5.41it/s] 81%|████████  | 1115/1380 [01:25<00:39,  6.73it/s] 81%|████████  | 1117/1380 [01:25<00:32,  8.12it/s] 81%|████████  | 1119/1380 [01:25<00:27,  9.53it/s] 81%|████████  | 1121/1380 [01:25<00:23, 10.84it/s] 81%|████████▏ | 1123/1380 [01:25<00:21, 11.98it/s] 82%|████████▏ | 1125/1380 [01:25<00:19, 12.97it/s] 82%|████████▏ | 1127/1380 [01:25<00:18, 13.76it/s] 82%|████████▏ | 1129/1380 [01:26<00:17, 14.35it/s] 82%|████████▏ | 1131/1380 [01:26<00:16, 14.79it/s] 82%|████████▏ | 1133/1380 [01:26<00:16, 15.15it/s] 82%|████████▏ | 1135/1380 [01:26<00:15, 15.40it/s] 82%|████████▏ | 1137/1380 [01:26<00:15, 15.36it/s] 83%|████████▎ | 1139/1380 [01:26<00:15, 15.54it/s] 83%|████████▎ | 1141/1380 [01:26<00:15, 15.67it/s] 83%|████████▎ | 1143/1380 [01:26<00:15, 15.65it/s] 83%|████████▎ | 1145/1380 [01:27<00:14, 15.76it/s] 83%|████████▎ | 1147/1380 [01:27<00:14, 15.83it/s] 83%|████████▎ | 1149/1380 [01:27<00:14, 15.86it/s] 83%|████████▎ | 1151/1380 [01:27<00:14, 15.88it/s] 84%|████████▎ | 1153/1380 [01:27<00:14, 15.88it/s] 84%|████████▎ | 1155/1380 [01:27<00:14, 15.83it/s] 84%|████████▍ | 1157/1380 [01:27<00:14, 15.67it/s] 84%|████████▍ | 1159/1380 [01:27<00:14, 15.58it/s] 84%|████████▍ | 1161/1380 [01:28<00:14, 15.52it/s] 84%|████████▍ | 1163/1380 [01:28<00:14, 15.45it/s] 84%|████████▍ | 1165/1380 [01:28<00:13, 15.42it/s] 85%|████████▍ | 1167/1380 [01:28<00:13, 15.33it/s] 85%|████████▍ | 1169/1380 [01:28<00:14, 15.02it/s] 85%|████████▍ | 1171/1380 [01:28<00:13, 15.05it/s] 85%|████████▌ | 1173/1380 [01:28<00:13, 15.12it/s] 85%|████████▌ | 1175/1380 [01:29<00:13, 15.17it/s] 85%|████████▌ | 1177/1380 [01:29<00:13, 15.25it/s] 85%|████████▌ | 1179/1380 [01:29<00:12, 15.47it/s] 86%|████████▌ | 1181/1380 [01:29<00:12, 15.59it/s] 86%|████████▌ | 1183/1380 [01:29<00:12, 15.67it/s] 86%|████████▌ | 1185/1380 [01:29<00:12, 15.76it/s] 86%|████████▌ | 1187/1380 [01:29<00:12, 15.83it/s] 86%|████████▌ | 1189/1380 [01:29<00:12, 15.61it/s] 86%|████████▋ | 1191/1380 [01:30<00:12, 15.65it/s] 86%|████████▋ | 1193/1380 [01:30<00:11, 15.73it/s] 87%|████████▋ | 1195/1380 [01:30<00:11, 15.77it/s] 87%|████████▋ | 1197/1380 [01:30<00:11, 15.86it/s] 87%|████████▋ | 1199/1380 [01:30<00:11, 15.91it/s] 87%|████████▋ | 1201/1380 [01:30<00:11, 15.93it/s] 87%|████████▋ | 1203/1380 [01:30<00:11, 15.93it/s] 87%|████████▋ | 1205/1380 [01:30<00:10, 15.98it/s] 87%|████████▋ | 1207/1380 [01:31<00:10, 15.99it/s] 88%|████████▊ | 1209/1380 [01:31<00:10, 15.98it/s] 88%|████████▊ | 1211/1380 [01:31<00:10, 15.97it/s] 88%|████████▊ | 1213/1380 [01:31<00:10, 15.99it/s] 88%|████████▊ | 1215/1380 [01:31<00:10, 15.74it/s] 88%|████████▊ | 1217/1380 [01:31<00:10, 15.81it/s] 88%|████████▊ | 1219/1380 [01:31<00:10, 15.88it/s] 88%|████████▊ | 1221/1380 [01:31<00:09, 15.91it/s] 89%|████████▊ | 1223/1380 [01:32<00:09, 15.90it/s] 89%|████████▉ | 1225/1380 [01:32<00:09, 15.92it/s] 89%|████████▉ | 1227/1380 [01:32<00:09, 15.95it/s] 89%|████████▉ | 1229/1380 [01:32<00:09, 15.95it/s] 89%|████████▉ | 1231/1380 [01:32<00:09, 15.95it/s] 89%|████████▉ | 1233/1380 [01:32<00:09, 15.97it/s] 89%|████████▉ | 1235/1380 [01:32<00:09, 15.97it/s] 90%|████████▉ | 1237/1380 [01:32<00:08, 15.95it/s] 90%|████████▉ | 1239/1380 [01:33<00:08, 15.94it/s] 90%|████████▉ | 1241/1380 [01:33<00:08, 15.96it/s] 90%|█████████ | 1243/1380 [01:33<00:08, 15.96it/s] 90%|█████████ | 1245/1380 [01:33<00:08, 15.93it/s] 90%|█████████ | 1247/1380 [01:33<00:08, 15.92it/s] 91%|█████████ | 1249/1380 [01:33<00:08, 15.91it/s] 91%|█████████ | 1251/1380 [01:33<00:08, 15.91it/s] 91%|█████████ | 1253/1380 [01:33<00:07, 15.91it/s] 91%|█████████ | 1255/1380 [01:34<00:07, 15.94it/s] 91%|█████████ | 1257/1380 [01:34<00:07, 15.93it/s] 91%|█████████ | 1259/1380 [01:34<00:07, 15.89it/s] 91%|█████████▏| 1261/1380 [01:34<00:07, 15.87it/s] 92%|█████████▏| 1263/1380 [01:34<00:07, 15.89it/s] 92%|█████████▏| 1265/1380 [01:34<00:07, 15.87it/s] 92%|█████████▏| 1267/1380 [01:34<00:07, 15.79it/s] 92%|█████████▏| 1269/1380 [01:34<00:06, 15.86it/s] 92%|█████████▏| 1271/1380 [01:35<00:06, 15.88it/s] 92%|█████████▏| 1273/1380 [01:35<00:06, 15.87it/s] 92%|█████████▏| 1275/1380 [01:35<00:06, 15.81it/s] 93%|█████████▎| 1277/1380 [01:35<00:06, 15.70it/s] 93%|█████████▎| 1279/1380 [01:35<00:06, 15.78it/s] 93%|█████████▎| 1281/1380 [01:35<00:06, 15.85it/s] 93%|█████████▎| 1283/1380 [01:35<00:06, 15.89it/s] 93%|█████████▎| 1285/1380 [01:35<00:05, 15.90it/s] 93%|█████████▎| 1287/1380 [01:36<00:05, 15.93it/s] 93%|█████████▎| 1289/1380 [01:36<00:05, 15.95it/s] 94%|█████████▎| 1291/1380 [01:36<00:05, 15.85it/s] 94%|█████████▎| 1293/1380 [01:36<00:05, 15.88it/s] 94%|█████████▍| 1295/1380 [01:36<00:05, 15.93it/s] 94%|█████████▍| 1297/1380 [01:36<00:05, 15.93it/s] 94%|█████████▍| 1299/1380 [01:36<00:05, 15.92it/s] 94%|█████████▍| 1301/1380 [01:36<00:04, 15.95it/s] 94%|█████████▍| 1303/1380 [01:37<00:04, 15.96it/s] 95%|█████████▍| 1305/1380 [01:37<00:04, 15.96it/s] 95%|█████████▍| 1307/1380 [01:37<00:04, 15.95it/s] 95%|█████████▍| 1309/1380 [01:37<00:04, 15.99it/s] 95%|█████████▌| 1311/1380 [01:37<00:04, 15.99it/s] 95%|█████████▌| 1313/1380 [01:37<00:04, 15.96it/s] 95%|█████████▌| 1315/1380 [01:37<00:04, 15.95it/s] 95%|█████████▌| 1317/1380 [01:37<00:03, 15.98it/s] 96%|█████████▌| 1319/1380 [01:38<00:03, 15.98it/s] 96%|█████████▌| 1321/1380 [01:38<00:03, 15.95it/s] 96%|█████████▌| 1323/1380 [01:38<00:03, 15.95it/s] 96%|█████████▌| 1325/1380 [01:38<00:03, 15.99it/s] 96%|█████████▌| 1327/1380 [01:38<00:03, 15.98it/s] 96%|█████████▋| 1329/1380 [01:38<00:03, 15.90it/s] 96%|█████████▋| 1331/1380 [01:38<00:03, 15.87it/s] 97%|█████████▋| 1333/1380 [01:38<00:02, 15.89it/s] 97%|█████████▋| 1335/1380 [01:39<00:02, 15.89it/s] 97%|█████████▋| 1337/1380 [01:39<00:02, 15.91it/s] 97%|█████████▋| 1339/1380 [01:39<00:02, 15.93it/s] 97%|█████████▋| 1341/1380 [01:39<00:02, 15.93it/s] 97%|█████████▋| 1343/1380 [01:39<00:02, 15.91it/s] 97%|█████████▋| 1345/1380 [01:39<00:02, 15.87it/s] 98%|█████████▊| 1347/1380 [01:39<00:02, 15.85it/s] 98%|█████████▊| 1349/1380 [01:39<00:01, 15.85it/s] 98%|█████████▊| 1351/1380 [01:40<00:01, 15.89it/s] 98%|█████████▊| 1353/1380 [01:40<00:01, 15.91it/s] 98%|█████████▊| 1355/1380 [01:40<00:01, 15.74it/s] 98%|█████████▊| 1357/1380 [01:40<00:01, 15.73it/s] 98%|█████████▊| 1359/1380 [01:40<00:01, 15.78it/s] 99%|█████████▊| 1361/1380 [01:40<00:01, 15.81it/s] 99%|█████████▉| 1363/1380 [01:40<00:01, 15.87it/s] 99%|█████████▉| 1365/1380 [01:40<00:00, 15.91it/s] 99%|█████████▉| 1367/1380 [01:41<00:00, 15.91it/s] 99%|█████████▉| 1369/1380 [01:41<00:00, 15.91it/s] 99%|█████████▉| 1371/1380 [01:41<00:00, 15.96it/s] 99%|█████████▉| 1373/1380 [01:41<00:00, 15.95it/s]100%|█████████▉| 1375/1380 [01:41<00:00, 15.80it/s]100%|█████████▉| 1377/1380 [01:41<00:00, 15.86it/s]100%|█████████▉| 1379/1380 [01:41<00:00, 15.92it/s]                                                   100%|██████████| 1380/1380 [01:41<00:00, 15.92it/s][INFO|trainer.py:755] 2023-11-15 23:23:06,794 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 23:23:06,796 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 23:23:06,796 >>   Num examples = 2204
[INFO|trainer.py:3134] 2023-11-15 23:23:06,797 >>   Batch size = 8
{'eval_loss': 0.3733028173446655, 'eval_accuracy': 0.8602540834845736, 'eval_micro_f1': 0.8602540834845736, 'eval_macro_f1': 0.8476358925530034, 'eval_runtime': 3.408, 'eval_samples_per_second': 646.707, 'eval_steps_per_second': 80.985, 'epoch': 4.0}
{'loss': 0.2608, 'learning_rate': 0.0, 'epoch': 5.0}

  0%|          | 0/276 [00:00<?, ?it/s][A
  4%|▎         | 10/276 [00:00<00:02, 93.93it/s][A
  7%|▋         | 20/276 [00:00<00:02, 86.93it/s][A
 11%|█         | 29/276 [00:00<00:02, 85.07it/s][A
 14%|█▍        | 38/276 [00:00<00:02, 84.15it/s][A
 17%|█▋        | 47/276 [00:00<00:02, 83.39it/s][A
 20%|██        | 56/276 [00:00<00:02, 82.58it/s][A
 24%|██▎       | 65/276 [00:00<00:02, 82.48it/s][A
 27%|██▋       | 74/276 [00:00<00:02, 82.53it/s][A
 30%|███       | 83/276 [00:00<00:02, 82.62it/s][A
 33%|███▎      | 92/276 [00:01<00:02, 82.74it/s][A
 37%|███▋      | 101/276 [00:01<00:02, 82.70it/s][A
 40%|███▉      | 110/276 [00:01<00:02, 82.73it/s][A
 43%|████▎     | 119/276 [00:01<00:01, 82.88it/s][A
 46%|████▋     | 128/276 [00:01<00:01, 82.91it/s][A
 50%|████▉     | 137/276 [00:01<00:01, 83.03it/s][A
 53%|█████▎    | 146/276 [00:01<00:01, 82.90it/s][A
 56%|█████▌    | 155/276 [00:01<00:01, 82.88it/s][A
 59%|█████▉    | 164/276 [00:01<00:01, 82.96it/s][A
 63%|██████▎   | 173/276 [00:02<00:01, 82.75it/s][A
 66%|██████▌   | 182/276 [00:02<00:01, 82.82it/s][A
 69%|██████▉   | 191/276 [00:02<00:01, 81.43it/s][A
 72%|███████▏  | 200/276 [00:02<00:00, 81.96it/s][A
 76%|███████▌  | 209/276 [00:02<00:00, 82.30it/s][A
 79%|███████▉  | 218/276 [00:02<00:00, 82.72it/s][A
 82%|████████▏ | 227/276 [00:02<00:00, 82.39it/s][A
 86%|████████▌ | 236/276 [00:02<00:00, 81.96it/s][A
 89%|████████▉ | 245/276 [00:02<00:00, 82.24it/s][A
 92%|█████████▏| 254/276 [00:03<00:00, 82.27it/s][A
 95%|█████████▌| 263/276 [00:03<00:00, 81.93it/s][A
 99%|█████████▊| 272/276 [00:03<00:00, 82.22it/s][A                                                   
                                                 [A100%|██████████| 1380/1380 [01:45<00:00, 15.92it/s]
100%|██████████| 276/276 [00:03<00:00, 82.22it/s][A
                                                 [A[INFO|trainer.py:1963] 2023-11-15 23:23:10,195 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                   100%|██████████| 1380/1380 [01:45<00:00, 15.92it/s]100%|██████████| 1380/1380 [01:45<00:00, 13.11it/s]
[INFO|trainer.py:2855] 2023-11-15 23:23:10,198 >> Saving model checkpoint to ./result/acl_roberta-base_seed1_adapter
[INFO|configuration_utils.py:460] 2023-11-15 23:23:10,202 >> Configuration saved in ./result/acl_roberta-base_seed1_adapter/config.json
[INFO|modeling_utils.py:1997] 2023-11-15 23:23:11,327 >> Model weights saved in ./result/acl_roberta-base_seed1_adapter/pytorch_model.bin
[INFO|tokenization_utils_base.py:2235] 2023-11-15 23:23:11,331 >> tokenizer config file saved in ./result/acl_roberta-base_seed1_adapter/tokenizer_config.json
[INFO|tokenization_utils_base.py:2242] 2023-11-15 23:23:11,333 >> Special tokens file saved in ./result/acl_roberta-base_seed1_adapter/special_tokens_map.json
{'eval_loss': 0.40083372592926025, 'eval_accuracy': 0.8579854809437386, 'eval_micro_f1': 0.8579854809437386, 'eval_macro_f1': 0.8454737103230042, 'eval_runtime': 3.3947, 'eval_samples_per_second': 649.253, 'eval_steps_per_second': 81.304, 'epoch': 5.0}
{'train_runtime': 105.2967, 'train_samples_per_second': 418.627, 'train_steps_per_second': 13.106, 'train_loss': 0.357260026793549, 'epoch': 5.0}
***** train metrics *****
  epoch                    =        5.0
  train_loss               =     0.3573
  train_runtime            = 0:01:45.29
  train_samples            =       8816
  train_samples_per_second =    418.627
  train_steps_per_second   =     13.106
11/15/2023 23:23:11 - INFO - __main__ - *** Evaluate ***
[INFO|trainer.py:755] 2023-11-15 23:23:11,470 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 23:23:11,471 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 23:23:11,472 >>   Num examples = 2204
[INFO|trainer.py:3134] 2023-11-15 23:23:11,472 >>   Batch size = 8
  0%|          | 0/276 [00:00<?, ?it/s]  4%|▎         | 10/276 [00:00<00:02, 93.70it/s]  7%|▋         | 20/276 [00:00<00:02, 87.52it/s] 11%|█         | 29/276 [00:00<00:02, 84.74it/s] 14%|█▍        | 38/276 [00:00<00:02, 83.56it/s] 17%|█▋        | 47/276 [00:00<00:02, 83.50it/s] 20%|██        | 56/276 [00:00<00:02, 83.18it/s] 24%|██▎       | 65/276 [00:00<00:02, 83.28it/s] 27%|██▋       | 74/276 [00:00<00:02, 83.12it/s] 30%|███       | 83/276 [00:00<00:02, 83.08it/s] 33%|███▎      | 92/276 [00:01<00:02, 83.06it/s] 37%|███▋      | 101/276 [00:01<00:02, 83.17it/s] 40%|███▉      | 110/276 [00:01<00:02, 82.88it/s] 43%|████▎     | 119/276 [00:01<00:01, 83.08it/s] 46%|████▋     | 128/276 [00:01<00:01, 83.32it/s] 50%|████▉     | 137/276 [00:01<00:01, 83.18it/s] 53%|█████▎    | 146/276 [00:01<00:01, 83.38it/s] 56%|█████▌    | 155/276 [00:01<00:01, 83.46it/s] 59%|█████▉    | 164/276 [00:01<00:01, 83.59it/s] 63%|██████▎   | 173/276 [00:02<00:01, 83.43it/s] 66%|██████▌   | 182/276 [00:02<00:01, 83.50it/s] 69%|██████▉   | 191/276 [00:02<00:01, 83.64it/s] 72%|███████▏  | 200/276 [00:02<00:00, 83.66it/s] 76%|███████▌  | 209/276 [00:02<00:00, 83.78it/s] 79%|███████▉  | 218/276 [00:02<00:00, 84.00it/s] 82%|████████▏ | 227/276 [00:02<00:00, 83.39it/s] 86%|████████▌ | 236/276 [00:02<00:00, 82.92it/s] 89%|████████▉ | 245/276 [00:02<00:00, 82.93it/s] 92%|█████████▏| 254/276 [00:03<00:00, 83.11it/s] 95%|█████████▌| 263/276 [00:03<00:00, 82.89it/s] 99%|█████████▊| 272/276 [00:03<00:00, 82.87it/s]100%|██████████| 276/276 [00:03<00:00, 82.46it/s]
***** eval metrics *****
  epoch                   =        5.0
  eval_accuracy           =      0.858
  eval_loss               =     0.4008
  eval_macro_f1           =     0.8455
  eval_micro_f1           =      0.858
  eval_runtime            = 0:00:03.36
  eval_samples            =       2204
  eval_samples_per_second =    655.364
  eval_steps_per_second   =     82.069
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                  eval/accuracy ▅▁▆█▆▆
wandb:                      eval/loss ▆█▃▁▆▆
wandb:                  eval/macro_f1 ▁▁▄█▇▇
wandb:                  eval/micro_f1 ▅▁▆█▆▆
wandb:                   eval/runtime █▆█▆▅▁
wandb:        eval/samples_per_second ▁▃▁▃▄█
wandb:          eval/steps_per_second ▁▃▁▃▄█
wandb:                    train/epoch ▁▁▃▃▅▅▆▆████
wandb:              train/global_step ▁▁▃▃▅▅▆▆████
wandb:            train/learning_rate █▆▅▃▁
wandb:                     train/loss █▄▃▂▁
wandb:               train/total_flos ▁
wandb:               train/train_loss ▁
wandb:            train/train_runtime ▁
wandb: train/train_samples_per_second ▁
wandb:   train/train_steps_per_second ▁
wandb: 
wandb: Run summary:
wandb:                  eval/accuracy 0.85799
wandb:                      eval/loss 0.40083
wandb:                  eval/macro_f1 0.84547
wandb:                  eval/micro_f1 0.85799
wandb:                   eval/runtime 3.363
wandb:        eval/samples_per_second 655.364
wandb:          eval/steps_per_second 82.069
wandb:                    train/epoch 5.0
wandb:              train/global_step 1380
wandb:            train/learning_rate 0.0
wandb:                     train/loss 0.2608
wandb:               train/total_flos 1464896356669440.0
wandb:               train/train_loss 0.35726
wandb:            train/train_runtime 105.2967
wandb: train/train_samples_per_second 418.627
wandb:   train/train_steps_per_second 13.106
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /ceph/home/wangyifei/a3/wandb/offline-run-20231115_232006-xg8rm5dn
wandb: Find logs at: ./wandb/offline-run-20231115_232006-xg8rm5dn/logs
(ModelArguments(model_name_or_path='roberta-base', cache_dir='./cache', use_adapter=True, use_lora=False), DataTrainingArguments(dataset_name='agnews_sup', max_seq_length=64), TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.0005,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=./result/agnews_sup_roberta-base_seed1_adapter/runs/Nov15_23-23-27_node013,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.EPOCH,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=5.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
output_dir=./result/agnews_sup_roberta-base_seed1_adapter,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
resume_from_checkpoint=None,
run_name=./result/agnews_sup_roberta-base_seed1_adapter,
save_on_each_node=False,
save_safetensors=False,
save_steps=500,
save_strategy=IntervalStrategy.NO,
save_total_limit=None,
seed=222,
sharded_ddp=[],
skip_memory_metrics=True,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
))
11/15/2023 23:23:27 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False
11/15/2023 23:23:27 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.0005,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=./result/agnews_sup_roberta-base_seed1_adapter/runs/Nov15_23-23-26_node013,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.EPOCH,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=5.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
output_dir=./result/agnews_sup_roberta-base_seed1_adapter,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
resume_from_checkpoint=None,
run_name=./result/agnews_sup_roberta-base_seed1_adapter,
save_on_each_node=False,
save_safetensors=False,
save_steps=500,
save_strategy=IntervalStrategy.NO,
save_total_limit=None,
seed=222,
sharded_ddp=[],
skip_memory_metrics=True,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
wandb: Tracking run with wandb version 0.15.12
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[INFO|configuration_utils.py:715] 2023-11-15 23:23:43,062 >> loading configuration file config.json from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/config.json
[INFO|configuration_utils.py:775] 2023-11-15 23:23:43,071 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.33.3",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_auto.py:535] 2023-11-15 23:23:53,088 >> Could not locate the tokenizer configuration file, will try to use the model config instead.
[INFO|configuration_utils.py:715] 2023-11-15 23:24:03,104 >> loading configuration file config.json from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/config.json
[INFO|configuration_utils.py:775] 2023-11-15 23:24:03,105 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.33.3",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:1852] 2023-11-15 23:24:23,147 >> loading file vocab.json from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/vocab.json
[INFO|tokenization_utils_base.py:1852] 2023-11-15 23:24:23,148 >> loading file merges.txt from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/merges.txt
[INFO|tokenization_utils_base.py:1852] 2023-11-15 23:24:23,148 >> loading file tokenizer.json from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/tokenizer.json
[INFO|tokenization_utils_base.py:1852] 2023-11-15 23:24:23,148 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1852] 2023-11-15 23:24:23,149 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1852] 2023-11-15 23:24:23,149 >> loading file tokenizer_config.json from cache at None
[INFO|configuration_utils.py:715] 2023-11-15 23:24:23,150 >> loading configuration file config.json from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/config.json
[INFO|configuration_utils.py:775] 2023-11-15 23:24:23,151 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.33.3",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:2862] 2023-11-15 23:24:43,318 >> loading weights file model.safetensors from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/model.safetensors
[INFO|modeling_utils.py:3638] 2023-11-15 23:24:44,046 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3650] 2023-11-15 23:24:44,047 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
parameters 1488196
model_max_length: 512
Running tokenizer on dataset:   0%|          | 0/6840 [00:00<?, ? examples/s]Running tokenizer on dataset:  29%|██▉       | 2000/6840 [00:00<00:00, 14400.13 examples/s]Running tokenizer on dataset:  58%|█████▊    | 4000/6840 [00:00<00:00, 15337.12 examples/s]Running tokenizer on dataset:  88%|████████▊ | 6000/6840 [00:00<00:00, 17072.94 examples/s]Running tokenizer on dataset: 100%|██████████| 6840/6840 [00:00<00:00, 16672.80 examples/s]
Running tokenizer on dataset:   0%|          | 0/760 [00:00<?, ? examples/s]Running tokenizer on dataset: 100%|██████████| 760/760 [00:00<00:00, 21436.64 examples/s]
11/15/2023 23:24:45 - INFO - __main__ - Sample 6380 of the training set: {'text': 'Mich. Elephant Gets Therapy for Arthritis ROYAL OAK, Mich. - Like any patient, Wanda needs positive reinforcement to wrestle through her physical therapy...', 'label': 3, 'input_ids': [0, 40648, 4, 36516, 32810, 25889, 13, 1586, 44491, 10033, 975, 2118, 384, 7140, 6, 9605, 4, 111, 2011, 143, 3186, 6, 305, 5219, 782, 1313, 37700, 7, 27881, 149, 69, 2166, 5804, 734, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
11/15/2023 23:24:45 - INFO - __main__ - Sample 883 of the training set: {'text': "Chicago to Hold EBay Auction to Raise Money for Cultural Programs City officials hope there are people willing to pay plenty of money to own a vintage Playboy Bunny costume, toss green dye into the Chicago River or throw a dinner party prepared by Oprah Winfrey's chef.", 'label': 2, 'input_ids': [0, 21897, 7, 10357, 381, 20861, 26342, 7, 39208, 8028, 13, 15309, 25740, 412, 503, 1034, 89, 32, 82, 2882, 7, 582, 2710, 9, 418, 7, 308, 10, 12669, 24526, 33470, 12111, 6, 13027, 2272, 31800, 88, 5, 1568, 1995, 50, 3211, 10, 3630, 537, 2460, 30, 20015, 5711, 16127, 18, 8172, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
11/15/2023 23:24:45 - INFO - __main__ - Sample 1927 of the training set: {'text': 'Astros 10, Pirates 5 HOUSTON Mike Lamb went four-for-five with a homer and four RB-Is to lead the Houston Astros to their ninth straight win with a 10-to-five victory over the Pittsburgh Pirates today.', 'label': 0, 'input_ids': [0, 39021, 3985, 158, 6, 11114, 195, 30392, 12917, 1483, 13132, 439, 237, 12, 1990, 12, 9579, 19, 10, 8646, 8, 237, 11191, 12, 6209, 7, 483, 5, 2499, 10938, 7, 49, 5127, 1359, 339, 19, 10, 158, 12, 560, 12, 9579, 1124, 81, 5, 4386, 11114, 452, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
11/15/2023 23:24:45 - WARNING - accelerate.utils.other - Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[INFO|trainer.py:755] 2023-11-15 23:24:46,557 >> The following columns in the training set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:1715] 2023-11-15 23:24:46,568 >> ***** Running training *****
[INFO|trainer.py:1716] 2023-11-15 23:24:46,569 >>   Num examples = 6,840
[INFO|trainer.py:1717] 2023-11-15 23:24:46,569 >>   Num Epochs = 5
[INFO|trainer.py:1718] 2023-11-15 23:24:46,569 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1721] 2023-11-15 23:24:46,569 >>   Total train batch size (w. parallel, distributed & accumulation) = 32
[INFO|trainer.py:1722] 2023-11-15 23:24:46,570 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:1723] 2023-11-15 23:24:46,570 >>   Total optimization steps = 1,070
[INFO|trainer.py:1724] 2023-11-15 23:24:46,571 >>   Number of trainable parameters = 1,488,196
[INFO|integration_utils.py:716] 2023-11-15 23:24:46,572 >> Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"
  0%|          | 0/1070 [00:00<?, ?it/s]  0%|          | 1/1070 [00:01<18:04,  1.01s/it]  0%|          | 3/1070 [00:01<05:30,  3.23it/s]  0%|          | 5/1070 [00:01<03:14,  5.48it/s]  1%|          | 7/1070 [00:01<02:20,  7.58it/s]  1%|          | 9/1070 [00:01<01:52,  9.42it/s]  1%|          | 11/1070 [00:01<01:36, 10.95it/s]  1%|          | 13/1070 [00:01<01:26, 12.20it/s]  1%|▏         | 15/1070 [00:01<01:20, 13.09it/s]  2%|▏         | 17/1070 [00:02<01:16, 13.85it/s]  2%|▏         | 19/1070 [00:02<01:12, 14.41it/s]  2%|▏         | 21/1070 [00:02<01:10, 14.78it/s]  2%|▏         | 23/1070 [00:02<01:09, 15.17it/s]  2%|▏         | 25/1070 [00:02<01:07, 15.43it/s]  3%|▎         | 27/1070 [00:02<01:06, 15.61it/s]  3%|▎         | 29/1070 [00:02<01:06, 15.71it/s]  3%|▎         | 31/1070 [00:02<01:06, 15.69it/s]  3%|▎         | 33/1070 [00:03<01:05, 15.76it/s]  3%|▎         | 35/1070 [00:03<01:05, 15.75it/s]  3%|▎         | 37/1070 [00:03<01:05, 15.85it/s]  4%|▎         | 39/1070 [00:03<01:05, 15.86it/s]  4%|▍         | 41/1070 [00:03<01:04, 15.83it/s]  4%|▍         | 43/1070 [00:03<01:05, 15.76it/s]  4%|▍         | 45/1070 [00:03<01:05, 15.67it/s]  4%|▍         | 47/1070 [00:03<01:06, 15.49it/s]  5%|▍         | 49/1070 [00:04<01:05, 15.50it/s]  5%|▍         | 51/1070 [00:04<01:05, 15.54it/s]  5%|▍         | 53/1070 [00:04<01:05, 15.62it/s]  5%|▌         | 55/1070 [00:04<01:04, 15.75it/s]  5%|▌         | 57/1070 [00:04<01:03, 15.86it/s]  6%|▌         | 59/1070 [00:04<01:03, 15.94it/s]  6%|▌         | 61/1070 [00:04<01:03, 15.83it/s]  6%|▌         | 63/1070 [00:04<01:03, 15.90it/s]  6%|▌         | 65/1070 [00:05<01:02, 15.97it/s]  6%|▋         | 67/1070 [00:05<01:02, 16.05it/s]  6%|▋         | 69/1070 [00:05<01:02, 16.09it/s]  7%|▋         | 71/1070 [00:05<01:02, 16.06it/s]  7%|▋         | 73/1070 [00:05<01:02, 16.07it/s]  7%|▋         | 75/1070 [00:05<01:01, 16.08it/s]  7%|▋         | 77/1070 [00:05<01:01, 16.13it/s]  7%|▋         | 79/1070 [00:05<01:01, 16.15it/s]  8%|▊         | 81/1070 [00:06<01:01, 16.13it/s]  8%|▊         | 83/1070 [00:06<01:01, 16.13it/s]  8%|▊         | 85/1070 [00:06<01:01, 16.13it/s]  8%|▊         | 87/1070 [00:06<01:01, 16.03it/s]  8%|▊         | 89/1070 [00:06<01:01, 16.06it/s]  9%|▊         | 91/1070 [00:06<01:00, 16.06it/s]  9%|▊         | 93/1070 [00:06<01:00, 16.03it/s]  9%|▉         | 95/1070 [00:06<01:01, 15.96it/s]  9%|▉         | 97/1070 [00:07<01:01, 15.92it/s]  9%|▉         | 99/1070 [00:07<01:01, 15.84it/s]  9%|▉         | 101/1070 [00:07<01:01, 15.85it/s] 10%|▉         | 103/1070 [00:07<01:00, 15.87it/s] 10%|▉         | 105/1070 [00:07<01:00, 15.84it/s] 10%|█         | 107/1070 [00:07<01:00, 15.89it/s] 10%|█         | 109/1070 [00:07<01:00, 15.91it/s] 10%|█         | 111/1070 [00:07<01:00, 15.94it/s] 11%|█         | 113/1070 [00:08<01:00, 15.95it/s] 11%|█         | 115/1070 [00:08<00:59, 16.00it/s] 11%|█         | 117/1070 [00:08<00:59, 16.03it/s] 11%|█         | 119/1070 [00:08<00:59, 15.93it/s] 11%|█▏        | 121/1070 [00:08<00:59, 15.94it/s] 11%|█▏        | 123/1070 [00:08<00:59, 16.02it/s] 12%|█▏        | 125/1070 [00:08<00:58, 16.07it/s] 12%|█▏        | 127/1070 [00:08<00:58, 16.09it/s] 12%|█▏        | 129/1070 [00:09<00:58, 16.09it/s] 12%|█▏        | 131/1070 [00:09<00:58, 16.04it/s] 12%|█▏        | 133/1070 [00:09<00:58, 16.05it/s] 13%|█▎        | 135/1070 [00:09<00:59, 15.60it/s] 13%|█▎        | 137/1070 [00:09<01:00, 15.54it/s] 13%|█▎        | 139/1070 [00:09<01:00, 15.47it/s] 13%|█▎        | 141/1070 [00:09<00:59, 15.50it/s] 13%|█▎        | 143/1070 [00:09<00:59, 15.61it/s] 14%|█▎        | 145/1070 [00:10<00:58, 15.80it/s] 14%|█▎        | 147/1070 [00:10<00:57, 15.94it/s] 14%|█▍        | 149/1070 [00:10<00:57, 15.93it/s] 14%|█▍        | 151/1070 [00:10<00:57, 15.96it/s] 14%|█▍        | 153/1070 [00:10<00:57, 16.00it/s] 14%|█▍        | 155/1070 [00:10<00:57, 16.05it/s] 15%|█▍        | 157/1070 [00:10<00:57, 16.01it/s] 15%|█▍        | 159/1070 [00:10<00:56, 16.01it/s] 15%|█▌        | 161/1070 [00:11<00:56, 15.99it/s] 15%|█▌        | 163/1070 [00:11<00:56, 16.07it/s] 15%|█▌        | 165/1070 [00:11<00:56, 16.12it/s] 16%|█▌        | 167/1070 [00:11<00:55, 16.15it/s] 16%|█▌        | 169/1070 [00:11<00:55, 16.15it/s] 16%|█▌        | 171/1070 [00:11<00:55, 16.13it/s] 16%|█▌        | 173/1070 [00:11<00:55, 16.14it/s] 16%|█▋        | 175/1070 [00:11<00:55, 16.18it/s] 17%|█▋        | 177/1070 [00:12<00:55, 16.17it/s] 17%|█▋        | 179/1070 [00:12<00:55, 16.15it/s] 17%|█▋        | 181/1070 [00:12<00:55, 16.05it/s] 17%|█▋        | 183/1070 [00:12<00:55, 16.00it/s] 17%|█▋        | 185/1070 [00:12<00:55, 15.95it/s] 17%|█▋        | 187/1070 [00:12<00:55, 15.91it/s] 18%|█▊        | 189/1070 [00:12<00:55, 15.84it/s] 18%|█▊        | 191/1070 [00:12<00:55, 15.86it/s] 18%|█▊        | 193/1070 [00:13<00:55, 15.84it/s] 18%|█▊        | 195/1070 [00:13<00:55, 15.79it/s] 18%|█▊        | 197/1070 [00:13<00:54, 15.88it/s] 19%|█▊        | 199/1070 [00:13<00:54, 15.87it/s] 19%|█▉        | 201/1070 [00:13<00:54, 15.86it/s] 19%|█▉        | 203/1070 [00:13<00:54, 15.93it/s] 19%|█▉        | 205/1070 [00:13<00:54, 15.92it/s] 19%|█▉        | 207/1070 [00:13<00:54, 15.83it/s] 20%|█▉        | 209/1070 [00:14<00:54, 15.92it/s] 20%|█▉        | 211/1070 [00:14<00:53, 15.97it/s] 20%|█▉        | 213/1070 [00:14<00:53, 15.99it/s]                                                   20%|██        | 214/1070 [00:14<00:53, 15.99it/s][INFO|trainer.py:755] 2023-11-15 23:25:00,955 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 23:25:00,957 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 23:25:00,957 >>   Num examples = 760
[INFO|trainer.py:3134] 2023-11-15 23:25:00,957 >>   Batch size = 8
{'loss': 0.427, 'learning_rate': 0.0004, 'epoch': 1.0}

  0%|          | 0/95 [00:00<?, ?it/s][A
 11%|█         | 10/95 [00:00<00:00, 95.78it/s][A
 21%|██        | 20/95 [00:00<00:00, 89.55it/s][A
 31%|███       | 29/95 [00:00<00:00, 87.98it/s][A
 40%|████      | 38/95 [00:00<00:00, 86.72it/s][A
 49%|████▉     | 47/95 [00:00<00:00, 81.74it/s][A
 59%|█████▉    | 56/95 [00:00<00:00, 80.07it/s][A
 68%|██████▊   | 65/95 [00:00<00:00, 78.65it/s][A
 77%|███████▋  | 73/95 [00:00<00:00, 77.98it/s][A
 85%|████████▌ | 81/95 [00:00<00:00, 77.83it/s][A
 95%|█████████▍| 90/95 [00:01<00:00, 80.97it/s][A                                                  
                                               [A 20%|██        | 214/1070 [00:15<00:53, 15.99it/s]
100%|██████████| 95/95 [00:01<00:00, 80.97it/s][A
                                               [A 20%|██        | 215/1070 [00:15<03:26,  4.14it/s] 20%|██        | 217/1070 [00:15<02:40,  5.32it/s] 20%|██        | 219/1070 [00:15<02:07,  6.66it/s] 21%|██        | 221/1070 [00:16<01:44,  8.10it/s] 21%|██        | 223/1070 [00:16<01:28,  9.52it/s] 21%|██        | 225/1070 [00:16<01:17, 10.86it/s] 21%|██        | 227/1070 [00:16<01:10, 12.03it/s] 21%|██▏       | 229/1070 [00:16<01:04, 13.00it/s] 22%|██▏       | 231/1070 [00:16<01:00, 13.83it/s] 22%|██▏       | 233/1070 [00:16<00:57, 14.46it/s] 22%|██▏       | 235/1070 [00:16<00:55, 14.92it/s] 22%|██▏       | 237/1070 [00:17<00:54, 15.26it/s] 22%|██▏       | 239/1070 [00:17<00:53, 15.48it/s] 23%|██▎       | 241/1070 [00:17<00:52, 15.70it/s] 23%|██▎       | 243/1070 [00:17<00:52, 15.84it/s] 23%|██▎       | 245/1070 [00:17<00:51, 15.92it/s] 23%|██▎       | 247/1070 [00:17<00:51, 15.98it/s] 23%|██▎       | 249/1070 [00:17<00:51, 16.02it/s] 23%|██▎       | 251/1070 [00:17<00:51, 16.06it/s] 24%|██▎       | 253/1070 [00:18<00:51, 15.98it/s] 24%|██▍       | 255/1070 [00:18<00:51, 15.88it/s] 24%|██▍       | 257/1070 [00:18<00:51, 15.84it/s] 24%|██▍       | 259/1070 [00:18<00:51, 15.79it/s] 24%|██▍       | 261/1070 [00:18<00:51, 15.76it/s] 25%|██▍       | 263/1070 [00:18<00:50, 15.84it/s] 25%|██▍       | 265/1070 [00:18<00:50, 15.88it/s] 25%|██▍       | 267/1070 [00:18<00:50, 15.84it/s] 25%|██▌       | 269/1070 [00:19<00:50, 15.79it/s] 25%|██▌       | 271/1070 [00:19<00:50, 15.83it/s] 26%|██▌       | 273/1070 [00:19<00:50, 15.76it/s] 26%|██▌       | 275/1070 [00:19<00:50, 15.74it/s] 26%|██▌       | 277/1070 [00:19<00:50, 15.77it/s] 26%|██▌       | 279/1070 [00:19<00:50, 15.76it/s] 26%|██▋       | 281/1070 [00:19<00:49, 15.79it/s] 26%|██▋       | 283/1070 [00:19<00:49, 15.84it/s] 27%|██▋       | 285/1070 [00:20<00:49, 15.87it/s] 27%|██▋       | 287/1070 [00:20<00:49, 15.91it/s] 27%|██▋       | 289/1070 [00:20<00:48, 15.97it/s] 27%|██▋       | 291/1070 [00:20<00:48, 15.92it/s] 27%|██▋       | 293/1070 [00:20<00:49, 15.78it/s] 28%|██▊       | 295/1070 [00:20<00:49, 15.52it/s] 28%|██▊       | 297/1070 [00:20<00:49, 15.49it/s] 28%|██▊       | 299/1070 [00:20<00:49, 15.42it/s] 28%|██▊       | 301/1070 [00:21<00:49, 15.47it/s] 28%|██▊       | 303/1070 [00:21<00:49, 15.58it/s] 29%|██▊       | 305/1070 [00:21<00:48, 15.68it/s] 29%|██▊       | 307/1070 [00:21<00:48, 15.79it/s] 29%|██▉       | 309/1070 [00:21<00:47, 15.90it/s] 29%|██▉       | 311/1070 [00:21<00:47, 15.96it/s] 29%|██▉       | 313/1070 [00:21<00:47, 15.98it/s] 29%|██▉       | 315/1070 [00:21<00:47, 15.97it/s] 30%|██▉       | 317/1070 [00:22<00:47, 16.01it/s] 30%|██▉       | 319/1070 [00:22<00:46, 16.05it/s] 30%|███       | 321/1070 [00:22<00:46, 16.06it/s] 30%|███       | 323/1070 [00:22<00:46, 16.03it/s] 30%|███       | 325/1070 [00:22<00:46, 16.03it/s] 31%|███       | 327/1070 [00:22<00:46, 16.04it/s] 31%|███       | 329/1070 [00:22<00:46, 16.05it/s] 31%|███       | 331/1070 [00:22<00:46, 16.05it/s] 31%|███       | 333/1070 [00:23<00:45, 16.05it/s] 31%|███▏      | 335/1070 [00:23<00:45, 16.04it/s] 31%|███▏      | 337/1070 [00:23<00:45, 16.01it/s] 32%|███▏      | 339/1070 [00:23<00:45, 16.00it/s] 32%|███▏      | 341/1070 [00:23<00:45, 15.94it/s] 32%|███▏      | 343/1070 [00:23<00:45, 15.91it/s] 32%|███▏      | 345/1070 [00:23<00:45, 15.85it/s] 32%|███▏      | 347/1070 [00:23<00:45, 15.80it/s] 33%|███▎      | 349/1070 [00:24<00:45, 15.69it/s] 33%|███▎      | 351/1070 [00:24<00:45, 15.64it/s] 33%|███▎      | 353/1070 [00:24<00:45, 15.72it/s] 33%|███▎      | 355/1070 [00:24<00:45, 15.71it/s] 33%|███▎      | 357/1070 [00:24<00:45, 15.78it/s] 34%|███▎      | 359/1070 [00:24<00:44, 15.82it/s] 34%|███▎      | 361/1070 [00:24<00:45, 15.67it/s] 34%|███▍      | 363/1070 [00:24<00:44, 15.78it/s] 34%|███▍      | 365/1070 [00:25<00:44, 15.67it/s] 34%|███▍      | 367/1070 [00:25<00:44, 15.69it/s] 34%|███▍      | 369/1070 [00:25<00:44, 15.74it/s] 35%|███▍      | 371/1070 [00:25<00:44, 15.72it/s] 35%|███▍      | 373/1070 [00:25<00:44, 15.79it/s] 35%|███▌      | 375/1070 [00:25<00:44, 15.76it/s] 35%|███▌      | 377/1070 [00:25<00:44, 15.64it/s] 35%|███▌      | 379/1070 [00:25<00:44, 15.67it/s] 36%|███▌      | 381/1070 [00:26<00:44, 15.51it/s] 36%|███▌      | 383/1070 [00:26<00:44, 15.40it/s] 36%|███▌      | 385/1070 [00:26<00:44, 15.44it/s] 36%|███▌      | 387/1070 [00:26<00:44, 15.45it/s] 36%|███▋      | 389/1070 [00:26<00:43, 15.52it/s] 37%|███▋      | 391/1070 [00:26<00:43, 15.66it/s] 37%|███▋      | 393/1070 [00:26<00:42, 15.81it/s] 37%|███▋      | 395/1070 [00:27<00:42, 15.92it/s] 37%|███▋      | 397/1070 [00:27<00:42, 15.96it/s] 37%|███▋      | 399/1070 [00:27<00:42, 15.97it/s] 37%|███▋      | 401/1070 [00:27<00:41, 15.99it/s] 38%|███▊      | 403/1070 [00:27<00:41, 16.04it/s] 38%|███▊      | 405/1070 [00:27<00:41, 16.05it/s] 38%|███▊      | 407/1070 [00:27<00:41, 16.05it/s] 38%|███▊      | 409/1070 [00:27<00:41, 16.04it/s] 38%|███▊      | 411/1070 [00:27<00:41, 16.06it/s] 39%|███▊      | 413/1070 [00:28<00:40, 16.06it/s] 39%|███▉      | 415/1070 [00:28<00:40, 16.08it/s] 39%|███▉      | 417/1070 [00:28<00:40, 16.03it/s] 39%|███▉      | 419/1070 [00:28<00:40, 16.03it/s] 39%|███▉      | 421/1070 [00:28<00:40, 16.04it/s] 40%|███▉      | 423/1070 [00:28<00:40, 16.03it/s] 40%|███▉      | 425/1070 [00:28<00:40, 15.99it/s] 40%|███▉      | 427/1070 [00:28<00:40, 15.98it/s]                                                   40%|████      | 428/1070 [00:29<00:40, 15.98it/s][INFO|trainer.py:755] 2023-11-15 23:25:15,620 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 23:25:15,622 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 23:25:15,622 >>   Num examples = 760
[INFO|trainer.py:3134] 2023-11-15 23:25:15,623 >>   Batch size = 8
{'eval_loss': 0.27470487356185913, 'eval_accuracy': 0.9078947368421053, 'eval_micro_f1': 0.9078947368421053, 'eval_macro_f1': 0.904639107053496, 'eval_runtime': 1.2009, 'eval_samples_per_second': 632.877, 'eval_steps_per_second': 79.11, 'epoch': 1.0}
{'loss': 0.2611, 'learning_rate': 0.0003, 'epoch': 2.0}

  0%|          | 0/95 [00:00<?, ?it/s][A
 11%|█         | 10/95 [00:00<00:00, 95.16it/s][A
 21%|██        | 20/95 [00:00<00:00, 88.45it/s][A
 31%|███       | 29/95 [00:00<00:00, 85.99it/s][A
 40%|████      | 38/95 [00:00<00:00, 86.22it/s][A
 49%|████▉     | 47/95 [00:00<00:00, 84.35it/s][A
 59%|█████▉    | 56/95 [00:00<00:00, 84.70it/s][A
 68%|██████▊   | 65/95 [00:00<00:00, 85.40it/s][A
 78%|███████▊  | 74/95 [00:00<00:00, 85.44it/s][A
 87%|████████▋ | 83/95 [00:00<00:00, 85.56it/s][A
 97%|█████████▋| 92/95 [00:01<00:00, 85.41it/s][A                                                  
                                               [A 40%|████      | 428/1070 [00:30<00:40, 15.98it/s]
100%|██████████| 95/95 [00:01<00:00, 85.41it/s][A
                                               [A 40%|████      | 429/1070 [00:30<02:30,  4.25it/s] 40%|████      | 431/1070 [00:30<01:57,  5.45it/s] 40%|████      | 433/1070 [00:30<01:33,  6.79it/s] 41%|████      | 435/1070 [00:30<01:17,  8.20it/s] 41%|████      | 437/1070 [00:30<01:06,  9.58it/s] 41%|████      | 439/1070 [00:30<00:57, 10.89it/s] 41%|████      | 441/1070 [00:31<00:52, 11.99it/s] 41%|████▏     | 443/1070 [00:31<00:48, 12.95it/s] 42%|████▏     | 445/1070 [00:31<00:45, 13.75it/s] 42%|████▏     | 447/1070 [00:31<00:43, 14.35it/s] 42%|████▏     | 449/1070 [00:31<00:42, 14.74it/s] 42%|████▏     | 451/1070 [00:31<00:41, 15.02it/s] 42%|████▏     | 453/1070 [00:31<00:40, 15.18it/s] 43%|████▎     | 455/1070 [00:31<00:40, 15.24it/s] 43%|████▎     | 457/1070 [00:32<00:39, 15.37it/s] 43%|████▎     | 459/1070 [00:32<00:39, 15.44it/s] 43%|████▎     | 461/1070 [00:32<00:39, 15.61it/s] 43%|████▎     | 463/1070 [00:32<00:38, 15.75it/s] 43%|████▎     | 465/1070 [00:32<00:38, 15.83it/s] 44%|████▎     | 467/1070 [00:32<00:37, 15.88it/s] 44%|████▍     | 469/1070 [00:32<00:37, 15.93it/s] 44%|████▍     | 471/1070 [00:32<00:37, 15.99it/s] 44%|████▍     | 473/1070 [00:33<00:37, 16.00it/s] 44%|████▍     | 475/1070 [00:33<00:37, 15.99it/s] 45%|████▍     | 477/1070 [00:33<00:37, 16.01it/s] 45%|████▍     | 479/1070 [00:33<00:36, 16.00it/s] 45%|████▍     | 481/1070 [00:33<00:36, 15.95it/s] 45%|████▌     | 483/1070 [00:33<00:36, 15.97it/s] 45%|████▌     | 485/1070 [00:33<00:36, 15.98it/s] 46%|████▌     | 487/1070 [00:33<00:36, 16.02it/s] 46%|████▌     | 489/1070 [00:34<00:36, 16.01it/s] 46%|████▌     | 491/1070 [00:34<00:36, 16.00it/s] 46%|████▌     | 493/1070 [00:34<00:36, 16.01it/s] 46%|████▋     | 495/1070 [00:34<00:35, 16.03it/s] 46%|████▋     | 497/1070 [00:34<00:35, 16.03it/s] 47%|████▋     | 499/1070 [00:34<00:35, 16.00it/s] 47%|████▋     | 501/1070 [00:34<00:35, 15.90it/s] 47%|████▋     | 503/1070 [00:34<00:35, 15.82it/s] 47%|████▋     | 505/1070 [00:35<00:35, 15.76it/s] 47%|████▋     | 507/1070 [00:35<00:35, 15.74it/s] 48%|████▊     | 509/1070 [00:35<00:35, 15.71it/s] 48%|████▊     | 511/1070 [00:35<00:35, 15.74it/s] 48%|████▊     | 513/1070 [00:35<00:35, 15.73it/s] 48%|████▊     | 515/1070 [00:35<00:35, 15.71it/s] 48%|████▊     | 517/1070 [00:35<00:35, 15.76it/s] 49%|████▊     | 519/1070 [00:35<00:34, 15.81it/s] 49%|████▊     | 521/1070 [00:36<00:34, 15.80it/s] 49%|████▉     | 523/1070 [00:36<00:34, 15.83it/s] 49%|████▉     | 525/1070 [00:36<00:34, 15.81it/s] 49%|████▉     | 527/1070 [00:36<00:34, 15.78it/s] 49%|████▉     | 529/1070 [00:36<00:34, 15.79it/s] 50%|████▉     | 531/1070 [00:36<00:34, 15.85it/s] 50%|████▉     | 533/1070 [00:36<00:33, 15.84it/s] 50%|█████     | 535/1070 [00:36<00:33, 15.84it/s] 50%|█████     | 537/1070 [00:37<00:33, 15.87it/s] 50%|█████     | 539/1070 [00:37<00:33, 15.81it/s] 51%|█████     | 541/1070 [00:37<00:33, 15.70it/s] 51%|█████     | 543/1070 [00:37<00:33, 15.71it/s] 51%|█████     | 545/1070 [00:37<00:33, 15.69it/s] 51%|█████     | 547/1070 [00:37<00:33, 15.73it/s] 51%|█████▏    | 549/1070 [00:37<00:33, 15.74it/s] 51%|█████▏    | 551/1070 [00:37<00:32, 15.82it/s] 52%|█████▏    | 553/1070 [00:38<00:32, 15.88it/s] 52%|█████▏    | 555/1070 [00:38<00:32, 15.91it/s] 52%|█████▏    | 557/1070 [00:38<00:32, 15.96it/s] 52%|█████▏    | 559/1070 [00:38<00:32, 15.96it/s] 52%|█████▏    | 561/1070 [00:38<00:31, 15.97it/s] 53%|█████▎    | 563/1070 [00:38<00:31, 16.02it/s] 53%|█████▎    | 565/1070 [00:38<00:31, 16.05it/s] 53%|█████▎    | 567/1070 [00:38<00:31, 16.03it/s] 53%|█████▎    | 569/1070 [00:39<00:31, 16.01it/s] 53%|█████▎    | 571/1070 [00:39<00:31, 16.05it/s] 54%|█████▎    | 573/1070 [00:39<00:30, 16.06it/s] 54%|█████▎    | 575/1070 [00:39<00:30, 16.03it/s] 54%|█████▍    | 577/1070 [00:39<00:30, 16.02it/s] 54%|█████▍    | 579/1070 [00:39<00:30, 16.04it/s] 54%|█████▍    | 581/1070 [00:39<00:30, 16.06it/s] 54%|█████▍    | 583/1070 [00:39<00:30, 16.07it/s] 55%|█████▍    | 585/1070 [00:40<00:30, 16.03it/s] 55%|█████▍    | 587/1070 [00:40<00:30, 16.02it/s] 55%|█████▌    | 589/1070 [00:40<00:30, 16.00it/s] 55%|█████▌    | 591/1070 [00:40<00:30, 15.86it/s] 55%|█████▌    | 593/1070 [00:40<00:30, 15.82it/s] 56%|█████▌    | 595/1070 [00:40<00:30, 15.80it/s] 56%|█████▌    | 597/1070 [00:40<00:30, 15.73it/s] 56%|█████▌    | 599/1070 [00:40<00:29, 15.75it/s] 56%|█████▌    | 601/1070 [00:41<00:29, 15.73it/s] 56%|█████▋    | 603/1070 [00:41<00:29, 15.71it/s] 57%|█████▋    | 605/1070 [00:41<00:29, 15.77it/s] 57%|█████▋    | 607/1070 [00:41<00:29, 15.78it/s] 57%|█████▋    | 609/1070 [00:41<00:29, 15.77it/s] 57%|█████▋    | 611/1070 [00:41<00:29, 15.79it/s] 57%|█████▋    | 613/1070 [00:41<00:28, 15.85it/s] 57%|█████▋    | 615/1070 [00:41<00:28, 15.82it/s] 58%|█████▊    | 617/1070 [00:42<00:28, 15.80it/s] 58%|█████▊    | 619/1070 [00:42<00:28, 15.69it/s] 58%|█████▊    | 621/1070 [00:42<00:28, 15.73it/s] 58%|█████▊    | 623/1070 [00:42<00:28, 15.79it/s] 58%|█████▊    | 625/1070 [00:42<00:28, 15.84it/s] 59%|█████▊    | 627/1070 [00:42<00:28, 15.74it/s] 59%|█████▉    | 629/1070 [00:42<00:27, 15.77it/s] 59%|█████▉    | 631/1070 [00:43<00:28, 15.54it/s] 59%|█████▉    | 633/1070 [00:43<00:28, 15.44it/s] 59%|█████▉    | 635/1070 [00:43<00:28, 15.42it/s] 60%|█████▉    | 637/1070 [00:43<00:28, 15.42it/s] 60%|█████▉    | 639/1070 [00:43<00:27, 15.45it/s] 60%|█████▉    | 641/1070 [00:43<00:27, 15.61it/s]                                                   60%|██████    | 642/1070 [00:43<00:27, 15.61it/s][INFO|trainer.py:755] 2023-11-15 23:25:30,279 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 23:25:30,281 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 23:25:30,281 >>   Num examples = 760
[INFO|trainer.py:3134] 2023-11-15 23:25:30,282 >>   Batch size = 8
{'eval_loss': 0.31809571385383606, 'eval_accuracy': 0.9052631578947369, 'eval_micro_f1': 0.9052631578947369, 'eval_macro_f1': 0.9025063268197113, 'eval_runtime': 1.1553, 'eval_samples_per_second': 657.814, 'eval_steps_per_second': 82.227, 'epoch': 2.0}
{'loss': 0.2122, 'learning_rate': 0.0002, 'epoch': 3.0}

  0%|          | 0/95 [00:00<?, ?it/s][A
 11%|█         | 10/95 [00:00<00:00, 97.60it/s][A
 21%|██        | 20/95 [00:00<00:00, 91.62it/s][A
 32%|███▏      | 30/95 [00:00<00:00, 89.70it/s][A
 41%|████      | 39/95 [00:00<00:00, 89.30it/s][A
 51%|█████     | 48/95 [00:00<00:00, 88.99it/s][A
 60%|██████    | 57/95 [00:00<00:00, 88.39it/s][A
 69%|██████▉   | 66/95 [00:00<00:00, 88.75it/s][A
 79%|███████▉  | 75/95 [00:00<00:00, 88.09it/s][A
 88%|████████▊ | 84/95 [00:00<00:00, 88.05it/s][A
 98%|█████████▊| 93/95 [00:01<00:00, 88.54it/s][A                                                  
                                               [A 60%|██████    | 642/1070 [00:44<00:27, 15.61it/s]
100%|██████████| 95/95 [00:01<00:00, 88.54it/s][A
                                               [A 60%|██████    | 643/1070 [00:44<01:37,  4.37it/s] 60%|██████    | 645/1070 [00:45<01:16,  5.58it/s] 60%|██████    | 647/1070 [00:45<01:00,  6.94it/s] 61%|██████    | 649/1070 [00:45<00:50,  8.35it/s] 61%|██████    | 651/1070 [00:45<00:43,  9.73it/s] 61%|██████    | 653/1070 [00:45<00:37, 11.03it/s] 61%|██████    | 655/1070 [00:45<00:34, 12.17it/s] 61%|██████▏   | 657/1070 [00:45<00:31, 13.11it/s] 62%|██████▏   | 659/1070 [00:45<00:29, 13.86it/s] 62%|██████▏   | 661/1070 [00:46<00:28, 14.43it/s] 62%|██████▏   | 663/1070 [00:46<00:27, 14.81it/s] 62%|██████▏   | 665/1070 [00:46<00:26, 15.07it/s] 62%|██████▏   | 667/1070 [00:46<00:26, 15.25it/s] 63%|██████▎   | 669/1070 [00:46<00:26, 15.38it/s] 63%|██████▎   | 671/1070 [00:46<00:25, 15.42it/s] 63%|██████▎   | 673/1070 [00:46<00:25, 15.54it/s] 63%|██████▎   | 675/1070 [00:46<00:25, 15.61it/s] 63%|██████▎   | 677/1070 [00:47<00:25, 15.63it/s] 63%|██████▎   | 679/1070 [00:47<00:24, 15.72it/s] 64%|██████▎   | 681/1070 [00:47<00:24, 15.77it/s] 64%|██████▍   | 683/1070 [00:47<00:24, 15.80it/s] 64%|██████▍   | 685/1070 [00:47<00:24, 15.82it/s] 64%|██████▍   | 687/1070 [00:47<00:24, 15.83it/s] 64%|██████▍   | 689/1070 [00:47<00:24, 15.79it/s] 65%|██████▍   | 691/1070 [00:47<00:23, 15.81it/s] 65%|██████▍   | 693/1070 [00:48<00:23, 15.85it/s] 65%|██████▍   | 695/1070 [00:48<00:23, 15.84it/s] 65%|██████▌   | 697/1070 [00:48<00:23, 15.84it/s] 65%|██████▌   | 699/1070 [00:48<00:23, 15.82it/s] 66%|██████▌   | 701/1070 [00:48<00:23, 15.74it/s] 66%|██████▌   | 703/1070 [00:48<00:23, 15.72it/s] 66%|██████▌   | 705/1070 [00:48<00:23, 15.68it/s] 66%|██████▌   | 707/1070 [00:48<00:23, 15.68it/s] 66%|██████▋   | 709/1070 [00:49<00:22, 15.70it/s] 66%|██████▋   | 711/1070 [00:49<00:22, 15.78it/s] 67%|██████▋   | 713/1070 [00:49<00:22, 15.81it/s] 67%|██████▋   | 715/1070 [00:49<00:22, 15.91it/s] 67%|██████▋   | 717/1070 [00:49<00:22, 15.95it/s] 67%|██████▋   | 719/1070 [00:49<00:21, 15.97it/s] 67%|██████▋   | 721/1070 [00:49<00:21, 15.95it/s] 68%|██████▊   | 723/1070 [00:49<00:21, 15.95it/s] 68%|██████▊   | 725/1070 [00:50<00:21, 15.97it/s] 68%|██████▊   | 727/1070 [00:50<00:21, 15.94it/s] 68%|██████▊   | 729/1070 [00:50<00:21, 15.85it/s] 68%|██████▊   | 731/1070 [00:50<00:21, 15.87it/s] 69%|██████▊   | 733/1070 [00:50<00:21, 15.85it/s] 69%|██████▊   | 735/1070 [00:50<00:21, 15.89it/s] 69%|██████▉   | 737/1070 [00:50<00:20, 15.95it/s] 69%|██████▉   | 739/1070 [00:50<00:20, 15.97it/s] 69%|██████▉   | 741/1070 [00:51<00:20, 15.97it/s] 69%|██████▉   | 743/1070 [00:51<00:20, 15.96it/s] 70%|██████▉   | 745/1070 [00:51<00:20, 16.00it/s] 70%|██████▉   | 747/1070 [00:51<00:20, 16.00it/s] 70%|███████   | 749/1070 [00:51<00:20, 15.98it/s] 70%|███████   | 751/1070 [00:51<00:19, 15.96it/s] 70%|███████   | 753/1070 [00:51<00:19, 15.90it/s] 71%|███████   | 755/1070 [00:51<00:19, 15.83it/s] 71%|███████   | 757/1070 [00:52<00:19, 15.74it/s] 71%|███████   | 759/1070 [00:52<00:19, 15.71it/s] 71%|███████   | 761/1070 [00:52<00:19, 15.72it/s] 71%|███████▏  | 763/1070 [00:52<00:19, 15.74it/s] 71%|███████▏  | 765/1070 [00:52<00:19, 15.74it/s] 72%|███████▏  | 767/1070 [00:52<00:19, 15.70it/s] 72%|███████▏  | 769/1070 [00:52<00:19, 15.69it/s] 72%|███████▏  | 771/1070 [00:52<00:19, 15.66it/s] 72%|███████▏  | 773/1070 [00:53<00:18, 15.73it/s] 72%|███████▏  | 775/1070 [00:53<00:18, 15.72it/s] 73%|███████▎  | 777/1070 [00:53<00:18, 15.72it/s] 73%|███████▎  | 779/1070 [00:53<00:18, 15.74it/s] 73%|███████▎  | 781/1070 [00:53<00:18, 15.69it/s] 73%|███████▎  | 783/1070 [00:53<00:18, 15.74it/s] 73%|███████▎  | 785/1070 [00:53<00:18, 15.79it/s] 74%|███████▎  | 787/1070 [00:53<00:17, 15.76it/s] 74%|███████▎  | 789/1070 [00:54<00:17, 15.66it/s] 74%|███████▍  | 791/1070 [00:54<00:18, 15.42it/s] 74%|███████▍  | 793/1070 [00:54<00:18, 15.26it/s] 74%|███████▍  | 795/1070 [00:54<00:17, 15.36it/s] 74%|███████▍  | 797/1070 [00:54<00:17, 15.48it/s] 75%|███████▍  | 799/1070 [00:54<00:17, 15.58it/s] 75%|███████▍  | 801/1070 [00:54<00:17, 15.70it/s] 75%|███████▌  | 803/1070 [00:55<00:16, 15.82it/s] 75%|███████▌  | 805/1070 [00:55<00:16, 15.88it/s] 75%|███████▌  | 807/1070 [00:55<00:16, 15.92it/s] 76%|███████▌  | 809/1070 [00:55<00:16, 15.92it/s] 76%|███████▌  | 811/1070 [00:55<00:16, 15.96it/s] 76%|███████▌  | 813/1070 [00:55<00:16, 15.93it/s] 76%|███████▌  | 815/1070 [00:55<00:15, 15.95it/s] 76%|███████▋  | 817/1070 [00:55<00:15, 15.92it/s] 77%|███████▋  | 819/1070 [00:56<00:15, 15.93it/s] 77%|███████▋  | 821/1070 [00:56<00:15, 15.89it/s] 77%|███████▋  | 823/1070 [00:56<00:15, 15.80it/s] 77%|███████▋  | 825/1070 [00:56<00:15, 15.85it/s] 77%|███████▋  | 827/1070 [00:56<00:15, 15.88it/s] 77%|███████▋  | 829/1070 [00:56<00:15, 15.88it/s] 78%|███████▊  | 831/1070 [00:56<00:15, 15.92it/s] 78%|███████▊  | 833/1070 [00:56<00:14, 15.96it/s] 78%|███████▊  | 835/1070 [00:57<00:14, 15.98it/s] 78%|███████▊  | 837/1070 [00:57<00:14, 15.91it/s] 78%|███████▊  | 839/1070 [00:57<00:14, 15.96it/s] 79%|███████▊  | 841/1070 [00:57<00:14, 15.91it/s] 79%|███████▉  | 843/1070 [00:57<00:14, 15.80it/s] 79%|███████▉  | 845/1070 [00:57<00:14, 15.78it/s] 79%|███████▉  | 847/1070 [00:57<00:14, 15.72it/s] 79%|███████▉  | 849/1070 [00:57<00:14, 15.70it/s] 80%|███████▉  | 851/1070 [00:58<00:13, 15.73it/s] 80%|███████▉  | 853/1070 [00:58<00:13, 15.69it/s] 80%|███████▉  | 855/1070 [00:58<00:13, 15.71it/s]                                                   80%|████████  | 856/1070 [00:58<00:13, 15.71it/s][INFO|trainer.py:755] 2023-11-15 23:25:44,913 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 23:25:44,915 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 23:25:44,916 >>   Num examples = 760
[INFO|trainer.py:3134] 2023-11-15 23:25:44,916 >>   Batch size = 8
{'eval_loss': 0.2909742593765259, 'eval_accuracy': 0.9144736842105263, 'eval_micro_f1': 0.9144736842105263, 'eval_macro_f1': 0.9119797103401569, 'eval_runtime': 1.1095, 'eval_samples_per_second': 684.993, 'eval_steps_per_second': 85.624, 'epoch': 3.0}
{'loss': 0.1681, 'learning_rate': 0.0001, 'epoch': 4.0}

  0%|          | 0/95 [00:00<?, ?it/s][A
 11%|█         | 10/95 [00:00<00:00, 97.48it/s][A
 21%|██        | 20/95 [00:00<00:00, 91.96it/s][A
 32%|███▏      | 30/95 [00:00<00:00, 88.93it/s][A
 41%|████      | 39/95 [00:00<00:00, 88.78it/s][A
 51%|█████     | 48/95 [00:00<00:00, 87.37it/s][A
 60%|██████    | 57/95 [00:00<00:00, 85.84it/s][A
 69%|██████▉   | 66/95 [00:00<00:00, 86.09it/s][A
 79%|███████▉  | 75/95 [00:00<00:00, 86.40it/s][A
 88%|████████▊ | 84/95 [00:00<00:00, 86.51it/s][A
 98%|█████████▊| 93/95 [00:01<00:00, 86.68it/s][A                                                  
                                               [A 80%|████████  | 856/1070 [00:59<00:13, 15.71it/s]
100%|██████████| 95/95 [00:01<00:00, 86.68it/s][A
                                               [A 80%|████████  | 857/1070 [00:59<00:49,  4.31it/s] 80%|████████  | 859/1070 [00:59<00:38,  5.51it/s] 80%|████████  | 861/1070 [00:59<00:30,  6.84it/s] 81%|████████  | 863/1070 [00:59<00:25,  8.22it/s] 81%|████████  | 865/1070 [01:00<00:21,  9.59it/s] 81%|████████  | 867/1070 [01:00<00:18, 10.85it/s] 81%|████████  | 869/1070 [01:00<00:16, 11.93it/s] 81%|████████▏ | 871/1070 [01:00<00:15, 12.91it/s] 82%|████████▏ | 873/1070 [01:00<00:14, 13.68it/s] 82%|████████▏ | 875/1070 [01:00<00:13, 14.20it/s] 82%|████████▏ | 877/1070 [01:00<00:13, 14.68it/s] 82%|████████▏ | 879/1070 [01:00<00:12, 15.03it/s] 82%|████████▏ | 881/1070 [01:01<00:12, 15.24it/s] 83%|████████▎ | 883/1070 [01:01<00:12, 15.47it/s] 83%|████████▎ | 885/1070 [01:01<00:11, 15.62it/s] 83%|████████▎ | 887/1070 [01:01<00:11, 15.69it/s] 83%|████████▎ | 889/1070 [01:01<00:11, 15.76it/s] 83%|████████▎ | 891/1070 [01:01<00:11, 15.85it/s] 83%|████████▎ | 893/1070 [01:01<00:11, 15.89it/s] 84%|████████▎ | 895/1070 [01:01<00:11, 15.89it/s] 84%|████████▍ | 897/1070 [01:02<00:10, 15.91it/s] 84%|████████▍ | 899/1070 [01:02<00:10, 15.95it/s] 84%|████████▍ | 901/1070 [01:02<00:10, 15.95it/s] 84%|████████▍ | 903/1070 [01:02<00:10, 15.91it/s] 85%|████████▍ | 905/1070 [01:02<00:10, 15.96it/s] 85%|████████▍ | 907/1070 [01:02<00:10, 15.99it/s] 85%|████████▍ | 909/1070 [01:02<00:10, 15.98it/s] 85%|████████▌ | 911/1070 [01:02<00:10, 15.86it/s] 85%|████████▌ | 913/1070 [01:03<00:09, 15.83it/s] 86%|████████▌ | 915/1070 [01:03<00:09, 15.77it/s] 86%|████████▌ | 917/1070 [01:03<00:09, 15.73it/s] 86%|████████▌ | 919/1070 [01:03<00:09, 15.72it/s] 86%|████████▌ | 921/1070 [01:03<00:09, 15.71it/s] 86%|████████▋ | 923/1070 [01:03<00:09, 15.72it/s] 86%|████████▋ | 925/1070 [01:03<00:09, 15.66it/s] 87%|████████▋ | 927/1070 [01:03<00:09, 15.71it/s] 87%|████████▋ | 929/1070 [01:04<00:08, 15.79it/s] 87%|████████▋ | 931/1070 [01:04<00:08, 15.80it/s] 87%|████████▋ | 933/1070 [01:04<00:08, 15.81it/s] 87%|████████▋ | 935/1070 [01:04<00:08, 15.80it/s] 88%|████████▊ | 937/1070 [01:04<00:08, 15.75it/s] 88%|████████▊ | 939/1070 [01:04<00:08, 15.76it/s] 88%|████████▊ | 941/1070 [01:04<00:08, 15.76it/s] 88%|████████▊ | 943/1070 [01:04<00:08, 15.80it/s] 88%|████████▊ | 945/1070 [01:05<00:07, 15.77it/s] 89%|████████▊ | 947/1070 [01:05<00:07, 15.75it/s] 89%|████████▊ | 949/1070 [01:05<00:07, 15.48it/s] 89%|████████▉ | 951/1070 [01:05<00:07, 15.44it/s] 89%|████████▉ | 953/1070 [01:05<00:07, 15.46it/s] 89%|████████▉ | 955/1070 [01:05<00:07, 15.54it/s] 89%|████████▉ | 957/1070 [01:05<00:07, 15.65it/s] 90%|████████▉ | 959/1070 [01:06<00:07, 15.72it/s] 90%|████████▉ | 961/1070 [01:06<00:06, 15.78it/s] 90%|█████████ | 963/1070 [01:06<00:06, 15.80it/s] 90%|█████████ | 965/1070 [01:06<00:06, 15.80it/s] 90%|█████████ | 967/1070 [01:06<00:06, 15.85it/s] 91%|█████████ | 969/1070 [01:06<00:06, 15.89it/s] 91%|█████████ | 971/1070 [01:06<00:06, 15.91it/s] 91%|█████████ | 973/1070 [01:06<00:06, 15.90it/s] 91%|█████████ | 975/1070 [01:07<00:05, 15.94it/s] 91%|█████████▏| 977/1070 [01:07<00:05, 15.91it/s] 91%|█████████▏| 979/1070 [01:07<00:05, 15.90it/s] 92%|█████████▏| 981/1070 [01:07<00:05, 15.95it/s] 92%|█████████▏| 983/1070 [01:07<00:05, 15.97it/s] 92%|█████████▏| 985/1070 [01:07<00:05, 15.94it/s] 92%|█████████▏| 987/1070 [01:07<00:05, 15.93it/s] 92%|█████████▏| 989/1070 [01:07<00:05, 15.89it/s] 93%|█████████▎| 991/1070 [01:08<00:04, 15.80it/s] 93%|█████████▎| 993/1070 [01:08<00:04, 15.81it/s] 93%|█████████▎| 995/1070 [01:08<00:04, 15.89it/s] 93%|█████████▎| 997/1070 [01:08<00:04, 15.88it/s] 93%|█████████▎| 999/1070 [01:08<00:04, 15.80it/s] 94%|█████████▎| 1001/1070 [01:08<00:04, 15.77it/s] 94%|█████████▎| 1003/1070 [01:08<00:04, 15.70it/s] 94%|█████████▍| 1005/1070 [01:08<00:04, 15.67it/s] 94%|█████████▍| 1007/1070 [01:09<00:04, 15.68it/s] 94%|█████████▍| 1009/1070 [01:09<00:03, 15.68it/s] 94%|█████████▍| 1011/1070 [01:09<00:03, 15.67it/s] 95%|█████████▍| 1013/1070 [01:09<00:03, 15.70it/s] 95%|█████████▍| 1015/1070 [01:09<00:03, 15.73it/s] 95%|█████████▌| 1017/1070 [01:09<00:03, 15.80it/s] 95%|█████████▌| 1019/1070 [01:09<00:03, 15.79it/s] 95%|█████████▌| 1021/1070 [01:09<00:03, 15.80it/s] 96%|█████████▌| 1023/1070 [01:10<00:02, 15.78it/s] 96%|█████████▌| 1025/1070 [01:10<00:02, 15.76it/s] 96%|█████████▌| 1027/1070 [01:10<00:02, 15.75it/s] 96%|█████████▌| 1029/1070 [01:10<00:02, 15.78it/s] 96%|█████████▋| 1031/1070 [01:10<00:02, 15.80it/s] 97%|█████████▋| 1033/1070 [01:10<00:02, 15.81it/s] 97%|█████████▋| 1035/1070 [01:10<00:02, 15.77it/s] 97%|█████████▋| 1037/1070 [01:10<00:02, 15.71it/s] 97%|█████████▋| 1039/1070 [01:11<00:01, 15.69it/s] 97%|█████████▋| 1041/1070 [01:11<00:01, 15.69it/s] 97%|█████████▋| 1043/1070 [01:11<00:01, 15.59it/s] 98%|█████████▊| 1045/1070 [01:11<00:01, 15.69it/s] 98%|█████████▊| 1047/1070 [01:11<00:01, 15.75it/s] 98%|█████████▊| 1049/1070 [01:11<00:01, 15.78it/s] 98%|█████████▊| 1051/1070 [01:11<00:01, 15.86it/s] 98%|█████████▊| 1053/1070 [01:11<00:01, 15.88it/s] 99%|█████████▊| 1055/1070 [01:12<00:00, 15.88it/s] 99%|█████████▉| 1057/1070 [01:12<00:00, 15.85it/s] 99%|█████████▉| 1059/1070 [01:12<00:00, 15.84it/s] 99%|█████████▉| 1061/1070 [01:12<00:00, 15.81it/s] 99%|█████████▉| 1063/1070 [01:12<00:00, 15.86it/s]100%|█████████▉| 1065/1070 [01:12<00:00, 15.92it/s]100%|█████████▉| 1067/1070 [01:12<00:00, 15.92it/s]100%|█████████▉| 1069/1070 [01:12<00:00, 15.92it/s]                                                   100%|██████████| 1070/1070 [01:13<00:00, 15.92it/s][INFO|trainer.py:755] 2023-11-15 23:25:59,581 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 23:25:59,582 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 23:25:59,582 >>   Num examples = 760
[INFO|trainer.py:3134] 2023-11-15 23:25:59,583 >>   Batch size = 8
{'eval_loss': 0.2737162113189697, 'eval_accuracy': 0.9171052631578948, 'eval_micro_f1': 0.9171052631578948, 'eval_macro_f1': 0.9146856668998841, 'eval_runtime': 1.1302, 'eval_samples_per_second': 672.443, 'eval_steps_per_second': 84.055, 'epoch': 4.0}
{'loss': 0.1324, 'learning_rate': 0.0, 'epoch': 5.0}

  0%|          | 0/95 [00:00<?, ?it/s][A
 11%|█         | 10/95 [00:00<00:00, 98.53it/s][A
 21%|██        | 20/95 [00:00<00:00, 91.25it/s][A
 32%|███▏      | 30/95 [00:00<00:00, 89.53it/s][A
 41%|████      | 39/95 [00:00<00:00, 88.94it/s][A
 51%|█████     | 48/95 [00:00<00:00, 87.53it/s][A
 60%|██████    | 57/95 [00:00<00:00, 86.65it/s][A
 69%|██████▉   | 66/95 [00:00<00:00, 86.94it/s][A
 79%|███████▉  | 75/95 [00:00<00:00, 86.63it/s][A
 88%|████████▊ | 84/95 [00:00<00:00, 86.07it/s][A
 98%|█████████▊| 93/95 [00:01<00:00, 83.97it/s][A                                                   
                                               [A100%|██████████| 1070/1070 [01:14<00:00, 15.92it/s]
100%|██████████| 95/95 [00:01<00:00, 83.97it/s][A
                                               [A[INFO|trainer.py:1963] 2023-11-15 23:26:00,740 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                   100%|██████████| 1070/1070 [01:14<00:00, 15.92it/s]100%|██████████| 1070/1070 [01:14<00:00, 14.43it/s]
[INFO|trainer.py:2855] 2023-11-15 23:26:00,743 >> Saving model checkpoint to ./result/agnews_sup_roberta-base_seed1_adapter
[INFO|configuration_utils.py:460] 2023-11-15 23:26:00,746 >> Configuration saved in ./result/agnews_sup_roberta-base_seed1_adapter/config.json
[INFO|modeling_utils.py:1997] 2023-11-15 23:26:01,855 >> Model weights saved in ./result/agnews_sup_roberta-base_seed1_adapter/pytorch_model.bin
[INFO|tokenization_utils_base.py:2235] 2023-11-15 23:26:01,858 >> tokenizer config file saved in ./result/agnews_sup_roberta-base_seed1_adapter/tokenizer_config.json
[INFO|tokenization_utils_base.py:2242] 2023-11-15 23:26:01,860 >> Special tokens file saved in ./result/agnews_sup_roberta-base_seed1_adapter/special_tokens_map.json
{'eval_loss': 0.2905980944633484, 'eval_accuracy': 0.9144736842105263, 'eval_micro_f1': 0.9144736842105263, 'eval_macro_f1': 0.9119650737994749, 'eval_runtime': 1.137, 'eval_samples_per_second': 668.433, 'eval_steps_per_second': 83.554, 'epoch': 5.0}
{'train_runtime': 74.1694, 'train_samples_per_second': 461.106, 'train_steps_per_second': 14.426, 'train_loss': 0.2401798070034134, 'epoch': 5.0}
***** train metrics *****
  epoch                    =        5.0
  train_loss               =     0.2402
  train_runtime            = 0:01:14.16
  train_samples            =       6840
  train_samples_per_second =    461.106
  train_steps_per_second   =     14.426
11/15/2023 23:26:01 - INFO - __main__ - *** Evaluate ***
[INFO|trainer.py:755] 2023-11-15 23:26:01,986 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 23:26:01,987 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 23:26:01,988 >>   Num examples = 760
[INFO|trainer.py:3134] 2023-11-15 23:26:01,988 >>   Batch size = 8
  0%|          | 0/95 [00:00<?, ?it/s] 11%|█         | 10/95 [00:00<00:00, 93.87it/s] 21%|██        | 20/95 [00:00<00:00, 89.00it/s] 31%|███       | 29/95 [00:00<00:00, 87.66it/s] 40%|████      | 38/95 [00:00<00:00, 87.54it/s] 49%|████▉     | 47/95 [00:00<00:00, 86.81it/s] 59%|█████▉    | 56/95 [00:00<00:00, 85.38it/s] 68%|██████▊   | 65/95 [00:00<00:00, 85.80it/s] 78%|███████▊  | 74/95 [00:00<00:00, 85.13it/s] 87%|████████▋ | 83/95 [00:00<00:00, 85.79it/s] 97%|█████████▋| 92/95 [00:01<00:00, 85.18it/s]100%|██████████| 95/95 [00:01<00:00, 83.88it/s]
***** eval metrics *****
  epoch                   =        5.0
  eval_accuracy           =     0.9145
  eval_loss               =     0.2906
  eval_macro_f1           =      0.912
  eval_micro_f1           =     0.9145
  eval_runtime            = 0:00:01.14
  eval_samples            =        760
  eval_samples_per_second =    662.281
  eval_steps_per_second   =     82.785
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                  eval/accuracy ▃▁▆█▆▆
wandb:                      eval/loss ▁█▄▁▄▄
wandb:                  eval/macro_f1 ▂▁▆█▆▆
wandb:                  eval/micro_f1 ▃▁▆█▆▆
wandb:                   eval/runtime █▅▁▃▃▄
wandb:        eval/samples_per_second ▁▄█▆▆▅
wandb:          eval/steps_per_second ▁▄█▆▆▅
wandb:                    train/epoch ▁▁▃▃▅▅▆▆████
wandb:              train/global_step ▁▁▃▃▄▄▆▆████
wandb:            train/learning_rate █▆▅▃▁
wandb:                     train/loss █▄▃▂▁
wandb:               train/total_flos ▁
wandb:               train/train_loss ▁
wandb:            train/train_runtime ▁
wandb: train/train_samples_per_second ▁
wandb:   train/train_steps_per_second ▁
wandb: 
wandb: Run summary:
wandb:                  eval/accuracy 0.91447
wandb:                      eval/loss 0.2906
wandb:                  eval/macro_f1 0.91197
wandb:                  eval/micro_f1 0.91447
wandb:                   eval/runtime 1.1475
wandb:        eval/samples_per_second 662.281
wandb:          eval/steps_per_second 82.785
wandb:                    train/epoch 5.0
wandb:              train/global_step 1070
wandb:            train/learning_rate 0.0
wandb:                     train/loss 0.1324
wandb:               train/total_flos 1136567617228800.0
wandb:               train/train_loss 0.24018
wandb:            train/train_runtime 74.1694
wandb: train/train_samples_per_second 461.106
wandb:   train/train_steps_per_second 14.426
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /ceph/home/wangyifei/a3/wandb/offline-run-20231115_232328-195dg5i1
wandb: Find logs at: ./wandb/offline-run-20231115_232328-195dg5i1/logs
(ModelArguments(model_name_or_path='roberta-base', cache_dir='./cache', use_adapter=True, use_lora=False), DataTrainingArguments(dataset_name='restaurant', max_seq_length=64), TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.0005,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=./result/restaurant_roberta-base_seed2_adapter/runs/Nov15_23-26-14_node013,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.EPOCH,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=5.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
output_dir=./result/restaurant_roberta-base_seed2_adapter,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
resume_from_checkpoint=None,
run_name=./result/restaurant_roberta-base_seed2_adapter,
save_on_each_node=False,
save_safetensors=False,
save_steps=500,
save_strategy=IntervalStrategy.NO,
save_total_limit=None,
seed=333,
sharded_ddp=[],
skip_memory_metrics=True,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
))
11/15/2023 23:26:14 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False
11/15/2023 23:26:14 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.0005,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=./result/restaurant_roberta-base_seed2_adapter/runs/Nov15_23-26-13_node013,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.EPOCH,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=5.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
output_dir=./result/restaurant_roberta-base_seed2_adapter,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
resume_from_checkpoint=None,
run_name=./result/restaurant_roberta-base_seed2_adapter,
save_on_each_node=False,
save_safetensors=False,
save_steps=500,
save_strategy=IntervalStrategy.NO,
save_total_limit=None,
seed=333,
sharded_ddp=[],
skip_memory_metrics=True,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
wandb: Tracking run with wandb version 0.15.12
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
Map:   0%|          | 0/4722 [00:00<?, ? examples/s]Map:  88%|████████▊ | 4135/4722 [00:00<00:00, 39838.03 examples/s]Map: 100%|██████████| 4722/4722 [00:00<00:00, 39169.80 examples/s]
[INFO|configuration_utils.py:715] 2023-11-15 23:26:29,773 >> loading configuration file config.json from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/config.json
[INFO|configuration_utils.py:775] 2023-11-15 23:26:29,782 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.33.3",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_auto.py:535] 2023-11-15 23:26:39,798 >> Could not locate the tokenizer configuration file, will try to use the model config instead.
[INFO|configuration_utils.py:715] 2023-11-15 23:26:49,815 >> loading configuration file config.json from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/config.json
[INFO|configuration_utils.py:775] 2023-11-15 23:26:49,815 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.33.3",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:1852] 2023-11-15 23:27:09,864 >> loading file vocab.json from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/vocab.json
[INFO|tokenization_utils_base.py:1852] 2023-11-15 23:27:09,864 >> loading file merges.txt from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/merges.txt
[INFO|tokenization_utils_base.py:1852] 2023-11-15 23:27:09,864 >> loading file tokenizer.json from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/tokenizer.json
[INFO|tokenization_utils_base.py:1852] 2023-11-15 23:27:09,865 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1852] 2023-11-15 23:27:09,865 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1852] 2023-11-15 23:27:09,865 >> loading file tokenizer_config.json from cache at None
[INFO|configuration_utils.py:715] 2023-11-15 23:27:09,866 >> loading configuration file config.json from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/config.json
[INFO|configuration_utils.py:775] 2023-11-15 23:27:09,867 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.33.3",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:2862] 2023-11-15 23:27:30,097 >> loading weights file model.safetensors from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/model.safetensors
[INFO|modeling_utils.py:3638] 2023-11-15 23:27:30,842 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3650] 2023-11-15 23:27:30,843 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
parameters 1487427
model_max_length: 512
Running tokenizer on dataset:   0%|          | 0/3777 [00:00<?, ? examples/s]Running tokenizer on dataset:  79%|███████▉  | 3000/3777 [00:00<00:00, 20685.75 examples/s]Running tokenizer on dataset: 100%|██████████| 3777/3777 [00:00<00:00, 19056.78 examples/s]
Running tokenizer on dataset:   0%|          | 0/945 [00:00<?, ? examples/s]Running tokenizer on dataset: 100%|██████████| 945/945 [00:00<00:00, 16944.47 examples/s]
11/15/2023 23:27:31 - INFO - __main__ - Sample 2272 of the training set: {'text': 'Carinthia cheese ravioli with wild mushrooms <SEP> Innovations are just as assured, from the simple Carinthia cheese ravioli with wild mushrooms to the caviar-topped sturgeon, beautifully matched with a bright green spinach-vodka sauce.', 'label': 0, 'input_ids': [0, 9518, 35744, 493, 7134, 25283, 14215, 118, 19, 3418, 25038, 28696, 3388, 510, 15698, 20067, 1635, 32, 95, 25, 7189, 6, 31, 5, 2007, 1653, 35744, 493, 7134, 25283, 14215, 118, 19, 3418, 25038, 7, 5, 32426, 12202, 12, 560, 5686, 1690, 710, 20989, 6, 16467, 9184, 19, 10, 4520, 2272, 31225, 12, 705, 40677, 8929, 4, 2, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0]}.
11/15/2023 23:27:31 - INFO - __main__ - Sample 1436 of the training set: {'text': 'jazz singer <SEP> jazz singer had a nice voice + she made us all get up to dance to shake some cals to eat some more.', 'label': 0, 'input_ids': [0, 267, 7706, 3250, 28696, 3388, 510, 15698, 11057, 3250, 56, 10, 2579, 2236, 2055, 79, 156, 201, 70, 120, 62, 7, 3836, 7, 8559, 103, 740, 1536, 7, 3529, 103, 55, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
11/15/2023 23:27:31 - INFO - __main__ - Sample 1446 of the training set: {'text': 'iceberg <SEP> The bruscetta is a bit soggy, but the salads were fresh, included a nice mix of greens (not iceberg) all dishes are served piping hot from the kitchen.', 'label': 1, 'input_ids': [0, 2463, 2865, 28696, 3388, 510, 15698, 20, 5378, 20214, 10464, 16, 10, 828, 579, 2154, 4740, 6, 53, 5, 26924, 58, 2310, 6, 1165, 10, 2579, 3344, 9, 16543, 36, 3654, 26937, 43, 70, 10230, 32, 1665, 37273, 2131, 31, 5, 4647, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
11/15/2023 23:27:32 - WARNING - accelerate.utils.other - Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[INFO|trainer.py:755] 2023-11-15 23:27:33,383 >> The following columns in the training set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:1715] 2023-11-15 23:27:33,393 >> ***** Running training *****
[INFO|trainer.py:1716] 2023-11-15 23:27:33,394 >>   Num examples = 3,777
[INFO|trainer.py:1717] 2023-11-15 23:27:33,394 >>   Num Epochs = 5
[INFO|trainer.py:1718] 2023-11-15 23:27:33,394 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1721] 2023-11-15 23:27:33,395 >>   Total train batch size (w. parallel, distributed & accumulation) = 32
[INFO|trainer.py:1722] 2023-11-15 23:27:33,395 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:1723] 2023-11-15 23:27:33,395 >>   Total optimization steps = 595
[INFO|trainer.py:1724] 2023-11-15 23:27:33,396 >>   Number of trainable parameters = 1,487,427
[INFO|integration_utils.py:716] 2023-11-15 23:27:33,397 >> Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"
  0%|          | 0/595 [00:00<?, ?it/s]  0%|          | 1/595 [00:01<10:08,  1.02s/it]  1%|          | 3/595 [00:01<03:04,  3.21it/s]  1%|          | 5/595 [00:01<01:47,  5.46it/s]  1%|          | 7/595 [00:01<01:17,  7.60it/s]  2%|▏         | 9/595 [00:01<01:02,  9.45it/s]  2%|▏         | 11/595 [00:01<00:53, 10.98it/s]  2%|▏         | 13/595 [00:01<00:47, 12.23it/s]  3%|▎         | 15/595 [00:01<00:44, 13.18it/s]  3%|▎         | 17/595 [00:02<00:41, 13.90it/s]  3%|▎         | 19/595 [00:02<00:39, 14.43it/s]  4%|▎         | 21/595 [00:02<00:38, 14.87it/s]  4%|▍         | 23/595 [00:02<00:37, 15.13it/s]  4%|▍         | 25/595 [00:02<00:37, 15.27it/s]  5%|▍         | 27/595 [00:02<00:36, 15.46it/s]  5%|▍         | 29/595 [00:02<00:36, 15.64it/s]  5%|▌         | 31/595 [00:02<00:35, 15.76it/s]  6%|▌         | 33/595 [00:03<00:35, 15.81it/s]  6%|▌         | 35/595 [00:03<00:35, 15.79it/s]  6%|▌         | 37/595 [00:03<00:35, 15.86it/s]  7%|▋         | 39/595 [00:03<00:34, 15.92it/s]  7%|▋         | 41/595 [00:03<00:34, 15.96it/s]  7%|▋         | 43/595 [00:03<00:34, 15.85it/s]  8%|▊         | 45/595 [00:03<00:34, 15.95it/s]  8%|▊         | 47/595 [00:03<00:34, 16.00it/s]  8%|▊         | 49/595 [00:04<00:34, 15.93it/s]  9%|▊         | 51/595 [00:04<00:34, 15.66it/s]  9%|▉         | 53/595 [00:04<00:35, 15.46it/s]  9%|▉         | 55/595 [00:04<00:34, 15.56it/s] 10%|▉         | 57/595 [00:04<00:34, 15.59it/s] 10%|▉         | 59/595 [00:04<00:34, 15.58it/s] 10%|█         | 61/595 [00:04<00:33, 15.75it/s] 11%|█         | 63/595 [00:04<00:33, 15.85it/s] 11%|█         | 65/595 [00:05<00:33, 15.92it/s] 11%|█▏        | 67/595 [00:05<00:33, 15.98it/s] 12%|█▏        | 69/595 [00:05<00:32, 16.05it/s] 12%|█▏        | 71/595 [00:05<00:32, 16.09it/s] 12%|█▏        | 73/595 [00:05<00:32, 16.11it/s] 13%|█▎        | 75/595 [00:05<00:32, 16.10it/s] 13%|█▎        | 77/595 [00:05<00:32, 16.11it/s] 13%|█▎        | 79/595 [00:05<00:31, 16.13it/s] 14%|█▎        | 81/595 [00:06<00:31, 16.14it/s] 14%|█▍        | 83/595 [00:06<00:31, 16.10it/s] 14%|█▍        | 85/595 [00:06<00:31, 16.10it/s] 15%|█▍        | 87/595 [00:06<00:31, 16.12it/s] 15%|█▍        | 89/595 [00:06<00:31, 16.16it/s] 15%|█▌        | 91/595 [00:06<00:31, 16.17it/s] 16%|█▌        | 93/595 [00:06<00:31, 16.14it/s] 16%|█▌        | 95/595 [00:06<00:31, 15.78it/s] 16%|█▋        | 97/595 [00:07<00:31, 15.81it/s] 17%|█▋        | 99/595 [00:07<00:31, 15.85it/s] 17%|█▋        | 101/595 [00:07<00:31, 15.84it/s] 17%|█▋        | 103/595 [00:07<00:31, 15.84it/s] 18%|█▊        | 105/595 [00:07<00:31, 15.77it/s] 18%|█▊        | 107/595 [00:07<00:31, 15.74it/s] 18%|█▊        | 109/595 [00:07<00:30, 15.77it/s] 19%|█▊        | 111/595 [00:07<00:30, 15.79it/s] 19%|█▉        | 113/595 [00:08<00:30, 15.81it/s] 19%|█▉        | 115/595 [00:08<00:30, 15.80it/s] 20%|█▉        | 117/595 [00:08<00:30, 15.81it/s]                                                  20%|██        | 119/595 [00:08<00:30, 15.81it/s][INFO|trainer.py:755] 2023-11-15 23:27:41,815 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 23:27:41,817 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 23:27:41,818 >>   Num examples = 945
[INFO|trainer.py:3134] 2023-11-15 23:27:41,818 >>   Batch size = 8
{'loss': 0.6951, 'learning_rate': 0.0004, 'epoch': 1.0}

  0%|          | 0/119 [00:00<?, ?it/s][A
  8%|▊         | 10/119 [00:00<00:01, 89.04it/s][A
 16%|█▌        | 19/119 [00:00<00:01, 85.64it/s][A
 24%|██▎       | 28/119 [00:00<00:01, 84.31it/s][A
 31%|███       | 37/119 [00:00<00:00, 82.86it/s][A
 39%|███▊      | 46/119 [00:00<00:00, 82.84it/s][A
 46%|████▌     | 55/119 [00:00<00:00, 82.51it/s][A
 54%|█████▍    | 64/119 [00:00<00:00, 82.85it/s][A
 61%|██████▏   | 73/119 [00:00<00:00, 81.59it/s][A
 69%|██████▉   | 82/119 [00:00<00:00, 82.19it/s][A
 76%|███████▋  | 91/119 [00:01<00:00, 81.67it/s][A
 84%|████████▍ | 100/119 [00:01<00:00, 81.54it/s][A
 92%|█████████▏| 109/119 [00:01<00:00, 81.05it/s][A
 99%|█████████▉| 118/119 [00:01<00:00, 78.36it/s][A                                                 
                                                 [A 20%|██        | 119/595 [00:09<00:30, 15.81it/s]
100%|██████████| 119/119 [00:01<00:00, 78.36it/s][A
                                                 [A 20%|██        | 120/595 [00:09<02:02,  3.89it/s] 21%|██        | 122/595 [00:10<01:36,  4.89it/s] 21%|██        | 124/595 [00:10<01:17,  6.06it/s] 21%|██        | 126/595 [00:10<01:03,  7.36it/s] 22%|██▏       | 128/595 [00:10<00:53,  8.73it/s] 22%|██▏       | 130/595 [00:10<00:46, 10.09it/s] 22%|██▏       | 132/595 [00:10<00:40, 11.35it/s] 23%|██▎       | 134/595 [00:10<00:37, 12.46it/s] 23%|██▎       | 136/595 [00:10<00:34, 13.37it/s] 23%|██▎       | 138/595 [00:11<00:32, 14.09it/s] 24%|██▎       | 140/595 [00:11<00:31, 14.64it/s] 24%|██▍       | 142/595 [00:11<00:30, 15.09it/s] 24%|██▍       | 144/595 [00:11<00:29, 15.42it/s] 25%|██▍       | 146/595 [00:11<00:28, 15.64it/s] 25%|██▍       | 148/595 [00:11<00:28, 15.75it/s] 25%|██▌       | 150/595 [00:11<00:28, 15.85it/s] 26%|██▌       | 152/595 [00:11<00:27, 15.96it/s] 26%|██▌       | 154/595 [00:12<00:27, 16.05it/s] 26%|██▌       | 156/595 [00:12<00:27, 16.07it/s] 27%|██▋       | 158/595 [00:12<00:27, 16.08it/s] 27%|██▋       | 160/595 [00:12<00:27, 16.08it/s] 27%|██▋       | 162/595 [00:12<00:26, 16.12it/s] 28%|██▊       | 164/595 [00:12<00:26, 16.15it/s] 28%|██▊       | 166/595 [00:12<00:26, 16.01it/s] 28%|██▊       | 168/595 [00:12<00:26, 15.93it/s] 29%|██▊       | 170/595 [00:13<00:26, 15.94it/s] 29%|██▉       | 172/595 [00:13<00:26, 15.90it/s] 29%|██▉       | 174/595 [00:13<00:26, 15.81it/s] 30%|██▉       | 176/595 [00:13<00:26, 15.83it/s] 30%|██▉       | 178/595 [00:13<00:26, 15.85it/s] 30%|███       | 180/595 [00:13<00:26, 15.83it/s] 31%|███       | 182/595 [00:13<00:26, 15.88it/s] 31%|███       | 184/595 [00:13<00:25, 15.93it/s] 31%|███▏      | 186/595 [00:14<00:25, 15.90it/s] 32%|███▏      | 188/595 [00:14<00:25, 15.89it/s] 32%|███▏      | 190/595 [00:14<00:25, 15.87it/s] 32%|███▏      | 192/595 [00:14<00:25, 15.88it/s] 33%|███▎      | 194/595 [00:14<00:25, 15.89it/s] 33%|███▎      | 196/595 [00:14<00:25, 15.94it/s] 33%|███▎      | 198/595 [00:14<00:24, 15.99it/s] 34%|███▎      | 200/595 [00:14<00:24, 15.96it/s] 34%|███▍      | 202/595 [00:15<00:24, 15.91it/s] 34%|███▍      | 204/595 [00:15<00:24, 15.90it/s] 35%|███▍      | 206/595 [00:15<00:24, 15.83it/s] 35%|███▍      | 208/595 [00:15<00:24, 15.77it/s] 35%|███▌      | 210/595 [00:15<00:24, 15.78it/s] 36%|███▌      | 212/595 [00:15<00:24, 15.77it/s] 36%|███▌      | 214/595 [00:15<00:24, 15.80it/s] 36%|███▋      | 216/595 [00:16<00:23, 15.95it/s] 37%|███▋      | 218/595 [00:16<00:23, 16.02it/s] 37%|███▋      | 220/595 [00:16<00:23, 16.05it/s] 37%|███▋      | 222/595 [00:16<00:23, 16.06it/s] 38%|███▊      | 224/595 [00:16<00:23, 16.06it/s] 38%|███▊      | 226/595 [00:16<00:22, 16.12it/s] 38%|███▊      | 228/595 [00:16<00:22, 16.12it/s] 39%|███▊      | 230/595 [00:16<00:22, 16.10it/s] 39%|███▉      | 232/595 [00:16<00:22, 16.05it/s] 39%|███▉      | 234/595 [00:17<00:22, 16.07it/s] 40%|███▉      | 236/595 [00:17<00:22, 16.13it/s]                                                  40%|████      | 238/595 [00:17<00:22, 16.13it/s][INFO|trainer.py:755] 2023-11-15 23:27:50,733 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 23:27:50,735 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 23:27:50,735 >>   Num examples = 945
[INFO|trainer.py:3134] 2023-11-15 23:27:50,736 >>   Batch size = 8
{'eval_loss': 0.6044871211051941, 'eval_accuracy': 0.7513227513227513, 'eval_micro_f1': 0.7513227513227513, 'eval_macro_f1': 0.6481325606447235, 'eval_runtime': 1.5096, 'eval_samples_per_second': 626.003, 'eval_steps_per_second': 78.83, 'epoch': 1.0}
{'loss': 0.4921, 'learning_rate': 0.0003, 'epoch': 2.0}

  0%|          | 0/119 [00:00<?, ?it/s][A
  8%|▊         | 10/119 [00:00<00:01, 95.65it/s][A
 17%|█▋        | 20/119 [00:00<00:01, 89.43it/s][A
 24%|██▍       | 29/119 [00:00<00:01, 87.13it/s][A
 32%|███▏      | 38/119 [00:00<00:00, 85.19it/s][A
 39%|███▉      | 47/119 [00:00<00:00, 84.78it/s][A
 47%|████▋     | 56/119 [00:00<00:00, 85.04it/s][A
 55%|█████▍    | 65/119 [00:00<00:00, 84.99it/s][A
 62%|██████▏   | 74/119 [00:00<00:00, 84.70it/s][A
 70%|██████▉   | 83/119 [00:00<00:00, 83.87it/s][A
 77%|███████▋  | 92/119 [00:01<00:00, 83.60it/s][A
 85%|████████▍ | 101/119 [00:01<00:00, 83.50it/s][A
 92%|█████████▏| 110/119 [00:01<00:00, 83.18it/s][A
100%|██████████| 119/119 [00:01<00:00, 82.91it/s][A                                                 
                                                 [A 40%|████      | 238/595 [00:18<00:22, 16.13it/s]
100%|██████████| 119/119 [00:01<00:00, 82.91it/s][A
                                                 [A 40%|████      | 239/595 [00:18<01:28,  4.02it/s] 41%|████      | 241/595 [00:18<01:10,  5.04it/s] 41%|████      | 243/595 [00:19<00:56,  6.23it/s] 41%|████      | 245/595 [00:19<00:46,  7.53it/s] 42%|████▏     | 247/595 [00:19<00:39,  8.89it/s] 42%|████▏     | 249/595 [00:19<00:33, 10.21it/s] 42%|████▏     | 251/595 [00:19<00:30, 11.40it/s] 43%|████▎     | 253/595 [00:19<00:27, 12.45it/s] 43%|████▎     | 255/595 [00:19<00:25, 13.24it/s] 43%|████▎     | 257/595 [00:19<00:24, 13.91it/s] 44%|████▎     | 259/595 [00:20<00:23, 14.49it/s] 44%|████▍     | 261/595 [00:20<00:22, 14.92it/s] 44%|████▍     | 263/595 [00:20<00:21, 15.20it/s] 45%|████▍     | 265/595 [00:20<00:21, 15.42it/s] 45%|████▍     | 267/595 [00:20<00:21, 15.60it/s] 45%|████▌     | 269/595 [00:20<00:20, 15.66it/s] 46%|████▌     | 271/595 [00:20<00:20, 15.63it/s] 46%|████▌     | 273/595 [00:21<00:20, 15.70it/s] 46%|████▌     | 275/595 [00:21<00:20, 15.70it/s] 47%|████▋     | 277/595 [00:21<00:20, 15.70it/s] 47%|████▋     | 279/595 [00:21<00:20, 15.80it/s] 47%|████▋     | 281/595 [00:21<00:19, 15.88it/s] 48%|████▊     | 283/595 [00:21<00:19, 15.92it/s] 48%|████▊     | 285/595 [00:21<00:19, 15.96it/s] 48%|████▊     | 287/595 [00:21<00:19, 16.03it/s] 49%|████▊     | 289/595 [00:22<00:19, 16.07it/s] 49%|████▉     | 291/595 [00:22<00:18, 16.02it/s] 49%|████▉     | 293/595 [00:22<00:18, 16.03it/s] 50%|████▉     | 295/595 [00:22<00:18, 16.05it/s] 50%|████▉     | 297/595 [00:22<00:18, 16.10it/s] 50%|█████     | 299/595 [00:22<00:18, 16.10it/s] 51%|█████     | 301/595 [00:22<00:18, 16.03it/s] 51%|█████     | 303/595 [00:22<00:18, 15.97it/s] 51%|█████▏    | 305/595 [00:23<00:18, 16.01it/s] 52%|█████▏    | 307/595 [00:23<00:17, 16.04it/s] 52%|█████▏    | 309/595 [00:23<00:17, 15.99it/s] 52%|█████▏    | 311/595 [00:23<00:17, 16.01it/s] 53%|█████▎    | 313/595 [00:23<00:17, 16.08it/s] 53%|█████▎    | 315/595 [00:23<00:17, 16.09it/s] 53%|█████▎    | 317/595 [00:23<00:17, 16.03it/s] 54%|█████▎    | 319/595 [00:23<00:17, 15.92it/s] 54%|█████▍    | 321/595 [00:24<00:17, 15.92it/s] 54%|█████▍    | 323/595 [00:24<00:17, 15.81it/s] 55%|█████▍    | 325/595 [00:24<00:17, 15.75it/s] 55%|█████▍    | 327/595 [00:24<00:16, 15.78it/s] 55%|█████▌    | 329/595 [00:24<00:16, 15.75it/s] 56%|█████▌    | 331/595 [00:24<00:16, 15.75it/s] 56%|█████▌    | 333/595 [00:24<00:16, 15.79it/s] 56%|█████▋    | 335/595 [00:24<00:16, 15.82it/s] 57%|█████▋    | 337/595 [00:25<00:16, 15.86it/s] 57%|█████▋    | 339/595 [00:25<00:16, 15.88it/s] 57%|█████▋    | 341/595 [00:25<00:15, 15.88it/s] 58%|█████▊    | 343/595 [00:25<00:15, 15.83it/s] 58%|█████▊    | 345/595 [00:25<00:15, 15.89it/s] 58%|█████▊    | 347/595 [00:25<00:15, 15.90it/s] 59%|█████▊    | 349/595 [00:25<00:15, 15.87it/s] 59%|█████▉    | 351/595 [00:25<00:15, 15.88it/s] 59%|█████▉    | 353/595 [00:26<00:15, 15.85it/s] 60%|█████▉    | 355/595 [00:26<00:15, 15.78it/s]                                                  60%|██████    | 357/595 [00:26<00:15, 15.78it/s][INFO|trainer.py:755] 2023-11-15 23:27:59,655 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 23:27:59,656 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 23:27:59,657 >>   Num examples = 945
[INFO|trainer.py:3134] 2023-11-15 23:27:59,657 >>   Batch size = 8
{'eval_loss': 0.47981950640678406, 'eval_accuracy': 0.8, 'eval_micro_f1': 0.8000000000000002, 'eval_macro_f1': 0.7026017029572698, 'eval_runtime': 1.458, 'eval_samples_per_second': 648.168, 'eval_steps_per_second': 81.621, 'epoch': 2.0}
{'loss': 0.3811, 'learning_rate': 0.0002, 'epoch': 3.0}

  0%|          | 0/119 [00:00<?, ?it/s][A
  8%|▊         | 9/119 [00:00<00:01, 83.53it/s][A
 15%|█▌        | 18/119 [00:00<00:01, 77.14it/s][A
 22%|██▏       | 26/119 [00:00<00:01, 75.15it/s][A
 29%|██▊       | 34/119 [00:00<00:01, 75.01it/s][A
 36%|███▌      | 43/119 [00:00<00:00, 77.51it/s][A
 44%|████▎     | 52/119 [00:00<00:00, 79.27it/s][A
 51%|█████▏    | 61/119 [00:00<00:00, 80.63it/s][A
 59%|█████▉    | 70/119 [00:00<00:00, 81.67it/s][A
 66%|██████▋   | 79/119 [00:00<00:00, 82.31it/s][A
 74%|███████▍  | 88/119 [00:01<00:00, 82.41it/s][A
 82%|████████▏ | 97/119 [00:01<00:00, 83.36it/s][A
 89%|████████▉ | 106/119 [00:01<00:00, 83.61it/s][A
 97%|█████████▋| 115/119 [00:01<00:00, 83.47it/s][A                                                 
                                                 [A 60%|██████    | 357/595 [00:27<00:15, 15.78it/s]
100%|██████████| 119/119 [00:01<00:00, 83.47it/s][A
                                                 [A 60%|██████    | 358/595 [00:27<01:01,  3.88it/s] 61%|██████    | 360/595 [00:27<00:48,  4.89it/s] 61%|██████    | 362/595 [00:28<00:38,  6.08it/s] 61%|██████    | 364/595 [00:28<00:31,  7.39it/s] 62%|██████▏   | 366/595 [00:28<00:26,  8.75it/s] 62%|██████▏   | 368/595 [00:28<00:22, 10.08it/s] 62%|██████▏   | 370/595 [00:28<00:19, 11.34it/s] 63%|██████▎   | 372/595 [00:28<00:17, 12.39it/s] 63%|██████▎   | 374/595 [00:28<00:16, 13.25it/s] 63%|██████▎   | 376/595 [00:28<00:15, 13.97it/s] 64%|██████▎   | 378/595 [00:29<00:14, 14.55it/s] 64%|██████▍   | 380/595 [00:29<00:14, 14.94it/s] 64%|██████▍   | 382/595 [00:29<00:14, 15.12it/s] 65%|██████▍   | 384/595 [00:29<00:13, 15.33it/s] 65%|██████▍   | 386/595 [00:29<00:13, 15.44it/s] 65%|██████▌   | 388/595 [00:29<00:13, 15.50it/s] 66%|██████▌   | 390/595 [00:29<00:13, 15.62it/s] 66%|██████▌   | 392/595 [00:29<00:12, 15.66it/s] 66%|██████▌   | 394/595 [00:30<00:12, 15.66it/s] 67%|██████▋   | 396/595 [00:30<00:12, 15.73it/s] 67%|██████▋   | 398/595 [00:30<00:12, 15.78it/s] 67%|██████▋   | 400/595 [00:30<00:12, 15.76it/s] 68%|██████▊   | 402/595 [00:30<00:12, 15.82it/s] 68%|██████▊   | 404/595 [00:30<00:12, 15.86it/s] 68%|██████▊   | 406/595 [00:30<00:11, 15.78it/s] 69%|██████▊   | 408/595 [00:30<00:11, 15.84it/s] 69%|██████▉   | 410/595 [00:31<00:11, 15.86it/s] 69%|██████▉   | 412/595 [00:31<00:11, 15.86it/s] 70%|██████▉   | 414/595 [00:31<00:11, 15.87it/s] 70%|██████▉   | 416/595 [00:31<00:11, 15.92it/s] 70%|███████   | 418/595 [00:31<00:11, 15.83it/s] 71%|███████   | 420/595 [00:31<00:11, 15.73it/s] 71%|███████   | 422/595 [00:31<00:11, 15.69it/s] 71%|███████▏  | 424/595 [00:31<00:10, 15.67it/s] 72%|███████▏  | 426/595 [00:32<00:10, 15.70it/s] 72%|███████▏  | 428/595 [00:32<00:10, 15.74it/s] 72%|███████▏  | 430/595 [00:32<00:10, 15.78it/s] 73%|███████▎  | 432/595 [00:32<00:10, 15.89it/s] 73%|███████▎  | 434/595 [00:32<00:10, 15.95it/s] 73%|███████▎  | 436/595 [00:32<00:09, 15.95it/s] 74%|███████▎  | 438/595 [00:32<00:09, 15.93it/s] 74%|███████▍  | 440/595 [00:32<00:09, 15.99it/s] 74%|███████▍  | 442/595 [00:33<00:09, 16.01it/s] 75%|███████▍  | 444/595 [00:33<00:09, 15.99it/s] 75%|███████▍  | 446/595 [00:33<00:09, 15.95it/s] 75%|███████▌  | 448/595 [00:33<00:09, 15.98it/s] 76%|███████▌  | 450/595 [00:33<00:09, 16.00it/s] 76%|███████▌  | 452/595 [00:33<00:08, 16.00it/s] 76%|███████▋  | 454/595 [00:33<00:08, 15.99it/s] 77%|███████▋  | 456/595 [00:33<00:08, 16.04it/s] 77%|███████▋  | 458/595 [00:34<00:08, 16.07it/s] 77%|███████▋  | 460/595 [00:34<00:08, 16.05it/s] 78%|███████▊  | 462/595 [00:34<00:08, 16.01it/s] 78%|███████▊  | 464/595 [00:34<00:08, 16.03it/s] 78%|███████▊  | 466/595 [00:34<00:08, 16.03it/s] 79%|███████▊  | 468/595 [00:34<00:07, 15.92it/s] 79%|███████▉  | 470/595 [00:34<00:07, 15.86it/s] 79%|███████▉  | 472/595 [00:35<00:07, 15.85it/s] 80%|███████▉  | 474/595 [00:35<00:07, 15.78it/s]                                                  80%|████████  | 476/595 [00:35<00:07, 15.78it/s][INFO|trainer.py:755] 2023-11-15 23:28:08,625 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 23:28:08,626 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 23:28:08,627 >>   Num examples = 945
[INFO|trainer.py:3134] 2023-11-15 23:28:08,627 >>   Batch size = 8
{'eval_loss': 0.44272780418395996, 'eval_accuracy': 0.8328042328042328, 'eval_micro_f1': 0.8328042328042328, 'eval_macro_f1': 0.7687502748721214, 'eval_runtime': 1.5108, 'eval_samples_per_second': 625.506, 'eval_steps_per_second': 78.767, 'epoch': 3.0}
{'loss': 0.3024, 'learning_rate': 0.0001, 'epoch': 4.0}

  0%|          | 0/119 [00:00<?, ?it/s][A
  8%|▊         | 10/119 [00:00<00:01, 95.21it/s][A
 17%|█▋        | 20/119 [00:00<00:01, 89.86it/s][A
 25%|██▌       | 30/119 [00:00<00:01, 88.70it/s][A
 33%|███▎      | 39/119 [00:00<00:00, 87.11it/s][A
 40%|████      | 48/119 [00:00<00:00, 87.18it/s][A
 48%|████▊     | 57/119 [00:00<00:00, 86.95it/s][A
 55%|█████▌    | 66/119 [00:00<00:00, 86.42it/s][A
 63%|██████▎   | 75/119 [00:00<00:00, 86.93it/s][A
 71%|███████   | 84/119 [00:00<00:00, 86.29it/s][A
 78%|███████▊  | 93/119 [00:01<00:00, 86.56it/s][A
 86%|████████▌ | 102/119 [00:01<00:00, 86.87it/s][A
 93%|█████████▎| 111/119 [00:01<00:00, 87.22it/s][A                                                 
                                                 [A 80%|████████  | 476/595 [00:36<00:07, 15.78it/s]
100%|██████████| 119/119 [00:01<00:00, 87.22it/s][A
                                                 [A 80%|████████  | 477/595 [00:36<00:28,  4.11it/s] 81%|████████  | 479/595 [00:36<00:22,  5.15it/s] 81%|████████  | 481/595 [00:36<00:17,  6.35it/s] 81%|████████  | 483/595 [00:37<00:14,  7.66it/s] 82%|████████▏ | 485/595 [00:37<00:12,  8.99it/s] 82%|████████▏ | 487/595 [00:37<00:10, 10.24it/s] 82%|████████▏ | 489/595 [00:37<00:09, 11.43it/s] 83%|████████▎ | 491/595 [00:37<00:08, 12.37it/s] 83%|████████▎ | 493/595 [00:37<00:07, 13.26it/s] 83%|████████▎ | 495/595 [00:37<00:07, 14.01it/s] 84%|████████▎ | 497/595 [00:37<00:06, 14.57it/s] 84%|████████▍ | 499/595 [00:38<00:06, 14.97it/s] 84%|████████▍ | 501/595 [00:38<00:06, 15.27it/s] 85%|████████▍ | 503/595 [00:38<00:05, 15.52it/s] 85%|████████▍ | 505/595 [00:38<00:05, 15.70it/s] 85%|████████▌ | 507/595 [00:38<00:05, 15.78it/s] 86%|████████▌ | 509/595 [00:38<00:05, 15.84it/s] 86%|████████▌ | 511/595 [00:38<00:05, 15.92it/s] 86%|████████▌ | 513/595 [00:38<00:05, 15.99it/s] 87%|████████▋ | 515/595 [00:39<00:04, 16.02it/s] 87%|████████▋ | 517/595 [00:39<00:04, 16.02it/s] 87%|████████▋ | 519/595 [00:39<00:04, 16.01it/s] 88%|████████▊ | 521/595 [00:39<00:04, 16.07it/s] 88%|████████▊ | 523/595 [00:39<00:04, 16.02it/s] 88%|████████▊ | 525/595 [00:39<00:04, 15.89it/s] 89%|████████▊ | 527/595 [00:39<00:04, 15.94it/s] 89%|████████▉ | 529/595 [00:39<00:04, 16.00it/s] 89%|████████▉ | 531/595 [00:40<00:03, 16.01it/s] 90%|████████▉ | 533/595 [00:40<00:03, 15.83it/s] 90%|████████▉ | 535/595 [00:40<00:03, 15.89it/s] 90%|█████████ | 537/595 [00:40<00:03, 15.83it/s] 91%|█████████ | 539/595 [00:40<00:03, 15.78it/s] 91%|█████████ | 541/595 [00:40<00:03, 15.74it/s] 91%|█████████▏| 543/595 [00:40<00:03, 15.68it/s] 92%|█████████▏| 545/595 [00:40<00:03, 15.70it/s] 92%|█████████▏| 547/595 [00:41<00:03, 15.72it/s] 92%|█████████▏| 549/595 [00:41<00:02, 15.70it/s] 93%|█████████▎| 551/595 [00:41<00:02, 15.81it/s] 93%|█████████▎| 553/595 [00:41<00:02, 15.88it/s] 93%|█████████▎| 555/595 [00:41<00:02, 15.88it/s] 94%|█████████▎| 557/595 [00:41<00:02, 15.88it/s] 94%|█████████▍| 559/595 [00:41<00:02, 15.93it/s] 94%|█████████▍| 561/595 [00:41<00:02, 15.84it/s] 95%|█████████▍| 563/595 [00:42<00:02, 15.83it/s] 95%|█████████▍| 565/595 [00:42<00:01, 15.87it/s] 95%|█████████▌| 567/595 [00:42<00:01, 15.91it/s] 96%|█████████▌| 569/595 [00:42<00:01, 15.91it/s] 96%|█████████▌| 571/595 [00:42<00:01, 15.89it/s] 96%|█████████▋| 573/595 [00:42<00:01, 15.85it/s] 97%|█████████▋| 575/595 [00:42<00:01, 15.80it/s] 97%|█████████▋| 577/595 [00:42<00:01, 15.65it/s] 97%|█████████▋| 579/595 [00:43<00:01, 15.58it/s] 98%|█████████▊| 581/595 [00:43<00:00, 15.72it/s] 98%|█████████▊| 583/595 [00:43<00:00, 15.87it/s] 98%|█████████▊| 585/595 [00:43<00:00, 15.95it/s] 99%|█████████▊| 587/595 [00:43<00:00, 15.98it/s] 99%|█████████▉| 589/595 [00:43<00:00, 15.99it/s] 99%|█████████▉| 591/595 [00:43<00:00, 15.98it/s]100%|█████████▉| 593/595 [00:43<00:00, 16.03it/s]                                                 100%|██████████| 595/595 [00:44<00:00, 16.03it/s][INFO|trainer.py:755] 2023-11-15 23:28:17,489 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 23:28:17,490 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 23:28:17,490 >>   Num examples = 945
[INFO|trainer.py:3134] 2023-11-15 23:28:17,490 >>   Batch size = 8
{'eval_loss': 0.461010605096817, 'eval_accuracy': 0.8444444444444444, 'eval_micro_f1': 0.8444444444444444, 'eval_macro_f1': 0.7802582730489241, 'eval_runtime': 1.4061, 'eval_samples_per_second': 672.09, 'eval_steps_per_second': 84.634, 'epoch': 4.0}
{'loss': 0.2549, 'learning_rate': 0.0, 'epoch': 5.0}

  0%|          | 0/119 [00:00<?, ?it/s][A
  8%|▊         | 10/119 [00:00<00:01, 96.98it/s][A
 17%|█▋        | 20/119 [00:00<00:01, 91.83it/s][A
 25%|██▌       | 30/119 [00:00<00:00, 90.13it/s][A
 34%|███▎      | 40/119 [00:00<00:00, 89.87it/s][A
 41%|████      | 49/119 [00:00<00:00, 89.69it/s][A
 49%|████▊     | 58/119 [00:00<00:00, 88.85it/s][A
 56%|█████▋    | 67/119 [00:00<00:00, 88.49it/s][A
 64%|██████▍   | 76/119 [00:00<00:00, 88.10it/s][A
 71%|███████▏  | 85/119 [00:00<00:00, 88.54it/s][A
 79%|███████▉  | 94/119 [00:01<00:00, 88.49it/s][A
 87%|████████▋ | 103/119 [00:01<00:00, 87.88it/s][A
 94%|█████████▍| 112/119 [00:01<00:00, 88.40it/s][A                                                 
                                                 [A100%|██████████| 595/595 [00:45<00:00, 16.03it/s]
100%|██████████| 119/119 [00:01<00:00, 88.40it/s][A
                                                 [A[INFO|trainer.py:1963] 2023-11-15 23:28:18,875 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                 100%|██████████| 595/595 [00:45<00:00, 16.03it/s]100%|██████████| 595/595 [00:45<00:00, 13.08it/s]
[INFO|trainer.py:2855] 2023-11-15 23:28:18,878 >> Saving model checkpoint to ./result/restaurant_roberta-base_seed2_adapter
[INFO|configuration_utils.py:460] 2023-11-15 23:28:18,881 >> Configuration saved in ./result/restaurant_roberta-base_seed2_adapter/config.json
[INFO|modeling_utils.py:1997] 2023-11-15 23:28:20,008 >> Model weights saved in ./result/restaurant_roberta-base_seed2_adapter/pytorch_model.bin
[INFO|tokenization_utils_base.py:2235] 2023-11-15 23:28:20,010 >> tokenizer config file saved in ./result/restaurant_roberta-base_seed2_adapter/tokenizer_config.json
[INFO|tokenization_utils_base.py:2242] 2023-11-15 23:28:20,012 >> Special tokens file saved in ./result/restaurant_roberta-base_seed2_adapter/special_tokens_map.json
{'eval_loss': 0.4492655396461487, 'eval_accuracy': 0.8391534391534392, 'eval_micro_f1': 0.8391534391534392, 'eval_macro_f1': 0.7751535696931972, 'eval_runtime': 1.3817, 'eval_samples_per_second': 683.922, 'eval_steps_per_second': 86.124, 'epoch': 5.0}
{'train_runtime': 45.4788, 'train_samples_per_second': 415.249, 'train_steps_per_second': 13.083, 'train_loss': 0.4251177491260176, 'epoch': 5.0}
***** train metrics *****
  epoch                    =        5.0
  train_loss               =     0.4251
  train_runtime            = 0:00:45.47
  train_samples            =       3777
  train_samples_per_second =    415.249
  train_steps_per_second   =     13.083
11/15/2023 23:28:20 - INFO - __main__ - *** Evaluate ***
[INFO|trainer.py:755] 2023-11-15 23:28:20,152 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 23:28:20,153 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 23:28:20,153 >>   Num examples = 945
[INFO|trainer.py:3134] 2023-11-15 23:28:20,154 >>   Batch size = 8
  0%|          | 0/119 [00:00<?, ?it/s]  8%|▊         | 10/119 [00:00<00:01, 93.65it/s] 17%|█▋        | 20/119 [00:00<00:01, 88.55it/s] 24%|██▍       | 29/119 [00:00<00:01, 85.80it/s] 32%|███▏      | 38/119 [00:00<00:00, 85.89it/s] 39%|███▉      | 47/119 [00:00<00:00, 85.72it/s] 47%|████▋     | 56/119 [00:00<00:00, 85.89it/s] 55%|█████▍    | 65/119 [00:00<00:00, 85.23it/s] 62%|██████▏   | 74/119 [00:00<00:00, 85.21it/s] 70%|██████▉   | 83/119 [00:00<00:00, 84.90it/s] 77%|███████▋  | 92/119 [00:01<00:00, 85.13it/s] 85%|████████▍ | 101/119 [00:01<00:00, 85.26it/s] 92%|█████████▏| 110/119 [00:01<00:00, 84.86it/s]100%|██████████| 119/119 [00:01<00:00, 83.04it/s]100%|██████████| 119/119 [00:01<00:00, 82.87it/s]
***** eval metrics *****
  epoch                   =        5.0
  eval_accuracy           =     0.8392
  eval_loss               =     0.4493
  eval_macro_f1           =     0.7752
  eval_micro_f1           =     0.8392
  eval_runtime            = 0:00:01.45
  eval_samples            =        945
  eval_samples_per_second =    651.096
  eval_steps_per_second   =      81.99
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                  eval/accuracy ▁▅▇███
wandb:                      eval/loss █▃▁▂▁▁
wandb:                  eval/macro_f1 ▁▄▇███
wandb:                  eval/micro_f1 ▁▅▇███
wandb:                   eval/runtime █▅█▂▁▅
wandb:        eval/samples_per_second ▁▄▁▇█▄
wandb:          eval/steps_per_second ▁▄▁▇█▄
wandb:                    train/epoch ▁▁▃▃▅▅▆▆████
wandb:              train/global_step ▁▁▃▃▅▅▆▆████
wandb:            train/learning_rate █▆▅▃▁
wandb:                     train/loss █▅▃▂▁
wandb:               train/total_flos ▁
wandb:               train/train_loss ▁
wandb:            train/train_runtime ▁
wandb: train/train_samples_per_second ▁
wandb:   train/train_steps_per_second ▁
wandb: 
wandb: Run summary:
wandb:                  eval/accuracy 0.83915
wandb:                      eval/loss 0.44927
wandb:                  eval/macro_f1 0.77515
wandb:                  eval/micro_f1 0.83915
wandb:                   eval/runtime 1.4514
wandb:        eval/samples_per_second 651.096
wandb:          eval/steps_per_second 81.99
wandb:                    train/epoch 5.0
wandb:              train/global_step 595
wandb:            train/learning_rate 0.0
wandb:                     train/loss 0.2549
wandb:               train/total_flos 627599085655680.0
wandb:               train/train_loss 0.42512
wandb:            train/train_runtime 45.4788
wandb: train/train_samples_per_second 415.249
wandb:   train/train_steps_per_second 13.083
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /ceph/home/wangyifei/a3/wandb/offline-run-20231115_232615-2c03ayp6
wandb: Find logs at: ./wandb/offline-run-20231115_232615-2c03ayp6/logs
(ModelArguments(model_name_or_path='roberta-base', cache_dir='./cache', use_adapter=True, use_lora=False), DataTrainingArguments(dataset_name='acl', max_seq_length=64), TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.0005,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=./result/acl_roberta-base_seed2_adapter/runs/Nov15_23-28-31_node013,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.EPOCH,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=5.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
output_dir=./result/acl_roberta-base_seed2_adapter,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
resume_from_checkpoint=None,
run_name=./result/acl_roberta-base_seed2_adapter,
save_on_each_node=False,
save_safetensors=False,
save_steps=500,
save_strategy=IntervalStrategy.NO,
save_total_limit=None,
seed=333,
sharded_ddp=[],
skip_memory_metrics=True,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
))
11/15/2023 23:28:31 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False
11/15/2023 23:28:31 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.0005,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=./result/acl_roberta-base_seed2_adapter/runs/Nov15_23-28-31_node013,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.EPOCH,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=5.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
output_dir=./result/acl_roberta-base_seed2_adapter,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
resume_from_checkpoint=None,
run_name=./result/acl_roberta-base_seed2_adapter,
save_on_each_node=False,
save_safetensors=False,
save_steps=500,
save_strategy=IntervalStrategy.NO,
save_total_limit=None,
seed=333,
sharded_ddp=[],
skip_memory_metrics=True,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
wandb: Tracking run with wandb version 0.15.12
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
Map:   0%|          | 0/11020 [00:00<?, ? examples/s]Map:  36%|███▌      | 3951/11020 [00:00<00:00, 39299.97 examples/s]Map:  73%|███████▎  | 8000/11020 [00:00<00:00, 38723.98 examples/s]Map: 100%|██████████| 11020/11020 [00:00<00:00, 38461.19 examples/s]
[INFO|configuration_utils.py:715] 2023-11-15 23:28:47,573 >> loading configuration file config.json from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/config.json
[INFO|configuration_utils.py:775] 2023-11-15 23:28:47,582 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.33.3",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_auto.py:535] 2023-11-15 23:28:57,598 >> Could not locate the tokenizer configuration file, will try to use the model config instead.
[INFO|configuration_utils.py:715] 2023-11-15 23:29:07,614 >> loading configuration file config.json from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/config.json
[INFO|configuration_utils.py:775] 2023-11-15 23:29:07,615 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.33.3",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:1852] 2023-11-15 23:29:27,662 >> loading file vocab.json from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/vocab.json
[INFO|tokenization_utils_base.py:1852] 2023-11-15 23:29:27,663 >> loading file merges.txt from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/merges.txt
[INFO|tokenization_utils_base.py:1852] 2023-11-15 23:29:27,663 >> loading file tokenizer.json from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/tokenizer.json
[INFO|tokenization_utils_base.py:1852] 2023-11-15 23:29:27,663 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1852] 2023-11-15 23:29:27,664 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1852] 2023-11-15 23:29:27,664 >> loading file tokenizer_config.json from cache at None
[INFO|configuration_utils.py:715] 2023-11-15 23:29:27,665 >> loading configuration file config.json from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/config.json
[INFO|configuration_utils.py:775] 2023-11-15 23:29:27,666 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.33.3",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:2862] 2023-11-15 23:29:47,844 >> loading weights file model.safetensors from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/model.safetensors
[INFO|modeling_utils.py:3638] 2023-11-15 23:29:48,590 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3650] 2023-11-15 23:29:48,591 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
parameters 1487427
model_max_length: 512
Running tokenizer on dataset:   0%|          | 0/8816 [00:00<?, ? examples/s]Running tokenizer on dataset:  23%|██▎       | 2000/8816 [00:00<00:00, 15945.80 examples/s]Running tokenizer on dataset:  45%|████▌     | 4000/8816 [00:00<00:00, 17082.86 examples/s]Running tokenizer on dataset:  68%|██████▊   | 6000/8816 [00:00<00:00, 17504.12 examples/s]Running tokenizer on dataset:  91%|█████████ | 8000/8816 [00:00<00:00, 18071.81 examples/s]Running tokenizer on dataset: 100%|██████████| 8816/8816 [00:00<00:00, 17758.66 examples/s]
Running tokenizer on dataset:   0%|          | 0/2204 [00:00<?, ? examples/s]Running tokenizer on dataset: 100%|██████████| 2204/2204 [00:00<00:00, 19939.96 examples/s]Running tokenizer on dataset: 100%|██████████| 2204/2204 [00:00<00:00, 19662.08 examples/s]
11/15/2023 23:29:50 - INFO - __main__ - Sample 5747 of the training set: {'text': 'While the redox buffer pairs (e.g. GSH/GSSG, reduced PC/oxidised PC, and reduced Protein/oxidised Protein) can protect cells from oxidative damage (Tsuji et al., 2002), this produces an imbalance in the redox status that may lead to other unwanted effects such as changes in intracellular pH, which…', 'label': 0, 'input_ids': [0, 5771, 5, 1275, 4325, 21944, 15029, 36, 242, 4, 571, 4, 272, 10237, 73, 534, 8108, 534, 6, 2906, 4985, 73, 4325, 808, 1720, 4985, 6, 8, 2906, 34786, 73, 4325, 808, 1720, 34786, 43, 64, 1744, 4590, 31, 46099, 1880, 36, 565, 9228, 5186, 4400, 1076, 482, 5241, 238, 42, 9108, 41, 27340, 11, 5, 1275, 4325, 2194, 14, 189, 483, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.
11/15/2023 23:29:50 - INFO - __main__ - Sample 5785 of the training set: {'text': 'BDNF has been shown to interact with several neurotrans-\nmitter systems, including the DA (Spenger et al., 1995), serotonin (5-HT) (Lyons et al., 1999; Rumajogee et al., 2002), and NPY systems (Barnea and Roberts, 2001; Nawa et al., 1993, 1994).', 'label': 0, 'input_ids': [0, 18941, 25356, 34, 57, 2343, 7, 10754, 19, 484, 44755, 12, 50118, 44370, 1743, 6, 217, 5, 9036, 36, 104, 9675, 2403, 4400, 1076, 482, 7969, 238, 43399, 36, 245, 12, 14469, 43, 36, 38683, 1790, 4400, 1076, 482, 6193, 131, 13772, 13745, 25601, 4400, 1076, 482, 5241, 238, 8, 26266, 975, 1743, 36, 14507, 22423, 8, 6274, 6, 5155, 131, 234, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.
11/15/2023 23:29:50 - INFO - __main__ - Sample 3534 of the training set: {'text': 'Studies have shown that alcohol consumption may contribute to the spread of HIV/AIDS by diminishing sexual inhibitions and interfering with one’s ability to adequately assess risk (Gordon et al. 1997; MacDonald et al. 2000a, b; Maisto et al. 2004).', 'label': 0, 'input_ids': [0, 46000, 33, 2343, 14, 3766, 4850, 189, 5042, 7, 5, 2504, 9, 7947, 73, 30968, 30, 28953, 1363, 38512, 8237, 8, 23524, 19, 65, 17, 27, 29, 1460, 7, 17327, 7118, 810, 36, 43226, 4400, 1076, 4, 7528, 131, 22207, 4400, 1076, 4, 3788, 102, 6, 741, 131, 3066, 661, 139, 4400, 1076, 4, 4482, 322, 2, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]}.
11/15/2023 23:29:50 - WARNING - accelerate.utils.other - Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[INFO|trainer.py:755] 2023-11-15 23:29:51,338 >> The following columns in the training set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:1715] 2023-11-15 23:29:51,349 >> ***** Running training *****
[INFO|trainer.py:1716] 2023-11-15 23:29:51,349 >>   Num examples = 8,816
[INFO|trainer.py:1717] 2023-11-15 23:29:51,349 >>   Num Epochs = 5
[INFO|trainer.py:1718] 2023-11-15 23:29:51,349 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1721] 2023-11-15 23:29:51,350 >>   Total train batch size (w. parallel, distributed & accumulation) = 32
[INFO|trainer.py:1722] 2023-11-15 23:29:51,350 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:1723] 2023-11-15 23:29:51,350 >>   Total optimization steps = 1,380
[INFO|trainer.py:1724] 2023-11-15 23:29:51,351 >>   Number of trainable parameters = 1,487,427
[INFO|integration_utils.py:716] 2023-11-15 23:29:51,352 >> Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"
  0%|          | 0/1380 [00:00<?, ?it/s]  0%|          | 1/1380 [00:01<23:21,  1.02s/it]  0%|          | 3/1380 [00:01<07:06,  3.23it/s]  0%|          | 5/1380 [00:01<04:10,  5.48it/s]  1%|          | 7/1380 [00:01<03:00,  7.62it/s]  1%|          | 9/1380 [00:01<02:23,  9.54it/s]  1%|          | 11/1380 [00:01<02:02, 11.16it/s]  1%|          | 13/1380 [00:01<01:49, 12.46it/s]  1%|          | 15/1380 [00:01<01:41, 13.46it/s]  1%|          | 17/1380 [00:02<01:35, 14.22it/s]  1%|▏         | 19/1380 [00:02<01:32, 14.70it/s]  2%|▏         | 21/1380 [00:02<01:29, 15.15it/s]  2%|▏         | 23/1380 [00:02<01:27, 15.43it/s]  2%|▏         | 25/1380 [00:02<01:27, 15.55it/s]  2%|▏         | 27/1380 [00:02<01:26, 15.64it/s]  2%|▏         | 29/1380 [00:02<01:25, 15.74it/s]  2%|▏         | 31/1380 [00:02<01:25, 15.73it/s]  2%|▏         | 33/1380 [00:03<01:25, 15.76it/s]  3%|▎         | 35/1380 [00:03<01:25, 15.79it/s]  3%|▎         | 37/1380 [00:03<01:25, 15.78it/s]  3%|▎         | 39/1380 [00:03<01:24, 15.78it/s]  3%|▎         | 41/1380 [00:03<01:24, 15.90it/s]  3%|▎         | 43/1380 [00:03<01:23, 15.94it/s]  3%|▎         | 45/1380 [00:03<01:23, 15.97it/s]  3%|▎         | 47/1380 [00:03<01:23, 15.96it/s]  4%|▎         | 49/1380 [00:04<01:23, 15.96it/s]  4%|▎         | 51/1380 [00:04<01:23, 15.96it/s]  4%|▍         | 53/1380 [00:04<01:23, 15.97it/s]  4%|▍         | 55/1380 [00:04<01:22, 16.00it/s]  4%|▍         | 57/1380 [00:04<01:22, 16.05it/s]  4%|▍         | 59/1380 [00:04<01:22, 16.06it/s]  4%|▍         | 61/1380 [00:04<01:22, 15.99it/s]  5%|▍         | 63/1380 [00:04<01:22, 15.98it/s]  5%|▍         | 65/1380 [00:05<01:22, 15.93it/s]  5%|▍         | 67/1380 [00:05<01:22, 15.84it/s]  5%|▌         | 69/1380 [00:05<01:22, 15.83it/s]  5%|▌         | 71/1380 [00:05<01:22, 15.85it/s]  5%|▌         | 73/1380 [00:05<01:22, 15.83it/s]  5%|▌         | 75/1380 [00:05<01:21, 15.92it/s]  6%|▌         | 77/1380 [00:05<01:21, 16.00it/s]  6%|▌         | 79/1380 [00:05<01:21, 16.04it/s]  6%|▌         | 81/1380 [00:06<01:20, 16.05it/s]  6%|▌         | 83/1380 [00:06<01:20, 16.05it/s]  6%|▌         | 85/1380 [00:06<01:20, 16.08it/s]  6%|▋         | 87/1380 [00:06<01:20, 16.12it/s]  6%|▋         | 89/1380 [00:06<01:19, 16.16it/s]  7%|▋         | 91/1380 [00:06<01:19, 16.18it/s]  7%|▋         | 93/1380 [00:06<01:19, 16.18it/s]  7%|▋         | 95/1380 [00:06<01:19, 16.16it/s]  7%|▋         | 97/1380 [00:07<01:19, 16.15it/s]  7%|▋         | 99/1380 [00:07<01:19, 16.17it/s]  7%|▋         | 101/1380 [00:07<01:18, 16.19it/s]  7%|▋         | 103/1380 [00:07<01:18, 16.19it/s]  8%|▊         | 105/1380 [00:07<01:18, 16.15it/s]  8%|▊         | 107/1380 [00:07<01:18, 16.13it/s]  8%|▊         | 109/1380 [00:07<01:18, 16.16it/s]  8%|▊         | 111/1380 [00:07<01:18, 16.18it/s]  8%|▊         | 113/1380 [00:07<01:18, 16.06it/s]  8%|▊         | 115/1380 [00:08<01:19, 15.93it/s]  8%|▊         | 117/1380 [00:08<01:19, 15.93it/s]  9%|▊         | 119/1380 [00:08<01:19, 15.85it/s]  9%|▉         | 121/1380 [00:08<01:19, 15.82it/s]  9%|▉         | 123/1380 [00:08<01:19, 15.84it/s]  9%|▉         | 125/1380 [00:08<01:19, 15.78it/s]  9%|▉         | 127/1380 [00:08<01:19, 15.78it/s]  9%|▉         | 129/1380 [00:09<01:18, 15.88it/s]  9%|▉         | 131/1380 [00:09<01:18, 15.89it/s] 10%|▉         | 133/1380 [00:09<01:18, 15.87it/s] 10%|▉         | 135/1380 [00:09<01:18, 15.93it/s] 10%|▉         | 137/1380 [00:09<01:18, 15.91it/s] 10%|█         | 139/1380 [00:09<01:18, 15.88it/s] 10%|█         | 141/1380 [00:09<01:18, 15.88it/s] 10%|█         | 143/1380 [00:09<01:17, 15.93it/s] 11%|█         | 145/1380 [00:10<01:17, 15.97it/s] 11%|█         | 147/1380 [00:10<01:17, 15.98it/s] 11%|█         | 149/1380 [00:10<01:17, 15.94it/s] 11%|█         | 151/1380 [00:10<01:17, 15.91it/s] 11%|█         | 153/1380 [00:10<01:17, 15.88it/s] 11%|█         | 155/1380 [00:10<01:17, 15.84it/s] 11%|█▏        | 157/1380 [00:10<01:17, 15.86it/s] 12%|█▏        | 159/1380 [00:10<01:16, 15.86it/s] 12%|█▏        | 161/1380 [00:11<01:16, 15.90it/s] 12%|█▏        | 163/1380 [00:11<01:16, 16.01it/s] 12%|█▏        | 165/1380 [00:11<01:15, 16.12it/s] 12%|█▏        | 167/1380 [00:11<01:15, 16.12it/s] 12%|█▏        | 169/1380 [00:11<01:14, 16.16it/s] 12%|█▏        | 171/1380 [00:11<01:14, 16.16it/s] 13%|█▎        | 173/1380 [00:11<01:14, 16.19it/s] 13%|█▎        | 175/1380 [00:11<01:14, 16.21it/s] 13%|█▎        | 177/1380 [00:12<01:14, 16.24it/s] 13%|█▎        | 179/1380 [00:12<01:13, 16.23it/s] 13%|█▎        | 181/1380 [00:12<01:13, 16.23it/s] 13%|█▎        | 183/1380 [00:12<01:13, 16.22it/s] 13%|█▎        | 185/1380 [00:12<01:13, 16.22it/s] 14%|█▎        | 187/1380 [00:12<01:13, 16.21it/s] 14%|█▎        | 189/1380 [00:12<01:13, 16.24it/s] 14%|█▍        | 191/1380 [00:12<01:13, 16.23it/s] 14%|█▍        | 193/1380 [00:12<01:13, 16.23it/s] 14%|█▍        | 195/1380 [00:13<01:13, 16.21it/s] 14%|█▍        | 197/1380 [00:13<01:12, 16.21it/s] 14%|█▍        | 199/1380 [00:13<01:12, 16.21it/s] 15%|█▍        | 201/1380 [00:13<01:13, 16.14it/s] 15%|█▍        | 203/1380 [00:13<01:13, 16.08it/s] 15%|█▍        | 205/1380 [00:13<01:13, 16.02it/s] 15%|█▌        | 207/1380 [00:13<01:13, 16.01it/s] 15%|█▌        | 209/1380 [00:13<01:13, 15.99it/s] 15%|█▌        | 211/1380 [00:14<01:13, 15.95it/s] 15%|█▌        | 213/1380 [00:14<01:13, 15.90it/s] 16%|█▌        | 215/1380 [00:14<01:12, 15.97it/s] 16%|█▌        | 217/1380 [00:14<01:12, 16.00it/s] 16%|█▌        | 219/1380 [00:14<01:12, 15.98it/s] 16%|█▌        | 221/1380 [00:14<01:12, 16.01it/s] 16%|█▌        | 223/1380 [00:14<01:12, 16.06it/s] 16%|█▋        | 225/1380 [00:14<01:12, 16.01it/s] 16%|█▋        | 227/1380 [00:15<01:12, 15.96it/s] 17%|█▋        | 229/1380 [00:15<01:11, 15.99it/s] 17%|█▋        | 231/1380 [00:15<01:11, 16.07it/s] 17%|█▋        | 233/1380 [00:15<01:11, 16.06it/s] 17%|█▋        | 235/1380 [00:15<01:11, 16.07it/s] 17%|█▋        | 237/1380 [00:15<01:11, 16.05it/s] 17%|█▋        | 239/1380 [00:15<01:11, 15.99it/s] 17%|█▋        | 241/1380 [00:15<01:11, 15.91it/s] 18%|█▊        | 243/1380 [00:16<01:11, 15.85it/s] 18%|█▊        | 245/1380 [00:16<01:11, 15.86it/s] 18%|█▊        | 247/1380 [00:16<01:11, 15.85it/s] 18%|█▊        | 249/1380 [00:16<01:10, 15.94it/s] 18%|█▊        | 251/1380 [00:16<01:10, 16.00it/s] 18%|█▊        | 253/1380 [00:16<01:10, 16.05it/s] 18%|█▊        | 255/1380 [00:16<01:09, 16.14it/s] 19%|█▊        | 257/1380 [00:16<01:09, 16.18it/s] 19%|█▉        | 259/1380 [00:17<01:09, 16.20it/s] 19%|█▉        | 261/1380 [00:17<01:09, 16.20it/s] 19%|█▉        | 263/1380 [00:17<01:09, 16.17it/s] 19%|█▉        | 265/1380 [00:17<01:08, 16.18it/s] 19%|█▉        | 267/1380 [00:17<01:08, 16.21it/s] 19%|█▉        | 269/1380 [00:17<01:08, 16.22it/s] 20%|█▉        | 271/1380 [00:17<01:08, 16.18it/s] 20%|█▉        | 273/1380 [00:17<01:08, 16.15it/s] 20%|█▉        | 275/1380 [00:18<01:08, 16.17it/s]                                                   20%|██        | 276/1380 [00:18<01:08, 16.17it/s][INFO|trainer.py:755] 2023-11-15 23:30:09,495 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 23:30:09,497 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 23:30:09,497 >>   Num examples = 2204
[INFO|trainer.py:3134] 2023-11-15 23:30:09,497 >>   Batch size = 8
{'loss': 0.4891, 'learning_rate': 0.0004, 'epoch': 1.0}

  0%|          | 0/276 [00:00<?, ?it/s][A
  4%|▎         | 10/276 [00:00<00:02, 96.32it/s][A
  7%|▋         | 20/276 [00:00<00:02, 90.55it/s][A
 11%|█         | 30/276 [00:00<00:02, 88.87it/s][A
 14%|█▍        | 39/276 [00:00<00:02, 88.20it/s][A
 17%|█▋        | 48/276 [00:00<00:02, 87.78it/s][A
 21%|██        | 57/276 [00:00<00:02, 87.62it/s][A
 24%|██▍       | 66/276 [00:00<00:02, 86.90it/s][A
 27%|██▋       | 75/276 [00:00<00:02, 85.78it/s][A
 30%|███       | 84/276 [00:00<00:02, 86.11it/s][A
 34%|███▎      | 93/276 [00:01<00:02, 85.92it/s][A
 37%|███▋      | 102/276 [00:01<00:02, 84.48it/s][A
 40%|████      | 111/276 [00:01<00:01, 83.66it/s][A
 43%|████▎     | 120/276 [00:01<00:01, 83.01it/s][A
 47%|████▋     | 129/276 [00:01<00:01, 83.65it/s][A
 50%|█████     | 138/276 [00:01<00:01, 83.68it/s][A
 53%|█████▎    | 147/276 [00:01<00:01, 83.61it/s][A
 57%|█████▋    | 156/276 [00:01<00:01, 83.88it/s][A
 60%|█████▉    | 165/276 [00:01<00:01, 83.69it/s][A
 63%|██████▎   | 174/276 [00:02<00:01, 83.98it/s][A
 66%|██████▋   | 183/276 [00:02<00:01, 84.29it/s][A
 70%|██████▉   | 192/276 [00:02<00:01, 83.67it/s][A
 73%|███████▎  | 201/276 [00:02<00:00, 84.07it/s][A
 76%|███████▌  | 210/276 [00:02<00:00, 84.40it/s][A
 79%|███████▉  | 219/276 [00:02<00:00, 84.56it/s][A
 83%|████████▎ | 228/276 [00:02<00:00, 84.15it/s][A
 86%|████████▌ | 237/276 [00:02<00:00, 84.58it/s][A
 89%|████████▉ | 246/276 [00:02<00:00, 84.39it/s][A
 92%|█████████▏| 255/276 [00:02<00:00, 85.14it/s][A
 96%|█████████▌| 264/276 [00:03<00:00, 85.08it/s][A
 99%|█████████▉| 273/276 [00:03<00:00, 85.34it/s][A                                                  
                                                 [A 20%|██        | 276/1380 [00:21<01:08, 16.17it/s]
100%|██████████| 276/276 [00:03<00:00, 85.34it/s][A
                                                 [A 20%|██        | 277/1380 [00:21<10:12,  1.80it/s] 20%|██        | 279/1380 [00:21<07:29,  2.45it/s] 20%|██        | 281/1380 [00:21<05:34,  3.28it/s] 21%|██        | 283/1380 [00:21<04:15,  4.30it/s] 21%|██        | 285/1380 [00:22<03:18,  5.51it/s] 21%|██        | 287/1380 [00:22<02:39,  6.87it/s] 21%|██        | 289/1380 [00:22<02:11,  8.30it/s] 21%|██        | 291/1380 [00:22<01:52,  9.70it/s] 21%|██        | 293/1380 [00:22<01:38, 11.01it/s] 21%|██▏       | 295/1380 [00:22<01:29, 12.18it/s] 22%|██▏       | 297/1380 [00:22<01:22, 13.16it/s] 22%|██▏       | 299/1380 [00:22<01:17, 13.93it/s] 22%|██▏       | 301/1380 [00:23<01:14, 14.51it/s] 22%|██▏       | 303/1380 [00:23<01:12, 14.95it/s] 22%|██▏       | 305/1380 [00:23<01:10, 15.31it/s] 22%|██▏       | 307/1380 [00:23<01:09, 15.54it/s] 22%|██▏       | 309/1380 [00:23<01:08, 15.70it/s] 23%|██▎       | 311/1380 [00:23<01:07, 15.79it/s] 23%|██▎       | 313/1380 [00:23<01:07, 15.82it/s] 23%|██▎       | 315/1380 [00:23<01:07, 15.89it/s] 23%|██▎       | 317/1380 [00:24<01:06, 15.96it/s] 23%|██▎       | 319/1380 [00:24<01:06, 15.99it/s] 23%|██▎       | 321/1380 [00:24<01:06, 16.01it/s] 23%|██▎       | 323/1380 [00:24<01:05, 16.04it/s] 24%|██▎       | 325/1380 [00:24<01:06, 15.95it/s] 24%|██▎       | 327/1380 [00:24<01:06, 15.86it/s] 24%|██▍       | 329/1380 [00:24<01:06, 15.85it/s] 24%|██▍       | 331/1380 [00:24<01:06, 15.81it/s] 24%|██▍       | 333/1380 [00:25<01:06, 15.76it/s] 24%|██▍       | 335/1380 [00:25<01:06, 15.74it/s] 24%|██▍       | 337/1380 [00:25<01:06, 15.78it/s] 25%|██▍       | 339/1380 [00:25<01:06, 15.76it/s] 25%|██▍       | 341/1380 [00:25<01:05, 15.83it/s] 25%|██▍       | 343/1380 [00:25<01:05, 15.83it/s] 25%|██▌       | 345/1380 [00:25<01:05, 15.77it/s] 25%|██▌       | 347/1380 [00:25<01:05, 15.86it/s] 25%|██▌       | 349/1380 [00:26<01:04, 15.91it/s] 25%|██▌       | 351/1380 [00:26<01:05, 15.83it/s] 26%|██▌       | 353/1380 [00:26<01:04, 15.83it/s] 26%|██▌       | 355/1380 [00:26<01:04, 15.89it/s] 26%|██▌       | 357/1380 [00:26<01:04, 15.93it/s] 26%|██▌       | 359/1380 [00:26<01:04, 15.89it/s] 26%|██▌       | 361/1380 [00:26<01:03, 15.94it/s] 26%|██▋       | 363/1380 [00:26<01:03, 15.97it/s] 26%|██▋       | 365/1380 [00:27<01:03, 15.89it/s] 27%|██▋       | 367/1380 [00:27<01:03, 15.83it/s] 27%|██▋       | 369/1380 [00:27<01:04, 15.78it/s] 27%|██▋       | 371/1380 [00:27<01:04, 15.75it/s] 27%|██▋       | 373/1380 [00:27<01:03, 15.75it/s] 27%|██▋       | 375/1380 [00:27<01:03, 15.77it/s] 27%|██▋       | 377/1380 [00:27<01:03, 15.86it/s] 27%|██▋       | 379/1380 [00:27<01:02, 15.93it/s] 28%|██▊       | 381/1380 [00:28<01:02, 15.99it/s] 28%|██▊       | 383/1380 [00:28<01:02, 16.02it/s] 28%|██▊       | 385/1380 [00:28<01:02, 16.01it/s] 28%|██▊       | 387/1380 [00:28<01:01, 16.03it/s] 28%|██▊       | 389/1380 [00:28<01:01, 16.05it/s] 28%|██▊       | 391/1380 [00:28<01:01, 16.06it/s] 28%|██▊       | 393/1380 [00:28<01:01, 16.04it/s] 29%|██▊       | 395/1380 [00:28<01:01, 16.03it/s] 29%|██▉       | 397/1380 [00:29<01:01, 16.08it/s] 29%|██▉       | 399/1380 [00:29<01:00, 16.11it/s] 29%|██▉       | 401/1380 [00:29<01:00, 16.12it/s] 29%|██▉       | 403/1380 [00:29<01:00, 16.10it/s] 29%|██▉       | 405/1380 [00:29<01:00, 16.05it/s] 29%|██▉       | 407/1380 [00:29<01:00, 15.98it/s] 30%|██▉       | 409/1380 [00:29<01:00, 16.00it/s] 30%|██▉       | 411/1380 [00:29<01:00, 16.03it/s] 30%|██▉       | 413/1380 [00:30<01:00, 16.04it/s] 30%|███       | 415/1380 [00:30<01:00, 16.00it/s] 30%|███       | 417/1380 [00:30<01:00, 15.93it/s] 30%|███       | 419/1380 [00:30<01:00, 15.88it/s] 31%|███       | 421/1380 [00:30<01:00, 15.85it/s] 31%|███       | 423/1380 [00:30<01:00, 15.83it/s] 31%|███       | 425/1380 [00:30<01:00, 15.67it/s] 31%|███       | 427/1380 [00:30<01:00, 15.70it/s] 31%|███       | 429/1380 [00:31<01:00, 15.71it/s] 31%|███       | 431/1380 [00:31<01:00, 15.75it/s] 31%|███▏      | 433/1380 [00:31<00:59, 15.82it/s] 32%|███▏      | 435/1380 [00:31<00:59, 15.84it/s] 32%|███▏      | 437/1380 [00:31<00:59, 15.88it/s] 32%|███▏      | 439/1380 [00:31<00:59, 15.88it/s] 32%|███▏      | 441/1380 [00:31<00:59, 15.82it/s] 32%|███▏      | 443/1380 [00:31<00:59, 15.75it/s] 32%|███▏      | 445/1380 [00:32<00:58, 15.85it/s] 32%|███▏      | 447/1380 [00:32<00:58, 15.82it/s] 33%|███▎      | 449/1380 [00:32<00:58, 15.82it/s] 33%|███▎      | 451/1380 [00:32<00:58, 15.79it/s] 33%|███▎      | 453/1380 [00:32<00:58, 15.77it/s] 33%|███▎      | 455/1380 [00:32<00:59, 15.56it/s] 33%|███▎      | 457/1380 [00:32<00:59, 15.40it/s] 33%|███▎      | 459/1380 [00:32<00:59, 15.43it/s] 33%|███▎      | 461/1380 [00:33<00:59, 15.32it/s] 34%|███▎      | 463/1380 [00:33<00:59, 15.46it/s] 34%|███▎      | 465/1380 [00:33<00:58, 15.62it/s] 34%|███▍      | 467/1380 [00:33<00:58, 15.72it/s] 34%|███▍      | 469/1380 [00:33<00:57, 15.84it/s] 34%|███▍      | 471/1380 [00:33<00:57, 15.91it/s] 34%|███▍      | 473/1380 [00:33<00:56, 15.95it/s] 34%|███▍      | 475/1380 [00:33<00:56, 15.94it/s] 35%|███▍      | 477/1380 [00:34<00:56, 15.97it/s] 35%|███▍      | 479/1380 [00:34<00:56, 16.01it/s] 35%|███▍      | 481/1380 [00:34<00:56, 16.02it/s] 35%|███▌      | 483/1380 [00:34<00:56, 16.01it/s] 35%|███▌      | 485/1380 [00:34<00:55, 16.01it/s] 35%|███▌      | 487/1380 [00:34<00:55, 16.06it/s] 35%|███▌      | 489/1380 [00:34<00:55, 16.07it/s] 36%|███▌      | 491/1380 [00:34<00:55, 16.03it/s] 36%|███▌      | 493/1380 [00:35<00:55, 16.02it/s] 36%|███▌      | 495/1380 [00:35<00:55, 16.06it/s] 36%|███▌      | 497/1380 [00:35<00:55, 16.03it/s] 36%|███▌      | 499/1380 [00:35<00:55, 16.02it/s] 36%|███▋      | 501/1380 [00:35<00:55, 15.95it/s] 36%|███▋      | 503/1380 [00:35<00:55, 15.89it/s] 37%|███▋      | 505/1380 [00:35<00:55, 15.84it/s] 37%|███▋      | 507/1380 [00:35<00:55, 15.82it/s] 37%|███▋      | 509/1380 [00:36<00:55, 15.79it/s] 37%|███▋      | 511/1380 [00:36<00:55, 15.72it/s] 37%|███▋      | 513/1380 [00:36<00:55, 15.63it/s] 37%|███▋      | 515/1380 [00:36<00:55, 15.68it/s] 37%|███▋      | 517/1380 [00:36<00:54, 15.69it/s] 38%|███▊      | 519/1380 [00:36<00:54, 15.72it/s] 38%|███▊      | 521/1380 [00:36<00:54, 15.64it/s] 38%|███▊      | 523/1380 [00:37<00:54, 15.66it/s] 38%|███▊      | 525/1380 [00:37<00:54, 15.69it/s] 38%|███▊      | 527/1380 [00:37<00:54, 15.75it/s] 38%|███▊      | 529/1380 [00:37<00:54, 15.76it/s] 38%|███▊      | 531/1380 [00:37<00:53, 15.80it/s] 39%|███▊      | 533/1380 [00:37<00:53, 15.79it/s] 39%|███▉      | 535/1380 [00:37<00:53, 15.78it/s] 39%|███▉      | 537/1380 [00:37<00:53, 15.84it/s] 39%|███▉      | 539/1380 [00:38<00:53, 15.72it/s] 39%|███▉      | 541/1380 [00:38<00:53, 15.57it/s] 39%|███▉      | 543/1380 [00:38<00:53, 15.60it/s] 39%|███▉      | 545/1380 [00:38<00:53, 15.64it/s] 40%|███▉      | 547/1380 [00:38<00:53, 15.69it/s] 40%|███▉      | 549/1380 [00:38<00:52, 15.76it/s] 40%|███▉      | 551/1380 [00:38<00:52, 15.88it/s]                                                   40%|████      | 552/1380 [00:38<00:52, 15.88it/s][INFO|trainer.py:755] 2023-11-15 23:30:30,169 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 23:30:30,171 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 23:30:30,171 >>   Num examples = 2204
[INFO|trainer.py:3134] 2023-11-15 23:30:30,172 >>   Batch size = 8
{'eval_loss': 0.3621892035007477, 'eval_accuracy': 0.8593466424682396, 'eval_micro_f1': 0.8593466424682396, 'eval_macro_f1': 0.8415235503223989, 'eval_runtime': 3.3033, 'eval_samples_per_second': 667.202, 'eval_steps_per_second': 83.552, 'epoch': 1.0}
{'loss': 0.3644, 'learning_rate': 0.0003, 'epoch': 2.0}

  0%|          | 0/276 [00:00<?, ?it/s][A
  4%|▎         | 10/276 [00:00<00:02, 97.52it/s][A
  7%|▋         | 20/276 [00:00<00:02, 90.68it/s][A
 11%|█         | 30/276 [00:00<00:02, 89.73it/s][A
 14%|█▍        | 39/276 [00:00<00:02, 88.89it/s][A
 17%|█▋        | 48/276 [00:00<00:02, 88.45it/s][A
 21%|██        | 57/276 [00:00<00:02, 87.67it/s][A
 24%|██▍       | 66/276 [00:00<00:02, 87.89it/s][A
 27%|██▋       | 75/276 [00:00<00:02, 87.61it/s][A
 30%|███       | 84/276 [00:00<00:02, 87.73it/s][A
 34%|███▎      | 93/276 [00:01<00:02, 87.95it/s][A
 37%|███▋      | 102/276 [00:01<00:01, 87.86it/s][A
 40%|████      | 111/276 [00:01<00:01, 87.16it/s][A
 43%|████▎     | 120/276 [00:01<00:01, 87.52it/s][A
 47%|████▋     | 129/276 [00:01<00:01, 87.79it/s][A
 50%|█████     | 138/276 [00:01<00:01, 87.47it/s][A
 53%|█████▎    | 147/276 [00:01<00:01, 87.75it/s][A
 57%|█████▋    | 156/276 [00:01<00:01, 87.86it/s][A
 60%|█████▉    | 165/276 [00:01<00:01, 87.96it/s][A
 63%|██████▎   | 174/276 [00:01<00:01, 87.86it/s][A
 66%|██████▋   | 183/276 [00:02<00:01, 87.57it/s][A
 70%|██████▉   | 192/276 [00:02<00:00, 87.69it/s][A
 73%|███████▎  | 201/276 [00:02<00:00, 87.03it/s][A
 76%|███████▌  | 210/276 [00:02<00:00, 86.13it/s][A
 79%|███████▉  | 219/276 [00:02<00:00, 85.62it/s][A
 83%|████████▎ | 228/276 [00:02<00:00, 85.70it/s][A
 86%|████████▌ | 237/276 [00:02<00:00, 85.18it/s][A
 89%|████████▉ | 246/276 [00:02<00:00, 84.16it/s][A
 92%|█████████▏| 255/276 [00:02<00:00, 84.45it/s][A
 96%|█████████▌| 264/276 [00:03<00:00, 84.78it/s][A
 99%|█████████▉| 273/276 [00:03<00:00, 85.20it/s][A                                                  
                                                 [A 40%|████      | 552/1380 [00:42<00:52, 15.88it/s]
100%|██████████| 276/276 [00:03<00:00, 85.20it/s][A
                                                 [A 40%|████      | 553/1380 [00:42<07:30,  1.84it/s] 40%|████      | 555/1380 [00:42<05:30,  2.50it/s] 40%|████      | 557/1380 [00:42<04:06,  3.34it/s] 41%|████      | 559/1380 [00:42<03:07,  4.38it/s] 41%|████      | 561/1380 [00:42<02:26,  5.59it/s] 41%|████      | 563/1380 [00:42<01:57,  6.94it/s] 41%|████      | 565/1380 [00:42<01:37,  8.34it/s] 41%|████      | 567/1380 [00:42<01:23,  9.72it/s] 41%|████      | 569/1380 [00:43<01:13, 11.02it/s] 41%|████▏     | 571/1380 [00:43<01:06, 12.12it/s] 42%|████▏     | 573/1380 [00:43<01:01, 13.04it/s] 42%|████▏     | 575/1380 [00:43<00:58, 13.75it/s] 42%|████▏     | 577/1380 [00:43<00:56, 14.24it/s] 42%|████▏     | 579/1380 [00:43<00:54, 14.67it/s] 42%|████▏     | 581/1380 [00:43<00:53, 14.96it/s] 42%|████▏     | 583/1380 [00:44<00:52, 15.17it/s] 42%|████▏     | 585/1380 [00:44<00:51, 15.39it/s] 43%|████▎     | 587/1380 [00:44<00:50, 15.59it/s] 43%|████▎     | 589/1380 [00:44<00:50, 15.67it/s] 43%|████▎     | 591/1380 [00:44<00:50, 15.70it/s] 43%|████▎     | 593/1380 [00:44<00:49, 15.79it/s] 43%|████▎     | 595/1380 [00:44<00:49, 15.82it/s] 43%|████▎     | 597/1380 [00:44<00:49, 15.88it/s] 43%|████▎     | 599/1380 [00:45<00:48, 15.97it/s] 44%|████▎     | 601/1380 [00:45<00:48, 16.02it/s] 44%|████▎     | 603/1380 [00:45<00:48, 16.04it/s] 44%|████▍     | 605/1380 [00:45<00:48, 16.04it/s] 44%|████▍     | 607/1380 [00:45<00:48, 16.04it/s] 44%|████▍     | 609/1380 [00:45<00:47, 16.08it/s] 44%|████▍     | 611/1380 [00:45<00:47, 16.09it/s] 44%|████▍     | 613/1380 [00:45<00:47, 16.07it/s] 45%|████▍     | 615/1380 [00:46<00:47, 16.06it/s] 45%|████▍     | 617/1380 [00:46<00:47, 16.09it/s] 45%|████▍     | 619/1380 [00:46<00:47, 16.10it/s] 45%|████▌     | 621/1380 [00:46<00:47, 16.10it/s] 45%|████▌     | 623/1380 [00:46<00:47, 16.07it/s] 45%|████▌     | 625/1380 [00:46<00:47, 16.05it/s] 45%|████▌     | 627/1380 [00:46<00:47, 16.02it/s] 46%|████▌     | 629/1380 [00:46<00:47, 15.92it/s] 46%|████▌     | 631/1380 [00:47<00:47, 15.86it/s] 46%|████▌     | 633/1380 [00:47<00:47, 15.82it/s] 46%|████▌     | 635/1380 [00:47<00:47, 15.79it/s] 46%|████▌     | 637/1380 [00:47<00:47, 15.76it/s] 46%|████▋     | 639/1380 [00:47<00:46, 15.83it/s] 46%|████▋     | 641/1380 [00:47<00:46, 15.80it/s] 47%|████▋     | 643/1380 [00:47<00:46, 15.80it/s] 47%|████▋     | 645/1380 [00:47<00:46, 15.89it/s] 47%|████▋     | 647/1380 [00:48<00:46, 15.92it/s] 47%|████▋     | 649/1380 [00:48<00:45, 15.94it/s] 47%|████▋     | 651/1380 [00:48<00:45, 15.96it/s] 47%|████▋     | 653/1380 [00:48<00:45, 15.93it/s] 47%|████▋     | 655/1380 [00:48<00:45, 15.92it/s] 48%|████▊     | 657/1380 [00:48<00:45, 15.92it/s] 48%|████▊     | 659/1380 [00:48<00:45, 15.96it/s] 48%|████▊     | 661/1380 [00:48<00:44, 15.98it/s] 48%|████▊     | 663/1380 [00:49<00:44, 15.95it/s] 48%|████▊     | 665/1380 [00:49<00:45, 15.86it/s] 48%|████▊     | 667/1380 [00:49<00:45, 15.79it/s] 48%|████▊     | 669/1380 [00:49<00:45, 15.61it/s] 49%|████▊     | 671/1380 [00:49<00:45, 15.56it/s] 49%|████▉     | 673/1380 [00:49<00:45, 15.67it/s] 49%|████▉     | 675/1380 [00:49<00:44, 15.79it/s] 49%|████▉     | 677/1380 [00:49<00:44, 15.90it/s] 49%|████▉     | 679/1380 [00:50<00:43, 15.97it/s] 49%|████▉     | 681/1380 [00:50<00:43, 15.99it/s] 49%|████▉     | 683/1380 [00:50<00:43, 15.97it/s] 50%|████▉     | 685/1380 [00:50<00:43, 16.01it/s] 50%|████▉     | 687/1380 [00:50<00:43, 16.05it/s] 50%|████▉     | 689/1380 [00:50<00:43, 16.06it/s] 50%|█████     | 691/1380 [00:50<00:42, 16.05it/s] 50%|█████     | 693/1380 [00:50<00:42, 16.06it/s] 50%|█████     | 695/1380 [00:51<00:42, 16.09it/s] 51%|█████     | 697/1380 [00:51<00:42, 16.10it/s] 51%|█████     | 699/1380 [00:51<00:42, 16.07it/s] 51%|█████     | 701/1380 [00:51<00:42, 16.06it/s] 51%|█████     | 703/1380 [00:51<00:42, 16.06it/s] 51%|█████     | 705/1380 [00:51<00:42, 16.04it/s] 51%|█████     | 707/1380 [00:51<00:42, 15.96it/s] 51%|█████▏    | 709/1380 [00:51<00:42, 15.96it/s] 52%|█████▏    | 711/1380 [00:52<00:41, 16.01it/s] 52%|█████▏    | 713/1380 [00:52<00:41, 15.99it/s] 52%|█████▏    | 715/1380 [00:52<00:41, 15.88it/s] 52%|█████▏    | 717/1380 [00:52<00:41, 15.84it/s] 52%|█████▏    | 719/1380 [00:52<00:41, 15.78it/s] 52%|█████▏    | 721/1380 [00:52<00:42, 15.68it/s] 52%|█████▏    | 723/1380 [00:52<00:42, 15.60it/s] 53%|█████▎    | 725/1380 [00:52<00:42, 15.59it/s] 53%|█████▎    | 727/1380 [00:53<00:41, 15.61it/s] 53%|█████▎    | 729/1380 [00:53<00:41, 15.64it/s] 53%|█████▎    | 731/1380 [00:53<00:41, 15.74it/s] 53%|█████▎    | 733/1380 [00:53<00:40, 15.81it/s] 53%|█████▎    | 735/1380 [00:53<00:40, 15.80it/s] 53%|█████▎    | 737/1380 [00:53<00:40, 15.83it/s] 54%|█████▎    | 739/1380 [00:53<00:40, 15.73it/s] 54%|█████▎    | 741/1380 [00:53<00:40, 15.70it/s] 54%|█████▍    | 743/1380 [00:54<00:40, 15.77it/s] 54%|█████▍    | 745/1380 [00:54<00:40, 15.80it/s] 54%|█████▍    | 747/1380 [00:54<00:39, 15.86it/s] 54%|█████▍    | 749/1380 [00:54<00:39, 15.87it/s] 54%|█████▍    | 751/1380 [00:54<00:39, 15.81it/s] 55%|█████▍    | 753/1380 [00:54<00:40, 15.53it/s] 55%|█████▍    | 755/1380 [00:54<00:40, 15.43it/s] 55%|█████▍    | 757/1380 [00:54<00:40, 15.51it/s] 55%|█████▌    | 759/1380 [00:55<00:39, 15.60it/s] 55%|█████▌    | 761/1380 [00:55<00:39, 15.63it/s] 55%|█████▌    | 763/1380 [00:55<00:39, 15.75it/s] 55%|█████▌    | 765/1380 [00:55<00:38, 15.84it/s] 56%|█████▌    | 767/1380 [00:55<00:38, 15.93it/s] 56%|█████▌    | 769/1380 [00:55<00:38, 15.92it/s] 56%|█████▌    | 771/1380 [00:55<00:38, 15.95it/s] 56%|█████▌    | 773/1380 [00:55<00:37, 15.99it/s] 56%|█████▌    | 775/1380 [00:56<00:37, 16.03it/s] 56%|█████▋    | 777/1380 [00:56<00:37, 16.04it/s] 56%|█████▋    | 779/1380 [00:56<00:37, 16.04it/s] 57%|█████▋    | 781/1380 [00:56<00:37, 16.04it/s] 57%|█████▋    | 783/1380 [00:56<00:37, 16.07it/s] 57%|█████▋    | 785/1380 [00:56<00:37, 16.08it/s] 57%|█████▋    | 787/1380 [00:56<00:36, 16.07it/s] 57%|█████▋    | 789/1380 [00:56<00:36, 16.04it/s] 57%|█████▋    | 791/1380 [00:57<00:36, 16.04it/s] 57%|█████▋    | 793/1380 [00:57<00:36, 16.03it/s] 58%|█████▊    | 795/1380 [00:57<00:36, 16.01it/s] 58%|█████▊    | 797/1380 [00:57<00:36, 15.95it/s] 58%|█████▊    | 799/1380 [00:57<00:36, 15.93it/s] 58%|█████▊    | 801/1380 [00:57<00:36, 15.85it/s] 58%|█████▊    | 803/1380 [00:57<00:36, 15.80it/s] 58%|█████▊    | 805/1380 [00:57<00:36, 15.79it/s] 58%|█████▊    | 807/1380 [00:58<00:36, 15.79it/s] 59%|█████▊    | 809/1380 [00:58<00:36, 15.74it/s] 59%|█████▉    | 811/1380 [00:58<00:36, 15.73it/s] 59%|█████▉    | 813/1380 [00:58<00:36, 15.73it/s] 59%|█████▉    | 815/1380 [00:58<00:35, 15.77it/s] 59%|█████▉    | 817/1380 [00:58<00:35, 15.79it/s] 59%|█████▉    | 819/1380 [00:58<00:35, 15.77it/s] 59%|█████▉    | 821/1380 [00:58<00:35, 15.78it/s] 60%|█████▉    | 823/1380 [00:59<00:35, 15.79it/s] 60%|█████▉    | 825/1380 [00:59<00:35, 15.80it/s] 60%|█████▉    | 827/1380 [00:59<00:34, 15.85it/s]                                                   60%|██████    | 828/1380 [00:59<00:34, 15.85it/s][INFO|trainer.py:755] 2023-11-15 23:30:50,764 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 23:30:50,765 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 23:30:50,766 >>   Num examples = 2204
[INFO|trainer.py:3134] 2023-11-15 23:30:50,766 >>   Batch size = 8
{'eval_loss': 0.37184566259384155, 'eval_accuracy': 0.8593466424682396, 'eval_micro_f1': 0.8593466424682396, 'eval_macro_f1': 0.8399494239419214, 'eval_runtime': 3.2291, 'eval_samples_per_second': 682.553, 'eval_steps_per_second': 85.474, 'epoch': 2.0}
{'loss': 0.332, 'learning_rate': 0.0002, 'epoch': 3.0}

  0%|          | 0/276 [00:00<?, ?it/s][A
  4%|▎         | 10/276 [00:00<00:02, 94.93it/s][A
  7%|▋         | 20/276 [00:00<00:02, 89.85it/s][A
 11%|█         | 30/276 [00:00<00:02, 88.22it/s][A
 14%|█▍        | 39/276 [00:00<00:02, 87.35it/s][A
 17%|█▋        | 48/276 [00:00<00:02, 86.52it/s][A
 21%|██        | 57/276 [00:00<00:02, 85.84it/s][A
 24%|██▍       | 66/276 [00:00<00:02, 83.00it/s][A
 27%|██▋       | 75/276 [00:00<00:02, 80.56it/s][A
 30%|███       | 84/276 [00:01<00:02, 79.79it/s][A
 33%|███▎      | 92/276 [00:01<00:02, 78.76it/s][A
 36%|███▌      | 100/276 [00:01<00:02, 78.18it/s][A
 39%|███▉      | 109/276 [00:01<00:02, 79.65it/s][A
 43%|████▎     | 118/276 [00:01<00:01, 81.92it/s][A
 46%|████▌     | 127/276 [00:01<00:01, 83.59it/s][A
 49%|████▉     | 136/276 [00:01<00:01, 84.97it/s][A
 53%|█████▎    | 145/276 [00:01<00:01, 86.16it/s][A
 56%|█████▌    | 154/276 [00:01<00:01, 87.05it/s][A
 59%|█████▉    | 163/276 [00:01<00:01, 87.25it/s][A
 62%|██████▏   | 172/276 [00:02<00:01, 87.69it/s][A
 66%|██████▌   | 181/276 [00:02<00:01, 86.98it/s][A
 69%|██████▉   | 190/276 [00:02<00:00, 86.67it/s][A
 72%|███████▏  | 199/276 [00:02<00:00, 87.02it/s][A
 75%|███████▌  | 208/276 [00:02<00:00, 87.47it/s][A
 79%|███████▊  | 217/276 [00:02<00:00, 87.63it/s][A
 82%|████████▏ | 226/276 [00:02<00:00, 87.80it/s][A
 85%|████████▌ | 235/276 [00:02<00:00, 87.68it/s][A
 88%|████████▊ | 244/276 [00:02<00:00, 87.48it/s][A
 92%|█████████▏| 253/276 [00:02<00:00, 87.88it/s][A
 95%|█████████▍| 262/276 [00:03<00:00, 88.11it/s][A
 98%|█████████▊| 271/276 [00:03<00:00, 87.71it/s][A                                                  
                                                 [A 60%|██████    | 828/1380 [01:02<00:34, 15.85it/s]
100%|██████████| 276/276 [00:03<00:00, 87.71it/s][A
                                                 [A 60%|██████    | 829/1380 [01:02<05:03,  1.81it/s] 60%|██████    | 831/1380 [01:02<03:42,  2.47it/s] 60%|██████    | 833/1380 [01:03<02:45,  3.31it/s] 61%|██████    | 835/1380 [01:03<02:05,  4.34it/s] 61%|██████    | 837/1380 [01:03<01:37,  5.55it/s] 61%|██████    | 839/1380 [01:03<01:18,  6.88it/s] 61%|██████    | 841/1380 [01:03<01:05,  8.28it/s] 61%|██████    | 843/1380 [01:03<00:55,  9.65it/s] 61%|██████    | 845/1380 [01:03<00:49, 10.89it/s] 61%|██████▏   | 847/1380 [01:03<00:44, 12.01it/s] 62%|██████▏   | 849/1380 [01:04<00:41, 12.94it/s] 62%|██████▏   | 851/1380 [01:04<00:38, 13.70it/s] 62%|██████▏   | 853/1380 [01:04<00:36, 14.32it/s] 62%|██████▏   | 855/1380 [01:04<00:35, 14.72it/s] 62%|██████▏   | 857/1380 [01:04<00:34, 15.00it/s] 62%|██████▏   | 859/1380 [01:04<00:34, 15.30it/s] 62%|██████▏   | 861/1380 [01:04<00:33, 15.45it/s] 63%|██████▎   | 863/1380 [01:04<00:33, 15.58it/s] 63%|██████▎   | 865/1380 [01:05<00:32, 15.62it/s] 63%|██████▎   | 867/1380 [01:05<00:32, 15.68it/s] 63%|██████▎   | 869/1380 [01:05<00:32, 15.71it/s] 63%|██████▎   | 871/1380 [01:05<00:32, 15.74it/s] 63%|██████▎   | 873/1380 [01:05<00:32, 15.82it/s] 63%|██████▎   | 875/1380 [01:05<00:31, 15.81it/s] 64%|██████▎   | 877/1380 [01:05<00:31, 15.78it/s] 64%|██████▎   | 879/1380 [01:05<00:31, 15.73it/s] 64%|██████▍   | 881/1380 [01:06<00:31, 15.70it/s] 64%|██████▍   | 883/1380 [01:06<00:31, 15.73it/s] 64%|██████▍   | 885/1380 [01:06<00:31, 15.71it/s] 64%|██████▍   | 887/1380 [01:06<00:31, 15.77it/s] 64%|██████▍   | 889/1380 [01:06<00:30, 15.86it/s] 65%|██████▍   | 891/1380 [01:06<00:30, 15.93it/s] 65%|██████▍   | 893/1380 [01:06<00:30, 15.98it/s] 65%|██████▍   | 895/1380 [01:06<00:30, 15.96it/s] 65%|██████▌   | 897/1380 [01:07<00:30, 15.98it/s] 65%|██████▌   | 899/1380 [01:07<00:30, 16.02it/s] 65%|██████▌   | 901/1380 [01:07<00:29, 15.99it/s] 65%|██████▌   | 903/1380 [01:07<00:29, 16.00it/s] 66%|██████▌   | 905/1380 [01:07<00:29, 16.00it/s] 66%|██████▌   | 907/1380 [01:07<00:29, 16.04it/s] 66%|██████▌   | 909/1380 [01:07<00:29, 16.03it/s] 66%|██████▌   | 911/1380 [01:07<00:29, 16.01it/s] 66%|██████▌   | 913/1380 [01:08<00:29, 16.01it/s] 66%|██████▋   | 915/1380 [01:08<00:28, 16.05it/s] 66%|██████▋   | 917/1380 [01:08<00:28, 16.04it/s] 67%|██████▋   | 919/1380 [01:08<00:28, 16.03it/s] 67%|██████▋   | 921/1380 [01:08<00:28, 16.02it/s] 67%|██████▋   | 923/1380 [01:08<00:28, 16.01it/s] 67%|██████▋   | 925/1380 [01:08<00:28, 15.94it/s] 67%|██████▋   | 927/1380 [01:08<00:28, 15.82it/s] 67%|██████▋   | 929/1380 [01:09<00:28, 15.80it/s] 67%|██████▋   | 931/1380 [01:09<00:28, 15.72it/s] 68%|██████▊   | 933/1380 [01:09<00:28, 15.69it/s] 68%|██████▊   | 935/1380 [01:09<00:28, 15.72it/s] 68%|██████▊   | 937/1380 [01:09<00:28, 15.69it/s] 68%|██████▊   | 939/1380 [01:09<00:28, 15.75it/s] 68%|██████▊   | 941/1380 [01:09<00:27, 15.79it/s] 68%|██████▊   | 943/1380 [01:09<00:27, 15.77it/s] 68%|██████▊   | 945/1380 [01:10<00:27, 15.72it/s] 69%|██████▊   | 947/1380 [01:10<00:27, 15.71it/s] 69%|██████▉   | 949/1380 [01:10<00:27, 15.64it/s] 69%|██████▉   | 951/1380 [01:10<00:27, 15.72it/s] 69%|██████▉   | 953/1380 [01:10<00:27, 15.70it/s] 69%|██████▉   | 955/1380 [01:10<00:27, 15.69it/s] 69%|██████▉   | 957/1380 [01:10<00:27, 15.64it/s] 69%|██████▉   | 959/1380 [01:10<00:27, 15.46it/s] 70%|██████▉   | 961/1380 [01:11<00:27, 15.50it/s] 70%|██████▉   | 963/1380 [01:11<00:26, 15.55it/s] 70%|██████▉   | 965/1380 [01:11<00:26, 15.41it/s] 70%|███████   | 967/1380 [01:11<00:27, 15.26it/s] 70%|███████   | 969/1380 [01:11<00:26, 15.31it/s] 70%|███████   | 971/1380 [01:11<00:26, 15.32it/s] 71%|███████   | 973/1380 [01:11<00:26, 15.27it/s] 71%|███████   | 975/1380 [01:12<00:26, 15.45it/s] 71%|███████   | 977/1380 [01:12<00:25, 15.62it/s] 71%|███████   | 979/1380 [01:12<00:25, 15.73it/s] 71%|███████   | 981/1380 [01:12<00:25, 15.79it/s] 71%|███████   | 983/1380 [01:12<00:24, 15.88it/s] 71%|███████▏  | 985/1380 [01:12<00:24, 15.93it/s] 72%|███████▏  | 987/1380 [01:12<00:24, 15.94it/s] 72%|███████▏  | 989/1380 [01:12<00:24, 15.94it/s] 72%|███████▏  | 991/1380 [01:13<00:24, 15.99it/s] 72%|███████▏  | 993/1380 [01:13<00:24, 15.99it/s] 72%|███████▏  | 995/1380 [01:13<00:24, 15.99it/s] 72%|███████▏  | 997/1380 [01:13<00:23, 16.01it/s] 72%|███████▏  | 999/1380 [01:13<00:23, 16.03it/s] 73%|███████▎  | 1001/1380 [01:13<00:23, 16.03it/s] 73%|███████▎  | 1003/1380 [01:13<00:23, 16.01it/s] 73%|███████▎  | 1005/1380 [01:13<00:23, 16.00it/s] 73%|███████▎  | 1007/1380 [01:14<00:23, 16.03it/s] 73%|███████▎  | 1009/1380 [01:14<00:23, 16.04it/s] 73%|███████▎  | 1011/1380 [01:14<00:23, 15.98it/s] 73%|███████▎  | 1013/1380 [01:14<00:22, 15.97it/s] 74%|███████▎  | 1015/1380 [01:14<00:22, 15.93it/s] 74%|███████▎  | 1017/1380 [01:14<00:22, 15.83it/s] 74%|███████▍  | 1019/1380 [01:14<00:22, 15.78it/s] 74%|███████▍  | 1021/1380 [01:14<00:22, 15.72it/s] 74%|███████▍  | 1023/1380 [01:15<00:22, 15.72it/s] 74%|███████▍  | 1025/1380 [01:15<00:22, 15.78it/s] 74%|███████▍  | 1027/1380 [01:15<00:22, 15.80it/s] 75%|███████▍  | 1029/1380 [01:15<00:22, 15.83it/s] 75%|███████▍  | 1031/1380 [01:15<00:22, 15.86it/s] 75%|███████▍  | 1033/1380 [01:15<00:21, 15.83it/s] 75%|███████▌  | 1035/1380 [01:15<00:21, 15.82it/s] 75%|███████▌  | 1037/1380 [01:15<00:21, 15.84it/s] 75%|███████▌  | 1039/1380 [01:16<00:21, 15.90it/s] 75%|███████▌  | 1041/1380 [01:16<00:21, 15.86it/s] 76%|███████▌  | 1043/1380 [01:16<00:21, 15.88it/s] 76%|███████▌  | 1045/1380 [01:16<00:21, 15.88it/s] 76%|███████▌  | 1047/1380 [01:16<00:21, 15.84it/s] 76%|███████▌  | 1049/1380 [01:16<00:20, 15.83it/s] 76%|███████▌  | 1051/1380 [01:16<00:20, 15.82it/s] 76%|███████▋  | 1053/1380 [01:16<00:20, 15.79it/s] 76%|███████▋  | 1055/1380 [01:17<00:20, 15.62it/s] 77%|███████▋  | 1057/1380 [01:17<00:20, 15.41it/s] 77%|███████▋  | 1059/1380 [01:17<00:20, 15.41it/s] 77%|███████▋  | 1061/1380 [01:17<00:20, 15.44it/s] 77%|███████▋  | 1063/1380 [01:17<00:20, 15.50it/s] 77%|███████▋  | 1065/1380 [01:17<00:20, 15.62it/s] 77%|███████▋  | 1067/1380 [01:17<00:19, 15.71it/s] 77%|███████▋  | 1069/1380 [01:17<00:19, 15.80it/s] 78%|███████▊  | 1071/1380 [01:18<00:19, 15.89it/s] 78%|███████▊  | 1073/1380 [01:18<00:19, 15.94it/s] 78%|███████▊  | 1075/1380 [01:18<00:19, 15.94it/s] 78%|███████▊  | 1077/1380 [01:18<00:19, 15.94it/s] 78%|███████▊  | 1079/1380 [01:18<00:18, 15.98it/s] 78%|███████▊  | 1081/1380 [01:18<00:18, 16.00it/s] 78%|███████▊  | 1083/1380 [01:18<00:18, 16.00it/s] 79%|███████▊  | 1085/1380 [01:18<00:18, 15.97it/s] 79%|███████▉  | 1087/1380 [01:19<00:18, 16.01it/s] 79%|███████▉  | 1089/1380 [01:19<00:18, 16.02it/s] 79%|███████▉  | 1091/1380 [01:19<00:18, 16.00it/s] 79%|███████▉  | 1093/1380 [01:19<00:17, 15.99it/s] 79%|███████▉  | 1095/1380 [01:19<00:17, 16.03it/s] 79%|███████▉  | 1097/1380 [01:19<00:17, 16.04it/s] 80%|███████▉  | 1099/1380 [01:19<00:17, 16.02it/s] 80%|███████▉  | 1101/1380 [01:19<00:17, 16.00it/s] 80%|███████▉  | 1103/1380 [01:20<00:17, 16.03it/s]                                                    80%|████████  | 1104/1380 [01:20<00:17, 16.03it/s][INFO|trainer.py:755] 2023-11-15 23:31:11,460 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 23:31:11,461 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 23:31:11,461 >>   Num examples = 2204
[INFO|trainer.py:3134] 2023-11-15 23:31:11,462 >>   Batch size = 8
{'eval_loss': 0.36115437746047974, 'eval_accuracy': 0.867513611615245, 'eval_micro_f1': 0.867513611615245, 'eval_macro_f1': 0.8512657431164401, 'eval_runtime': 3.2784, 'eval_samples_per_second': 672.286, 'eval_steps_per_second': 84.188, 'epoch': 3.0}
{'loss': 0.2957, 'learning_rate': 0.0001, 'epoch': 4.0}

  0%|          | 0/276 [00:00<?, ?it/s][A
  4%|▎         | 10/276 [00:00<00:02, 95.61it/s][A
  7%|▋         | 20/276 [00:00<00:02, 89.89it/s][A
 11%|█         | 30/276 [00:00<00:02, 86.69it/s][A
 14%|█▍        | 39/276 [00:00<00:02, 84.70it/s][A
 17%|█▋        | 48/276 [00:00<00:02, 85.70it/s][A
 21%|██        | 57/276 [00:00<00:02, 84.90it/s][A
 24%|██▍       | 66/276 [00:00<00:02, 85.37it/s][A
 27%|██▋       | 75/276 [00:00<00:02, 85.29it/s][A
 30%|███       | 84/276 [00:00<00:02, 85.31it/s][A
 34%|███▎      | 93/276 [00:01<00:02, 85.56it/s][A
 37%|███▋      | 102/276 [00:01<00:02, 86.15it/s][A
 40%|████      | 111/276 [00:01<00:01, 86.01it/s][A
 43%|████▎     | 120/276 [00:01<00:01, 85.79it/s][A
 47%|████▋     | 129/276 [00:01<00:01, 86.13it/s][A
 50%|█████     | 138/276 [00:01<00:01, 86.01it/s][A
 53%|█████▎    | 147/276 [00:01<00:01, 86.32it/s][A
 57%|█████▋    | 156/276 [00:01<00:01, 86.12it/s][A
 60%|█████▉    | 165/276 [00:01<00:01, 86.28it/s][A
 63%|██████▎   | 174/276 [00:02<00:01, 86.21it/s][A
 66%|██████▋   | 183/276 [00:02<00:01, 85.90it/s][A
 70%|██████▉   | 192/276 [00:02<00:00, 85.79it/s][A
 73%|███████▎  | 201/276 [00:02<00:00, 85.45it/s][A
 76%|███████▌  | 210/276 [00:02<00:00, 85.28it/s][A
 79%|███████▉  | 219/276 [00:02<00:00, 81.67it/s][A
 83%|████████▎ | 228/276 [00:02<00:00, 80.88it/s][A
 86%|████████▌ | 237/276 [00:02<00:00, 79.02it/s][A
 89%|████████▉ | 245/276 [00:02<00:00, 77.86it/s][A
 92%|█████████▏| 254/276 [00:03<00:00, 80.73it/s][A
 95%|█████████▌| 263/276 [00:03<00:00, 82.98it/s][A
 99%|█████████▊| 272/276 [00:03<00:00, 83.41it/s][A                                                   
                                                 [A 80%|████████  | 1104/1380 [01:23<00:17, 16.03it/s]
100%|██████████| 276/276 [00:03<00:00, 83.41it/s][A
                                                 [A 80%|████████  | 1105/1380 [01:23<02:33,  1.80it/s] 80%|████████  | 1107/1380 [01:23<01:51,  2.45it/s] 80%|████████  | 1109/1380 [01:23<01:22,  3.28it/s] 81%|████████  | 1111/1380 [01:23<01:02,  4.31it/s] 81%|████████  | 1113/1380 [01:23<00:48,  5.52it/s] 81%|████████  | 1115/1380 [01:24<00:38,  6.86it/s] 81%|████████  | 1117/1380 [01:24<00:31,  8.28it/s] 81%|████████  | 1119/1380 [01:24<00:26,  9.68it/s] 81%|████████  | 1121/1380 [01:24<00:23, 10.98it/s] 81%|████████▏ | 1123/1380 [01:24<00:21, 12.12it/s] 82%|████████▏ | 1125/1380 [01:24<00:19, 13.07it/s] 82%|████████▏ | 1127/1380 [01:24<00:18, 13.85it/s] 82%|████████▏ | 1129/1380 [01:24<00:17, 14.44it/s] 82%|████████▏ | 1131/1380 [01:25<00:16, 14.83it/s] 82%|████████▏ | 1133/1380 [01:25<00:16, 15.15it/s] 82%|████████▏ | 1135/1380 [01:25<00:15, 15.42it/s] 82%|████████▏ | 1137/1380 [01:25<00:15, 15.60it/s] 83%|████████▎ | 1139/1380 [01:25<00:15, 15.62it/s] 83%|████████▎ | 1141/1380 [01:25<00:15, 15.62it/s] 83%|████████▎ | 1143/1380 [01:25<00:15, 15.61it/s] 83%|████████▎ | 1145/1380 [01:26<00:15, 15.58it/s] 83%|████████▎ | 1147/1380 [01:26<00:14, 15.62it/s] 83%|████████▎ | 1149/1380 [01:26<00:14, 15.61it/s] 83%|████████▎ | 1151/1380 [01:26<00:14, 15.64it/s] 84%|████████▎ | 1153/1380 [01:26<00:14, 15.69it/s] 84%|████████▎ | 1155/1380 [01:26<00:14, 15.74it/s] 84%|████████▍ | 1157/1380 [01:26<00:14, 15.77it/s] 84%|████████▍ | 1159/1380 [01:26<00:14, 15.78it/s] 84%|████████▍ | 1161/1380 [01:27<00:13, 15.79it/s] 84%|████████▍ | 1163/1380 [01:27<00:13, 15.82it/s] 84%|████████▍ | 1165/1380 [01:27<00:13, 15.84it/s] 85%|████████▍ | 1167/1380 [01:27<00:13, 15.85it/s] 85%|████████▍ | 1169/1380 [01:27<00:13, 15.78it/s] 85%|████████▍ | 1171/1380 [01:27<00:13, 15.80it/s] 85%|████████▌ | 1173/1380 [01:27<00:13, 15.79it/s] 85%|████████▌ | 1175/1380 [01:27<00:12, 15.83it/s] 85%|████████▌ | 1177/1380 [01:28<00:12, 15.80it/s] 85%|████████▌ | 1179/1380 [01:28<00:12, 15.73it/s] 86%|████████▌ | 1181/1380 [01:28<00:12, 15.71it/s] 86%|████████▌ | 1183/1380 [01:28<00:12, 15.71it/s] 86%|████████▌ | 1185/1380 [01:28<00:12, 15.69it/s] 86%|████████▌ | 1187/1380 [01:28<00:12, 15.64it/s] 86%|████████▌ | 1189/1380 [01:28<00:12, 15.72it/s] 86%|████████▋ | 1191/1380 [01:28<00:11, 15.78it/s] 86%|████████▋ | 1193/1380 [01:29<00:11, 15.88it/s] 87%|████████▋ | 1195/1380 [01:29<00:11, 15.94it/s] 87%|████████▋ | 1197/1380 [01:29<00:11, 15.93it/s] 87%|████████▋ | 1199/1380 [01:29<00:11, 15.94it/s] 87%|████████▋ | 1201/1380 [01:29<00:11, 15.95it/s] 87%|████████▋ | 1203/1380 [01:29<00:11, 15.98it/s] 87%|████████▋ | 1205/1380 [01:29<00:10, 15.97it/s] 87%|████████▋ | 1207/1380 [01:29<00:10, 15.98it/s] 88%|████████▊ | 1209/1380 [01:30<00:10, 15.96it/s] 88%|████████▊ | 1211/1380 [01:30<00:10, 15.98it/s] 88%|████████▊ | 1213/1380 [01:30<00:10, 15.96it/s] 88%|████████▊ | 1215/1380 [01:30<00:10, 15.96it/s] 88%|████████▊ | 1217/1380 [01:30<00:10, 16.00it/s] 88%|████████▊ | 1219/1380 [01:30<00:10, 16.01it/s] 88%|████████▊ | 1221/1380 [01:30<00:09, 15.98it/s] 89%|████████▊ | 1223/1380 [01:30<00:09, 15.98it/s] 89%|████████▉ | 1225/1380 [01:31<00:09, 15.99it/s] 89%|████████▉ | 1227/1380 [01:31<00:09, 15.92it/s] 89%|████████▉ | 1229/1380 [01:31<00:09, 15.81it/s] 89%|████████▉ | 1231/1380 [01:31<00:09, 15.80it/s] 89%|████████▉ | 1233/1380 [01:31<00:09, 15.73it/s] 89%|████████▉ | 1235/1380 [01:31<00:09, 15.73it/s] 90%|████████▉ | 1237/1380 [01:31<00:09, 15.65it/s] 90%|████████▉ | 1239/1380 [01:31<00:08, 15.68it/s] 90%|████████▉ | 1241/1380 [01:32<00:08, 15.76it/s] 90%|█████████ | 1243/1380 [01:32<00:08, 15.74it/s] 90%|█████████ | 1245/1380 [01:32<00:08, 15.72it/s] 90%|█████████ | 1247/1380 [01:32<00:08, 15.74it/s] 91%|█████████ | 1249/1380 [01:32<00:08, 15.76it/s] 91%|█████████ | 1251/1380 [01:32<00:08, 15.81it/s] 91%|█████████ | 1253/1380 [01:32<00:08, 15.85it/s] 91%|█████████ | 1255/1380 [01:32<00:07, 15.87it/s] 91%|█████████ | 1257/1380 [01:33<00:07, 15.85it/s] 91%|█████████ | 1259/1380 [01:33<00:07, 15.82it/s] 91%|█████████▏| 1261/1380 [01:33<00:07, 15.69it/s] 92%|█████████▏| 1263/1380 [01:33<00:07, 15.75it/s] 92%|█████████▏| 1265/1380 [01:33<00:07, 15.80it/s] 92%|█████████▏| 1267/1380 [01:33<00:07, 15.57it/s] 92%|█████████▏| 1269/1380 [01:33<00:07, 15.38it/s] 92%|█████████▏| 1271/1380 [01:33<00:07, 15.37it/s] 92%|█████████▏| 1273/1380 [01:34<00:06, 15.43it/s] 92%|█████████▏| 1275/1380 [01:34<00:06, 15.49it/s] 93%|█████████▎| 1277/1380 [01:34<00:06, 15.64it/s] 93%|█████████▎| 1279/1380 [01:34<00:06, 15.75it/s] 93%|█████████▎| 1281/1380 [01:34<00:06, 15.82it/s] 93%|█████████▎| 1283/1380 [01:34<00:06, 15.84it/s] 93%|█████████▎| 1285/1380 [01:34<00:05, 15.86it/s] 93%|█████████▎| 1287/1380 [01:34<00:05, 15.91it/s] 93%|█████████▎| 1289/1380 [01:35<00:05, 15.91it/s] 94%|█████████▎| 1291/1380 [01:35<00:05, 15.93it/s] 94%|█████████▎| 1293/1380 [01:35<00:05, 15.97it/s] 94%|█████████▍| 1295/1380 [01:35<00:05, 15.96it/s] 94%|█████████▍| 1297/1380 [01:35<00:05, 15.95it/s] 94%|█████████▍| 1299/1380 [01:35<00:05, 15.96it/s] 94%|█████████▍| 1301/1380 [01:35<00:04, 15.95it/s] 94%|█████████▍| 1303/1380 [01:35<00:04, 15.96it/s] 95%|█████████▍| 1305/1380 [01:36<00:04, 15.94it/s] 95%|█████████▍| 1307/1380 [01:36<00:04, 15.97it/s] 95%|█████████▍| 1309/1380 [01:36<00:04, 15.95it/s] 95%|█████████▌| 1311/1380 [01:36<00:04, 15.94it/s] 95%|█████████▌| 1313/1380 [01:36<00:04, 15.94it/s] 95%|█████████▌| 1315/1380 [01:36<00:04, 15.91it/s] 95%|█████████▌| 1317/1380 [01:36<00:03, 15.77it/s] 96%|█████████▌| 1319/1380 [01:37<00:03, 15.77it/s] 96%|█████████▌| 1321/1380 [01:37<00:03, 15.74it/s] 96%|█████████▌| 1323/1380 [01:37<00:03, 15.71it/s] 96%|█████████▌| 1325/1380 [01:37<00:03, 15.79it/s] 96%|█████████▌| 1327/1380 [01:37<00:03, 15.45it/s] 96%|█████████▋| 1329/1380 [01:37<00:03, 15.44it/s] 96%|█████████▋| 1331/1380 [01:37<00:03, 15.55it/s] 97%|█████████▋| 1333/1380 [01:37<00:03, 15.67it/s] 97%|█████████▋| 1335/1380 [01:38<00:02, 15.73it/s] 97%|█████████▋| 1337/1380 [01:38<00:02, 15.65it/s] 97%|█████████▋| 1339/1380 [01:38<00:02, 15.72it/s] 97%|█████████▋| 1341/1380 [01:38<00:02, 15.74it/s] 97%|█████████▋| 1343/1380 [01:38<00:02, 15.77it/s] 97%|█████████▋| 1345/1380 [01:38<00:02, 15.79it/s] 98%|█████████▊| 1347/1380 [01:38<00:02, 15.81it/s] 98%|█████████▊| 1349/1380 [01:38<00:01, 15.76it/s] 98%|█████████▊| 1351/1380 [01:39<00:01, 15.80it/s] 98%|█████████▊| 1353/1380 [01:39<00:01, 15.76it/s] 98%|█████████▊| 1355/1380 [01:39<00:01, 15.71it/s] 98%|█████████▊| 1357/1380 [01:39<00:01, 15.66it/s] 98%|█████████▊| 1359/1380 [01:39<00:01, 15.62it/s] 99%|█████████▊| 1361/1380 [01:39<00:01, 15.63it/s] 99%|█████████▉| 1363/1380 [01:39<00:01, 15.63it/s] 99%|█████████▉| 1365/1380 [01:39<00:00, 15.64it/s] 99%|█████████▉| 1367/1380 [01:40<00:00, 15.74it/s] 99%|█████████▉| 1369/1380 [01:40<00:00, 15.78it/s] 99%|█████████▉| 1371/1380 [01:40<00:00, 15.81it/s] 99%|█████████▉| 1373/1380 [01:40<00:00, 15.87it/s]100%|█████████▉| 1375/1380 [01:40<00:00, 15.90it/s]100%|█████████▉| 1377/1380 [01:40<00:00, 15.92it/s]100%|█████████▉| 1379/1380 [01:40<00:00, 15.90it/s]                                                   100%|██████████| 1380/1380 [01:40<00:00, 15.90it/s][INFO|trainer.py:755] 2023-11-15 23:31:32,209 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 23:31:32,210 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 23:31:32,211 >>   Num examples = 2204
[INFO|trainer.py:3134] 2023-11-15 23:31:32,211 >>   Batch size = 8
{'eval_loss': 0.3597547709941864, 'eval_accuracy': 0.8647912885662432, 'eval_micro_f1': 0.8647912885662432, 'eval_macro_f1': 0.8470467356719978, 'eval_runtime': 3.315, 'eval_samples_per_second': 664.865, 'eval_steps_per_second': 83.259, 'epoch': 4.0}
{'loss': 0.2612, 'learning_rate': 0.0, 'epoch': 5.0}

  0%|          | 0/276 [00:00<?, ?it/s][A
  4%|▎         | 10/276 [00:00<00:02, 98.45it/s][A
  7%|▋         | 20/276 [00:00<00:02, 92.15it/s][A
 11%|█         | 30/276 [00:00<00:02, 90.29it/s][A
 14%|█▍        | 40/276 [00:00<00:02, 88.80it/s][A
 18%|█▊        | 49/276 [00:00<00:02, 88.27it/s][A
 21%|██        | 58/276 [00:00<00:02, 87.81it/s][A
 24%|██▍       | 67/276 [00:00<00:02, 87.41it/s][A
 28%|██▊       | 76/276 [00:00<00:02, 87.01it/s][A
 31%|███       | 85/276 [00:00<00:02, 86.86it/s][A
 34%|███▍      | 94/276 [00:01<00:02, 86.97it/s][A
 37%|███▋      | 103/276 [00:01<00:01, 87.07it/s][A
 41%|████      | 112/276 [00:01<00:01, 87.37it/s][A
 44%|████▍     | 121/276 [00:01<00:01, 86.89it/s][A
 47%|████▋     | 130/276 [00:01<00:01, 86.91it/s][A
 50%|█████     | 139/276 [00:01<00:01, 86.23it/s][A
 54%|█████▎    | 148/276 [00:01<00:01, 85.61it/s][A
 57%|█████▋    | 157/276 [00:01<00:01, 85.49it/s][A
 60%|██████    | 166/276 [00:01<00:01, 85.46it/s][A
 63%|██████▎   | 175/276 [00:02<00:01, 84.94it/s][A
 67%|██████▋   | 184/276 [00:02<00:01, 83.48it/s][A
 70%|██████▉   | 193/276 [00:02<00:01, 82.73it/s][A
 73%|███████▎  | 202/276 [00:02<00:00, 83.02it/s][A
 76%|███████▋  | 211/276 [00:02<00:00, 83.24it/s][A
 80%|███████▉  | 220/276 [00:02<00:00, 83.47it/s][A
 83%|████████▎ | 229/276 [00:02<00:00, 83.49it/s][A
 86%|████████▌ | 238/276 [00:02<00:00, 84.79it/s][A
 89%|████████▉ | 247/276 [00:02<00:00, 84.60it/s][A
 93%|█████████▎| 256/276 [00:02<00:00, 84.18it/s][A
 96%|█████████▌| 265/276 [00:03<00:00, 84.07it/s][A
 99%|█████████▉| 274/276 [00:03<00:00, 84.16it/s][A                                                   
                                                 [A100%|██████████| 1380/1380 [01:44<00:00, 15.90it/s]
100%|██████████| 276/276 [00:03<00:00, 84.16it/s][A
                                                 [A[INFO|trainer.py:1963] 2023-11-15 23:31:35,490 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                   100%|██████████| 1380/1380 [01:44<00:00, 15.90it/s]100%|██████████| 1380/1380 [01:44<00:00, 13.25it/s]
[INFO|trainer.py:2855] 2023-11-15 23:31:35,494 >> Saving model checkpoint to ./result/acl_roberta-base_seed2_adapter
[INFO|configuration_utils.py:460] 2023-11-15 23:31:35,498 >> Configuration saved in ./result/acl_roberta-base_seed2_adapter/config.json
[INFO|modeling_utils.py:1997] 2023-11-15 23:31:36,615 >> Model weights saved in ./result/acl_roberta-base_seed2_adapter/pytorch_model.bin
[INFO|tokenization_utils_base.py:2235] 2023-11-15 23:31:36,618 >> tokenizer config file saved in ./result/acl_roberta-base_seed2_adapter/tokenizer_config.json
[INFO|tokenization_utils_base.py:2242] 2023-11-15 23:31:36,620 >> Special tokens file saved in ./result/acl_roberta-base_seed2_adapter/special_tokens_map.json
{'eval_loss': 0.38509008288383484, 'eval_accuracy': 0.8638838475499092, 'eval_micro_f1': 0.8638838475499093, 'eval_macro_f1': 0.8475191383892069, 'eval_runtime': 3.2763, 'eval_samples_per_second': 672.703, 'eval_steps_per_second': 84.24, 'epoch': 5.0}
{'train_runtime': 104.1392, 'train_samples_per_second': 423.279, 'train_steps_per_second': 13.251, 'train_loss': 0.34849641772284023, 'epoch': 5.0}
***** train metrics *****
  epoch                    =        5.0
  train_loss               =     0.3485
  train_runtime            = 0:01:44.13
  train_samples            =       8816
  train_samples_per_second =    423.279
  train_steps_per_second   =     13.251
11/15/2023 23:31:36 - INFO - __main__ - *** Evaluate ***
[INFO|trainer.py:755] 2023-11-15 23:31:36,778 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 23:31:36,779 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 23:31:36,780 >>   Num examples = 2204
[INFO|trainer.py:3134] 2023-11-15 23:31:36,780 >>   Batch size = 8
  0%|          | 0/276 [00:00<?, ?it/s]  3%|▎         | 9/276 [00:00<00:03, 82.20it/s]  7%|▋         | 18/276 [00:00<00:03, 85.49it/s] 10%|▉         | 27/276 [00:00<00:02, 86.68it/s] 13%|█▎        | 36/276 [00:00<00:02, 87.50it/s] 16%|█▋        | 45/276 [00:00<00:02, 87.83it/s] 20%|█▉        | 54/276 [00:00<00:02, 88.00it/s] 23%|██▎       | 63/276 [00:00<00:02, 87.84it/s] 26%|██▌       | 72/276 [00:00<00:02, 87.79it/s] 29%|██▉       | 81/276 [00:00<00:02, 87.28it/s] 33%|███▎      | 90/276 [00:01<00:02, 87.52it/s] 36%|███▌      | 99/276 [00:01<00:02, 87.87it/s] 39%|███▉      | 108/276 [00:01<00:01, 88.02it/s] 42%|████▏     | 117/276 [00:01<00:01, 88.23it/s] 46%|████▌     | 126/276 [00:01<00:01, 88.27it/s] 49%|████▉     | 135/276 [00:01<00:01, 87.91it/s] 52%|█████▏    | 144/276 [00:01<00:01, 87.97it/s] 55%|█████▌    | 153/276 [00:01<00:01, 88.18it/s] 59%|█████▊    | 162/276 [00:01<00:01, 87.44it/s] 62%|██████▏   | 171/276 [00:01<00:01, 87.80it/s] 65%|██████▌   | 180/276 [00:02<00:01, 87.41it/s] 68%|██████▊   | 189/276 [00:02<00:00, 87.39it/s] 72%|███████▏  | 198/276 [00:02<00:00, 87.47it/s] 75%|███████▌  | 207/276 [00:02<00:00, 87.52it/s] 78%|███████▊  | 216/276 [00:02<00:00, 87.55it/s] 82%|████████▏ | 225/276 [00:02<00:00, 87.71it/s] 85%|████████▍ | 234/276 [00:02<00:00, 86.70it/s] 88%|████████▊ | 243/276 [00:02<00:00, 86.24it/s] 91%|█████████▏| 252/276 [00:02<00:00, 85.39it/s] 95%|█████████▍| 261/276 [00:02<00:00, 85.01it/s] 98%|█████████▊| 270/276 [00:03<00:00, 83.85it/s]100%|██████████| 276/276 [00:03<00:00, 85.76it/s]
***** eval metrics *****
  epoch                   =        5.0
  eval_accuracy           =     0.8639
  eval_loss               =     0.3851
  eval_macro_f1           =     0.8475
  eval_micro_f1           =     0.8639
  eval_runtime            = 0:00:03.23
  eval_samples            =       2204
  eval_samples_per_second =    680.931
  eval_steps_per_second   =     85.271
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                  eval/accuracy ▁▁█▆▅▅
wandb:                      eval/loss ▂▄▁▁██
wandb:                  eval/macro_f1 ▂▁█▅▆▆
wandb:                  eval/micro_f1 ▁▁█▆▅▅
wandb:                   eval/runtime ▇▁▅█▅▂
wandb:        eval/samples_per_second ▂█▄▁▄▇
wandb:          eval/steps_per_second ▂█▄▁▄▇
wandb:                    train/epoch ▁▁▃▃▅▅▆▆████
wandb:              train/global_step ▁▁▃▃▅▅▆▆████
wandb:            train/learning_rate █▆▅▃▁
wandb:                     train/loss █▄▃▂▁
wandb:               train/total_flos ▁
wandb:               train/train_loss ▁
wandb:            train/train_runtime ▁
wandb: train/train_samples_per_second ▁
wandb:   train/train_steps_per_second ▁
wandb: 
wandb: Run summary:
wandb:                  eval/accuracy 0.86388
wandb:                      eval/loss 0.38509
wandb:                  eval/macro_f1 0.84752
wandb:                  eval/micro_f1 0.86388
wandb:                   eval/runtime 3.2367
wandb:        eval/samples_per_second 680.931
wandb:          eval/steps_per_second 85.271
wandb:                    train/epoch 5.0
wandb:              train/global_step 1380
wandb:            train/learning_rate 0.0
wandb:                     train/loss 0.2612
wandb:               train/total_flos 1464896356669440.0
wandb:               train/train_loss 0.3485
wandb:            train/train_runtime 104.1392
wandb: train/train_samples_per_second 423.279
wandb:   train/train_steps_per_second 13.251
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /ceph/home/wangyifei/a3/wandb/offline-run-20231115_232832-eihptx0q
wandb: Find logs at: ./wandb/offline-run-20231115_232832-eihptx0q/logs
(ModelArguments(model_name_or_path='roberta-base', cache_dir='./cache', use_adapter=True, use_lora=False), DataTrainingArguments(dataset_name='agnews_sup', max_seq_length=64), TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.0005,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=./result/agnews_sup_roberta-base_seed2_adapter/runs/Nov15_23-31-49_node013,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.EPOCH,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=5.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
output_dir=./result/agnews_sup_roberta-base_seed2_adapter,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
resume_from_checkpoint=None,
run_name=./result/agnews_sup_roberta-base_seed2_adapter,
save_on_each_node=False,
save_safetensors=False,
save_steps=500,
save_strategy=IntervalStrategy.NO,
save_total_limit=None,
seed=333,
sharded_ddp=[],
skip_memory_metrics=True,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
))
11/15/2023 23:31:49 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False
11/15/2023 23:31:49 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.0005,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=./result/agnews_sup_roberta-base_seed2_adapter/runs/Nov15_23-31-49_node013,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.EPOCH,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=5.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
output_dir=./result/agnews_sup_roberta-base_seed2_adapter,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
resume_from_checkpoint=None,
run_name=./result/agnews_sup_roberta-base_seed2_adapter,
save_on_each_node=False,
save_safetensors=False,
save_steps=500,
save_strategy=IntervalStrategy.NO,
save_total_limit=None,
seed=333,
sharded_ddp=[],
skip_memory_metrics=True,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
wandb: Tracking run with wandb version 0.15.12
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[INFO|configuration_utils.py:715] 2023-11-15 23:32:05,426 >> loading configuration file config.json from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/config.json
[INFO|configuration_utils.py:775] 2023-11-15 23:32:05,436 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.33.3",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_auto.py:535] 2023-11-15 23:32:15,497 >> Could not locate the tokenizer configuration file, will try to use the model config instead.
[INFO|configuration_utils.py:715] 2023-11-15 23:32:25,515 >> loading configuration file config.json from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/config.json
[INFO|configuration_utils.py:775] 2023-11-15 23:32:25,515 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.33.3",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:1852] 2023-11-15 23:32:45,565 >> loading file vocab.json from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/vocab.json
[INFO|tokenization_utils_base.py:1852] 2023-11-15 23:32:45,566 >> loading file merges.txt from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/merges.txt
[INFO|tokenization_utils_base.py:1852] 2023-11-15 23:32:45,566 >> loading file tokenizer.json from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/tokenizer.json
[INFO|tokenization_utils_base.py:1852] 2023-11-15 23:32:45,566 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1852] 2023-11-15 23:32:45,566 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1852] 2023-11-15 23:32:45,567 >> loading file tokenizer_config.json from cache at None
[INFO|configuration_utils.py:715] 2023-11-15 23:32:45,568 >> loading configuration file config.json from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/config.json
[INFO|configuration_utils.py:775] 2023-11-15 23:32:45,569 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.33.3",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:2862] 2023-11-15 23:33:05,736 >> loading weights file model.safetensors from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/model.safetensors
[INFO|modeling_utils.py:3638] 2023-11-15 23:33:06,436 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3650] 2023-11-15 23:33:06,437 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
parameters 1488196
model_max_length: 512
Running tokenizer on dataset:   0%|          | 0/6840 [00:00<?, ? examples/s]Running tokenizer on dataset:  29%|██▉       | 2000/6840 [00:00<00:00, 18585.43 examples/s]Running tokenizer on dataset:  88%|████████▊ | 6000/6840 [00:00<00:00, 20298.77 examples/s]Running tokenizer on dataset: 100%|██████████| 6840/6840 [00:00<00:00, 20065.02 examples/s]
Running tokenizer on dataset:   0%|          | 0/760 [00:00<?, ? examples/s]Running tokenizer on dataset: 100%|██████████| 760/760 [00:00<00:00, 21970.00 examples/s]
11/15/2023 23:33:07 - INFO - __main__ - Sample 4545 of the training set: {'text': "Yankees' Brown Has Successful Surgery Kevin Brown had successful surgery on his broken left hand Sunday and vowed to pitch again for the Yankees this season.", 'label': 0, 'input_ids': [0, 43033, 41563, 108, 1547, 6233, 14361, 2650, 26793, 2363, 1547, 56, 1800, 3012, 15, 39, 3187, 314, 865, 395, 8, 7588, 7, 3242, 456, 13, 5, 6742, 42, 191, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
11/15/2023 23:33:07 - INFO - __main__ - Sample 2873 of the training set: {'text': 'Bush shields shrimp industry The Bush administration yesterday said Chinese and Vietnamese shrimp are sold at unfairly low prices in the United States, siding with US fishermen as they try to fend off overseas competition.', 'label': 1, 'input_ids': [0, 43294, 31768, 22126, 539, 20, 3516, 942, 2350, 26, 1111, 8, 16859, 22126, 32, 1088, 23, 19106, 614, 850, 11, 5, 315, 532, 6, 579, 8231, 19, 382, 16516, 25, 51, 860, 7, 26885, 160, 4886, 1465, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
11/15/2023 23:33:07 - INFO - __main__ - Sample 2892 of the training set: {'text': 'How the credit policy will affect you The Reserve Bank of India announced the mid-term review of its monetary policy on Tuesday. Though the central Bank kept away from the much expected interest rate hike, the policy contained recommendations ', 'label': 1, 'input_ids': [0, 6179, 5, 1361, 714, 40, 3327, 47, 20, 3965, 788, 9, 666, 585, 5, 1084, 12, 1279, 1551, 9, 63, 5775, 714, 15, 294, 4, 3791, 5, 1353, 788, 1682, 409, 31, 5, 203, 421, 773, 731, 5960, 6, 5, 714, 5558, 4664, 1437, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
11/15/2023 23:33:07 - WARNING - accelerate.utils.other - Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[INFO|trainer.py:755] 2023-11-15 23:33:08,948 >> The following columns in the training set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:1715] 2023-11-15 23:33:08,959 >> ***** Running training *****
[INFO|trainer.py:1716] 2023-11-15 23:33:08,960 >>   Num examples = 6,840
[INFO|trainer.py:1717] 2023-11-15 23:33:08,960 >>   Num Epochs = 5
[INFO|trainer.py:1718] 2023-11-15 23:33:08,960 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1721] 2023-11-15 23:33:08,960 >>   Total train batch size (w. parallel, distributed & accumulation) = 32
[INFO|trainer.py:1722] 2023-11-15 23:33:08,961 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:1723] 2023-11-15 23:33:08,961 >>   Total optimization steps = 1,070
[INFO|trainer.py:1724] 2023-11-15 23:33:08,962 >>   Number of trainable parameters = 1,488,196
[INFO|integration_utils.py:716] 2023-11-15 23:33:08,963 >> Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"
  0%|          | 0/1070 [00:00<?, ?it/s]  0%|          | 1/1070 [00:01<18:46,  1.05s/it]  0%|          | 3/1070 [00:01<05:40,  3.13it/s]  0%|          | 5/1070 [00:01<03:19,  5.33it/s]  1%|          | 7/1070 [00:01<02:23,  7.43it/s]  1%|          | 9/1070 [00:01<01:54,  9.30it/s]  1%|          | 11/1070 [00:01<01:37, 10.89it/s]  1%|          | 13/1070 [00:01<01:27, 12.12it/s]  1%|▏         | 15/1070 [00:01<01:21, 12.96it/s]  2%|▏         | 17/1070 [00:02<01:17, 13.67it/s]  2%|▏         | 19/1070 [00:02<01:13, 14.25it/s]  2%|▏         | 21/1070 [00:02<01:11, 14.60it/s]  2%|▏         | 23/1070 [00:02<01:10, 14.94it/s]  2%|▏         | 25/1070 [00:02<01:08, 15.27it/s]  3%|▎         | 27/1070 [00:02<01:07, 15.53it/s]  3%|▎         | 29/1070 [00:02<01:06, 15.73it/s]  3%|▎         | 31/1070 [00:02<01:05, 15.88it/s]  3%|▎         | 33/1070 [00:03<01:04, 15.96it/s]  3%|▎         | 35/1070 [00:03<01:04, 16.02it/s]  3%|▎         | 37/1070 [00:03<01:04, 16.08it/s]  4%|▎         | 39/1070 [00:03<01:03, 16.11it/s]  4%|▍         | 41/1070 [00:03<01:03, 16.09it/s]  4%|▍         | 43/1070 [00:03<01:03, 16.10it/s]  4%|▍         | 45/1070 [00:03<01:03, 16.08it/s]  4%|▍         | 47/1070 [00:03<01:03, 16.10it/s]  5%|▍         | 49/1070 [00:04<01:03, 16.14it/s]  5%|▍         | 51/1070 [00:04<01:03, 16.14it/s]  5%|▍         | 53/1070 [00:04<01:03, 16.11it/s]  5%|▌         | 55/1070 [00:04<01:03, 16.08it/s]  5%|▌         | 57/1070 [00:04<01:02, 16.12it/s]  6%|▌         | 59/1070 [00:04<01:02, 16.11it/s]  6%|▌         | 61/1070 [00:04<01:02, 16.10it/s]  6%|▌         | 63/1070 [00:04<01:02, 16.04it/s]  6%|▌         | 65/1070 [00:05<01:02, 15.96it/s]  6%|▋         | 67/1070 [00:05<01:02, 15.93it/s]  6%|▋         | 69/1070 [00:05<01:03, 15.88it/s]  7%|▋         | 71/1070 [00:05<01:03, 15.76it/s]  7%|▋         | 73/1070 [00:05<01:03, 15.79it/s]  7%|▋         | 75/1070 [00:05<01:02, 15.80it/s]  7%|▋         | 77/1070 [00:05<01:02, 15.83it/s]  7%|▋         | 79/1070 [00:05<01:02, 15.88it/s]  8%|▊         | 81/1070 [00:06<01:02, 15.92it/s]  8%|▊         | 83/1070 [00:06<01:01, 15.94it/s]  8%|▊         | 85/1070 [00:06<01:01, 15.91it/s]  8%|▊         | 87/1070 [00:06<01:01, 15.94it/s]  8%|▊         | 89/1070 [00:06<01:01, 15.87it/s]  9%|▊         | 91/1070 [00:06<01:01, 15.94it/s]  9%|▊         | 93/1070 [00:06<01:01, 15.97it/s]  9%|▉         | 95/1070 [00:06<01:00, 15.99it/s]  9%|▉         | 97/1070 [00:07<01:01, 15.95it/s]  9%|▉         | 99/1070 [00:07<01:00, 15.99it/s]  9%|▉         | 101/1070 [00:07<01:00, 15.90it/s] 10%|▉         | 103/1070 [00:07<01:01, 15.68it/s] 10%|▉         | 105/1070 [00:07<01:02, 15.56it/s] 10%|█         | 107/1070 [00:07<01:01, 15.53it/s] 10%|█         | 109/1070 [00:07<01:01, 15.51it/s] 10%|█         | 111/1070 [00:07<01:01, 15.59it/s] 11%|█         | 113/1070 [00:08<01:00, 15.79it/s] 11%|█         | 115/1070 [00:08<01:00, 15.84it/s] 11%|█         | 117/1070 [00:08<01:00, 15.87it/s] 11%|█         | 119/1070 [00:08<00:59, 15.91it/s] 11%|█▏        | 121/1070 [00:08<00:59, 16.02it/s] 11%|█▏        | 123/1070 [00:08<00:58, 16.12it/s] 12%|█▏        | 125/1070 [00:08<00:58, 16.18it/s] 12%|█▏        | 127/1070 [00:08<00:58, 16.20it/s] 12%|█▏        | 129/1070 [00:09<00:58, 16.19it/s] 12%|█▏        | 131/1070 [00:09<00:58, 16.17it/s] 12%|█▏        | 133/1070 [00:09<00:57, 16.18it/s] 13%|█▎        | 135/1070 [00:09<00:57, 16.21it/s] 13%|█▎        | 137/1070 [00:09<00:57, 16.22it/s] 13%|█▎        | 139/1070 [00:09<00:57, 16.22it/s] 13%|█▎        | 141/1070 [00:09<00:57, 16.21it/s] 13%|█▎        | 143/1070 [00:09<00:57, 16.20it/s] 14%|█▎        | 145/1070 [00:10<00:57, 16.20it/s] 14%|█▎        | 147/1070 [00:10<00:56, 16.23it/s] 14%|█▍        | 149/1070 [00:10<00:56, 16.21it/s] 14%|█▍        | 151/1070 [00:10<00:56, 16.16it/s] 14%|█▍        | 153/1070 [00:10<00:57, 16.04it/s] 14%|█▍        | 155/1070 [00:10<00:57, 15.99it/s] 15%|█▍        | 157/1070 [00:10<00:57, 15.97it/s] 15%|█▍        | 159/1070 [00:10<00:57, 15.82it/s] 15%|█▌        | 161/1070 [00:11<00:57, 15.90it/s] 15%|█▌        | 163/1070 [00:11<00:57, 15.86it/s] 15%|█▌        | 165/1070 [00:11<00:57, 15.87it/s] 16%|█▌        | 167/1070 [00:11<00:56, 15.90it/s] 16%|█▌        | 169/1070 [00:11<00:56, 15.91it/s] 16%|█▌        | 171/1070 [00:11<00:56, 15.90it/s] 16%|█▌        | 173/1070 [00:11<00:56, 15.87it/s] 16%|█▋        | 175/1070 [00:11<00:56, 15.95it/s] 17%|█▋        | 177/1070 [00:12<00:56, 15.94it/s] 17%|█▋        | 179/1070 [00:12<00:56, 15.89it/s] 17%|█▋        | 181/1070 [00:12<00:55, 15.93it/s] 17%|█▋        | 183/1070 [00:12<00:55, 15.96it/s] 17%|█▋        | 185/1070 [00:12<00:55, 16.00it/s] 17%|█▋        | 187/1070 [00:12<00:55, 15.96it/s] 18%|█▊        | 189/1070 [00:12<00:55, 15.96it/s] 18%|█▊        | 191/1070 [00:12<00:55, 15.93it/s] 18%|█▊        | 193/1070 [00:13<00:55, 15.84it/s] 18%|█▊        | 195/1070 [00:13<00:55, 15.85it/s] 18%|█▊        | 197/1070 [00:13<00:55, 15.86it/s] 19%|█▊        | 199/1070 [00:13<00:54, 15.86it/s] 19%|█▉        | 201/1070 [00:13<00:54, 15.93it/s] 19%|█▉        | 203/1070 [00:13<00:54, 16.03it/s] 19%|█▉        | 205/1070 [00:13<00:53, 16.09it/s] 19%|█▉        | 207/1070 [00:13<00:53, 16.10it/s] 20%|█▉        | 209/1070 [00:14<00:53, 16.12it/s] 20%|█▉        | 211/1070 [00:14<00:53, 16.14it/s] 20%|█▉        | 213/1070 [00:14<00:53, 16.14it/s]                                                   20%|██        | 214/1070 [00:14<00:53, 16.14it/s][INFO|trainer.py:755] 2023-11-15 23:33:23,347 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 23:33:23,348 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 23:33:23,349 >>   Num examples = 760
[INFO|trainer.py:3134] 2023-11-15 23:33:23,349 >>   Batch size = 8
{'loss': 0.437, 'learning_rate': 0.0004, 'epoch': 1.0}

  0%|          | 0/95 [00:00<?, ?it/s][A
 11%|█         | 10/95 [00:00<00:00, 94.43it/s][A
 21%|██        | 20/95 [00:00<00:00, 89.02it/s][A
 31%|███       | 29/95 [00:00<00:00, 87.64it/s][A
 40%|████      | 38/95 [00:00<00:00, 86.97it/s][A
 49%|████▉     | 47/95 [00:00<00:00, 86.35it/s][A
 59%|█████▉    | 56/95 [00:00<00:00, 85.52it/s][A
 68%|██████▊   | 65/95 [00:00<00:00, 84.62it/s][A
 78%|███████▊  | 74/95 [00:00<00:00, 84.65it/s][A
 87%|████████▋ | 83/95 [00:00<00:00, 85.03it/s][A
 97%|█████████▋| 92/95 [00:01<00:00, 85.24it/s][A                                                  
                                               [A 20%|██        | 214/1070 [00:15<00:53, 16.14it/s]
100%|██████████| 95/95 [00:01<00:00, 85.24it/s][A
                                               [A 20%|██        | 215/1070 [00:15<03:19,  4.28it/s] 20%|██        | 217/1070 [00:15<02:35,  5.49it/s] 20%|██        | 219/1070 [00:15<02:04,  6.84it/s] 21%|██        | 221/1070 [00:15<01:42,  8.27it/s] 21%|██        | 223/1070 [00:16<01:27,  9.67it/s] 21%|██        | 225/1070 [00:16<01:17, 10.95it/s] 21%|██        | 227/1070 [00:16<01:09, 12.06it/s] 21%|██▏       | 229/1070 [00:16<01:04, 12.99it/s] 22%|██▏       | 231/1070 [00:16<01:01, 13.72it/s] 22%|██▏       | 233/1070 [00:16<00:58, 14.25it/s] 22%|██▏       | 235/1070 [00:16<00:56, 14.74it/s] 22%|██▏       | 237/1070 [00:16<00:55, 15.03it/s] 22%|██▏       | 239/1070 [00:17<00:54, 15.24it/s] 23%|██▎       | 241/1070 [00:17<00:53, 15.48it/s] 23%|██▎       | 243/1070 [00:17<00:52, 15.63it/s] 23%|██▎       | 245/1070 [00:17<00:52, 15.70it/s] 23%|██▎       | 247/1070 [00:17<00:52, 15.79it/s] 23%|██▎       | 249/1070 [00:17<00:51, 15.86it/s] 23%|██▎       | 251/1070 [00:17<00:51, 15.86it/s] 24%|██▎       | 253/1070 [00:17<00:51, 15.89it/s] 24%|██▍       | 255/1070 [00:18<00:51, 15.88it/s] 24%|██▍       | 257/1070 [00:18<00:51, 15.91it/s] 24%|██▍       | 259/1070 [00:18<00:50, 15.91it/s] 24%|██▍       | 261/1070 [00:18<00:51, 15.84it/s] 25%|██▍       | 263/1070 [00:18<00:51, 15.79it/s] 25%|██▍       | 265/1070 [00:18<00:50, 15.79it/s] 25%|██▍       | 267/1070 [00:18<00:50, 15.78it/s] 25%|██▌       | 269/1070 [00:18<00:50, 15.81it/s] 25%|██▌       | 271/1070 [00:19<00:50, 15.85it/s] 26%|██▌       | 273/1070 [00:19<00:50, 15.92it/s] 26%|██▌       | 275/1070 [00:19<00:50, 15.88it/s] 26%|██▌       | 277/1070 [00:19<00:49, 15.89it/s] 26%|██▌       | 279/1070 [00:19<00:49, 15.94it/s] 26%|██▋       | 281/1070 [00:19<00:49, 15.99it/s] 26%|██▋       | 283/1070 [00:19<00:48, 16.07it/s] 27%|██▋       | 285/1070 [00:19<00:48, 16.12it/s] 27%|██▋       | 287/1070 [00:20<00:48, 16.12it/s] 27%|██▋       | 289/1070 [00:20<00:48, 16.08it/s] 27%|██▋       | 291/1070 [00:20<00:48, 16.06it/s] 27%|██▋       | 293/1070 [00:20<00:48, 16.10it/s] 28%|██▊       | 295/1070 [00:20<00:48, 16.11it/s] 28%|██▊       | 297/1070 [00:20<00:48, 16.10it/s] 28%|██▊       | 299/1070 [00:20<00:47, 16.10it/s] 28%|██▊       | 301/1070 [00:20<00:47, 16.11it/s] 28%|██▊       | 303/1070 [00:21<00:47, 16.16it/s] 29%|██▊       | 305/1070 [00:21<00:47, 16.16it/s] 29%|██▊       | 307/1070 [00:21<00:47, 16.14it/s] 29%|██▉       | 309/1070 [00:21<00:47, 16.13it/s] 29%|██▉       | 311/1070 [00:21<00:47, 16.10it/s] 29%|██▉       | 313/1070 [00:21<00:47, 16.06it/s] 29%|██▉       | 315/1070 [00:21<00:47, 15.98it/s] 30%|██▉       | 317/1070 [00:21<00:47, 15.92it/s] 30%|██▉       | 319/1070 [00:22<00:47, 15.89it/s] 30%|███       | 321/1070 [00:22<00:47, 15.85it/s] 30%|███       | 323/1070 [00:22<00:47, 15.86it/s] 30%|███       | 325/1070 [00:22<00:46, 15.91it/s] 31%|███       | 327/1070 [00:22<00:46, 15.91it/s] 31%|███       | 329/1070 [00:22<00:46, 15.86it/s] 31%|███       | 331/1070 [00:22<00:46, 15.88it/s] 31%|███       | 333/1070 [00:22<00:46, 15.93it/s] 31%|███▏      | 335/1070 [00:23<00:46, 15.95it/s] 31%|███▏      | 337/1070 [00:23<00:46, 15.92it/s] 32%|███▏      | 339/1070 [00:23<00:45, 15.92it/s] 32%|███▏      | 341/1070 [00:23<00:45, 15.90it/s] 32%|███▏      | 343/1070 [00:23<00:45, 15.91it/s] 32%|███▏      | 345/1070 [00:23<00:45, 15.90it/s] 32%|███▏      | 347/1070 [00:23<00:45, 15.93it/s] 33%|███▎      | 349/1070 [00:24<00:45, 15.91it/s] 33%|███▎      | 351/1070 [00:24<00:45, 15.89it/s] 33%|███▎      | 353/1070 [00:24<00:45, 15.85it/s] 33%|███▎      | 355/1070 [00:24<00:45, 15.83it/s] 33%|███▎      | 357/1070 [00:24<00:45, 15.78it/s] 34%|███▎      | 359/1070 [00:24<00:44, 15.81it/s] 34%|███▎      | 361/1070 [00:24<00:44, 15.86it/s] 34%|███▍      | 363/1070 [00:24<00:44, 15.91it/s] 34%|███▍      | 365/1070 [00:25<00:44, 15.94it/s] 34%|███▍      | 367/1070 [00:25<00:43, 16.02it/s] 34%|███▍      | 369/1070 [00:25<00:43, 16.07it/s] 35%|███▍      | 371/1070 [00:25<00:43, 16.08it/s] 35%|███▍      | 373/1070 [00:25<00:43, 16.07it/s] 35%|███▌      | 375/1070 [00:25<00:43, 16.07it/s] 35%|███▌      | 377/1070 [00:25<00:43, 16.10it/s] 35%|███▌      | 379/1070 [00:25<00:42, 16.11it/s] 36%|███▌      | 381/1070 [00:26<00:42, 16.10it/s] 36%|███▌      | 383/1070 [00:26<00:42, 16.09it/s] 36%|███▌      | 385/1070 [00:26<00:42, 16.10it/s] 36%|███▌      | 387/1070 [00:26<00:42, 16.14it/s] 36%|███▋      | 389/1070 [00:26<00:42, 16.15it/s] 37%|███▋      | 391/1070 [00:26<00:42, 16.14it/s] 37%|███▋      | 393/1070 [00:26<00:42, 16.12it/s] 37%|███▋      | 395/1070 [00:26<00:42, 16.02it/s] 37%|███▋      | 397/1070 [00:26<00:41, 16.03it/s] 37%|███▋      | 399/1070 [00:27<00:41, 16.04it/s] 37%|███▋      | 401/1070 [00:27<00:41, 16.03it/s] 38%|███▊      | 403/1070 [00:27<00:41, 16.02it/s] 38%|███▊      | 405/1070 [00:27<00:41, 15.99it/s] 38%|███▊      | 407/1070 [00:27<00:41, 15.94it/s] 38%|███▊      | 409/1070 [00:27<00:41, 15.89it/s] 38%|███▊      | 411/1070 [00:27<00:41, 15.87it/s] 39%|███▊      | 413/1070 [00:28<00:41, 15.83it/s] 39%|███▉      | 415/1070 [00:28<00:41, 15.85it/s] 39%|███▉      | 417/1070 [00:28<00:41, 15.86it/s] 39%|███▉      | 419/1070 [00:28<00:41, 15.85it/s] 39%|███▉      | 421/1070 [00:28<00:40, 15.87it/s] 40%|███▉      | 423/1070 [00:28<00:40, 15.92it/s] 40%|███▉      | 425/1070 [00:28<00:40, 15.94it/s] 40%|███▉      | 427/1070 [00:28<00:40, 15.94it/s]                                                   40%|████      | 428/1070 [00:28<00:40, 15.94it/s][INFO|trainer.py:755] 2023-11-15 23:33:37,897 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 23:33:37,899 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 23:33:37,899 >>   Num examples = 760
[INFO|trainer.py:3134] 2023-11-15 23:33:37,900 >>   Batch size = 8
{'eval_loss': 0.2950809597969055, 'eval_accuracy': 0.9105263157894737, 'eval_micro_f1': 0.9105263157894739, 'eval_macro_f1': 0.9072585177628615, 'eval_runtime': 1.1512, 'eval_samples_per_second': 660.17, 'eval_steps_per_second': 82.521, 'epoch': 1.0}
{'loss': 0.2546, 'learning_rate': 0.0003, 'epoch': 2.0}

  0%|          | 0/95 [00:00<?, ?it/s][A
 11%|█         | 10/95 [00:00<00:00, 95.30it/s][A
 21%|██        | 20/95 [00:00<00:00, 89.54it/s][A
 31%|███       | 29/95 [00:00<00:00, 87.96it/s][A
 40%|████      | 38/95 [00:00<00:00, 87.03it/s][A
 49%|████▉     | 47/95 [00:00<00:00, 86.01it/s][A
 59%|█████▉    | 56/95 [00:00<00:00, 85.73it/s][A
 68%|██████▊   | 65/95 [00:00<00:00, 84.67it/s][A
 78%|███████▊  | 74/95 [00:00<00:00, 81.38it/s][A
 87%|████████▋ | 83/95 [00:00<00:00, 78.41it/s][A
 96%|█████████▌| 91/95 [00:01<00:00, 77.08it/s][A                                                  
                                               [A 40%|████      | 428/1070 [00:30<00:40, 15.94it/s]
100%|██████████| 95/95 [00:01<00:00, 77.08it/s][A
                                               [A 40%|████      | 429/1070 [00:30<02:36,  4.10it/s] 40%|████      | 431/1070 [00:30<02:01,  5.28it/s] 40%|████      | 433/1070 [00:30<01:36,  6.61it/s] 41%|████      | 435/1070 [00:30<01:19,  8.03it/s] 41%|████      | 437/1070 [00:30<01:06,  9.45it/s] 41%|████      | 439/1070 [00:30<00:58, 10.76it/s] 41%|████      | 441/1070 [00:30<00:52, 11.94it/s] 41%|████▏     | 443/1070 [00:31<00:48, 12.94it/s] 42%|████▏     | 445/1070 [00:31<00:45, 13.71it/s] 42%|████▏     | 447/1070 [00:31<00:43, 14.35it/s] 42%|████▏     | 449/1070 [00:31<00:41, 14.82it/s] 42%|████▏     | 451/1070 [00:31<00:40, 15.15it/s] 42%|████▏     | 453/1070 [00:31<00:40, 15.41it/s] 43%|████▎     | 455/1070 [00:31<00:39, 15.59it/s] 43%|████▎     | 457/1070 [00:31<00:38, 15.72it/s] 43%|████▎     | 459/1070 [00:32<00:38, 15.81it/s] 43%|████▎     | 461/1070 [00:32<00:38, 15.91it/s] 43%|████▎     | 463/1070 [00:32<00:38, 15.95it/s] 43%|████▎     | 465/1070 [00:32<00:37, 15.95it/s] 44%|████▎     | 467/1070 [00:32<00:37, 15.92it/s] 44%|████▍     | 469/1070 [00:32<00:37, 15.93it/s] 44%|████▍     | 471/1070 [00:32<00:37, 15.89it/s] 44%|████▍     | 473/1070 [00:32<00:37, 15.84it/s] 44%|████▍     | 475/1070 [00:33<00:37, 15.84it/s] 45%|████▍     | 477/1070 [00:33<00:37, 15.83it/s] 45%|████▍     | 479/1070 [00:33<00:37, 15.76it/s] 45%|████▍     | 481/1070 [00:33<00:37, 15.76it/s] 45%|████▌     | 483/1070 [00:33<00:37, 15.74it/s] 45%|████▌     | 485/1070 [00:33<00:37, 15.77it/s] 46%|████▌     | 487/1070 [00:33<00:36, 15.78it/s] 46%|████▌     | 489/1070 [00:33<00:36, 15.80it/s] 46%|████▌     | 491/1070 [00:34<00:36, 15.74it/s] 46%|████▌     | 493/1070 [00:34<00:36, 15.84it/s] 46%|████▋     | 495/1070 [00:34<00:36, 15.78it/s] 46%|████▋     | 497/1070 [00:34<00:36, 15.81it/s] 47%|████▋     | 499/1070 [00:34<00:36, 15.86it/s] 47%|████▋     | 501/1070 [00:34<00:35, 15.81it/s] 47%|████▋     | 503/1070 [00:34<00:36, 15.74it/s] 47%|████▋     | 505/1070 [00:34<00:35, 15.81it/s] 47%|████▋     | 507/1070 [00:35<00:35, 15.84it/s] 48%|████▊     | 509/1070 [00:35<00:35, 15.85it/s] 48%|████▊     | 511/1070 [00:35<00:35, 15.86it/s] 48%|████▊     | 513/1070 [00:35<00:35, 15.78it/s] 48%|████▊     | 515/1070 [00:35<00:35, 15.72it/s] 48%|████▊     | 517/1070 [00:35<00:35, 15.74it/s] 49%|████▊     | 519/1070 [00:35<00:35, 15.66it/s] 49%|████▊     | 521/1070 [00:36<00:34, 15.74it/s] 49%|████▉     | 523/1070 [00:36<00:34, 15.86it/s] 49%|████▉     | 525/1070 [00:36<00:34, 15.94it/s] 49%|████▉     | 527/1070 [00:36<00:34, 15.95it/s] 49%|████▉     | 529/1070 [00:36<00:33, 15.99it/s] 50%|████▉     | 531/1070 [00:36<00:33, 16.01it/s] 50%|████▉     | 533/1070 [00:36<00:33, 16.03it/s] 50%|█████     | 535/1070 [00:36<00:33, 16.03it/s] 50%|█████     | 537/1070 [00:37<00:33, 16.03it/s] 50%|█████     | 539/1070 [00:37<00:33, 16.08it/s] 51%|█████     | 541/1070 [00:37<00:32, 16.11it/s] 51%|█████     | 543/1070 [00:37<00:32, 16.11it/s] 51%|█████     | 545/1070 [00:37<00:32, 16.10it/s] 51%|█████     | 547/1070 [00:37<00:32, 16.06it/s] 51%|█████▏    | 549/1070 [00:37<00:32, 16.06it/s] 51%|█████▏    | 551/1070 [00:37<00:32, 16.07it/s] 52%|█████▏    | 553/1070 [00:37<00:32, 16.06it/s] 52%|█████▏    | 555/1070 [00:38<00:32, 16.03it/s] 52%|█████▏    | 557/1070 [00:38<00:31, 16.08it/s] 52%|█████▏    | 559/1070 [00:38<00:31, 16.11it/s] 52%|█████▏    | 561/1070 [00:38<00:31, 16.03it/s] 53%|█████▎    | 563/1070 [00:38<00:31, 15.91it/s] 53%|█████▎    | 565/1070 [00:38<00:31, 15.89it/s] 53%|█████▎    | 567/1070 [00:38<00:31, 15.86it/s] 53%|█████▎    | 569/1070 [00:39<00:31, 15.83it/s] 53%|█████▎    | 571/1070 [00:39<00:31, 15.87it/s] 54%|█████▎    | 573/1070 [00:39<00:31, 15.89it/s] 54%|█████▎    | 575/1070 [00:39<00:31, 15.91it/s] 54%|█████▍    | 577/1070 [00:39<00:31, 15.85it/s] 54%|█████▍    | 579/1070 [00:39<00:30, 15.86it/s] 54%|█████▍    | 581/1070 [00:39<00:30, 15.86it/s] 54%|█████▍    | 583/1070 [00:39<00:30, 15.82it/s] 55%|█████▍    | 585/1070 [00:40<00:30, 15.91it/s] 55%|█████▍    | 587/1070 [00:40<00:30, 15.89it/s] 55%|█████▌    | 589/1070 [00:40<00:30, 15.88it/s] 55%|█████▌    | 591/1070 [00:40<00:30, 15.88it/s] 55%|█████▌    | 593/1070 [00:40<00:30, 15.89it/s] 56%|█████▌    | 595/1070 [00:40<00:29, 15.86it/s] 56%|█████▌    | 597/1070 [00:40<00:29, 15.93it/s] 56%|█████▌    | 599/1070 [00:40<00:29, 15.98it/s] 56%|█████▌    | 601/1070 [00:41<00:29, 15.93it/s] 56%|█████▋    | 603/1070 [00:41<00:29, 15.76it/s] 57%|█████▋    | 605/1070 [00:41<00:29, 15.63it/s] 57%|█████▋    | 607/1070 [00:41<00:29, 15.55it/s] 57%|█████▋    | 609/1070 [00:41<00:29, 15.62it/s] 57%|█████▋    | 611/1070 [00:41<00:29, 15.75it/s] 57%|█████▋    | 613/1070 [00:41<00:28, 15.83it/s] 57%|█████▋    | 615/1070 [00:41<00:28, 15.91it/s] 58%|█████▊    | 617/1070 [00:42<00:28, 15.98it/s] 58%|█████▊    | 619/1070 [00:42<00:28, 16.01it/s] 58%|█████▊    | 621/1070 [00:42<00:28, 15.98it/s] 58%|█████▊    | 623/1070 [00:42<00:27, 16.04it/s] 58%|█████▊    | 625/1070 [00:42<00:27, 16.08it/s] 59%|█████▊    | 627/1070 [00:42<00:27, 16.09it/s] 59%|█████▉    | 629/1070 [00:42<00:27, 16.02it/s] 59%|█████▉    | 631/1070 [00:42<00:27, 16.03it/s] 59%|█████▉    | 633/1070 [00:43<00:27, 16.07it/s] 59%|█████▉    | 635/1070 [00:43<00:27, 16.05it/s] 60%|█████▉    | 637/1070 [00:43<00:27, 16.02it/s] 60%|█████▉    | 639/1070 [00:43<00:26, 16.03it/s] 60%|█████▉    | 641/1070 [00:43<00:26, 16.10it/s]                                                   60%|██████    | 642/1070 [00:43<00:26, 16.10it/s][INFO|trainer.py:755] 2023-11-15 23:33:52,540 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 23:33:52,541 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 23:33:52,542 >>   Num examples = 760
[INFO|trainer.py:3134] 2023-11-15 23:33:52,542 >>   Batch size = 8
{'eval_loss': 0.2754918336868286, 'eval_accuracy': 0.9118421052631579, 'eval_micro_f1': 0.9118421052631579, 'eval_macro_f1': 0.9090210075830721, 'eval_runtime': 1.212, 'eval_samples_per_second': 627.089, 'eval_steps_per_second': 78.386, 'epoch': 2.0}
{'loss': 0.2063, 'learning_rate': 0.0002, 'epoch': 3.0}

  0%|          | 0/95 [00:00<?, ?it/s][A
 11%|█         | 10/95 [00:00<00:00, 98.88it/s][A
 21%|██        | 20/95 [00:00<00:00, 91.22it/s][A
 32%|███▏      | 30/95 [00:00<00:00, 88.98it/s][A
 41%|████      | 39/95 [00:00<00:00, 87.04it/s][A
 51%|█████     | 48/95 [00:00<00:00, 84.88it/s][A
 60%|██████    | 57/95 [00:00<00:00, 84.68it/s][A
 69%|██████▉   | 66/95 [00:00<00:00, 84.92it/s][A
 79%|███████▉  | 75/95 [00:00<00:00, 84.49it/s][A
 88%|████████▊ | 84/95 [00:00<00:00, 84.86it/s][A
 98%|█████████▊| 93/95 [00:01<00:00, 83.72it/s][A                                                  
                                               [A 60%|██████    | 642/1070 [00:44<00:26, 16.10it/s]
100%|██████████| 95/95 [00:01<00:00, 83.72it/s][A
                                               [A 60%|██████    | 643/1070 [00:44<01:40,  4.27it/s] 60%|██████    | 645/1070 [00:44<01:17,  5.47it/s] 60%|██████    | 647/1070 [00:45<01:02,  6.81it/s] 61%|██████    | 649/1070 [00:45<00:51,  8.21it/s] 61%|██████    | 651/1070 [00:45<00:43,  9.60it/s] 61%|██████    | 653/1070 [00:45<00:38, 10.92it/s] 61%|██████    | 655/1070 [00:45<00:34, 12.08it/s] 61%|██████▏   | 657/1070 [00:45<00:31, 13.00it/s] 62%|██████▏   | 659/1070 [00:45<00:29, 13.75it/s] 62%|██████▏   | 661/1070 [00:45<00:28, 14.29it/s] 62%|██████▏   | 663/1070 [00:46<00:27, 14.73it/s] 62%|██████▏   | 665/1070 [00:46<00:26, 15.07it/s] 62%|██████▏   | 667/1070 [00:46<00:26, 15.36it/s] 63%|██████▎   | 669/1070 [00:46<00:25, 15.56it/s] 63%|██████▎   | 671/1070 [00:46<00:25, 15.64it/s] 63%|██████▎   | 673/1070 [00:46<00:25, 15.58it/s] 63%|██████▎   | 675/1070 [00:46<00:25, 15.51it/s] 63%|██████▎   | 677/1070 [00:46<00:25, 15.49it/s] 63%|██████▎   | 679/1070 [00:47<00:25, 15.49it/s] 64%|██████▎   | 681/1070 [00:47<00:24, 15.62it/s] 64%|██████▍   | 683/1070 [00:47<00:24, 15.78it/s] 64%|██████▍   | 685/1070 [00:47<00:24, 15.89it/s] 64%|██████▍   | 687/1070 [00:47<00:24, 15.91it/s] 64%|██████▍   | 689/1070 [00:47<00:23, 15.95it/s] 65%|██████▍   | 691/1070 [00:47<00:23, 16.01it/s] 65%|██████▍   | 693/1070 [00:47<00:23, 16.05it/s] 65%|██████▍   | 695/1070 [00:48<00:23, 16.07it/s] 65%|██████▌   | 697/1070 [00:48<00:23, 16.07it/s] 65%|██████▌   | 699/1070 [00:48<00:23, 16.07it/s] 66%|██████▌   | 701/1070 [00:48<00:22, 16.12it/s] 66%|██████▌   | 703/1070 [00:48<00:22, 16.14it/s] 66%|██████▌   | 705/1070 [00:48<00:22, 16.14it/s] 66%|██████▌   | 707/1070 [00:48<00:22, 16.11it/s] 66%|██████▋   | 709/1070 [00:48<00:22, 16.09it/s] 66%|██████▋   | 711/1070 [00:49<00:22, 16.12it/s] 67%|██████▋   | 713/1070 [00:49<00:22, 16.12it/s] 67%|██████▋   | 715/1070 [00:49<00:22, 16.09it/s] 67%|██████▋   | 717/1070 [00:49<00:21, 16.08it/s] 67%|██████▋   | 719/1070 [00:49<00:21, 16.05it/s] 67%|██████▋   | 721/1070 [00:49<00:21, 15.99it/s] 68%|██████▊   | 723/1070 [00:49<00:21, 15.87it/s] 68%|██████▊   | 725/1070 [00:49<00:21, 15.83it/s] 68%|██████▊   | 727/1070 [00:50<00:21, 15.74it/s] 68%|██████▊   | 729/1070 [00:50<00:21, 15.75it/s] 68%|██████▊   | 731/1070 [00:50<00:21, 15.74it/s] 69%|██████▊   | 733/1070 [00:50<00:21, 15.79it/s] 69%|██████▊   | 735/1070 [00:50<00:21, 15.76it/s] 69%|██████▉   | 737/1070 [00:50<00:21, 15.80it/s] 69%|██████▉   | 739/1070 [00:50<00:20, 15.81it/s] 69%|██████▉   | 741/1070 [00:50<00:20, 15.83it/s] 69%|██████▉   | 743/1070 [00:51<00:20, 15.88it/s] 70%|██████▉   | 745/1070 [00:51<00:20, 15.88it/s] 70%|██████▉   | 747/1070 [00:51<00:20, 15.88it/s] 70%|███████   | 749/1070 [00:51<00:20, 15.88it/s] 70%|███████   | 751/1070 [00:51<00:20, 15.84it/s] 70%|███████   | 753/1070 [00:51<00:20, 15.84it/s] 71%|███████   | 755/1070 [00:51<00:19, 15.86it/s] 71%|███████   | 757/1070 [00:51<00:19, 15.80it/s] 71%|███████   | 759/1070 [00:52<00:19, 15.65it/s] 71%|███████   | 761/1070 [00:52<00:19, 15.52it/s] 71%|███████▏  | 763/1070 [00:52<00:19, 15.41it/s] 71%|███████▏  | 765/1070 [00:52<00:19, 15.47it/s] 72%|███████▏  | 767/1070 [00:52<00:19, 15.43it/s] 72%|███████▏  | 769/1070 [00:52<00:19, 15.54it/s] 72%|███████▏  | 771/1070 [00:52<00:19, 15.69it/s] 72%|███████▏  | 773/1070 [00:52<00:18, 15.80it/s] 72%|███████▏  | 775/1070 [00:53<00:18, 15.87it/s] 73%|███████▎  | 777/1070 [00:53<00:18, 15.90it/s] 73%|███████▎  | 779/1070 [00:53<00:18, 15.93it/s] 73%|███████▎  | 781/1070 [00:53<00:18, 15.96it/s] 73%|███████▎  | 783/1070 [00:53<00:17, 15.98it/s] 73%|███████▎  | 785/1070 [00:53<00:17, 15.99it/s] 74%|███████▎  | 787/1070 [00:53<00:17, 16.01it/s] 74%|███████▎  | 789/1070 [00:53<00:17, 16.00it/s] 74%|███████▍  | 791/1070 [00:54<00:17, 15.98it/s] 74%|███████▍  | 793/1070 [00:54<00:17, 15.99it/s] 74%|███████▍  | 795/1070 [00:54<00:17, 16.00it/s] 74%|███████▍  | 797/1070 [00:54<00:17, 16.01it/s] 75%|███████▍  | 799/1070 [00:54<00:16, 16.02it/s] 75%|███████▍  | 801/1070 [00:54<00:16, 16.06it/s] 75%|███████▌  | 803/1070 [00:54<00:16, 16.08it/s] 75%|███████▌  | 805/1070 [00:54<00:16, 16.04it/s] 75%|███████▌  | 807/1070 [00:55<00:16, 16.03it/s] 76%|███████▌  | 809/1070 [00:55<00:16, 16.02it/s] 76%|███████▌  | 811/1070 [00:55<00:16, 15.96it/s] 76%|███████▌  | 813/1070 [00:55<00:16, 15.88it/s] 76%|███████▌  | 815/1070 [00:55<00:16, 15.85it/s] 76%|███████▋  | 817/1070 [00:55<00:16, 15.79it/s] 77%|███████▋  | 819/1070 [00:55<00:15, 15.79it/s] 77%|███████▋  | 821/1070 [00:56<00:15, 15.81it/s] 77%|███████▋  | 823/1070 [00:56<00:15, 15.80it/s] 77%|███████▋  | 825/1070 [00:56<00:15, 15.85it/s] 77%|███████▋  | 827/1070 [00:56<00:15, 15.83it/s] 77%|███████▋  | 829/1070 [00:56<00:15, 15.82it/s] 78%|███████▊  | 831/1070 [00:56<00:15, 15.84it/s] 78%|███████▊  | 833/1070 [00:56<00:14, 15.81it/s] 78%|███████▊  | 835/1070 [00:56<00:14, 15.89it/s] 78%|███████▊  | 837/1070 [00:57<00:14, 15.86it/s] 78%|███████▊  | 839/1070 [00:57<00:14, 15.85it/s] 79%|███████▊  | 841/1070 [00:57<00:14, 15.85it/s] 79%|███████▉  | 843/1070 [00:57<00:14, 15.84it/s] 79%|███████▉  | 845/1070 [00:57<00:14, 15.87it/s] 79%|███████▉  | 847/1070 [00:57<00:14, 15.91it/s] 79%|███████▉  | 849/1070 [00:57<00:13, 15.96it/s] 80%|███████▉  | 851/1070 [00:57<00:13, 15.86it/s] 80%|███████▉  | 853/1070 [00:58<00:13, 15.81it/s] 80%|███████▉  | 855/1070 [00:58<00:13, 15.80it/s]                                                   80%|████████  | 856/1070 [00:58<00:13, 15.80it/s][INFO|trainer.py:755] 2023-11-15 23:34:07,165 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 23:34:07,167 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 23:34:07,167 >>   Num examples = 760
[INFO|trainer.py:3134] 2023-11-15 23:34:07,167 >>   Batch size = 8
{'eval_loss': 0.29833826422691345, 'eval_accuracy': 0.9105263157894737, 'eval_micro_f1': 0.9105263157894739, 'eval_macro_f1': 0.9081868926292157, 'eval_runtime': 1.1552, 'eval_samples_per_second': 657.884, 'eval_steps_per_second': 82.235, 'epoch': 3.0}
{'loss': 0.1642, 'learning_rate': 0.0001, 'epoch': 4.0}

  0%|          | 0/95 [00:00<?, ?it/s][A
  9%|▉         | 9/95 [00:00<00:00, 86.55it/s][A
 19%|█▉        | 18/95 [00:00<00:00, 82.48it/s][A
 28%|██▊       | 27/95 [00:00<00:00, 84.71it/s][A
 38%|███▊      | 36/95 [00:00<00:00, 85.98it/s][A
 47%|████▋     | 45/95 [00:00<00:00, 86.34it/s][A
 57%|█████▋    | 54/95 [00:00<00:00, 87.12it/s][A
 66%|██████▋   | 63/95 [00:00<00:00, 87.66it/s][A
 76%|███████▌  | 72/95 [00:00<00:00, 87.98it/s][A
 85%|████████▌ | 81/95 [00:00<00:00, 88.12it/s][A
 95%|█████████▍| 90/95 [00:01<00:00, 88.18it/s][A                                                  
                                               [A 80%|████████  | 856/1070 [00:59<00:13, 15.80it/s]
100%|██████████| 95/95 [00:01<00:00, 88.18it/s][A
                                               [A 80%|████████  | 857/1070 [00:59<00:49,  4.30it/s] 80%|████████  | 859/1070 [00:59<00:38,  5.51it/s] 80%|████████  | 861/1070 [00:59<00:30,  6.86it/s] 81%|████████  | 863/1070 [00:59<00:24,  8.28it/s] 81%|████████  | 865/1070 [00:59<00:21,  9.69it/s] 81%|████████  | 867/1070 [01:00<00:18, 10.99it/s] 81%|████████  | 869/1070 [01:00<00:16, 12.11it/s] 81%|████████▏ | 871/1070 [01:00<00:15, 13.07it/s] 82%|████████▏ | 873/1070 [01:00<00:14, 13.86it/s] 82%|████████▏ | 875/1070 [01:00<00:13, 14.47it/s] 82%|████████▏ | 877/1070 [01:00<00:12, 14.87it/s] 82%|████████▏ | 879/1070 [01:00<00:12, 15.20it/s] 82%|████████▏ | 881/1070 [01:00<00:12, 15.45it/s] 83%|████████▎ | 883/1070 [01:01<00:12, 15.56it/s] 83%|████████▎ | 885/1070 [01:01<00:11, 15.60it/s] 83%|████████▎ | 887/1070 [01:01<00:11, 15.62it/s] 83%|████████▎ | 889/1070 [01:01<00:11, 15.63it/s] 83%|████████▎ | 891/1070 [01:01<00:11, 15.68it/s] 83%|████████▎ | 893/1070 [01:01<00:11, 15.75it/s] 84%|████████▎ | 895/1070 [01:01<00:11, 15.78it/s] 84%|████████▍ | 897/1070 [01:01<00:10, 15.81it/s] 84%|████████▍ | 899/1070 [01:02<00:10, 15.84it/s] 84%|████████▍ | 901/1070 [01:02<00:10, 15.83it/s] 84%|████████▍ | 903/1070 [01:02<00:10, 15.82it/s] 85%|████████▍ | 905/1070 [01:02<00:10, 15.83it/s] 85%|████████▍ | 907/1070 [01:02<00:10, 15.88it/s] 85%|████████▍ | 909/1070 [01:02<00:10, 15.86it/s] 85%|████████▌ | 911/1070 [01:02<00:10, 15.85it/s] 85%|████████▌ | 913/1070 [01:02<00:09, 15.84it/s] 86%|████████▌ | 915/1070 [01:03<00:09, 15.83it/s] 86%|████████▌ | 917/1070 [01:03<00:09, 15.86it/s] 86%|████████▌ | 919/1070 [01:03<00:09, 15.93it/s] 86%|████████▌ | 921/1070 [01:03<00:09, 15.94it/s] 86%|████████▋ | 923/1070 [01:03<00:09, 15.84it/s] 86%|████████▋ | 925/1070 [01:03<00:09, 15.79it/s] 87%|████████▋ | 927/1070 [01:03<00:09, 15.67it/s] 87%|████████▋ | 929/1070 [01:03<00:09, 15.62it/s] 87%|████████▋ | 931/1070 [01:04<00:08, 15.62it/s] 87%|████████▋ | 933/1070 [01:04<00:08, 15.70it/s] 87%|████████▋ | 935/1070 [01:04<00:08, 15.81it/s] 88%|████████▊ | 937/1070 [01:04<00:08, 15.92it/s] 88%|████████▊ | 939/1070 [01:04<00:08, 15.97it/s] 88%|████████▊ | 941/1070 [01:04<00:08, 15.98it/s] 88%|████████▊ | 943/1070 [01:04<00:07, 15.98it/s] 88%|████████▊ | 945/1070 [01:04<00:07, 16.04it/s] 89%|████████▊ | 947/1070 [01:05<00:07, 16.04it/s] 89%|████████▊ | 949/1070 [01:05<00:07, 16.01it/s] 89%|████████▉ | 951/1070 [01:05<00:07, 15.98it/s] 89%|████████▉ | 953/1070 [01:05<00:07, 16.03it/s] 89%|████████▉ | 955/1070 [01:05<00:07, 16.06it/s] 89%|████████▉ | 957/1070 [01:05<00:07, 16.04it/s] 90%|████████▉ | 959/1070 [01:05<00:06, 15.97it/s] 90%|████████▉ | 961/1070 [01:05<00:06, 16.02it/s] 90%|█████████ | 963/1070 [01:06<00:06, 16.05it/s] 90%|█████████ | 965/1070 [01:06<00:06, 16.05it/s] 90%|█████████ | 967/1070 [01:06<00:06, 16.01it/s] 91%|█████████ | 969/1070 [01:06<00:06, 16.02it/s] 91%|█████████ | 971/1070 [01:06<00:06, 15.96it/s] 91%|█████████ | 973/1070 [01:06<00:06, 15.86it/s] 91%|█████████ | 975/1070 [01:06<00:06, 15.79it/s] 91%|█████████▏| 977/1070 [01:06<00:05, 15.74it/s] 91%|█████████▏| 979/1070 [01:07<00:05, 15.70it/s] 92%|█████████▏| 981/1070 [01:07<00:05, 15.75it/s] 92%|█████████▏| 983/1070 [01:07<00:05, 15.77it/s] 92%|█████████▏| 985/1070 [01:07<00:05, 15.77it/s] 92%|█████████▏| 987/1070 [01:07<00:05, 15.76it/s] 92%|█████████▏| 989/1070 [01:07<00:05, 15.77it/s] 93%|█████████▎| 991/1070 [01:07<00:05, 15.74it/s] 93%|█████████▎| 993/1070 [01:07<00:04, 15.71it/s] 93%|█████████▎| 995/1070 [01:08<00:04, 15.70it/s] 93%|█████████▎| 997/1070 [01:08<00:04, 15.71it/s] 93%|█████████▎| 999/1070 [01:08<00:04, 15.76it/s] 94%|█████████▎| 1001/1070 [01:08<00:04, 15.82it/s] 94%|█████████▎| 1003/1070 [01:08<00:04, 15.82it/s] 94%|█████████▍| 1005/1070 [01:08<00:04, 15.83it/s] 94%|█████████▍| 1007/1070 [01:08<00:03, 15.77it/s] 94%|█████████▍| 1009/1070 [01:08<00:03, 15.75it/s] 94%|█████████▍| 1011/1070 [01:09<00:03, 15.70it/s] 95%|█████████▍| 1013/1070 [01:09<00:03, 15.45it/s] 95%|█████████▍| 1015/1070 [01:09<00:03, 15.41it/s] 95%|█████████▌| 1017/1070 [01:09<00:03, 15.48it/s] 95%|█████████▌| 1019/1070 [01:09<00:03, 15.53it/s] 95%|█████████▌| 1021/1070 [01:09<00:03, 15.61it/s] 96%|█████████▌| 1023/1070 [01:09<00:02, 15.74it/s] 96%|█████████▌| 1025/1070 [01:10<00:02, 15.84it/s] 96%|█████████▌| 1027/1070 [01:10<00:02, 15.90it/s] 96%|█████████▌| 1029/1070 [01:10<00:02, 15.93it/s] 96%|█████████▋| 1031/1070 [01:10<00:02, 15.99it/s] 97%|█████████▋| 1033/1070 [01:10<00:02, 16.01it/s] 97%|█████████▋| 1035/1070 [01:10<00:02, 16.02it/s] 97%|█████████▋| 1037/1070 [01:10<00:02, 16.00it/s] 97%|█████████▋| 1039/1070 [01:10<00:01, 16.02it/s] 97%|█████████▋| 1041/1070 [01:11<00:01, 16.05it/s] 97%|█████████▋| 1043/1070 [01:11<00:01, 16.06it/s] 98%|█████████▊| 1045/1070 [01:11<00:01, 16.03it/s] 98%|█████████▊| 1047/1070 [01:11<00:01, 16.03it/s] 98%|█████████▊| 1049/1070 [01:11<00:01, 16.06it/s] 98%|█████████▊| 1051/1070 [01:11<00:01, 16.07it/s] 98%|█████████▊| 1053/1070 [01:11<00:01, 16.04it/s] 99%|█████████▊| 1055/1070 [01:11<00:00, 16.02it/s] 99%|█████████▉| 1057/1070 [01:11<00:00, 16.03it/s] 99%|█████████▉| 1059/1070 [01:12<00:00, 16.03it/s] 99%|█████████▉| 1061/1070 [01:12<00:00, 15.90it/s] 99%|█████████▉| 1063/1070 [01:12<00:00, 15.86it/s]100%|█████████▉| 1065/1070 [01:12<00:00, 15.83it/s]100%|█████████▉| 1067/1070 [01:12<00:00, 15.77it/s]100%|█████████▉| 1069/1070 [01:12<00:00, 15.80it/s]                                                   100%|██████████| 1070/1070 [01:12<00:00, 15.80it/s][INFO|trainer.py:755] 2023-11-15 23:34:21,775 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 23:34:21,776 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 23:34:21,776 >>   Num examples = 760
[INFO|trainer.py:3134] 2023-11-15 23:34:21,777 >>   Batch size = 8
{'eval_loss': 0.29988977313041687, 'eval_accuracy': 0.9105263157894737, 'eval_micro_f1': 0.9105263157894739, 'eval_macro_f1': 0.9083621346202202, 'eval_runtime': 1.1364, 'eval_samples_per_second': 668.785, 'eval_steps_per_second': 83.598, 'epoch': 4.0}
{'loss': 0.1308, 'learning_rate': 0.0, 'epoch': 5.0}

  0%|          | 0/95 [00:00<?, ?it/s][A
 11%|█         | 10/95 [00:00<00:00, 92.30it/s][A
 21%|██        | 20/95 [00:00<00:00, 88.48it/s][A
 31%|███       | 29/95 [00:00<00:00, 87.54it/s][A
 40%|████      | 38/95 [00:00<00:00, 86.30it/s][A
 49%|████▉     | 47/95 [00:00<00:00, 86.51it/s][A
 59%|█████▉    | 56/95 [00:00<00:00, 85.56it/s][A
 68%|██████▊   | 65/95 [00:00<00:00, 86.38it/s][A
 78%|███████▊  | 74/95 [00:00<00:00, 85.77it/s][A
 87%|████████▋ | 83/95 [00:00<00:00, 85.86it/s][A
 97%|█████████▋| 92/95 [00:01<00:00, 85.30it/s][A                                                   
                                               [A100%|██████████| 1070/1070 [01:13<00:00, 15.80it/s]
100%|██████████| 95/95 [00:01<00:00, 85.30it/s][A
                                               [A[INFO|trainer.py:1963] 2023-11-15 23:34:22,938 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                   100%|██████████| 1070/1070 [01:13<00:00, 15.80it/s]100%|██████████| 1070/1070 [01:13<00:00, 14.46it/s]
[INFO|trainer.py:2855] 2023-11-15 23:34:22,941 >> Saving model checkpoint to ./result/agnews_sup_roberta-base_seed2_adapter
[INFO|configuration_utils.py:460] 2023-11-15 23:34:22,944 >> Configuration saved in ./result/agnews_sup_roberta-base_seed2_adapter/config.json
[INFO|modeling_utils.py:1997] 2023-11-15 23:34:24,100 >> Model weights saved in ./result/agnews_sup_roberta-base_seed2_adapter/pytorch_model.bin
[INFO|tokenization_utils_base.py:2235] 2023-11-15 23:34:24,104 >> tokenizer config file saved in ./result/agnews_sup_roberta-base_seed2_adapter/tokenizer_config.json
[INFO|tokenization_utils_base.py:2242] 2023-11-15 23:34:24,106 >> Special tokens file saved in ./result/agnews_sup_roberta-base_seed2_adapter/special_tokens_map.json
{'eval_loss': 0.3219882547855377, 'eval_accuracy': 0.9118421052631579, 'eval_micro_f1': 0.9118421052631579, 'eval_macro_f1': 0.9092710787484067, 'eval_runtime': 1.1448, 'eval_samples_per_second': 663.9, 'eval_steps_per_second': 82.988, 'epoch': 5.0}
{'train_runtime': 73.9766, 'train_samples_per_second': 462.308, 'train_steps_per_second': 14.464, 'train_loss': 0.23855500622330425, 'epoch': 5.0}
***** train metrics *****
  epoch                    =        5.0
  train_loss               =     0.2386
  train_runtime            = 0:01:13.97
  train_samples            =       6840
  train_samples_per_second =    462.308
  train_steps_per_second   =     14.464
11/15/2023 23:34:24 - INFO - __main__ - *** Evaluate ***
[INFO|trainer.py:755] 2023-11-15 23:34:24,266 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 23:34:24,267 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 23:34:24,268 >>   Num examples = 760
[INFO|trainer.py:3134] 2023-11-15 23:34:24,268 >>   Batch size = 8
  0%|          | 0/95 [00:00<?, ?it/s] 11%|█         | 10/95 [00:00<00:00, 93.75it/s] 21%|██        | 20/95 [00:00<00:00, 90.85it/s] 32%|███▏      | 30/95 [00:00<00:00, 90.20it/s] 42%|████▏     | 40/95 [00:00<00:00, 89.27it/s] 52%|█████▏    | 49/95 [00:00<00:00, 88.59it/s] 61%|██████    | 58/95 [00:00<00:00, 88.71it/s] 71%|███████   | 67/95 [00:00<00:00, 89.07it/s] 80%|████████  | 76/95 [00:00<00:00, 89.04it/s] 89%|████████▉ | 85/95 [00:00<00:00, 89.23it/s] 99%|█████████▉| 94/95 [00:01<00:00, 89.43it/s]100%|██████████| 95/95 [00:01<00:00, 87.38it/s]
***** eval metrics *****
  epoch                   =        5.0
  eval_accuracy           =     0.9118
  eval_loss               =      0.322
  eval_macro_f1           =     0.9093
  eval_micro_f1           =     0.9118
  eval_runtime            = 0:00:01.10
  eval_samples            =        760
  eval_samples_per_second =    687.263
  eval_steps_per_second   =     85.908
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                  eval/accuracy ▁█▁▁██
wandb:                      eval/loss ▄▁▄▅██
wandb:                  eval/macro_f1 ▁▇▄▅██
wandb:                  eval/micro_f1 ▁█▁▁██
wandb:                   eval/runtime ▄█▄▃▄▁
wandb:        eval/samples_per_second ▅▁▅▆▅█
wandb:          eval/steps_per_second ▅▁▅▆▅█
wandb:                    train/epoch ▁▁▃▃▅▅▆▆████
wandb:              train/global_step ▁▁▃▃▄▄▆▆████
wandb:            train/learning_rate █▆▅▃▁
wandb:                     train/loss █▄▃▂▁
wandb:               train/total_flos ▁
wandb:               train/train_loss ▁
wandb:            train/train_runtime ▁
wandb: train/train_samples_per_second ▁
wandb:   train/train_steps_per_second ▁
wandb: 
wandb: Run summary:
wandb:                  eval/accuracy 0.91184
wandb:                      eval/loss 0.32199
wandb:                  eval/macro_f1 0.90927
wandb:                  eval/micro_f1 0.91184
wandb:                   eval/runtime 1.1058
wandb:        eval/samples_per_second 687.263
wandb:          eval/steps_per_second 85.908
wandb:                    train/epoch 5.0
wandb:              train/global_step 1070
wandb:            train/learning_rate 0.0
wandb:                     train/loss 0.1308
wandb:               train/total_flos 1136567617228800.0
wandb:               train/train_loss 0.23856
wandb:            train/train_runtime 73.9766
wandb: train/train_samples_per_second 462.308
wandb:   train/train_steps_per_second 14.464
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /ceph/home/wangyifei/a3/wandb/offline-run-20231115_233151-i2pdhdmr
wandb: Find logs at: ./wandb/offline-run-20231115_233151-i2pdhdmr/logs
(ModelArguments(model_name_or_path='roberta-base', cache_dir='./cache', use_adapter=True, use_lora=False), DataTrainingArguments(dataset_name='restaurant', max_seq_length=64), TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.0005,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=./result/restaurant_roberta-base_seed3_adapter/runs/Nov15_23-34-35_node013,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.EPOCH,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=5.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
output_dir=./result/restaurant_roberta-base_seed3_adapter,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
resume_from_checkpoint=None,
run_name=./result/restaurant_roberta-base_seed3_adapter,
save_on_each_node=False,
save_safetensors=False,
save_steps=500,
save_strategy=IntervalStrategy.NO,
save_total_limit=None,
seed=444,
sharded_ddp=[],
skip_memory_metrics=True,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
))
11/15/2023 23:34:35 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False
11/15/2023 23:34:35 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.0005,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=./result/restaurant_roberta-base_seed3_adapter/runs/Nov15_23-34-34_node013,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.EPOCH,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=5.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
output_dir=./result/restaurant_roberta-base_seed3_adapter,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
resume_from_checkpoint=None,
run_name=./result/restaurant_roberta-base_seed3_adapter,
save_on_each_node=False,
save_safetensors=False,
save_steps=500,
save_strategy=IntervalStrategy.NO,
save_total_limit=None,
seed=444,
sharded_ddp=[],
skip_memory_metrics=True,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
wandb: Tracking run with wandb version 0.15.12
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
Map:   0%|          | 0/4722 [00:00<?, ? examples/s]Map:  85%|████████▍ | 4000/4722 [00:00<00:00, 39346.84 examples/s]Map: 100%|██████████| 4722/4722 [00:00<00:00, 38448.63 examples/s]
[INFO|configuration_utils.py:715] 2023-11-15 23:34:50,777 >> loading configuration file config.json from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/config.json
[INFO|configuration_utils.py:775] 2023-11-15 23:34:50,786 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.33.3",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_auto.py:535] 2023-11-15 23:35:00,803 >> Could not locate the tokenizer configuration file, will try to use the model config instead.
[INFO|configuration_utils.py:715] 2023-11-15 23:35:10,819 >> loading configuration file config.json from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/config.json
[INFO|configuration_utils.py:775] 2023-11-15 23:35:10,820 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.33.3",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:1852] 2023-11-15 23:35:30,867 >> loading file vocab.json from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/vocab.json
[INFO|tokenization_utils_base.py:1852] 2023-11-15 23:35:30,868 >> loading file merges.txt from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/merges.txt
[INFO|tokenization_utils_base.py:1852] 2023-11-15 23:35:30,868 >> loading file tokenizer.json from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/tokenizer.json
[INFO|tokenization_utils_base.py:1852] 2023-11-15 23:35:30,868 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1852] 2023-11-15 23:35:30,868 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1852] 2023-11-15 23:35:30,869 >> loading file tokenizer_config.json from cache at None
[INFO|configuration_utils.py:715] 2023-11-15 23:35:30,870 >> loading configuration file config.json from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/config.json
[INFO|configuration_utils.py:775] 2023-11-15 23:35:30,870 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.33.3",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:2862] 2023-11-15 23:35:51,032 >> loading weights file model.safetensors from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/model.safetensors
[INFO|modeling_utils.py:3638] 2023-11-15 23:35:51,744 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3650] 2023-11-15 23:35:51,745 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
parameters 1487427
model_max_length: 512
Running tokenizer on dataset:   0%|          | 0/3777 [00:00<?, ? examples/s]Running tokenizer on dataset: 100%|██████████| 3777/3777 [00:00<00:00, 26513.57 examples/s]Running tokenizer on dataset: 100%|██████████| 3777/3777 [00:00<00:00, 26173.08 examples/s]
Running tokenizer on dataset:   0%|          | 0/945 [00:00<?, ? examples/s]Running tokenizer on dataset: 100%|██████████| 945/945 [00:00<00:00, 29247.47 examples/s]
11/15/2023 23:35:52 - INFO - __main__ - Sample 1265 of the training set: {'text': 'cuisine <SEP> I love when restaurants think using fancy expensive ingrediants makes the food fine cuisine, even with no idea how to use them.', 'label': 0, 'input_ids': [0, 16312, 40116, 28696, 3388, 510, 15698, 38, 657, 77, 4329, 206, 634, 13185, 3214, 49567, 40679, 817, 5, 689, 2051, 18196, 6, 190, 19, 117, 1114, 141, 7, 304, 106, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
11/15/2023 23:35:52 - INFO - __main__ - Sample 1178 of the training set: {'text': "table <SEP> I went with 5 friends and we lingered at the table for a bit and didn't feel rushed at all even though there was a wait.", 'label': 1, 'input_ids': [0, 14595, 28696, 3388, 510, 15698, 38, 439, 19, 195, 964, 8, 52, 24433, 3215, 23, 5, 2103, 13, 10, 828, 8, 399, 75, 619, 6022, 23, 70, 190, 600, 89, 21, 10, 2067, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
11/15/2023 23:35:52 - INFO - __main__ - Sample 54 of the training set: {'text': "Halibut <SEP> The Halibut was too salty, dessert was so so (don't waste any of your calories) and service was poor.", 'label': 2, 'input_ids': [0, 40306, 1452, 1182, 28696, 3388, 510, 15698, 20, 6579, 1452, 1182, 21, 350, 31924, 6, 17927, 21, 98, 98, 36, 7254, 75, 3844, 143, 9, 110, 12951, 43, 8, 544, 21, 2129, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
11/15/2023 23:35:52 - WARNING - accelerate.utils.other - Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[INFO|trainer.py:755] 2023-11-15 23:35:53,892 >> The following columns in the training set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:1715] 2023-11-15 23:35:53,902 >> ***** Running training *****
[INFO|trainer.py:1716] 2023-11-15 23:35:53,902 >>   Num examples = 3,777
[INFO|trainer.py:1717] 2023-11-15 23:35:53,903 >>   Num Epochs = 5
[INFO|trainer.py:1718] 2023-11-15 23:35:53,903 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1721] 2023-11-15 23:35:53,903 >>   Total train batch size (w. parallel, distributed & accumulation) = 32
[INFO|trainer.py:1722] 2023-11-15 23:35:53,903 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:1723] 2023-11-15 23:35:53,904 >>   Total optimization steps = 595
[INFO|trainer.py:1724] 2023-11-15 23:35:53,905 >>   Number of trainable parameters = 1,487,427
[INFO|integration_utils.py:716] 2023-11-15 23:35:53,906 >> Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"
  0%|          | 0/595 [00:00<?, ?it/s]  0%|          | 1/595 [00:01<09:57,  1.01s/it]  1%|          | 3/595 [00:01<03:01,  3.25it/s]  1%|          | 5/595 [00:01<01:47,  5.49it/s]  1%|          | 7/595 [00:01<01:17,  7.59it/s]  2%|▏         | 9/595 [00:01<01:01,  9.46it/s]  2%|▏         | 11/595 [00:01<00:52, 11.04it/s]  2%|▏         | 13/595 [00:01<00:47, 12.30it/s]  3%|▎         | 15/595 [00:01<00:43, 13.31it/s]  3%|▎         | 17/595 [00:02<00:40, 14.10it/s]  3%|▎         | 19/595 [00:02<00:39, 14.64it/s]  4%|▎         | 21/595 [00:02<00:38, 14.98it/s]  4%|▍         | 23/595 [00:02<00:37, 15.26it/s]  4%|▍         | 25/595 [00:02<00:36, 15.46it/s]  5%|▍         | 27/595 [00:02<00:36, 15.56it/s]  5%|▍         | 29/595 [00:02<00:36, 15.68it/s]  5%|▌         | 31/595 [00:02<00:35, 15.78it/s]  6%|▌         | 33/595 [00:03<00:35, 15.75it/s]  6%|▌         | 35/595 [00:03<00:35, 15.76it/s]  6%|▌         | 37/595 [00:03<00:35, 15.85it/s]  7%|▋         | 39/595 [00:03<00:35, 15.87it/s]  7%|▋         | 41/595 [00:03<00:34, 15.86it/s]  7%|▋         | 43/595 [00:03<00:34, 15.88it/s]  8%|▊         | 45/595 [00:03<00:34, 15.85it/s]  8%|▊         | 47/595 [00:03<00:34, 15.77it/s]  8%|▊         | 49/595 [00:04<00:34, 15.79it/s]  9%|▊         | 51/595 [00:04<00:34, 15.73it/s]  9%|▉         | 53/595 [00:04<00:34, 15.51it/s]  9%|▉         | 55/595 [00:04<00:34, 15.46it/s] 10%|▉         | 57/595 [00:04<00:34, 15.60it/s] 10%|▉         | 59/595 [00:04<00:34, 15.75it/s] 10%|█         | 61/595 [00:04<00:33, 15.83it/s] 11%|█         | 63/595 [00:04<00:33, 15.86it/s] 11%|█         | 65/595 [00:05<00:33, 15.92it/s] 11%|█▏        | 67/595 [00:05<00:33, 15.97it/s] 12%|█▏        | 69/595 [00:05<00:32, 16.04it/s] 12%|█▏        | 71/595 [00:05<00:32, 16.06it/s] 12%|█▏        | 73/595 [00:05<00:32, 16.03it/s] 13%|█▎        | 75/595 [00:05<00:32, 16.08it/s] 13%|█▎        | 77/595 [00:05<00:32, 16.09it/s] 13%|█▎        | 79/595 [00:05<00:32, 16.01it/s] 14%|█▎        | 81/595 [00:06<00:32, 15.95it/s] 14%|█▍        | 83/595 [00:06<00:32, 15.96it/s] 14%|█▍        | 85/595 [00:06<00:31, 15.95it/s] 15%|█▍        | 87/595 [00:06<00:31, 15.97it/s] 15%|█▍        | 89/595 [00:06<00:31, 15.95it/s] 15%|█▌        | 91/595 [00:06<00:31, 15.89it/s] 16%|█▌        | 93/595 [00:06<00:31, 15.77it/s] 16%|█▌        | 95/595 [00:06<00:31, 15.73it/s] 16%|█▋        | 97/595 [00:07<00:31, 15.76it/s] 17%|█▋        | 99/595 [00:07<00:31, 15.67it/s] 17%|█▋        | 101/595 [00:07<00:31, 15.62it/s] 17%|█▋        | 103/595 [00:07<00:32, 15.34it/s] 18%|█▊        | 105/595 [00:07<00:32, 15.07it/s] 18%|█▊        | 107/595 [00:07<00:31, 15.30it/s] 18%|█▊        | 109/595 [00:07<00:31, 15.38it/s] 19%|█▊        | 111/595 [00:07<00:31, 15.54it/s] 19%|█▉        | 113/595 [00:08<00:30, 15.56it/s] 19%|█▉        | 115/595 [00:08<00:30, 15.58it/s] 20%|█▉        | 117/595 [00:08<00:30, 15.73it/s]                                                  20%|██        | 119/595 [00:08<00:30, 15.73it/s][INFO|trainer.py:755] 2023-11-15 23:36:02,350 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 23:36:02,352 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 23:36:02,352 >>   Num examples = 945
[INFO|trainer.py:3134] 2023-11-15 23:36:02,352 >>   Batch size = 8
{'loss': 0.7336, 'learning_rate': 0.0004, 'epoch': 1.0}

  0%|          | 0/119 [00:00<?, ?it/s][A
  8%|▊         | 10/119 [00:00<00:01, 93.07it/s][A
 17%|█▋        | 20/119 [00:00<00:01, 89.01it/s][A
 24%|██▍       | 29/119 [00:00<00:01, 76.72it/s][A
 31%|███       | 37/119 [00:00<00:01, 70.65it/s][A
 38%|███▊      | 45/119 [00:00<00:01, 73.37it/s][A
 45%|████▌     | 54/119 [00:00<00:00, 76.60it/s][A
 53%|█████▎    | 63/119 [00:00<00:00, 78.44it/s][A
 60%|█████▉    | 71/119 [00:00<00:00, 76.37it/s][A
 66%|██████▋   | 79/119 [00:01<00:00, 71.47it/s][A
 73%|███████▎  | 87/119 [00:01<00:00, 73.32it/s][A
 81%|████████  | 96/119 [00:01<00:00, 76.21it/s][A
 88%|████████▊ | 105/119 [00:01<00:00, 77.75it/s][A
 95%|█████████▍| 113/119 [00:01<00:00, 77.84it/s][A                                                 
                                                 [A 20%|██        | 119/595 [00:10<00:30, 15.73it/s]
100%|██████████| 119/119 [00:01<00:00, 77.84it/s][A
                                                 [A 20%|██        | 120/595 [00:10<02:08,  3.70it/s] 21%|██        | 122/595 [00:10<01:41,  4.66it/s] 21%|██        | 124/595 [00:10<01:21,  5.80it/s] 21%|██        | 126/595 [00:10<01:06,  7.09it/s] 22%|██▏       | 128/595 [00:10<00:55,  8.46it/s] 22%|██▏       | 130/595 [00:10<00:47,  9.83it/s] 22%|██▏       | 132/595 [00:10<00:41, 11.08it/s] 23%|██▎       | 134/595 [00:11<00:37, 12.18it/s] 23%|██▎       | 136/595 [00:11<00:34, 13.15it/s] 23%|██▎       | 138/595 [00:11<00:32, 13.88it/s] 24%|██▎       | 140/595 [00:11<00:31, 14.47it/s] 24%|██▍       | 142/595 [00:11<00:30, 14.92it/s] 24%|██▍       | 144/595 [00:11<00:29, 15.26it/s] 25%|██▍       | 146/595 [00:11<00:29, 15.46it/s] 25%|██▍       | 148/595 [00:11<00:28, 15.59it/s] 25%|██▌       | 150/595 [00:12<00:28, 15.69it/s] 26%|██▌       | 152/595 [00:12<00:28, 15.79it/s] 26%|██▌       | 154/595 [00:12<00:27, 15.84it/s] 26%|██▌       | 156/595 [00:12<00:27, 15.91it/s] 27%|██▋       | 158/595 [00:12<00:27, 15.99it/s] 27%|██▋       | 160/595 [00:12<00:27, 16.08it/s] 27%|██▋       | 162/595 [00:12<00:26, 16.07it/s] 28%|██▊       | 164/595 [00:12<00:27, 15.94it/s] 28%|██▊       | 166/595 [00:13<00:26, 15.90it/s] 28%|██▊       | 168/595 [00:13<00:26, 15.91it/s] 29%|██▊       | 170/595 [00:13<00:26, 15.87it/s] 29%|██▉       | 172/595 [00:13<00:26, 15.82it/s] 29%|██▉       | 174/595 [00:13<00:26, 15.84it/s] 30%|██▉       | 176/595 [00:13<00:26, 15.87it/s] 30%|██▉       | 178/595 [00:13<00:26, 15.84it/s] 30%|███       | 180/595 [00:13<00:26, 15.86it/s] 31%|███       | 182/595 [00:14<00:26, 15.88it/s] 31%|███       | 184/595 [00:14<00:25, 15.86it/s] 31%|███▏      | 186/595 [00:14<00:25, 15.86it/s] 32%|███▏      | 188/595 [00:14<00:25, 15.96it/s] 32%|███▏      | 190/595 [00:14<00:25, 15.98it/s] 32%|███▏      | 192/595 [00:14<00:25, 15.99it/s] 33%|███▎      | 194/595 [00:14<00:25, 15.94it/s] 33%|███▎      | 196/595 [00:14<00:24, 15.99it/s] 33%|███▎      | 198/595 [00:15<00:24, 15.96it/s] 34%|███▎      | 200/595 [00:15<00:24, 15.98it/s] 34%|███▍      | 202/595 [00:15<00:24, 15.89it/s] 34%|███▍      | 204/595 [00:15<00:24, 15.96it/s] 35%|███▍      | 206/595 [00:15<00:24, 15.89it/s] 35%|███▍      | 208/595 [00:15<00:24, 15.69it/s] 35%|███▌      | 210/595 [00:15<00:24, 15.53it/s] 36%|███▌      | 212/595 [00:15<00:24, 15.51it/s] 36%|███▌      | 214/595 [00:16<00:24, 15.58it/s] 36%|███▋      | 216/595 [00:16<00:24, 15.70it/s] 37%|███▋      | 218/595 [00:16<00:23, 15.88it/s] 37%|███▋      | 220/595 [00:16<00:23, 16.00it/s] 37%|███▋      | 222/595 [00:16<00:23, 16.07it/s] 38%|███▊      | 224/595 [00:16<00:23, 16.06it/s] 38%|███▊      | 226/595 [00:16<00:22, 16.05it/s] 38%|███▊      | 228/595 [00:16<00:22, 16.12it/s] 39%|███▊      | 230/595 [00:17<00:22, 16.17it/s] 39%|███▉      | 232/595 [00:17<00:22, 16.20it/s] 39%|███▉      | 234/595 [00:17<00:22, 16.21it/s] 40%|███▉      | 236/595 [00:17<00:22, 16.23it/s]                                                  40%|████      | 238/595 [00:17<00:22, 16.23it/s][INFO|trainer.py:755] 2023-11-15 23:36:11,388 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 23:36:11,389 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 23:36:11,390 >>   Num examples = 945
[INFO|trainer.py:3134] 2023-11-15 23:36:11,390 >>   Batch size = 8
{'eval_loss': 0.5803946256637573, 'eval_accuracy': 0.7661375661375661, 'eval_micro_f1': 0.7661375661375661, 'eval_macro_f1': 0.6649154649883107, 'eval_runtime': 1.6088, 'eval_samples_per_second': 587.401, 'eval_steps_per_second': 73.969, 'epoch': 1.0}
{'loss': 0.4756, 'learning_rate': 0.0003, 'epoch': 2.0}

  0%|          | 0/119 [00:00<?, ?it/s][A
  8%|▊         | 10/119 [00:00<00:01, 95.56it/s][A
 17%|█▋        | 20/119 [00:00<00:01, 90.37it/s][A
 25%|██▌       | 30/119 [00:00<00:01, 88.64it/s][A
 33%|███▎      | 39/119 [00:00<00:00, 88.14it/s][A
 40%|████      | 48/119 [00:00<00:00, 87.26it/s][A
 48%|████▊     | 57/119 [00:00<00:00, 86.97it/s][A
 55%|█████▌    | 66/119 [00:00<00:00, 85.82it/s][A
 63%|██████▎   | 75/119 [00:00<00:00, 86.48it/s][A
 71%|███████   | 84/119 [00:00<00:00, 86.42it/s][A
 78%|███████▊  | 93/119 [00:01<00:00, 87.02it/s][A
 86%|████████▌ | 102/119 [00:01<00:00, 85.77it/s][A
 93%|█████████▎| 111/119 [00:01<00:00, 85.30it/s][A                                                 
                                                 [A 40%|████      | 238/595 [00:18<00:22, 16.23it/s]
100%|██████████| 119/119 [00:01<00:00, 85.30it/s][A
                                                 [A 40%|████      | 239/595 [00:18<01:26,  4.12it/s] 41%|████      | 241/595 [00:19<01:08,  5.15it/s] 41%|████      | 243/595 [00:19<00:55,  6.35it/s] 41%|████      | 245/595 [00:19<00:45,  7.68it/s] 42%|████▏     | 247/595 [00:19<00:38,  9.03it/s] 42%|████▏     | 249/595 [00:19<00:33, 10.35it/s] 42%|████▏     | 251/595 [00:19<00:29, 11.56it/s] 43%|████▎     | 253/595 [00:19<00:27, 12.63it/s] 43%|████▎     | 255/595 [00:19<00:25, 13.45it/s] 43%|████▎     | 257/595 [00:20<00:23, 14.09it/s] 44%|████▎     | 259/595 [00:20<00:22, 14.65it/s] 44%|████▍     | 261/595 [00:20<00:22, 15.06it/s] 44%|████▍     | 263/595 [00:20<00:21, 15.35it/s] 45%|████▍     | 265/595 [00:20<00:21, 15.51it/s] 45%|████▍     | 267/595 [00:20<00:20, 15.62it/s] 45%|████▌     | 269/595 [00:20<00:20, 15.78it/s] 46%|████▌     | 271/595 [00:20<00:20, 15.88it/s] 46%|████▌     | 273/595 [00:21<00:20, 15.92it/s] 46%|████▌     | 275/595 [00:21<00:20, 15.92it/s] 47%|████▋     | 277/595 [00:21<00:20, 15.69it/s] 47%|████▋     | 279/595 [00:21<00:20, 15.48it/s] 47%|████▋     | 281/595 [00:21<00:20, 15.39it/s] 48%|████▊     | 283/595 [00:21<00:20, 15.41it/s] 48%|████▊     | 285/595 [00:21<00:19, 15.62it/s] 48%|████▊     | 287/595 [00:21<00:19, 15.78it/s] 49%|████▊     | 289/595 [00:22<00:19, 15.87it/s] 49%|████▉     | 291/595 [00:22<00:19, 15.98it/s] 49%|████▉     | 293/595 [00:22<00:18, 15.96it/s] 50%|████▉     | 295/595 [00:22<00:18, 15.96it/s] 50%|████▉     | 297/595 [00:22<00:18, 15.93it/s] 50%|█████     | 299/595 [00:22<00:18, 16.03it/s] 51%|█████     | 301/595 [00:22<00:18, 16.02it/s] 51%|█████     | 303/595 [00:22<00:18, 16.02it/s] 51%|█████▏    | 305/595 [00:23<00:18, 15.95it/s] 52%|█████▏    | 307/595 [00:23<00:17, 16.03it/s] 52%|█████▏    | 309/595 [00:23<00:17, 15.98it/s] 52%|█████▏    | 311/595 [00:23<00:17, 16.00it/s] 53%|█████▎    | 313/595 [00:23<00:17, 15.95it/s] 53%|█████▎    | 315/595 [00:23<00:17, 15.98it/s] 53%|█████▎    | 317/595 [00:23<00:17, 16.03it/s] 54%|█████▎    | 319/595 [00:23<00:17, 16.04it/s] 54%|█████▍    | 321/595 [00:24<00:17, 15.99it/s] 54%|█████▍    | 323/595 [00:24<00:17, 15.96it/s] 55%|█████▍    | 325/595 [00:24<00:16, 15.91it/s] 55%|█████▍    | 327/595 [00:24<00:16, 15.86it/s] 55%|█████▌    | 329/595 [00:24<00:16, 15.89it/s] 56%|█████▌    | 331/595 [00:24<00:16, 15.86it/s] 56%|█████▌    | 333/595 [00:24<00:16, 15.86it/s] 56%|█████▋    | 335/595 [00:24<00:16, 15.85it/s] 57%|█████▋    | 337/595 [00:25<00:16, 15.89it/s] 57%|█████▋    | 339/595 [00:25<00:16, 15.92it/s] 57%|█████▋    | 341/595 [00:25<00:16, 15.86it/s] 58%|█████▊    | 343/595 [00:25<00:15, 15.91it/s] 58%|█████▊    | 345/595 [00:25<00:15, 15.89it/s] 58%|█████▊    | 347/595 [00:25<00:15, 15.69it/s] 59%|█████▊    | 349/595 [00:25<00:15, 15.78it/s] 59%|█████▉    | 351/595 [00:26<00:15, 15.76it/s] 59%|█████▉    | 353/595 [00:26<00:15, 15.77it/s] 60%|█████▉    | 355/595 [00:26<00:15, 15.74it/s]                                                  60%|██████    | 357/595 [00:26<00:15, 15.74it/s][INFO|trainer.py:755] 2023-11-15 23:36:20,264 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 23:36:20,265 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 23:36:20,266 >>   Num examples = 945
[INFO|trainer.py:3134] 2023-11-15 23:36:20,266 >>   Batch size = 8
{'eval_loss': 0.4199589490890503, 'eval_accuracy': 0.8201058201058201, 'eval_micro_f1': 0.8201058201058201, 'eval_macro_f1': 0.7413821814832788, 'eval_runtime': 1.4174, 'eval_samples_per_second': 666.704, 'eval_steps_per_second': 83.955, 'epoch': 2.0}
{'loss': 0.397, 'learning_rate': 0.0002, 'epoch': 3.0}

  0%|          | 0/119 [00:00<?, ?it/s][A
  8%|▊         | 10/119 [00:00<00:01, 95.49it/s][A
 17%|█▋        | 20/119 [00:00<00:01, 90.26it/s][A
 25%|██▌       | 30/119 [00:00<00:01, 88.08it/s][A
 33%|███▎      | 39/119 [00:00<00:00, 86.76it/s][A
 40%|████      | 48/119 [00:00<00:00, 83.73it/s][A
 48%|████▊     | 57/119 [00:00<00:00, 80.23it/s][A
 55%|█████▌    | 66/119 [00:00<00:00, 79.53it/s][A
 62%|██████▏   | 74/119 [00:00<00:00, 77.99it/s][A
 69%|██████▉   | 82/119 [00:01<00:00, 77.52it/s][A
 76%|███████▋  | 91/119 [00:01<00:00, 80.29it/s][A
 84%|████████▍ | 100/119 [00:01<00:00, 82.55it/s][A
 92%|█████████▏| 109/119 [00:01<00:00, 83.44it/s][A
 99%|█████████▉| 118/119 [00:01<00:00, 85.06it/s][A                                                 
                                                 [A 60%|██████    | 357/595 [00:27<00:15, 15.74it/s]
100%|██████████| 119/119 [00:01<00:00, 85.06it/s][A
                                                 [A 60%|██████    | 358/595 [00:27<00:59,  3.97it/s] 61%|██████    | 360/595 [00:28<00:47,  4.99it/s] 61%|██████    | 362/595 [00:28<00:37,  6.19it/s] 61%|██████    | 364/595 [00:28<00:30,  7.51it/s] 62%|██████▏   | 366/595 [00:28<00:25,  8.88it/s] 62%|██████▏   | 368/595 [00:28<00:22, 10.24it/s] 62%|██████▏   | 370/595 [00:28<00:19, 11.49it/s] 63%|██████▎   | 372/595 [00:28<00:17, 12.56it/s] 63%|██████▎   | 374/595 [00:28<00:16, 13.38it/s] 63%|██████▎   | 376/595 [00:29<00:15, 14.06it/s] 64%|██████▎   | 378/595 [00:29<00:14, 14.63it/s] 64%|██████▍   | 380/595 [00:29<00:14, 15.06it/s] 64%|██████▍   | 382/595 [00:29<00:13, 15.35it/s] 65%|██████▍   | 384/595 [00:29<00:13, 15.55it/s] 65%|██████▍   | 386/595 [00:29<00:13, 15.75it/s] 65%|██████▌   | 388/595 [00:29<00:13, 15.82it/s] 66%|██████▌   | 390/595 [00:29<00:12, 15.87it/s] 66%|██████▌   | 392/595 [00:30<00:12, 15.80it/s] 66%|██████▌   | 394/595 [00:30<00:12, 15.83it/s] 67%|██████▋   | 396/595 [00:30<00:12, 15.82it/s] 67%|██████▋   | 398/595 [00:30<00:12, 15.78it/s] 67%|██████▋   | 400/595 [00:30<00:12, 15.85it/s] 68%|██████▊   | 402/595 [00:30<00:12, 15.85it/s] 68%|██████▊   | 404/595 [00:30<00:12, 15.87it/s] 68%|██████▊   | 406/595 [00:30<00:11, 15.92it/s] 69%|██████▊   | 408/595 [00:31<00:11, 15.97it/s] 69%|██████▉   | 410/595 [00:31<00:11, 15.93it/s] 69%|██████▉   | 412/595 [00:31<00:11, 15.90it/s] 70%|██████▉   | 414/595 [00:31<00:11, 15.95it/s] 70%|██████▉   | 416/595 [00:31<00:11, 15.95it/s] 70%|███████   | 418/595 [00:31<00:11, 15.89it/s] 71%|███████   | 420/595 [00:31<00:10, 15.93it/s] 71%|███████   | 422/595 [00:31<00:10, 15.95it/s] 71%|███████▏  | 424/595 [00:32<00:10, 15.91it/s] 72%|███████▏  | 426/595 [00:32<00:10, 15.89it/s] 72%|███████▏  | 428/595 [00:32<00:10, 15.88it/s] 72%|███████▏  | 430/595 [00:32<00:10, 15.80it/s] 73%|███████▎  | 432/595 [00:32<00:10, 15.75it/s] 73%|███████▎  | 434/595 [00:32<00:10, 15.70it/s] 73%|███████▎  | 436/595 [00:32<00:10, 15.67it/s] 74%|███████▎  | 438/595 [00:32<00:09, 15.73it/s] 74%|███████▍  | 440/595 [00:33<00:09, 15.77it/s] 74%|███████▍  | 442/595 [00:33<00:09, 15.82it/s] 75%|███████▍  | 444/595 [00:33<00:09, 15.88it/s] 75%|███████▍  | 446/595 [00:33<00:09, 15.98it/s] 75%|███████▌  | 448/595 [00:33<00:09, 15.96it/s] 76%|███████▌  | 450/595 [00:33<00:09, 15.88it/s] 76%|███████▌  | 452/595 [00:33<00:09, 15.84it/s] 76%|███████▋  | 454/595 [00:33<00:08, 15.79it/s] 77%|███████▋  | 456/595 [00:34<00:08, 15.73it/s] 77%|███████▋  | 458/595 [00:34<00:08, 15.78it/s] 77%|███████▋  | 460/595 [00:34<00:08, 15.78it/s] 78%|███████▊  | 462/595 [00:34<00:08, 15.75it/s] 78%|███████▊  | 464/595 [00:34<00:08, 15.79it/s] 78%|███████▊  | 466/595 [00:34<00:08, 15.81it/s] 79%|███████▊  | 468/595 [00:34<00:08, 15.87it/s] 79%|███████▉  | 470/595 [00:34<00:07, 15.94it/s] 79%|███████▉  | 472/595 [00:35<00:07, 16.01it/s] 80%|███████▉  | 474/595 [00:35<00:07, 15.92it/s]                                                  80%|████████  | 476/595 [00:35<00:07, 15.92it/s][INFO|trainer.py:755] 2023-11-15 23:36:29,184 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 23:36:29,186 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 23:36:29,186 >>   Num examples = 945
[INFO|trainer.py:3134] 2023-11-15 23:36:29,186 >>   Batch size = 8
{'eval_loss': 0.45201927423477173, 'eval_accuracy': 0.8328042328042328, 'eval_micro_f1': 0.8328042328042328, 'eval_macro_f1': 0.7484564181179246, 'eval_runtime': 1.4757, 'eval_samples_per_second': 640.371, 'eval_steps_per_second': 80.639, 'epoch': 3.0}
{'loss': 0.2982, 'learning_rate': 0.0001, 'epoch': 4.0}

  0%|          | 0/119 [00:00<?, ?it/s][A
  8%|▊         | 10/119 [00:00<00:01, 97.73it/s][A
 17%|█▋        | 20/119 [00:00<00:01, 91.24it/s][A
 25%|██▌       | 30/119 [00:00<00:01, 88.33it/s][A
 33%|███▎      | 39/119 [00:00<00:00, 86.66it/s][A
 40%|████      | 48/119 [00:00<00:00, 84.57it/s][A
 48%|████▊     | 57/119 [00:00<00:00, 83.88it/s][A
 55%|█████▌    | 66/119 [00:00<00:00, 83.59it/s][A
 63%|██████▎   | 75/119 [00:00<00:00, 83.04it/s][A
 71%|███████   | 84/119 [00:00<00:00, 82.73it/s][A
 78%|███████▊  | 93/119 [00:01<00:00, 82.92it/s][A
 86%|████████▌ | 102/119 [00:01<00:00, 83.22it/s][A
 93%|█████████▎| 111/119 [00:01<00:00, 83.63it/s][A                                                 
                                                 [A 80%|████████  | 476/595 [00:36<00:07, 15.92it/s]
100%|██████████| 119/119 [00:01<00:00, 83.63it/s][A
                                                 [A 80%|████████  | 477/595 [00:36<00:29,  4.03it/s] 81%|████████  | 479/595 [00:36<00:22,  5.06it/s] 81%|████████  | 481/595 [00:37<00:18,  6.25it/s] 81%|████████  | 483/595 [00:37<00:14,  7.55it/s] 82%|████████▏ | 485/595 [00:37<00:12,  8.92it/s] 82%|████████▏ | 487/595 [00:37<00:10, 10.23it/s] 82%|████████▏ | 489/595 [00:37<00:09, 11.45it/s] 83%|████████▎ | 491/595 [00:37<00:08, 12.47it/s] 83%|████████▎ | 493/595 [00:37<00:07, 13.31it/s] 83%|████████▎ | 495/595 [00:37<00:07, 13.95it/s] 84%|████████▎ | 497/595 [00:38<00:06, 14.48it/s] 84%|████████▍ | 499/595 [00:38<00:06, 14.91it/s] 84%|████████▍ | 501/595 [00:38<00:06, 15.13it/s] 85%|████████▍ | 503/595 [00:38<00:06, 15.26it/s] 85%|████████▍ | 505/595 [00:38<00:05, 15.42it/s] 85%|████████▌ | 507/595 [00:38<00:05, 15.47it/s] 86%|████████▌ | 509/595 [00:38<00:05, 15.60it/s] 86%|████████▌ | 511/595 [00:38<00:05, 15.76it/s] 86%|████████▌ | 513/595 [00:39<00:05, 15.76it/s] 87%|████████▋ | 515/595 [00:39<00:05, 15.70it/s] 87%|████████▋ | 517/595 [00:39<00:04, 15.76it/s] 87%|████████▋ | 519/595 [00:39<00:04, 15.83it/s] 88%|████████▊ | 521/595 [00:39<00:04, 15.88it/s] 88%|████████▊ | 523/595 [00:39<00:04, 15.96it/s] 88%|████████▊ | 525/595 [00:39<00:04, 16.01it/s] 89%|████████▊ | 527/595 [00:39<00:04, 16.00it/s] 89%|████████▉ | 529/595 [00:40<00:04, 15.98it/s] 89%|████████▉ | 531/595 [00:40<00:04, 15.99it/s] 90%|████████▉ | 533/595 [00:40<00:03, 15.99it/s] 90%|████████▉ | 535/595 [00:40<00:03, 16.01it/s] 90%|█████████ | 537/595 [00:40<00:03, 16.01it/s] 91%|█████████ | 539/595 [00:40<00:03, 15.97it/s] 91%|█████████ | 541/595 [00:40<00:03, 16.00it/s] 91%|█████████▏| 543/595 [00:40<00:03, 16.01it/s] 92%|█████████▏| 545/595 [00:41<00:03, 15.99it/s] 92%|█████████▏| 547/595 [00:41<00:03, 15.99it/s] 92%|█████████▏| 549/595 [00:41<00:02, 15.96it/s] 93%|█████████▎| 551/595 [00:41<00:02, 15.85it/s] 93%|█████████▎| 553/595 [00:41<00:02, 15.82it/s] 93%|█████████▎| 555/595 [00:41<00:02, 15.81it/s] 94%|█████████▎| 557/595 [00:41<00:02, 15.76it/s] 94%|█████████▍| 559/595 [00:41<00:02, 15.79it/s] 94%|█████████▍| 561/595 [00:42<00:02, 15.84it/s] 95%|█████████▍| 563/595 [00:42<00:02, 15.82it/s] 95%|█████████▍| 565/595 [00:42<00:01, 15.85it/s] 95%|█████████▌| 567/595 [00:42<00:01, 15.85it/s] 96%|█████████▌| 569/595 [00:42<00:01, 15.83it/s] 96%|█████████▌| 571/595 [00:42<00:01, 15.80it/s] 96%|█████████▋| 573/595 [00:42<00:01, 15.87it/s] 97%|█████████▋| 575/595 [00:42<00:01, 15.90it/s] 97%|█████████▋| 577/595 [00:43<00:01, 15.86it/s] 97%|█████████▋| 579/595 [00:43<00:01, 15.88it/s] 98%|█████████▊| 581/595 [00:43<00:00, 15.73it/s] 98%|█████████▊| 583/595 [00:43<00:00, 15.68it/s] 98%|█████████▊| 585/595 [00:43<00:00, 15.69it/s] 99%|█████████▊| 587/595 [00:43<00:00, 15.71it/s] 99%|█████████▉| 589/595 [00:43<00:00, 15.70it/s] 99%|█████████▉| 591/595 [00:43<00:00, 15.64it/s]100%|█████████▉| 593/595 [00:44<00:00, 15.63it/s]                                                 100%|██████████| 595/595 [00:44<00:00, 15.63it/s][INFO|trainer.py:755] 2023-11-15 23:36:38,117 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 23:36:38,118 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 23:36:38,119 >>   Num examples = 945
[INFO|trainer.py:3134] 2023-11-15 23:36:38,119 >>   Batch size = 8
{'eval_loss': 0.4435962736606598, 'eval_accuracy': 0.852910052910053, 'eval_micro_f1': 0.852910052910053, 'eval_macro_f1': 0.7863018182327751, 'eval_runtime': 1.4469, 'eval_samples_per_second': 653.103, 'eval_steps_per_second': 82.243, 'epoch': 4.0}
{'loss': 0.2694, 'learning_rate': 0.0, 'epoch': 5.0}

  0%|          | 0/119 [00:00<?, ?it/s][A
  8%|▊         | 9/119 [00:00<00:01, 84.46it/s][A
 15%|█▌        | 18/119 [00:00<00:01, 84.70it/s][A
 23%|██▎       | 27/119 [00:00<00:01, 85.38it/s][A
 30%|███       | 36/119 [00:00<00:00, 85.56it/s][A
 38%|███▊      | 45/119 [00:00<00:00, 86.31it/s][A
 45%|████▌     | 54/119 [00:00<00:00, 86.10it/s][A
 53%|█████▎    | 63/119 [00:00<00:00, 86.36it/s][A
 61%|██████    | 72/119 [00:00<00:00, 84.51it/s][A
 68%|██████▊   | 81/119 [00:00<00:00, 84.75it/s][A
 76%|███████▌  | 90/119 [00:01<00:00, 85.41it/s][A
 83%|████████▎ | 99/119 [00:01<00:00, 86.36it/s][A
 91%|█████████ | 108/119 [00:01<00:00, 86.07it/s][A
 98%|█████████▊| 117/119 [00:01<00:00, 86.40it/s][A                                                 
                                                 [A100%|██████████| 595/595 [00:45<00:00, 15.63it/s]
100%|██████████| 119/119 [00:01<00:00, 86.40it/s][A
                                                 [A[INFO|trainer.py:1963] 2023-11-15 23:36:39,553 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                 100%|██████████| 595/595 [00:45<00:00, 15.63it/s]100%|██████████| 595/595 [00:45<00:00, 13.03it/s]
[INFO|trainer.py:2855] 2023-11-15 23:36:39,557 >> Saving model checkpoint to ./result/restaurant_roberta-base_seed3_adapter
[INFO|configuration_utils.py:460] 2023-11-15 23:36:39,559 >> Configuration saved in ./result/restaurant_roberta-base_seed3_adapter/config.json
[INFO|modeling_utils.py:1997] 2023-11-15 23:36:40,664 >> Model weights saved in ./result/restaurant_roberta-base_seed3_adapter/pytorch_model.bin
[INFO|tokenization_utils_base.py:2235] 2023-11-15 23:36:40,667 >> tokenizer config file saved in ./result/restaurant_roberta-base_seed3_adapter/tokenizer_config.json
[INFO|tokenization_utils_base.py:2242] 2023-11-15 23:36:40,669 >> Special tokens file saved in ./result/restaurant_roberta-base_seed3_adapter/special_tokens_map.json
{'eval_loss': 0.39929401874542236, 'eval_accuracy': 0.8539682539682539, 'eval_micro_f1': 0.8539682539682539, 'eval_macro_f1': 0.7986848025252028, 'eval_runtime': 1.4315, 'eval_samples_per_second': 660.13, 'eval_steps_per_second': 83.127, 'epoch': 5.0}
{'train_runtime': 45.649, 'train_samples_per_second': 413.701, 'train_steps_per_second': 13.034, 'train_loss': 0.4347706898921678, 'epoch': 5.0}
***** train metrics *****
  epoch                    =        5.0
  train_loss               =     0.4348
  train_runtime            = 0:00:45.64
  train_samples            =       3777
  train_samples_per_second =    413.701
  train_steps_per_second   =     13.034
11/15/2023 23:36:40 - INFO - __main__ - *** Evaluate ***
[INFO|trainer.py:755] 2023-11-15 23:36:40,803 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 23:36:40,804 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 23:36:40,804 >>   Num examples = 945
[INFO|trainer.py:3134] 2023-11-15 23:36:40,804 >>   Batch size = 8
  0%|          | 0/119 [00:00<?, ?it/s]  8%|▊         | 10/119 [00:00<00:01, 89.26it/s] 16%|█▌        | 19/119 [00:00<00:01, 84.27it/s] 24%|██▎       | 28/119 [00:00<00:01, 84.70it/s] 31%|███       | 37/119 [00:00<00:00, 84.75it/s] 39%|███▊      | 46/119 [00:00<00:00, 83.29it/s] 46%|████▌     | 55/119 [00:00<00:00, 83.76it/s] 54%|█████▍    | 64/119 [00:00<00:00, 83.16it/s] 61%|██████▏   | 73/119 [00:00<00:00, 83.25it/s] 69%|██████▉   | 82/119 [00:00<00:00, 83.94it/s] 76%|███████▋  | 91/119 [00:01<00:00, 84.91it/s] 84%|████████▍ | 100/119 [00:01<00:00, 85.41it/s] 92%|█████████▏| 109/119 [00:01<00:00, 85.85it/s] 99%|█████████▉| 118/119 [00:01<00:00, 85.57it/s]100%|██████████| 119/119 [00:01<00:00, 83.00it/s]
***** eval metrics *****
  epoch                   =        5.0
  eval_accuracy           =      0.854
  eval_loss               =     0.3993
  eval_macro_f1           =     0.7987
  eval_micro_f1           =      0.854
  eval_runtime            = 0:00:01.44
  eval_samples            =        945
  eval_samples_per_second =    651.796
  eval_steps_per_second   =     82.078
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                  eval/accuracy ▁▅▆███
wandb:                      eval/loss █▂▃▃▁▁
wandb:                  eval/macro_f1 ▁▅▅▇██
wandb:                  eval/micro_f1 ▁▅▆███
wandb:                   eval/runtime █▁▃▂▂▂
wandb:        eval/samples_per_second ▁█▆▇▇▇
wandb:          eval/steps_per_second ▁█▆▇▇▇
wandb:                    train/epoch ▁▁▃▃▅▅▆▆████
wandb:              train/global_step ▁▁▃▃▅▅▆▆████
wandb:            train/learning_rate █▆▅▃▁
wandb:                     train/loss █▄▃▁▁
wandb:               train/total_flos ▁
wandb:               train/train_loss ▁
wandb:            train/train_runtime ▁
wandb: train/train_samples_per_second ▁
wandb:   train/train_steps_per_second ▁
wandb: 
wandb: Run summary:
wandb:                  eval/accuracy 0.85397
wandb:                      eval/loss 0.39929
wandb:                  eval/macro_f1 0.79868
wandb:                  eval/micro_f1 0.85397
wandb:                   eval/runtime 1.4498
wandb:        eval/samples_per_second 651.796
wandb:          eval/steps_per_second 82.078
wandb:                    train/epoch 5.0
wandb:              train/global_step 595
wandb:            train/learning_rate 0.0
wandb:                     train/loss 0.2694
wandb:               train/total_flos 627599085655680.0
wandb:               train/train_loss 0.43477
wandb:            train/train_runtime 45.649
wandb: train/train_samples_per_second 413.701
wandb:   train/train_steps_per_second 13.034
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /ceph/home/wangyifei/a3/wandb/offline-run-20231115_233436-h6s5t3ep
wandb: Find logs at: ./wandb/offline-run-20231115_233436-h6s5t3ep/logs
(ModelArguments(model_name_or_path='roberta-base', cache_dir='./cache', use_adapter=True, use_lora=False), DataTrainingArguments(dataset_name='acl', max_seq_length=64), TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.0005,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=./result/acl_roberta-base_seed3_adapter/runs/Nov15_23-36-52_node013,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.EPOCH,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=5.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
output_dir=./result/acl_roberta-base_seed3_adapter,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
resume_from_checkpoint=None,
run_name=./result/acl_roberta-base_seed3_adapter,
save_on_each_node=False,
save_safetensors=False,
save_steps=500,
save_strategy=IntervalStrategy.NO,
save_total_limit=None,
seed=444,
sharded_ddp=[],
skip_memory_metrics=True,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
))
11/15/2023 23:36:52 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False
11/15/2023 23:36:52 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.0005,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=./result/acl_roberta-base_seed3_adapter/runs/Nov15_23-36-51_node013,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.EPOCH,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=5.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
output_dir=./result/acl_roberta-base_seed3_adapter,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
resume_from_checkpoint=None,
run_name=./result/acl_roberta-base_seed3_adapter,
save_on_each_node=False,
save_safetensors=False,
save_steps=500,
save_strategy=IntervalStrategy.NO,
save_total_limit=None,
seed=444,
sharded_ddp=[],
skip_memory_metrics=True,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
wandb: Tracking run with wandb version 0.15.12
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
Map:   0%|          | 0/11020 [00:00<?, ? examples/s]Map:  36%|███▋      | 4000/11020 [00:00<00:00, 39119.49 examples/s]Map:  73%|███████▎  | 8000/11020 [00:00<00:00, 38950.41 examples/s]Map: 100%|██████████| 11020/11020 [00:00<00:00, 38746.85 examples/s]
[INFO|configuration_utils.py:715] 2023-11-15 23:37:08,349 >> loading configuration file config.json from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/config.json
[INFO|configuration_utils.py:775] 2023-11-15 23:37:08,358 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.33.3",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_auto.py:535] 2023-11-15 23:37:18,410 >> Could not locate the tokenizer configuration file, will try to use the model config instead.
[INFO|configuration_utils.py:715] 2023-11-15 23:37:28,426 >> loading configuration file config.json from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/config.json
[INFO|configuration_utils.py:775] 2023-11-15 23:37:28,427 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.33.3",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:1852] 2023-11-15 23:37:48,477 >> loading file vocab.json from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/vocab.json
[INFO|tokenization_utils_base.py:1852] 2023-11-15 23:37:48,477 >> loading file merges.txt from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/merges.txt
[INFO|tokenization_utils_base.py:1852] 2023-11-15 23:37:48,478 >> loading file tokenizer.json from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/tokenizer.json
[INFO|tokenization_utils_base.py:1852] 2023-11-15 23:37:48,478 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1852] 2023-11-15 23:37:48,478 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1852] 2023-11-15 23:37:48,478 >> loading file tokenizer_config.json from cache at None
[INFO|configuration_utils.py:715] 2023-11-15 23:37:48,480 >> loading configuration file config.json from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/config.json
[INFO|configuration_utils.py:775] 2023-11-15 23:37:48,480 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.33.3",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:2862] 2023-11-15 23:38:08,685 >> loading weights file model.safetensors from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/model.safetensors
[INFO|modeling_utils.py:3638] 2023-11-15 23:38:09,392 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.bias']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3650] 2023-11-15 23:38:09,393 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
parameters 1487427
model_max_length: 512
Running tokenizer on dataset:   0%|          | 0/8816 [00:00<?, ? examples/s]Running tokenizer on dataset:  23%|██▎       | 2000/8816 [00:00<00:00, 17059.69 examples/s]Running tokenizer on dataset:  45%|████▌     | 4000/8816 [00:00<00:00, 17065.28 examples/s]Running tokenizer on dataset:  68%|██████▊   | 6000/8816 [00:00<00:00, 17447.88 examples/s]Running tokenizer on dataset:  91%|█████████ | 8000/8816 [00:00<00:00, 18068.29 examples/s]Running tokenizer on dataset: 100%|██████████| 8816/8816 [00:00<00:00, 17822.90 examples/s]
Running tokenizer on dataset:   0%|          | 0/2204 [00:00<?, ? examples/s]Running tokenizer on dataset: 100%|██████████| 2204/2204 [00:00<00:00, 18833.18 examples/s]Running tokenizer on dataset: 100%|██████████| 2204/2204 [00:00<00:00, 18574.27 examples/s]
11/15/2023 23:38:10 - INFO - __main__ - Sample 5060 of the training set: {'text': 'After GABA immunostaining, as elaborated in earlier studies (Domenici et al. 1988; Granda and Crossland 1989), the cell bodies, fibers, and numerous terminals showed GABA-like immunoreactivity in the Imc nucleus.', 'label': 0, 'input_ids': [0, 4993, 47644, 13998, 2603, 8173, 6, 25, 35838, 11, 656, 3218, 36, 495, 14900, 13850, 4400, 1076, 4, 11151, 131, 2974, 5219, 8, 4415, 1245, 10206, 238, 5, 3551, 3738, 6, 32902, 6, 8, 3617, 20531, 969, 47644, 12, 3341, 13998, 1688, 30280, 11, 5, 5902, 438, 38531, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
11/15/2023 23:38:10 - INFO - __main__ - Sample 4715 of the training set: {'text': 'The dynamic nature of the Dlg ‘supertertiary’ core structure suggest precise regulatory inputs have likely evolved to control its signaling output [25, 26].', 'label': 0, 'input_ids': [0, 133, 6878, 2574, 9, 5, 211, 462, 571, 44, 711, 16101, 1334, 90, 17174, 17, 27, 2731, 3184, 3608, 12548, 4099, 16584, 33, 533, 12236, 7, 797, 63, 22436, 4195, 646, 1244, 6, 973, 8174, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
11/15/2023 23:38:10 - INFO - __main__ - Sample 216 of the training set: {'text': 'C57 mice, however, have been reported to be more sensitive to the incentive properties of other drugs of abuse including amphetamine, cocaine, methamphetamine, or nicotine than DBA mice (for review see Crawley et al. 1997; Cabib et al. 2000; Orsini et al. 2004; 2005; Grabus et al. 2006).', 'label': 0, 'input_ids': [0, 347, 4390, 15540, 6, 959, 6, 33, 57, 431, 7, 28, 55, 5685, 7, 5, 10814, 3611, 9, 97, 2196, 9, 2134, 217, 28127, 45634, 6, 9890, 6, 19118, 6, 50, 27730, 87, 211, 3813, 15540, 36, 1990, 1551, 192, 28040, 607, 4400, 1076, 4, 7528, 131, 8434, 1452, 4400, 1076, 4, 3788, 131, 1793, 29, 2531, 4400, 1076, 4, 4482, 131, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.
11/15/2023 23:38:10 - WARNING - accelerate.utils.other - Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[INFO|trainer.py:755] 2023-11-15 23:38:11,928 >> The following columns in the training set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:1715] 2023-11-15 23:38:11,939 >> ***** Running training *****
[INFO|trainer.py:1716] 2023-11-15 23:38:11,939 >>   Num examples = 8,816
[INFO|trainer.py:1717] 2023-11-15 23:38:11,940 >>   Num Epochs = 5
[INFO|trainer.py:1718] 2023-11-15 23:38:11,940 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1721] 2023-11-15 23:38:11,940 >>   Total train batch size (w. parallel, distributed & accumulation) = 32
[INFO|trainer.py:1722] 2023-11-15 23:38:11,940 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:1723] 2023-11-15 23:38:11,941 >>   Total optimization steps = 1,380
[INFO|trainer.py:1724] 2023-11-15 23:38:11,942 >>   Number of trainable parameters = 1,487,427
[INFO|integration_utils.py:716] 2023-11-15 23:38:11,943 >> Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"
  0%|          | 0/1380 [00:00<?, ?it/s]  0%|          | 1/1380 [00:01<24:02,  1.05s/it]  0%|          | 3/1380 [00:01<07:18,  3.14it/s]  0%|          | 5/1380 [00:01<04:17,  5.33it/s]  1%|          | 7/1380 [00:01<03:04,  7.43it/s]  1%|          | 9/1380 [00:01<02:26,  9.34it/s]  1%|          | 11/1380 [00:01<02:05, 10.89it/s]  1%|          | 13/1380 [00:01<01:51, 12.21it/s]  1%|          | 15/1380 [00:01<01:42, 13.27it/s]  1%|          | 17/1380 [00:02<01:36, 14.08it/s]  1%|▏         | 19/1380 [00:02<01:32, 14.66it/s]  2%|▏         | 21/1380 [00:02<01:30, 15.08it/s]  2%|▏         | 23/1380 [00:02<01:28, 15.39it/s]  2%|▏         | 25/1380 [00:02<01:26, 15.62it/s]  2%|▏         | 27/1380 [00:02<01:25, 15.80it/s]  2%|▏         | 29/1380 [00:02<01:24, 15.92it/s]  2%|▏         | 31/1380 [00:02<01:24, 15.98it/s]  2%|▏         | 33/1380 [00:03<01:24, 15.99it/s]  3%|▎         | 35/1380 [00:03<01:23, 16.03it/s]  3%|▎         | 37/1380 [00:03<01:23, 16.09it/s]  3%|▎         | 39/1380 [00:03<01:23, 16.12it/s]  3%|▎         | 41/1380 [00:03<01:23, 16.07it/s]  3%|▎         | 43/1380 [00:03<01:23, 16.06it/s]  3%|▎         | 45/1380 [00:03<01:23, 16.06it/s]  3%|▎         | 47/1380 [00:03<01:23, 15.93it/s]  4%|▎         | 49/1380 [00:04<01:24, 15.79it/s]  4%|▎         | 51/1380 [00:04<01:24, 15.78it/s]  4%|▍         | 53/1380 [00:04<01:24, 15.77it/s]  4%|▍         | 55/1380 [00:04<01:24, 15.77it/s]  4%|▍         | 57/1380 [00:04<01:23, 15.78it/s]  4%|▍         | 59/1380 [00:04<01:23, 15.74it/s]  4%|▍         | 61/1380 [00:04<01:23, 15.74it/s]  5%|▍         | 63/1380 [00:04<01:23, 15.78it/s]  5%|▍         | 65/1380 [00:05<01:23, 15.76it/s]  5%|▍         | 67/1380 [00:05<01:23, 15.78it/s]  5%|▌         | 69/1380 [00:05<01:22, 15.86it/s]  5%|▌         | 71/1380 [00:05<01:22, 15.87it/s]  5%|▌         | 73/1380 [00:05<01:22, 15.90it/s]  5%|▌         | 75/1380 [00:05<01:21, 15.93it/s]  6%|▌         | 77/1380 [00:05<01:21, 15.91it/s]  6%|▌         | 79/1380 [00:05<01:21, 15.89it/s]  6%|▌         | 81/1380 [00:06<01:21, 15.90it/s]  6%|▌         | 83/1380 [00:06<01:21, 15.95it/s]  6%|▌         | 85/1380 [00:06<01:21, 15.97it/s]  6%|▋         | 87/1380 [00:06<01:21, 15.91it/s]  6%|▋         | 89/1380 [00:06<01:21, 15.84it/s]  7%|▋         | 91/1380 [00:06<01:21, 15.79it/s]  7%|▋         | 93/1380 [00:06<01:21, 15.75it/s]  7%|▋         | 95/1380 [00:06<01:21, 15.75it/s]  7%|▋         | 97/1380 [00:07<01:20, 15.86it/s]  7%|▋         | 99/1380 [00:07<01:20, 15.85it/s]  7%|▋         | 101/1380 [00:07<01:20, 15.97it/s]  7%|▋         | 103/1380 [00:07<01:19, 16.07it/s]  8%|▊         | 105/1380 [00:07<01:19, 16.08it/s]  8%|▊         | 107/1380 [00:07<01:19, 16.08it/s]  8%|▊         | 109/1380 [00:07<01:18, 16.09it/s]  8%|▊         | 111/1380 [00:07<01:18, 16.12it/s]  8%|▊         | 113/1380 [00:08<01:18, 16.16it/s]  8%|▊         | 115/1380 [00:08<01:18, 16.21it/s]  8%|▊         | 117/1380 [00:08<01:17, 16.23it/s]  9%|▊         | 119/1380 [00:08<01:17, 16.23it/s]  9%|▉         | 121/1380 [00:08<01:17, 16.21it/s]  9%|▉         | 123/1380 [00:08<01:17, 16.21it/s]  9%|▉         | 125/1380 [00:08<01:17, 16.23it/s]  9%|▉         | 127/1380 [00:08<01:17, 16.26it/s]  9%|▉         | 129/1380 [00:09<01:16, 16.27it/s]  9%|▉         | 131/1380 [00:09<01:16, 16.25it/s] 10%|▉         | 133/1380 [00:09<01:17, 16.19it/s] 10%|▉         | 135/1380 [00:09<01:17, 16.16it/s] 10%|▉         | 137/1380 [00:09<01:17, 16.10it/s] 10%|█         | 139/1380 [00:09<01:17, 16.00it/s] 10%|█         | 141/1380 [00:09<01:18, 15.88it/s] 10%|█         | 143/1380 [00:09<01:17, 15.89it/s] 11%|█         | 145/1380 [00:10<01:17, 15.88it/s] 11%|█         | 147/1380 [00:10<01:17, 15.91it/s] 11%|█         | 149/1380 [00:10<01:17, 15.93it/s] 11%|█         | 151/1380 [00:10<01:17, 15.95it/s] 11%|█         | 153/1380 [00:10<01:16, 15.95it/s] 11%|█         | 155/1380 [00:10<01:17, 15.91it/s] 11%|█▏        | 157/1380 [00:10<01:16, 15.96it/s] 12%|█▏        | 159/1380 [00:10<01:16, 15.99it/s] 12%|█▏        | 161/1380 [00:11<01:16, 15.97it/s] 12%|█▏        | 163/1380 [00:11<01:16, 15.96it/s] 12%|█▏        | 165/1380 [00:11<01:15, 16.00it/s] 12%|█▏        | 167/1380 [00:11<01:15, 16.01it/s] 12%|█▏        | 169/1380 [00:11<01:15, 15.98it/s] 12%|█▏        | 171/1380 [00:11<01:15, 15.99it/s] 13%|█▎        | 173/1380 [00:11<01:15, 15.95it/s] 13%|█▎        | 175/1380 [00:11<01:15, 15.96it/s] 13%|█▎        | 177/1380 [00:12<01:15, 15.91it/s] 13%|█▎        | 179/1380 [00:12<01:15, 15.86it/s] 13%|█▎        | 181/1380 [00:12<01:15, 15.87it/s] 13%|█▎        | 183/1380 [00:12<01:15, 15.83it/s] 13%|█▎        | 185/1380 [00:12<01:15, 15.85it/s] 14%|█▎        | 187/1380 [00:12<01:14, 15.95it/s] 14%|█▎        | 189/1380 [00:12<01:14, 15.99it/s] 14%|█▍        | 191/1380 [00:12<01:14, 16.03it/s] 14%|█▍        | 193/1380 [00:13<01:13, 16.08it/s] 14%|█▍        | 195/1380 [00:13<01:13, 16.15it/s] 14%|█▍        | 197/1380 [00:13<01:13, 16.18it/s] 14%|█▍        | 199/1380 [00:13<01:12, 16.20it/s] 15%|█▍        | 201/1380 [00:13<01:12, 16.20it/s] 15%|█▍        | 203/1380 [00:13<01:12, 16.19it/s] 15%|█▍        | 205/1380 [00:13<01:12, 16.19it/s] 15%|█▌        | 207/1380 [00:13<01:12, 16.22it/s] 15%|█▌        | 209/1380 [00:14<01:12, 16.23it/s] 15%|█▌        | 211/1380 [00:14<01:12, 16.22it/s] 15%|█▌        | 213/1380 [00:14<01:12, 16.20it/s] 16%|█▌        | 215/1380 [00:14<01:12, 16.17it/s] 16%|█▌        | 217/1380 [00:14<01:11, 16.18it/s] 16%|█▌        | 219/1380 [00:14<01:11, 16.17it/s] 16%|█▌        | 221/1380 [00:14<01:11, 16.16it/s] 16%|█▌        | 223/1380 [00:14<01:11, 16.16it/s] 16%|█▋        | 225/1380 [00:15<01:11, 16.12it/s] 16%|█▋        | 227/1380 [00:15<01:11, 16.02it/s] 17%|█▋        | 229/1380 [00:15<01:12, 15.94it/s] 17%|█▋        | 231/1380 [00:15<01:12, 15.89it/s] 17%|█▋        | 233/1380 [00:15<01:12, 15.87it/s] 17%|█▋        | 235/1380 [00:15<01:12, 15.83it/s] 17%|█▋        | 237/1380 [00:15<01:12, 15.82it/s] 17%|█▋        | 239/1380 [00:15<01:11, 15.85it/s] 17%|█▋        | 241/1380 [00:16<01:11, 15.90it/s] 18%|█▊        | 243/1380 [00:16<01:11, 15.86it/s] 18%|█▊        | 245/1380 [00:16<01:11, 15.77it/s] 18%|█▊        | 247/1380 [00:16<01:11, 15.76it/s] 18%|█▊        | 249/1380 [00:16<01:11, 15.73it/s] 18%|█▊        | 251/1380 [00:16<01:11, 15.75it/s] 18%|█▊        | 253/1380 [00:16<01:11, 15.86it/s] 18%|█▊        | 255/1380 [00:16<01:10, 15.87it/s] 19%|█▊        | 257/1380 [00:17<01:10, 15.84it/s] 19%|█▉        | 259/1380 [00:17<01:10, 15.86it/s] 19%|█▉        | 261/1380 [00:17<01:10, 15.83it/s] 19%|█▉        | 263/1380 [00:17<01:10, 15.81it/s] 19%|█▉        | 265/1380 [00:17<01:11, 15.59it/s] 19%|█▉        | 267/1380 [00:17<01:12, 15.34it/s] 19%|█▉        | 269/1380 [00:17<01:12, 15.36it/s] 20%|█▉        | 271/1380 [00:17<01:11, 15.50it/s] 20%|█▉        | 273/1380 [00:18<01:10, 15.60it/s] 20%|█▉        | 275/1380 [00:18<01:10, 15.74it/s]                                                   20%|██        | 276/1380 [00:18<01:10, 15.74it/s][INFO|trainer.py:755] 2023-11-15 23:38:30,199 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 23:38:30,201 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 23:38:30,201 >>   Num examples = 2204
[INFO|trainer.py:3134] 2023-11-15 23:38:30,201 >>   Batch size = 8
{'loss': 0.5081, 'learning_rate': 0.0004, 'epoch': 1.0}

  0%|          | 0/276 [00:00<?, ?it/s][A
  4%|▎         | 10/276 [00:00<00:02, 97.26it/s][A
  7%|▋         | 20/276 [00:00<00:02, 90.24it/s][A
 11%|█         | 30/276 [00:00<00:02, 88.73it/s][A
 14%|█▍        | 39/276 [00:00<00:02, 87.81it/s][A
 17%|█▋        | 48/276 [00:00<00:02, 87.17it/s][A
 21%|██        | 57/276 [00:00<00:02, 86.75it/s][A
 24%|██▍       | 66/276 [00:00<00:02, 86.67it/s][A
 27%|██▋       | 75/276 [00:00<00:02, 86.25it/s][A
 30%|███       | 84/276 [00:00<00:02, 86.62it/s][A
 34%|███▎      | 93/276 [00:01<00:02, 86.91it/s][A
 37%|███▋      | 102/276 [00:01<00:01, 87.15it/s][A
 40%|████      | 111/276 [00:01<00:01, 86.74it/s][A
 43%|████▎     | 120/276 [00:01<00:01, 86.92it/s][A
 47%|████▋     | 129/276 [00:01<00:01, 86.62it/s][A
 50%|█████     | 138/276 [00:01<00:01, 86.49it/s][A
 53%|█████▎    | 147/276 [00:01<00:01, 86.35it/s][A
 57%|█████▋    | 156/276 [00:01<00:01, 86.56it/s][A
 60%|█████▉    | 165/276 [00:01<00:01, 85.23it/s][A
 63%|██████▎   | 174/276 [00:02<00:01, 84.39it/s][A
 66%|██████▋   | 183/276 [00:02<00:01, 84.84it/s][A
 70%|██████▉   | 192/276 [00:02<00:00, 85.28it/s][A
 73%|███████▎  | 201/276 [00:02<00:00, 84.18it/s][A
 76%|███████▌  | 210/276 [00:02<00:00, 83.87it/s][A
 79%|███████▉  | 219/276 [00:02<00:00, 84.03it/s][A
 83%|████████▎ | 228/276 [00:02<00:00, 83.38it/s][A
 86%|████████▌ | 237/276 [00:02<00:00, 82.16it/s][A
 89%|████████▉ | 246/276 [00:02<00:00, 83.15it/s][A
 92%|█████████▏| 255/276 [00:02<00:00, 83.21it/s][A
 96%|█████████▌| 264/276 [00:03<00:00, 83.04it/s][A
 99%|█████████▉| 273/276 [00:03<00:00, 83.66it/s][A                                                  
                                                 [A 20%|██        | 276/1380 [00:21<01:10, 15.74it/s]
100%|██████████| 276/276 [00:03<00:00, 83.66it/s][A
                                                 [A 20%|██        | 277/1380 [00:21<10:11,  1.81it/s] 20%|██        | 279/1380 [00:21<07:28,  2.46it/s] 20%|██        | 281/1380 [00:21<05:33,  3.29it/s] 21%|██        | 283/1380 [00:21<04:13,  4.32it/s] 21%|██        | 285/1380 [00:22<03:18,  5.52it/s] 21%|██        | 287/1380 [00:22<02:39,  6.87it/s] 21%|██        | 289/1380 [00:22<02:11,  8.27it/s] 21%|██        | 291/1380 [00:22<01:52,  9.64it/s] 21%|██        | 293/1380 [00:22<01:39, 10.94it/s] 21%|██▏       | 295/1380 [00:22<01:29, 12.06it/s] 22%|██▏       | 297/1380 [00:22<01:23, 13.01it/s] 22%|██▏       | 299/1380 [00:22<01:18, 13.73it/s] 22%|██▏       | 301/1380 [00:23<01:15, 14.26it/s] 22%|██▏       | 303/1380 [00:23<01:13, 14.61it/s] 22%|██▏       | 305/1380 [00:23<01:12, 14.91it/s] 22%|██▏       | 307/1380 [00:23<01:11, 15.10it/s] 22%|██▏       | 309/1380 [00:23<01:10, 15.29it/s] 23%|██▎       | 311/1380 [00:23<01:09, 15.47it/s] 23%|██▎       | 313/1380 [00:23<01:08, 15.62it/s] 23%|██▎       | 315/1380 [00:24<01:08, 15.66it/s] 23%|██▎       | 317/1380 [00:24<01:07, 15.74it/s] 23%|██▎       | 319/1380 [00:24<01:07, 15.80it/s] 23%|██▎       | 321/1380 [00:24<01:06, 15.84it/s] 23%|██▎       | 323/1380 [00:24<01:06, 15.87it/s] 24%|██▎       | 325/1380 [00:24<01:06, 15.84it/s] 24%|██▎       | 327/1380 [00:24<01:06, 15.78it/s] 24%|██▍       | 329/1380 [00:24<01:06, 15.82it/s] 24%|██▍       | 331/1380 [00:25<01:06, 15.82it/s] 24%|██▍       | 333/1380 [00:25<01:06, 15.83it/s] 24%|██▍       | 335/1380 [00:25<01:05, 15.86it/s] 24%|██▍       | 337/1380 [00:25<01:05, 15.93it/s] 25%|██▍       | 339/1380 [00:25<01:05, 15.96it/s] 25%|██▍       | 341/1380 [00:25<01:04, 15.99it/s] 25%|██▍       | 343/1380 [00:25<01:04, 15.97it/s] 25%|██▌       | 345/1380 [00:25<01:05, 15.91it/s] 25%|██▌       | 347/1380 [00:26<01:05, 15.85it/s] 25%|██▌       | 349/1380 [00:26<01:05, 15.78it/s] 25%|██▌       | 351/1380 [00:26<01:05, 15.77it/s] 26%|██▌       | 353/1380 [00:26<01:21, 12.64it/s] 26%|██▌       | 355/1380 [00:26<01:16, 13.35it/s] 26%|██▌       | 357/1380 [00:26<01:13, 13.89it/s] 26%|██▌       | 359/1380 [00:26<01:11, 14.30it/s] 26%|██▌       | 361/1380 [00:27<01:10, 14.55it/s] 26%|██▋       | 363/1380 [00:27<01:08, 14.78it/s] 26%|██▋       | 365/1380 [00:27<01:07, 14.95it/s] 27%|██▋       | 367/1380 [00:27<01:07, 15.03it/s] 27%|██▋       | 369/1380 [00:27<01:06, 15.13it/s] 27%|██▋       | 371/1380 [00:27<01:06, 15.16it/s] 27%|██▋       | 373/1380 [00:27<01:06, 15.25it/s] 27%|██▋       | 375/1380 [00:27<01:06, 15.23it/s] 27%|██▋       | 377/1380 [00:28<01:05, 15.25it/s] 27%|██▋       | 379/1380 [00:28<01:05, 15.31it/s] 28%|██▊       | 381/1380 [00:28<01:04, 15.39it/s] 28%|██▊       | 383/1380 [00:28<01:04, 15.40it/s] 28%|██▊       | 385/1380 [00:28<01:04, 15.43it/s] 28%|██▊       | 387/1380 [00:28<01:04, 15.40it/s] 28%|██▊       | 389/1380 [00:28<01:04, 15.48it/s] 28%|██▊       | 391/1380 [00:28<01:03, 15.48it/s] 28%|██▊       | 393/1380 [00:29<01:03, 15.53it/s] 29%|██▊       | 395/1380 [00:29<01:03, 15.54it/s] 29%|██▉       | 397/1380 [00:29<01:03, 15.51it/s] 29%|██▉       | 399/1380 [00:29<01:03, 15.57it/s] 29%|██▉       | 401/1380 [00:29<01:03, 15.52it/s] 29%|██▉       | 403/1380 [00:29<01:02, 15.58it/s] 29%|██▉       | 405/1380 [00:29<01:02, 15.56it/s] 29%|██▉       | 407/1380 [00:30<01:02, 15.57it/s] 30%|██▉       | 409/1380 [00:30<01:02, 15.61it/s] 30%|██▉       | 411/1380 [00:30<01:02, 15.59it/s] 30%|██▉       | 413/1380 [00:30<01:01, 15.64it/s] 30%|███       | 415/1380 [00:30<01:01, 15.63it/s] 30%|███       | 417/1380 [00:30<01:01, 15.59it/s] 30%|███       | 419/1380 [00:30<01:01, 15.59it/s] 31%|███       | 421/1380 [00:30<01:01, 15.60it/s] 31%|███       | 423/1380 [00:31<01:01, 15.63it/s] 31%|███       | 425/1380 [00:31<01:01, 15.65it/s] 31%|███       | 427/1380 [00:31<01:01, 15.59it/s] 31%|███       | 429/1380 [00:31<01:00, 15.61it/s] 31%|███       | 431/1380 [00:31<01:00, 15.59it/s] 31%|███▏      | 433/1380 [00:31<01:00, 15.61it/s] 32%|███▏      | 435/1380 [00:31<01:00, 15.58it/s] 32%|███▏      | 437/1380 [00:31<01:00, 15.59it/s] 32%|███▏      | 439/1380 [00:32<01:00, 15.54it/s] 32%|███▏      | 441/1380 [00:32<01:00, 15.56it/s] 32%|███▏      | 443/1380 [00:32<01:00, 15.56it/s] 32%|███▏      | 445/1380 [00:32<01:00, 15.52it/s] 32%|███▏      | 447/1380 [00:32<00:59, 15.55it/s] 33%|███▎      | 449/1380 [00:32<00:59, 15.54it/s] 33%|███▎      | 451/1380 [00:32<00:59, 15.56it/s] 33%|███▎      | 453/1380 [00:32<00:59, 15.55it/s] 33%|███▎      | 455/1380 [00:33<00:59, 15.54it/s] 33%|███▎      | 457/1380 [00:33<00:59, 15.55it/s] 33%|███▎      | 459/1380 [00:33<00:59, 15.52it/s] 33%|███▎      | 461/1380 [00:33<00:59, 15.52it/s] 34%|███▎      | 463/1380 [00:33<00:59, 15.51it/s] 34%|███▎      | 465/1380 [00:33<00:58, 15.55it/s] 34%|███▍      | 467/1380 [00:33<00:58, 15.54it/s] 34%|███▍      | 469/1380 [00:33<00:58, 15.56it/s] 34%|███▍      | 471/1380 [00:34<00:58, 15.56it/s] 34%|███▍      | 473/1380 [00:34<00:58, 15.57it/s] 34%|███▍      | 475/1380 [00:34<00:58, 15.58it/s] 35%|███▍      | 477/1380 [00:34<00:58, 15.36it/s] 35%|███▍      | 479/1380 [00:34<00:58, 15.41it/s] 35%|███▍      | 481/1380 [00:34<00:58, 15.45it/s] 35%|███▌      | 483/1380 [00:34<00:57, 15.47it/s] 35%|███▌      | 485/1380 [00:35<00:57, 15.47it/s] 35%|███▌      | 487/1380 [00:35<00:57, 15.51it/s] 35%|███▌      | 489/1380 [00:35<00:57, 15.54it/s] 36%|███▌      | 491/1380 [00:35<00:57, 15.57it/s] 36%|███▌      | 493/1380 [00:35<00:56, 15.57it/s] 36%|███▌      | 495/1380 [00:35<00:56, 15.59it/s] 36%|███▌      | 497/1380 [00:35<00:56, 15.63it/s] 36%|███▌      | 499/1380 [00:35<00:56, 15.60it/s] 36%|███▋      | 501/1380 [00:36<00:56, 15.59it/s] 36%|███▋      | 503/1380 [00:36<00:56, 15.54it/s] 37%|███▋      | 505/1380 [00:36<00:56, 15.57it/s] 37%|███▋      | 507/1380 [00:36<00:56, 15.59it/s] 37%|███▋      | 509/1380 [00:36<00:55, 15.56it/s] 37%|███▋      | 511/1380 [00:36<00:55, 15.60it/s] 37%|███▋      | 513/1380 [00:36<00:55, 15.56it/s] 37%|███▋      | 515/1380 [00:36<00:55, 15.57it/s] 37%|███▋      | 517/1380 [00:37<00:55, 15.54it/s] 38%|███▊      | 519/1380 [00:37<00:55, 15.54it/s] 38%|███▊      | 521/1380 [00:37<00:55, 15.56it/s] 38%|███▊      | 523/1380 [00:37<00:55, 15.56it/s] 38%|███▊      | 525/1380 [00:37<00:54, 15.61it/s] 38%|███▊      | 527/1380 [00:37<00:54, 15.59it/s] 38%|███▊      | 529/1380 [00:37<00:54, 15.57it/s] 38%|███▊      | 531/1380 [00:37<00:54, 15.53it/s] 39%|███▊      | 533/1380 [00:38<00:54, 15.58it/s] 39%|███▉      | 535/1380 [00:38<00:54, 15.58it/s] 39%|███▉      | 537/1380 [00:38<00:54, 15.56it/s] 39%|███▉      | 539/1380 [00:38<00:54, 15.55it/s] 39%|███▉      | 541/1380 [00:38<00:53, 15.54it/s] 39%|███▉      | 543/1380 [00:38<00:53, 15.57it/s] 39%|███▉      | 545/1380 [00:38<00:53, 15.62it/s] 40%|███▉      | 547/1380 [00:39<00:53, 15.59it/s] 40%|███▉      | 549/1380 [00:39<00:53, 15.60it/s] 40%|███▉      | 551/1380 [00:39<00:53, 15.61it/s]                                                   40%|████      | 552/1380 [00:39<00:53, 15.61it/s][INFO|trainer.py:755] 2023-11-15 23:38:51,253 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 23:38:51,254 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 23:38:51,255 >>   Num examples = 2204
[INFO|trainer.py:3134] 2023-11-15 23:38:51,255 >>   Batch size = 8
{'eval_loss': 0.37639421224594116, 'eval_accuracy': 0.8706896551724138, 'eval_micro_f1': 0.8706896551724138, 'eval_macro_f1': 0.848891317054298, 'eval_runtime': 3.2868, 'eval_samples_per_second': 670.552, 'eval_steps_per_second': 83.971, 'epoch': 1.0}
{'loss': 0.3726, 'learning_rate': 0.0003, 'epoch': 2.0}

  0%|          | 0/276 [00:00<?, ?it/s][A
  4%|▎         | 10/276 [00:00<00:02, 89.95it/s][A
  7%|▋         | 19/276 [00:00<00:03, 82.42it/s][A
 10%|█         | 28/276 [00:00<00:03, 81.45it/s][A
 13%|█▎        | 37/276 [00:00<00:02, 80.02it/s][A
 17%|█▋        | 46/276 [00:00<00:02, 80.13it/s][A
 20%|█▉        | 55/276 [00:00<00:02, 78.48it/s][A
 23%|██▎       | 63/276 [00:00<00:02, 78.33it/s][A
 26%|██▌       | 71/276 [00:00<00:02, 77.50it/s][A
 29%|██▊       | 79/276 [00:00<00:02, 77.50it/s][A
 32%|███▏      | 87/276 [00:01<00:02, 77.36it/s][A
 34%|███▍      | 95/276 [00:01<00:02, 77.49it/s][A
 37%|███▋      | 103/276 [00:01<00:02, 78.19it/s][A
 40%|████      | 111/276 [00:01<00:02, 78.56it/s][A
 43%|████▎     | 119/276 [00:01<00:02, 77.15it/s][A
 46%|████▌     | 127/276 [00:01<00:01, 77.17it/s][A
 49%|████▉     | 135/276 [00:01<00:01, 76.30it/s][A
 52%|█████▏    | 143/276 [00:01<00:01, 75.00it/s][A
 55%|█████▍    | 151/276 [00:01<00:01, 75.60it/s][A
 58%|█████▊    | 159/276 [00:02<00:01, 75.71it/s][A
 61%|██████    | 167/276 [00:02<00:01, 76.04it/s][A
 63%|██████▎   | 175/276 [00:02<00:01, 76.55it/s][A
 66%|██████▋   | 183/276 [00:02<00:01, 76.75it/s][A
 69%|██████▉   | 191/276 [00:02<00:01, 77.68it/s][A
 72%|███████▏  | 199/276 [00:02<00:00, 77.68it/s][A
 75%|███████▌  | 207/276 [00:02<00:00, 76.66it/s][A
 78%|███████▊  | 215/276 [00:02<00:00, 76.05it/s][A
 81%|████████  | 223/276 [00:02<00:00, 76.10it/s][A
 84%|████████▎ | 231/276 [00:02<00:00, 75.55it/s][A
 87%|████████▋ | 239/276 [00:03<00:00, 76.29it/s][A
 89%|████████▉ | 247/276 [00:03<00:00, 75.22it/s][A
 92%|█████████▏| 255/276 [00:03<00:00, 75.84it/s][A
 95%|█████████▌| 263/276 [00:03<00:00, 76.47it/s][A
 98%|█████████▊| 271/276 [00:03<00:00, 76.11it/s][A                                                  
                                                 [A 40%|████      | 552/1380 [00:42<00:53, 15.61it/s]
100%|██████████| 276/276 [00:03<00:00, 76.11it/s][A
                                                 [A 40%|████      | 553/1380 [00:43<08:21,  1.65it/s] 40%|████      | 555/1380 [00:43<06:06,  2.25it/s] 40%|████      | 557/1380 [00:43<04:32,  3.02it/s] 41%|████      | 559/1380 [00:43<03:25,  3.99it/s] 41%|████      | 561/1380 [00:43<02:39,  5.13it/s] 41%|████      | 563/1380 [00:43<02:07,  6.42it/s] 41%|████      | 565/1380 [00:43<01:44,  7.78it/s] 41%|████      | 567/1380 [00:43<01:29,  9.12it/s] 41%|████      | 569/1380 [00:44<01:18, 10.40it/s] 41%|████▏     | 571/1380 [00:44<01:10, 11.54it/s] 42%|████▏     | 573/1380 [00:44<01:04, 12.48it/s] 42%|████▏     | 575/1380 [00:44<01:00, 13.27it/s] 42%|████▏     | 577/1380 [00:44<00:58, 13.84it/s] 42%|████▏     | 579/1380 [00:44<00:55, 14.31it/s] 42%|████▏     | 581/1380 [00:44<00:54, 14.59it/s] 42%|████▏     | 583/1380 [00:44<00:53, 14.83it/s] 42%|████▏     | 585/1380 [00:45<00:53, 14.95it/s] 43%|████▎     | 587/1380 [00:45<00:52, 15.10it/s] 43%|████▎     | 589/1380 [00:45<00:52, 15.20it/s] 43%|████▎     | 591/1380 [00:45<00:51, 15.31it/s] 43%|████▎     | 593/1380 [00:45<00:51, 15.35it/s] 43%|████▎     | 595/1380 [00:45<00:50, 15.41it/s] 43%|████▎     | 597/1380 [00:45<00:50, 15.44it/s] 43%|████▎     | 599/1380 [00:45<00:50, 15.48it/s] 44%|████▎     | 601/1380 [00:46<00:50, 15.48it/s] 44%|████▎     | 603/1380 [00:46<00:50, 15.48it/s] 44%|████▍     | 605/1380 [00:46<00:50, 15.48it/s] 44%|████▍     | 607/1380 [00:46<00:49, 15.48it/s] 44%|████▍     | 609/1380 [00:46<00:49, 15.51it/s] 44%|████▍     | 611/1380 [00:46<00:49, 15.48it/s] 44%|████▍     | 613/1380 [00:46<00:49, 15.49it/s] 45%|████▍     | 615/1380 [00:47<00:49, 15.48it/s] 45%|████▍     | 617/1380 [00:47<00:49, 15.51it/s] 45%|████▍     | 619/1380 [00:47<00:49, 15.49it/s] 45%|████▌     | 621/1380 [00:47<00:48, 15.50it/s] 45%|████▌     | 623/1380 [00:47<00:48, 15.50it/s] 45%|████▌     | 625/1380 [00:47<00:48, 15.50it/s] 45%|████▌     | 627/1380 [00:47<00:48, 15.53it/s] 46%|████▌     | 629/1380 [00:47<00:48, 15.50it/s] 46%|████▌     | 631/1380 [00:48<00:48, 15.51it/s] 46%|████▌     | 633/1380 [00:48<00:48, 15.49it/s] 46%|████▌     | 635/1380 [00:48<00:48, 15.50it/s] 46%|████▌     | 637/1380 [00:48<00:48, 15.46it/s] 46%|████▋     | 639/1380 [00:48<00:47, 15.48it/s] 46%|████▋     | 641/1380 [00:48<00:47, 15.45it/s] 47%|████▋     | 643/1380 [00:48<00:47, 15.50it/s] 47%|████▋     | 645/1380 [00:48<00:47, 15.46it/s] 47%|████▋     | 647/1380 [00:49<00:47, 15.47it/s] 47%|████▋     | 649/1380 [00:49<00:47, 15.49it/s] 47%|████▋     | 651/1380 [00:49<00:47, 15.50it/s] 47%|████▋     | 653/1380 [00:49<00:46, 15.53it/s] 47%|████▋     | 655/1380 [00:49<00:46, 15.50it/s] 48%|████▊     | 657/1380 [00:49<00:46, 15.52it/s] 48%|████▊     | 659/1380 [00:49<00:46, 15.46it/s] 48%|████▊     | 661/1380 [00:49<00:46, 15.49it/s] 48%|████▊     | 663/1380 [00:50<00:46, 15.42it/s] 48%|████▊     | 665/1380 [00:50<00:46, 15.40it/s] 48%|████▊     | 667/1380 [00:50<00:46, 15.37it/s] 48%|████▊     | 669/1380 [00:50<00:46, 15.40it/s] 49%|████▊     | 671/1380 [00:50<00:46, 15.41it/s] 49%|████▉     | 673/1380 [00:50<00:45, 15.45it/s] 49%|████▉     | 675/1380 [00:50<00:45, 15.39it/s] 49%|████▉     | 677/1380 [00:51<00:45, 15.44it/s] 49%|████▉     | 679/1380 [00:51<00:45, 15.39it/s] 49%|████▉     | 681/1380 [00:51<00:45, 15.45it/s] 49%|████▉     | 683/1380 [00:51<00:45, 15.44it/s] 50%|████▉     | 685/1380 [00:51<00:44, 15.50it/s] 50%|████▉     | 687/1380 [00:51<00:44, 15.50it/s] 50%|████▉     | 689/1380 [00:51<00:44, 15.51it/s] 50%|█████     | 691/1380 [00:51<00:44, 15.54it/s] 50%|█████     | 693/1380 [00:52<00:44, 15.47it/s] 50%|█████     | 695/1380 [00:52<00:44, 15.52it/s] 51%|█████     | 697/1380 [00:52<00:44, 15.50it/s] 51%|█████     | 699/1380 [00:52<00:43, 15.54it/s] 51%|█████     | 701/1380 [00:52<00:43, 15.45it/s] 51%|█████     | 703/1380 [00:52<00:43, 15.51it/s] 51%|█████     | 705/1380 [00:52<00:43, 15.50it/s] 51%|█████     | 707/1380 [00:52<00:43, 15.50it/s] 51%|█████▏    | 709/1380 [00:53<00:43, 15.50it/s] 52%|█████▏    | 711/1380 [00:53<00:43, 15.51it/s] 52%|█████▏    | 713/1380 [00:53<00:43, 15.50it/s] 52%|█████▏    | 715/1380 [00:53<00:42, 15.50it/s] 52%|█████▏    | 717/1380 [00:53<00:42, 15.51it/s] 52%|█████▏    | 719/1380 [00:53<00:42, 15.46it/s] 52%|█████▏    | 721/1380 [00:53<00:42, 15.49it/s] 52%|█████▏    | 723/1380 [00:54<00:42, 15.47it/s] 53%|█████▎    | 725/1380 [00:54<00:42, 15.51it/s] 53%|█████▎    | 727/1380 [00:54<00:42, 15.51it/s] 53%|█████▎    | 729/1380 [00:54<00:42, 15.44it/s] 53%|█████▎    | 731/1380 [00:54<00:42, 15.45it/s] 53%|█████▎    | 733/1380 [00:54<00:41, 15.46it/s] 53%|█████▎    | 735/1380 [00:54<00:41, 15.44it/s] 53%|█████▎    | 737/1380 [00:54<00:41, 15.43it/s] 54%|█████▎    | 739/1380 [00:55<00:41, 15.43it/s] 54%|█████▎    | 741/1380 [00:55<00:41, 15.45it/s] 54%|█████▍    | 743/1380 [00:55<00:41, 15.49it/s] 54%|█████▍    | 745/1380 [00:55<00:41, 15.46it/s] 54%|█████▍    | 747/1380 [00:55<00:40, 15.50it/s] 54%|█████▍    | 749/1380 [00:55<00:40, 15.50it/s] 54%|█████▍    | 751/1380 [00:55<00:40, 15.51it/s] 55%|█████▍    | 753/1380 [00:55<00:40, 15.49it/s] 55%|█████▍    | 755/1380 [00:56<00:40, 15.50it/s] 55%|█████▍    | 757/1380 [00:56<00:40, 15.49it/s] 55%|█████▌    | 759/1380 [00:56<00:40, 15.51it/s] 55%|█████▌    | 761/1380 [00:56<00:39, 15.50it/s] 55%|█████▌    | 763/1380 [00:56<00:39, 15.46it/s] 55%|█████▌    | 765/1380 [00:56<00:39, 15.46it/s] 56%|█████▌    | 767/1380 [00:56<00:39, 15.46it/s] 56%|█████▌    | 769/1380 [00:56<00:39, 15.47it/s] 56%|█████▌    | 771/1380 [00:57<00:39, 15.47it/s] 56%|█████▌    | 773/1380 [00:57<00:39, 15.49it/s] 56%|█████▌    | 775/1380 [00:57<00:39, 15.44it/s] 56%|█████▋    | 777/1380 [00:57<00:38, 15.48it/s] 56%|█████▋    | 779/1380 [00:57<00:38, 15.43it/s] 57%|█████▋    | 781/1380 [00:57<00:38, 15.48it/s] 57%|█████▋    | 783/1380 [00:57<00:38, 15.45it/s] 57%|█████▋    | 785/1380 [00:58<00:38, 15.50it/s] 57%|█████▋    | 787/1380 [00:58<00:38, 15.48it/s] 57%|█████▋    | 789/1380 [00:58<00:38, 15.51it/s] 57%|█████▋    | 791/1380 [00:58<00:38, 15.49it/s] 57%|█████▋    | 793/1380 [00:58<00:37, 15.49it/s] 58%|█████▊    | 795/1380 [00:58<00:37, 15.47it/s] 58%|█████▊    | 797/1380 [00:58<00:37, 15.47it/s] 58%|█████▊    | 799/1380 [00:58<00:37, 15.46it/s] 58%|█████▊    | 801/1380 [00:59<00:37, 15.45it/s] 58%|█████▊    | 803/1380 [00:59<00:37, 15.49it/s] 58%|█████▊    | 805/1380 [00:59<00:37, 15.49it/s] 58%|█████▊    | 807/1380 [00:59<00:36, 15.51it/s] 59%|█████▊    | 809/1380 [00:59<00:36, 15.52it/s] 59%|█████▉    | 811/1380 [00:59<00:36, 15.53it/s] 59%|█████▉    | 813/1380 [00:59<00:36, 15.49it/s] 59%|█████▉    | 815/1380 [00:59<00:36, 15.50it/s] 59%|█████▉    | 817/1380 [01:00<00:36, 15.51it/s] 59%|█████▉    | 819/1380 [01:00<00:36, 15.50it/s] 59%|█████▉    | 821/1380 [01:00<00:36, 15.50it/s] 60%|█████▉    | 823/1380 [01:00<00:35, 15.49it/s] 60%|█████▉    | 825/1380 [01:00<00:35, 15.47it/s] 60%|█████▉    | 827/1380 [01:00<00:35, 15.51it/s]                                                   60%|██████    | 828/1380 [01:00<00:35, 15.51it/s][INFO|trainer.py:755] 2023-11-15 23:39:12,706 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 23:39:12,709 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 23:39:12,709 >>   Num examples = 2204
[INFO|trainer.py:3134] 2023-11-15 23:39:12,709 >>   Batch size = 8
{'eval_loss': 0.3700932562351227, 'eval_accuracy': 0.8738656987295825, 'eval_micro_f1': 0.8738656987295825, 'eval_macro_f1': 0.8533420987288305, 'eval_runtime': 3.6363, 'eval_samples_per_second': 606.118, 'eval_steps_per_second': 75.902, 'epoch': 2.0}
{'loss': 0.3322, 'learning_rate': 0.0002, 'epoch': 3.0}

  0%|          | 0/276 [00:00<?, ?it/s][A
  3%|▎         | 9/276 [00:00<00:03, 86.30it/s][A
  7%|▋         | 18/276 [00:00<00:03, 81.45it/s][A
 10%|▉         | 27/276 [00:00<00:03, 78.61it/s][A
 13%|█▎        | 35/276 [00:00<00:03, 78.93it/s][A
 16%|█▌        | 43/276 [00:00<00:02, 78.42it/s][A
 18%|█▊        | 51/276 [00:00<00:02, 78.59it/s][A
 21%|██▏       | 59/276 [00:00<00:02, 78.51it/s][A
 24%|██▍       | 67/276 [00:00<00:02, 77.92it/s][A
 27%|██▋       | 75/276 [00:00<00:02, 77.97it/s][A
 30%|███       | 83/276 [00:01<00:02, 78.09it/s][A
 33%|███▎      | 91/276 [00:01<00:02, 78.21it/s][A
 36%|███▌      | 99/276 [00:01<00:02, 78.04it/s][A
 39%|███▉      | 107/276 [00:01<00:02, 78.03it/s][A
 42%|████▏     | 115/276 [00:01<00:02, 77.68it/s][A
 45%|████▍     | 123/276 [00:01<00:01, 76.72it/s][A
 47%|████▋     | 131/276 [00:01<00:01, 77.57it/s][A
 50%|█████     | 139/276 [00:01<00:01, 76.88it/s][A
 53%|█████▎    | 147/276 [00:01<00:01, 76.50it/s][A
 56%|█████▌    | 155/276 [00:01<00:01, 77.20it/s][A
 59%|█████▉    | 163/276 [00:02<00:01, 76.77it/s][A
 62%|██████▏   | 171/276 [00:02<00:01, 76.18it/s][A
 65%|██████▍   | 179/276 [00:02<00:01, 75.73it/s][A
 68%|██████▊   | 187/276 [00:02<00:01, 75.80it/s][A
 71%|███████   | 195/276 [00:02<00:01, 76.43it/s][A
 74%|███████▎  | 203/276 [00:02<00:00, 76.07it/s][A
 76%|███████▋  | 211/276 [00:02<00:00, 76.42it/s][A
 79%|███████▉  | 219/276 [00:02<00:00, 76.58it/s][A
 82%|████████▏ | 227/276 [00:02<00:00, 76.29it/s][A
 85%|████████▌ | 235/276 [00:03<00:00, 76.73it/s][A
 88%|████████▊ | 243/276 [00:03<00:00, 76.16it/s][A
 91%|█████████ | 251/276 [00:03<00:00, 76.53it/s][A
 94%|█████████▍| 259/276 [00:03<00:00, 76.84it/s][A
 97%|█████████▋| 267/276 [00:03<00:00, 77.67it/s][A
100%|█████████▉| 275/276 [00:03<00:00, 77.52it/s][A                                                  
                                                 [A 60%|██████    | 828/1380 [01:04<00:35, 15.51it/s]
100%|██████████| 276/276 [00:03<00:00, 77.52it/s][A
                                                 [A 60%|██████    | 829/1380 [01:04<05:34,  1.65it/s] 60%|██████    | 831/1380 [01:04<04:03,  2.25it/s] 60%|██████    | 833/1380 [01:04<03:00,  3.03it/s] 61%|██████    | 835/1380 [01:04<02:16,  3.99it/s] 61%|██████    | 837/1380 [01:04<01:45,  5.13it/s] 61%|██████    | 839/1380 [01:05<01:24,  6.41it/s] 61%|██████    | 841/1380 [01:05<01:09,  7.77it/s] 61%|██████    | 843/1380 [01:05<00:58,  9.12it/s] 61%|██████    | 845/1380 [01:05<00:51, 10.39it/s] 61%|██████▏   | 847/1380 [01:05<00:46, 11.53it/s] 62%|██████▏   | 849/1380 [01:05<00:42, 12.48it/s] 62%|██████▏   | 851/1380 [01:05<00:39, 13.26it/s] 62%|██████▏   | 853/1380 [01:06<00:38, 13.82it/s] 62%|██████▏   | 855/1380 [01:06<00:36, 14.27it/s] 62%|██████▏   | 857/1380 [01:06<00:35, 14.56it/s] 62%|██████▏   | 859/1380 [01:06<00:35, 14.82it/s] 62%|██████▏   | 861/1380 [01:06<00:34, 14.98it/s] 63%|██████▎   | 863/1380 [01:06<00:34, 15.12it/s] 63%|██████▎   | 865/1380 [01:06<00:33, 15.20it/s] 63%|██████▎   | 867/1380 [01:06<00:33, 15.26it/s] 63%|██████▎   | 869/1380 [01:07<00:33, 15.31it/s] 63%|██████▎   | 871/1380 [01:07<00:33, 15.39it/s] 63%|██████▎   | 873/1380 [01:07<00:32, 15.39it/s] 63%|██████▎   | 875/1380 [01:07<00:32, 15.40it/s] 64%|██████▎   | 877/1380 [01:07<00:32, 15.40it/s] 64%|██████▎   | 879/1380 [01:07<00:32, 15.42it/s] 64%|██████▍   | 881/1380 [01:07<00:32, 15.41it/s] 64%|██████▍   | 883/1380 [01:07<00:32, 15.39it/s] 64%|██████▍   | 885/1380 [01:08<00:32, 15.39it/s] 64%|██████▍   | 887/1380 [01:08<00:32, 15.39it/s] 64%|██████▍   | 889/1380 [01:08<00:31, 15.41it/s] 65%|██████▍   | 891/1380 [01:08<00:31, 15.41it/s] 65%|██████▍   | 893/1380 [01:08<00:31, 15.40it/s] 65%|██████▍   | 895/1380 [01:08<00:31, 15.40it/s] 65%|██████▌   | 897/1380 [01:08<00:31, 15.41it/s] 65%|██████▌   | 899/1380 [01:09<00:31, 15.40it/s] 65%|██████▌   | 901/1380 [01:09<00:31, 15.38it/s] 65%|██████▌   | 903/1380 [01:09<00:30, 15.41it/s] 66%|██████▌   | 905/1380 [01:09<00:30, 15.40it/s] 66%|██████▌   | 907/1380 [01:09<00:30, 15.40it/s] 66%|██████▌   | 909/1380 [01:09<00:30, 15.43it/s] 66%|██████▌   | 911/1380 [01:09<00:30, 15.42it/s] 66%|██████▌   | 913/1380 [01:09<00:30, 15.40it/s] 66%|██████▋   | 915/1380 [01:10<00:30, 15.39it/s] 66%|██████▋   | 917/1380 [01:10<00:30, 15.40it/s] 67%|██████▋   | 919/1380 [01:10<00:29, 15.39it/s] 67%|██████▋   | 921/1380 [01:10<00:29, 15.37it/s] 67%|██████▋   | 923/1380 [01:10<00:30, 14.88it/s] 67%|██████▋   | 925/1380 [01:10<00:30, 15.05it/s] 67%|██████▋   | 927/1380 [01:10<00:29, 15.14it/s] 67%|██████▋   | 929/1380 [01:10<00:29, 15.22it/s] 67%|██████▋   | 931/1380 [01:11<00:29, 15.27it/s] 68%|██████▊   | 933/1380 [01:11<00:29, 15.34it/s] 68%|██████▊   | 935/1380 [01:11<00:28, 15.35it/s] 68%|██████▊   | 937/1380 [01:11<00:28, 15.38it/s] 68%|██████▊   | 939/1380 [01:11<00:28, 15.37it/s] 68%|██████▊   | 941/1380 [01:11<00:28, 15.43it/s] 68%|██████▊   | 943/1380 [01:11<00:28, 15.43it/s] 68%|██████▊   | 945/1380 [01:12<00:28, 15.42it/s] 69%|██████▊   | 947/1380 [01:12<00:28, 15.43it/s] 69%|██████▉   | 949/1380 [01:12<00:27, 15.43it/s] 69%|██████▉   | 951/1380 [01:12<00:27, 15.42it/s] 69%|██████▉   | 953/1380 [01:12<00:27, 15.40it/s] 69%|██████▉   | 955/1380 [01:12<00:27, 15.32it/s] 69%|██████▉   | 957/1380 [01:12<00:27, 15.32it/s] 69%|██████▉   | 959/1380 [01:12<00:27, 15.33it/s] 70%|██████▉   | 961/1380 [01:13<00:27, 15.38it/s] 70%|██████▉   | 963/1380 [01:13<00:27, 15.34it/s] 70%|██████▉   | 965/1380 [01:13<00:27, 15.34it/s] 70%|███████   | 967/1380 [01:13<00:26, 15.36it/s] 70%|███████   | 969/1380 [01:13<00:26, 15.39it/s] 70%|███████   | 971/1380 [01:13<00:26, 15.37it/s] 71%|███████   | 973/1380 [01:13<00:26, 15.41it/s] 71%|███████   | 975/1380 [01:13<00:26, 15.37it/s] 71%|███████   | 977/1380 [01:14<00:26, 15.36it/s] 71%|███████   | 979/1380 [01:14<00:26, 15.35it/s] 71%|███████   | 981/1380 [01:14<00:25, 15.37it/s] 71%|███████   | 983/1380 [01:14<00:25, 15.35it/s] 71%|███████▏  | 985/1380 [01:14<00:25, 15.38it/s] 72%|███████▏  | 987/1380 [01:14<00:25, 15.39it/s] 72%|███████▏  | 989/1380 [01:14<00:25, 15.40it/s] 72%|███████▏  | 991/1380 [01:14<00:25, 15.37it/s] 72%|███████▏  | 993/1380 [01:15<00:25, 15.36it/s] 72%|███████▏  | 995/1380 [01:15<00:25, 15.36it/s] 72%|███████▏  | 997/1380 [01:15<00:24, 15.42it/s] 72%|███████▏  | 999/1380 [01:15<00:24, 15.37it/s] 73%|███████▎  | 1001/1380 [01:15<00:24, 15.42it/s] 73%|███████▎  | 1003/1380 [01:15<00:24, 15.40it/s] 73%|███████▎  | 1005/1380 [01:15<00:24, 15.41it/s] 73%|███████▎  | 1007/1380 [01:16<00:24, 15.40it/s] 73%|███████▎  | 1009/1380 [01:16<00:24, 15.41it/s] 73%|███████▎  | 1011/1380 [01:16<00:24, 15.36it/s] 73%|███████▎  | 1013/1380 [01:16<00:23, 15.37it/s] 74%|███████▎  | 1015/1380 [01:16<00:23, 15.32it/s] 74%|███████▎  | 1017/1380 [01:16<00:23, 15.35it/s] 74%|███████▍  | 1019/1380 [01:16<00:23, 15.36it/s] 74%|███████▍  | 1021/1380 [01:16<00:23, 15.39it/s] 74%|███████▍  | 1023/1380 [01:17<00:23, 15.36it/s] 74%|███████▍  | 1025/1380 [01:17<00:23, 15.39it/s] 74%|███████▍  | 1027/1380 [01:17<00:22, 15.37it/s] 75%|███████▍  | 1029/1380 [01:17<00:22, 15.37it/s] 75%|███████▍  | 1031/1380 [01:17<00:22, 15.37it/s] 75%|███████▍  | 1033/1380 [01:17<00:22, 15.39it/s] 75%|███████▌  | 1035/1380 [01:17<00:22, 15.37it/s] 75%|███████▌  | 1037/1380 [01:17<00:22, 15.38it/s] 75%|███████▌  | 1039/1380 [01:18<00:22, 15.38it/s] 75%|███████▌  | 1041/1380 [01:18<00:22, 15.39it/s] 76%|███████▌  | 1043/1380 [01:18<00:21, 15.38it/s] 76%|███████▌  | 1045/1380 [01:18<00:21, 15.40it/s] 76%|███████▌  | 1047/1380 [01:18<00:21, 15.38it/s] 76%|███████▌  | 1049/1380 [01:18<00:21, 15.37it/s] 76%|███████▌  | 1051/1380 [01:18<00:21, 15.37it/s] 76%|███████▋  | 1053/1380 [01:19<00:21, 15.39it/s] 76%|███████▋  | 1055/1380 [01:19<00:21, 15.40it/s] 77%|███████▋  | 1057/1380 [01:19<00:20, 15.40it/s] 77%|███████▋  | 1059/1380 [01:19<00:20, 15.39it/s] 77%|███████▋  | 1061/1380 [01:19<00:20, 15.43it/s] 77%|███████▋  | 1063/1380 [01:19<00:20, 15.41it/s] 77%|███████▋  | 1065/1380 [01:19<00:20, 15.41it/s] 77%|███████▋  | 1067/1380 [01:19<00:20, 15.38it/s] 77%|███████▋  | 1069/1380 [01:20<00:20, 15.38it/s] 78%|███████▊  | 1071/1380 [01:20<00:20, 15.37it/s] 78%|███████▊  | 1073/1380 [01:20<00:19, 15.40it/s] 78%|███████▊  | 1075/1380 [01:20<00:19, 15.45it/s] 78%|███████▊  | 1077/1380 [01:20<00:19, 15.56it/s] 78%|███████▊  | 1079/1380 [01:20<00:19, 15.63it/s] 78%|███████▊  | 1081/1380 [01:20<00:19, 15.67it/s] 78%|███████▊  | 1083/1380 [01:20<00:18, 15.70it/s] 79%|███████▊  | 1085/1380 [01:21<00:18, 15.71it/s] 79%|███████▉  | 1087/1380 [01:21<00:18, 15.73it/s] 79%|███████▉  | 1089/1380 [01:21<00:18, 15.75it/s] 79%|███████▉  | 1091/1380 [01:21<00:18, 15.74it/s] 79%|███████▉  | 1093/1380 [01:21<00:18, 15.73it/s] 79%|███████▉  | 1095/1380 [01:21<00:18, 15.74it/s] 79%|███████▉  | 1097/1380 [01:21<00:17, 15.77it/s] 80%|███████▉  | 1099/1380 [01:21<00:17, 15.76it/s] 80%|███████▉  | 1101/1380 [01:22<00:17, 15.80it/s] 80%|███████▉  | 1103/1380 [01:22<00:17, 15.81it/s]                                                    80%|████████  | 1104/1380 [01:22<00:17, 15.81it/s][INFO|trainer.py:755] 2023-11-15 23:39:34,215 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 23:39:34,216 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 23:39:34,216 >>   Num examples = 2204
[INFO|trainer.py:3134] 2023-11-15 23:39:34,216 >>   Batch size = 8
{'eval_loss': 0.3946424722671509, 'eval_accuracy': 0.8525408348457351, 'eval_micro_f1': 0.8525408348457351, 'eval_macro_f1': 0.8337752243980424, 'eval_runtime': 3.6303, 'eval_samples_per_second': 607.112, 'eval_steps_per_second': 76.027, 'epoch': 3.0}
{'loss': 0.296, 'learning_rate': 0.0001, 'epoch': 4.0}

  0%|          | 0/276 [00:00<?, ?it/s][A
  4%|▎         | 10/276 [00:00<00:02, 94.61it/s][A
  7%|▋         | 20/276 [00:00<00:03, 83.26it/s][A
 11%|█         | 29/276 [00:00<00:02, 84.46it/s][A
 14%|█▍        | 38/276 [00:00<00:02, 84.58it/s][A
 17%|█▋        | 47/276 [00:00<00:02, 84.92it/s][A
 20%|██        | 56/276 [00:00<00:02, 84.67it/s][A
 24%|██▎       | 65/276 [00:00<00:02, 84.55it/s][A
 27%|██▋       | 74/276 [00:00<00:02, 82.59it/s][A
 30%|███       | 83/276 [00:01<00:02, 79.61it/s][A
 33%|███▎      | 92/276 [00:01<00:02, 81.40it/s][A
 37%|███▋      | 101/276 [00:01<00:02, 83.04it/s][A
 40%|███▉      | 110/276 [00:01<00:01, 83.00it/s][A
 43%|████▎     | 119/276 [00:01<00:01, 84.05it/s][A
 46%|████▋     | 128/276 [00:01<00:01, 84.30it/s][A
 50%|████▉     | 137/276 [00:01<00:02, 62.40it/s][A
 53%|█████▎    | 145/276 [00:01<00:02, 65.36it/s][A
 55%|█████▌    | 153/276 [00:01<00:01, 68.26it/s][A
 58%|█████▊    | 161/276 [00:02<00:01, 70.58it/s][A
 61%|██████    | 169/276 [00:02<00:01, 72.18it/s][A
 64%|██████▍   | 177/276 [00:02<00:01, 72.88it/s][A
 67%|██████▋   | 185/276 [00:02<00:01, 73.25it/s][A
 70%|██████▉   | 193/276 [00:02<00:01, 74.69it/s][A
 73%|███████▎  | 201/276 [00:02<00:01, 73.69it/s][A
 76%|███████▌  | 209/276 [00:02<00:00, 73.25it/s][A
 79%|███████▊  | 217/276 [00:02<00:00, 72.85it/s][A
 82%|████████▏ | 225/276 [00:02<00:00, 73.46it/s][A
 84%|████████▍ | 233/276 [00:03<00:00, 74.50it/s][A
 87%|████████▋ | 241/276 [00:03<00:00, 74.17it/s][A
 90%|█████████ | 249/276 [00:03<00:00, 74.12it/s][A
 93%|█████████▎| 257/276 [00:03<00:00, 74.03it/s][A
 96%|█████████▌| 265/276 [00:03<00:00, 74.50it/s][A
 99%|█████████▉| 273/276 [00:03<00:00, 74.30it/s][A                                                   
                                                 [A 80%|████████  | 1104/1380 [01:25<00:17, 15.81it/s]
100%|██████████| 276/276 [00:03<00:00, 74.30it/s][A
                                                 [A 80%|████████  | 1105/1380 [01:26<02:49,  1.62it/s] 80%|████████  | 1107/1380 [01:26<02:03,  2.21it/s] 80%|████████  | 1109/1380 [01:26<01:30,  2.98it/s] 81%|████████  | 1111/1380 [01:26<01:08,  3.93it/s] 81%|████████  | 1113/1380 [01:26<00:52,  5.07it/s] 81%|████████  | 1115/1380 [01:26<00:41,  6.35it/s] 81%|████████  | 1117/1380 [01:26<00:34,  7.72it/s] 81%|████████  | 1119/1380 [01:26<00:28,  9.09it/s] 81%|████████  | 1121/1380 [01:27<00:24, 10.36it/s] 81%|████████▏ | 1123/1380 [01:27<00:22, 11.49it/s] 82%|████████▏ | 1125/1380 [01:27<00:20, 12.41it/s] 82%|████████▏ | 1127/1380 [01:27<00:19, 13.17it/s] 82%|████████▏ | 1129/1380 [01:27<00:18, 13.80it/s] 82%|████████▏ | 1131/1380 [01:27<00:17, 14.20it/s] 82%|████████▏ | 1133/1380 [01:27<00:16, 14.56it/s] 82%|████████▏ | 1135/1380 [01:27<00:16, 14.78it/s] 82%|████████▏ | 1137/1380 [01:28<00:16, 14.97it/s] 83%|████████▎ | 1139/1380 [01:28<00:15, 15.08it/s] 83%|████████▎ | 1141/1380 [01:28<00:15, 15.22it/s] 83%|████████▎ | 1143/1380 [01:28<00:15, 15.28it/s] 83%|████████▎ | 1145/1380 [01:28<00:15, 15.33it/s] 83%|████████▎ | 1147/1380 [01:28<00:15, 15.36it/s] 83%|████████▎ | 1149/1380 [01:28<00:14, 15.41it/s] 83%|████████▎ | 1151/1380 [01:29<00:14, 15.38it/s] 84%|████████▎ | 1153/1380 [01:29<00:14, 15.41it/s] 84%|████████▎ | 1155/1380 [01:29<00:14, 15.41it/s] 84%|████████▍ | 1157/1380 [01:29<00:14, 15.44it/s] 84%|████████▍ | 1159/1380 [01:29<00:14, 15.43it/s] 84%|████████▍ | 1161/1380 [01:29<00:14, 15.44it/s] 84%|████████▍ | 1163/1380 [01:29<00:14, 15.47it/s] 84%|████████▍ | 1165/1380 [01:29<00:13, 15.44it/s] 85%|████████▍ | 1167/1380 [01:30<00:13, 15.39it/s] 85%|████████▍ | 1169/1380 [01:30<00:13, 15.40it/s] 85%|████████▍ | 1171/1380 [01:30<00:13, 15.41it/s] 85%|████████▌ | 1173/1380 [01:30<00:13, 15.40it/s] 85%|████████▌ | 1175/1380 [01:30<00:13, 15.43it/s] 85%|████████▌ | 1177/1380 [01:30<00:13, 15.42it/s] 85%|████████▌ | 1179/1380 [01:30<00:13, 15.41it/s] 86%|████████▌ | 1181/1380 [01:30<00:12, 15.38it/s] 86%|████████▌ | 1183/1380 [01:31<00:12, 15.41it/s] 86%|████████▌ | 1185/1380 [01:31<00:12, 15.44it/s] 86%|████████▌ | 1187/1380 [01:31<00:12, 15.49it/s] 86%|████████▌ | 1189/1380 [01:31<00:12, 15.48it/s] 86%|████████▋ | 1191/1380 [01:31<00:12, 15.51it/s] 86%|████████▋ | 1193/1380 [01:31<00:12, 15.50it/s] 87%|████████▋ | 1195/1380 [01:31<00:11, 15.48it/s] 87%|████████▋ | 1197/1380 [01:32<00:11, 15.44it/s] 87%|████████▋ | 1199/1380 [01:32<00:11, 15.44it/s] 87%|████████▋ | 1201/1380 [01:32<00:11, 15.41it/s] 87%|████████▋ | 1203/1380 [01:32<00:11, 15.38it/s] 87%|████████▋ | 1205/1380 [01:32<00:11, 15.40it/s] 87%|████████▋ | 1207/1380 [01:32<00:11, 15.43it/s] 88%|████████▊ | 1209/1380 [01:32<00:11, 15.45it/s] 88%|████████▊ | 1211/1380 [01:32<00:10, 15.40it/s] 88%|████████▊ | 1213/1380 [01:33<00:10, 15.37it/s] 88%|████████▊ | 1215/1380 [01:33<00:10, 15.38it/s] 88%|████████▊ | 1217/1380 [01:33<00:10, 15.40it/s] 88%|████████▊ | 1219/1380 [01:33<00:10, 15.44it/s] 88%|████████▊ | 1221/1380 [01:33<00:10, 15.45it/s] 89%|████████▊ | 1223/1380 [01:33<00:10, 15.46it/s] 89%|████████▉ | 1225/1380 [01:33<00:10, 15.48it/s] 89%|████████▉ | 1227/1380 [01:33<00:09, 15.46it/s] 89%|████████▉ | 1229/1380 [01:34<00:09, 15.48it/s] 89%|████████▉ | 1231/1380 [01:34<00:09, 15.45it/s] 89%|████████▉ | 1233/1380 [01:34<00:09, 15.47it/s] 89%|████████▉ | 1235/1380 [01:34<00:09, 15.42it/s] 90%|████████▉ | 1237/1380 [01:34<00:09, 15.45it/s] 90%|████████▉ | 1239/1380 [01:34<00:09, 15.42it/s] 90%|████████▉ | 1241/1380 [01:34<00:08, 15.46it/s] 90%|█████████ | 1243/1380 [01:34<00:08, 15.45it/s] 90%|█████████ | 1245/1380 [01:35<00:08, 15.44it/s] 90%|█████████ | 1247/1380 [01:35<00:08, 15.39it/s] 91%|█████████ | 1249/1380 [01:35<00:08, 15.43it/s] 91%|█████████ | 1251/1380 [01:35<00:08, 15.43it/s] 91%|█████████ | 1253/1380 [01:35<00:08, 15.41it/s] 91%|█████████ | 1255/1380 [01:35<00:08, 15.41it/s] 91%|█████████ | 1257/1380 [01:35<00:07, 15.41it/s] 91%|█████████ | 1259/1380 [01:36<00:07, 15.42it/s] 91%|█████████▏| 1261/1380 [01:36<00:07, 15.44it/s] 92%|█████████▏| 1263/1380 [01:36<00:07, 15.45it/s] 92%|█████████▏| 1265/1380 [01:36<00:07, 15.47it/s] 92%|█████████▏| 1267/1380 [01:36<00:07, 15.47it/s] 92%|█████████▏| 1269/1380 [01:36<00:07, 15.46it/s] 92%|█████████▏| 1271/1380 [01:36<00:07, 15.47it/s] 92%|█████████▏| 1273/1380 [01:36<00:06, 15.45it/s] 92%|█████████▏| 1275/1380 [01:37<00:06, 15.41it/s] 93%|█████████▎| 1277/1380 [01:37<00:06, 15.39it/s] 93%|█████████▎| 1279/1380 [01:37<00:06, 15.41it/s] 93%|█████████▎| 1281/1380 [01:37<00:06, 15.39it/s] 93%|█████████▎| 1283/1380 [01:37<00:06, 15.38it/s] 93%|█████████▎| 1285/1380 [01:37<00:06, 15.39it/s] 93%|█████████▎| 1287/1380 [01:37<00:06, 15.42it/s] 93%|█████████▎| 1289/1380 [01:37<00:05, 15.43it/s] 94%|█████████▎| 1291/1380 [01:38<00:05, 15.43it/s] 94%|█████████▎| 1293/1380 [01:38<00:05, 15.41it/s] 94%|█████████▍| 1295/1380 [01:38<00:05, 15.45it/s] 94%|█████████▍| 1297/1380 [01:38<00:05, 15.43it/s] 94%|█████████▍| 1299/1380 [01:38<00:05, 15.46it/s] 94%|█████████▍| 1301/1380 [01:38<00:05, 15.45it/s] 94%|█████████▍| 1303/1380 [01:38<00:04, 15.46it/s] 95%|█████████▍| 1305/1380 [01:39<00:04, 15.42it/s] 95%|█████████▍| 1307/1380 [01:39<00:04, 15.40it/s] 95%|█████████▍| 1309/1380 [01:39<00:04, 15.40it/s] 95%|█████████▌| 1311/1380 [01:39<00:04, 15.45it/s] 95%|█████████▌| 1313/1380 [01:39<00:04, 15.45it/s] 95%|█████████▌| 1315/1380 [01:39<00:04, 15.47it/s] 95%|█████████▌| 1317/1380 [01:39<00:04, 15.42it/s] 96%|█████████▌| 1319/1380 [01:39<00:03, 15.44it/s] 96%|█████████▌| 1321/1380 [01:40<00:03, 15.44it/s] 96%|█████████▌| 1323/1380 [01:40<00:03, 15.42it/s] 96%|█████████▌| 1325/1380 [01:40<00:03, 15.41it/s] 96%|█████████▌| 1327/1380 [01:40<00:03, 15.40it/s] 96%|█████████▋| 1329/1380 [01:40<00:03, 15.39it/s] 96%|█████████▋| 1331/1380 [01:40<00:03, 15.15it/s] 97%|█████████▋| 1333/1380 [01:40<00:03, 15.22it/s] 97%|█████████▋| 1335/1380 [01:40<00:02, 15.27it/s] 97%|█████████▋| 1337/1380 [01:41<00:02, 15.31it/s] 97%|█████████▋| 1339/1380 [01:41<00:02, 15.39it/s] 97%|█████████▋| 1341/1380 [01:41<00:02, 15.41it/s] 97%|█████████▋| 1343/1380 [01:41<00:02, 15.48it/s] 97%|█████████▋| 1345/1380 [01:41<00:02, 15.48it/s] 98%|█████████▊| 1347/1380 [01:41<00:02, 15.47it/s] 98%|█████████▊| 1349/1380 [01:41<00:02, 15.46it/s] 98%|█████████▊| 1351/1380 [01:41<00:01, 15.45it/s] 98%|█████████▊| 1353/1380 [01:42<00:01, 15.43it/s] 98%|█████████▊| 1355/1380 [01:42<00:01, 15.45it/s] 98%|█████████▊| 1357/1380 [01:42<00:01, 15.44it/s] 98%|█████████▊| 1359/1380 [01:42<00:01, 15.43it/s] 99%|█████████▊| 1361/1380 [01:42<00:01, 15.43it/s] 99%|█████████▉| 1363/1380 [01:42<00:01, 15.42it/s] 99%|█████████▉| 1365/1380 [01:42<00:00, 15.40it/s] 99%|█████████▉| 1367/1380 [01:43<00:00, 15.42it/s] 99%|█████████▉| 1369/1380 [01:43<00:00, 15.41it/s] 99%|█████████▉| 1371/1380 [01:43<00:00, 15.42it/s] 99%|█████████▉| 1373/1380 [01:43<00:00, 15.46it/s]100%|█████████▉| 1375/1380 [01:43<00:00, 15.46it/s]100%|█████████▉| 1377/1380 [01:43<00:00, 15.49it/s]100%|█████████▉| 1379/1380 [01:43<00:00, 15.51it/s]                                                   100%|██████████| 1380/1380 [01:43<00:00, 15.51it/s][INFO|trainer.py:755] 2023-11-15 23:39:55,793 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 23:39:55,795 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 23:39:55,795 >>   Num examples = 2204
[INFO|trainer.py:3134] 2023-11-15 23:39:55,795 >>   Batch size = 8
{'eval_loss': 0.40474218130111694, 'eval_accuracy': 0.8602540834845736, 'eval_micro_f1': 0.8602540834845736, 'eval_macro_f1': 0.8444545543009516, 'eval_runtime': 3.6977, 'eval_samples_per_second': 596.048, 'eval_steps_per_second': 74.641, 'epoch': 4.0}
{'loss': 0.2593, 'learning_rate': 0.0, 'epoch': 5.0}

  0%|          | 0/276 [00:00<?, ?it/s][A
  3%|▎         | 9/276 [00:00<00:02, 89.37it/s][A
  7%|▋         | 18/276 [00:00<00:03, 79.27it/s][A
 10%|▉         | 27/276 [00:00<00:03, 79.89it/s][A
 13%|█▎        | 36/276 [00:00<00:03, 79.55it/s][A
 16%|█▋        | 45/276 [00:00<00:02, 79.87it/s][A
 20%|█▉        | 54/276 [00:00<00:02, 79.92it/s][A
 23%|██▎       | 63/276 [00:00<00:02, 79.55it/s][A
 26%|██▌       | 72/276 [00:00<00:02, 80.61it/s][A
 29%|██▉       | 81/276 [00:01<00:02, 80.21it/s][A
 33%|███▎      | 90/276 [00:01<00:02, 80.03it/s][A
 36%|███▌      | 99/276 [00:01<00:02, 79.14it/s][A
 39%|███▉      | 107/276 [00:01<00:02, 79.33it/s][A
 42%|████▏     | 115/276 [00:01<00:02, 79.36it/s][A
 45%|████▍     | 123/276 [00:01<00:01, 79.08it/s][A
 47%|████▋     | 131/276 [00:01<00:01, 78.95it/s][A
 50%|█████     | 139/276 [00:01<00:01, 78.58it/s][A
 53%|█████▎    | 147/276 [00:01<00:01, 78.60it/s][A
 57%|█████▋    | 156/276 [00:01<00:01, 79.29it/s][A
 59%|█████▉    | 164/276 [00:02<00:01, 79.35it/s][A
 62%|██████▏   | 172/276 [00:02<00:01, 79.30it/s][A
 66%|██████▌   | 181/276 [00:02<00:01, 79.51it/s][A
 69%|██████▉   | 190/276 [00:02<00:01, 79.90it/s][A
 72%|███████▏  | 198/276 [00:02<00:00, 79.81it/s][A
 75%|███████▍  | 206/276 [00:02<00:00, 79.67it/s][A
 78%|███████▊  | 214/276 [00:02<00:00, 79.59it/s][A
 80%|████████  | 222/276 [00:02<00:00, 78.95it/s][A
 84%|████████▎ | 231/276 [00:02<00:00, 79.17it/s][A
 87%|████████▋ | 239/276 [00:03<00:00, 77.82it/s][A
 89%|████████▉ | 247/276 [00:03<00:00, 76.62it/s][A
 92%|█████████▏| 255/276 [00:03<00:00, 75.67it/s][A
 95%|█████████▌| 263/276 [00:03<00:00, 75.71it/s][A
 99%|█████████▊| 272/276 [00:03<00:00, 76.98it/s][A                                                   
                                                 [A100%|██████████| 1380/1380 [01:47<00:00, 15.51it/s]
100%|██████████| 276/276 [00:03<00:00, 76.98it/s][A
                                                 [A[INFO|trainer.py:1963] 2023-11-15 23:39:59,359 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                   100%|██████████| 1380/1380 [01:47<00:00, 15.51it/s]100%|██████████| 1380/1380 [01:47<00:00, 12.85it/s]
[INFO|trainer.py:2855] 2023-11-15 23:39:59,362 >> Saving model checkpoint to ./result/acl_roberta-base_seed3_adapter
[INFO|configuration_utils.py:460] 2023-11-15 23:39:59,365 >> Configuration saved in ./result/acl_roberta-base_seed3_adapter/config.json
[INFO|modeling_utils.py:1997] 2023-11-15 23:40:00,515 >> Model weights saved in ./result/acl_roberta-base_seed3_adapter/pytorch_model.bin
[INFO|tokenization_utils_base.py:2235] 2023-11-15 23:40:00,518 >> tokenizer config file saved in ./result/acl_roberta-base_seed3_adapter/tokenizer_config.json
[INFO|tokenization_utils_base.py:2242] 2023-11-15 23:40:00,520 >> Special tokens file saved in ./result/acl_roberta-base_seed3_adapter/special_tokens_map.json
{'eval_loss': 0.3764922022819519, 'eval_accuracy': 0.867513611615245, 'eval_micro_f1': 0.867513611615245, 'eval_macro_f1': 0.8501961247259637, 'eval_runtime': 3.5602, 'eval_samples_per_second': 619.071, 'eval_steps_per_second': 77.524, 'epoch': 5.0}
{'train_runtime': 107.4177, 'train_samples_per_second': 410.361, 'train_steps_per_second': 12.847, 'train_loss': 0.3536642931509709, 'epoch': 5.0}
***** train metrics *****
  epoch                    =        5.0
  train_loss               =     0.3537
  train_runtime            = 0:01:47.41
  train_samples            =       8816
  train_samples_per_second =    410.361
  train_steps_per_second   =     12.847
11/15/2023 23:40:00 - INFO - __main__ - *** Evaluate ***
[INFO|trainer.py:755] 2023-11-15 23:40:00,722 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 23:40:00,724 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 23:40:00,724 >>   Num examples = 2204
[INFO|trainer.py:3134] 2023-11-15 23:40:00,725 >>   Batch size = 8
  0%|          | 0/276 [00:00<?, ?it/s]  4%|▎         | 10/276 [00:00<00:02, 90.54it/s]  7%|▋         | 20/276 [00:00<00:03, 84.12it/s] 11%|█         | 29/276 [00:00<00:03, 80.00it/s] 14%|█▍        | 38/276 [00:00<00:02, 80.37it/s] 17%|█▋        | 47/276 [00:00<00:02, 80.05it/s] 20%|██        | 56/276 [00:00<00:02, 80.68it/s] 24%|██▎       | 65/276 [00:00<00:02, 80.63it/s] 27%|██▋       | 74/276 [00:00<00:02, 79.97it/s] 30%|███       | 83/276 [00:01<00:02, 80.09it/s] 33%|███▎      | 92/276 [00:01<00:02, 79.59it/s] 37%|███▋      | 101/276 [00:01<00:02, 79.92it/s] 40%|███▉      | 110/276 [00:01<00:02, 79.89it/s] 43%|████▎     | 119/276 [00:01<00:01, 80.03it/s] 46%|████▋     | 128/276 [00:01<00:01, 79.81it/s] 49%|████▉     | 136/276 [00:01<00:01, 79.31it/s] 53%|█████▎    | 145/276 [00:01<00:01, 80.41it/s] 56%|█████▌    | 154/276 [00:01<00:01, 79.64it/s] 59%|█████▉    | 163/276 [00:02<00:01, 79.97it/s] 62%|██████▏   | 171/276 [00:02<00:01, 79.47it/s] 65%|██████▍   | 179/276 [00:02<00:01, 78.69it/s] 68%|██████▊   | 187/276 [00:02<00:01, 79.05it/s] 71%|███████   | 195/276 [00:02<00:01, 78.92it/s] 74%|███████▍  | 204/276 [00:02<00:00, 80.07it/s] 77%|███████▋  | 213/276 [00:02<00:00, 80.89it/s] 80%|████████  | 222/276 [00:02<00:00, 80.87it/s] 84%|████████▎ | 231/276 [00:02<00:00, 80.54it/s] 87%|████████▋ | 240/276 [00:02<00:00, 80.71it/s] 90%|█████████ | 249/276 [00:03<00:00, 80.25it/s] 93%|█████████▎| 258/276 [00:03<00:00, 79.69it/s] 96%|█████████▋| 266/276 [00:03<00:00, 79.16it/s] 99%|█████████▉| 274/276 [00:03<00:00, 78.88it/s]100%|██████████| 276/276 [00:03<00:00, 79.05it/s]
***** eval metrics *****
  epoch                   =        5.0
  eval_accuracy           =     0.8675
  eval_loss               =     0.3765
  eval_macro_f1           =     0.8502
  eval_micro_f1           =     0.8675
  eval_runtime            = 0:00:03.50
  eval_samples            =       2204
  eval_samples_per_second =    628.287
  eval_steps_per_second   =     78.678
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                  eval/accuracy ▇█▁▄▆▆
wandb:                      eval/loss ▂▁▆█▂▂
wandb:                  eval/macro_f1 ▆█▁▅▇▇
wandb:                  eval/micro_f1 ▇█▁▄▆▆
wandb:                   eval/runtime ▁▇▇█▆▅
wandb:        eval/samples_per_second █▂▂▁▃▄
wandb:          eval/steps_per_second █▂▂▁▃▄
wandb:                    train/epoch ▁▁▃▃▅▅▆▆████
wandb:              train/global_step ▁▁▃▃▅▅▆▆████
wandb:            train/learning_rate █▆▅▃▁
wandb:                     train/loss █▄▃▂▁
wandb:               train/total_flos ▁
wandb:               train/train_loss ▁
wandb:            train/train_runtime ▁
wandb: train/train_samples_per_second ▁
wandb:   train/train_steps_per_second ▁
wandb: 
wandb: Run summary:
wandb:                  eval/accuracy 0.86751
wandb:                      eval/loss 0.37649
wandb:                  eval/macro_f1 0.8502
wandb:                  eval/micro_f1 0.86751
wandb:                   eval/runtime 3.508
wandb:        eval/samples_per_second 628.287
wandb:          eval/steps_per_second 78.678
wandb:                    train/epoch 5.0
wandb:              train/global_step 1380
wandb:            train/learning_rate 0.0
wandb:                     train/loss 0.2593
wandb:               train/total_flos 1464896356669440.0
wandb:               train/train_loss 0.35366
wandb:            train/train_runtime 107.4177
wandb: train/train_samples_per_second 410.361
wandb:   train/train_steps_per_second 12.847
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /ceph/home/wangyifei/a3/wandb/offline-run-20231115_233653-zj4v1yfz
wandb: Find logs at: ./wandb/offline-run-20231115_233653-zj4v1yfz/logs
(ModelArguments(model_name_or_path='roberta-base', cache_dir='./cache', use_adapter=True, use_lora=False), DataTrainingArguments(dataset_name='agnews_sup', max_seq_length=64), TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.0005,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=./result/agnews_sup_roberta-base_seed3_adapter/runs/Nov15_23-40-14_node013,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.EPOCH,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=5.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
output_dir=./result/agnews_sup_roberta-base_seed3_adapter,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
resume_from_checkpoint=None,
run_name=./result/agnews_sup_roberta-base_seed3_adapter,
save_on_each_node=False,
save_safetensors=False,
save_steps=500,
save_strategy=IntervalStrategy.NO,
save_total_limit=None,
seed=444,
sharded_ddp=[],
skip_memory_metrics=True,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
))
11/15/2023 23:40:14 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False
11/15/2023 23:40:14 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.0005,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=./result/agnews_sup_roberta-base_seed3_adapter/runs/Nov15_23-40-14_node013,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.EPOCH,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=5.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
output_dir=./result/agnews_sup_roberta-base_seed3_adapter,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
resume_from_checkpoint=None,
run_name=./result/agnews_sup_roberta-base_seed3_adapter,
save_on_each_node=False,
save_safetensors=False,
save_steps=500,
save_strategy=IntervalStrategy.NO,
save_total_limit=None,
seed=444,
sharded_ddp=[],
skip_memory_metrics=True,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
wandb: Tracking run with wandb version 0.15.12
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[INFO|configuration_utils.py:715] 2023-11-15 23:40:30,236 >> loading configuration file config.json from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/config.json
[INFO|configuration_utils.py:775] 2023-11-15 23:40:30,246 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.33.3",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_auto.py:535] 2023-11-15 23:40:40,263 >> Could not locate the tokenizer configuration file, will try to use the model config instead.
[INFO|configuration_utils.py:715] 2023-11-15 23:40:50,281 >> loading configuration file config.json from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/config.json
[INFO|configuration_utils.py:775] 2023-11-15 23:40:50,281 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.33.3",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:1852] 2023-11-15 23:41:10,372 >> loading file vocab.json from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/vocab.json
[INFO|tokenization_utils_base.py:1852] 2023-11-15 23:41:10,373 >> loading file merges.txt from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/merges.txt
[INFO|tokenization_utils_base.py:1852] 2023-11-15 23:41:10,373 >> loading file tokenizer.json from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/tokenizer.json
[INFO|tokenization_utils_base.py:1852] 2023-11-15 23:41:10,373 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1852] 2023-11-15 23:41:10,374 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1852] 2023-11-15 23:41:10,374 >> loading file tokenizer_config.json from cache at None
[INFO|configuration_utils.py:715] 2023-11-15 23:41:10,375 >> loading configuration file config.json from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/config.json
[INFO|configuration_utils.py:775] 2023-11-15 23:41:10,376 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.33.3",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:2862] 2023-11-15 23:41:30,557 >> loading weights file model.safetensors from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/model.safetensors
[INFO|modeling_utils.py:3638] 2023-11-15 23:41:31,285 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.bias']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3650] 2023-11-15 23:41:31,285 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
parameters 1488196
model_max_length: 512
Running tokenizer on dataset:   0%|          | 0/6840 [00:00<?, ? examples/s]Running tokenizer on dataset:  29%|██▉       | 2000/6840 [00:00<00:00, 16628.79 examples/s]Running tokenizer on dataset:  58%|█████▊    | 4000/6840 [00:00<00:00, 16863.60 examples/s]Running tokenizer on dataset:  88%|████████▊ | 6000/6840 [00:00<00:00, 17139.35 examples/s]Running tokenizer on dataset: 100%|██████████| 6840/6840 [00:00<00:00, 17047.09 examples/s]
Running tokenizer on dataset:   0%|          | 0/760 [00:00<?, ? examples/s]Running tokenizer on dataset: 100%|██████████| 760/760 [00:00<00:00, 12724.47 examples/s]
11/15/2023 23:41:32 - INFO - __main__ - Sample 2530 of the training set: {'text': 'Invasion of the Data Snatchers (washingtonpost.com) washingtonpost.com - Think your PC is safe? Think again. A new study indicates your home computer is likely bogged down with spyware, viruses and other scourges wrought by hackers and PC pranksters. Ignorance may be bliss for some people, but for computer users, not knowing can be costly and inefficient.', 'label': 2, 'input_ids': [0, 42782, 27720, 9, 5, 5423, 7500, 415, 7873, 36, 605, 40886, 7049, 4, 175, 43, 14784, 1054, 7049, 4, 175, 111, 9387, 110, 4985, 16, 1522, 116, 9387, 456, 4, 83, 92, 892, 8711, 110, 184, 3034, 16, 533, 28423, 4462, 159, 19, 10258, 10680, 6, 21717, 8, 97, 2850, 2126, 5641, 37058, 30, 11344, 8, 4985, 25828, 9230, 4, 18762, 368, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.
11/15/2023 23:41:32 - INFO - __main__ - Sample 2357 of the training set: {'text': 'Corning begins work on Taiwan LCD facility Encouraged by the demand for LCDs, glass maker Corning on Thursday said it has broken ground for a second manufacturing facility in Taiwan.', 'label': 2, 'input_ids': [0, 15228, 3509, 3772, 173, 15, 6951, 20808, 2122, 14813, 2126, 4628, 30, 5, 1077, 13, 20808, 29, 6, 4049, 4403, 2812, 3509, 15, 296, 26, 24, 34, 3187, 1255, 13, 10, 200, 3021, 2122, 11, 6951, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
11/15/2023 23:41:32 - INFO - __main__ - Sample 108 of the training set: {'text': "In Asia, Powell defends N. Korea policy SEOUL -- Secretary of State Colin L. Powell yesterday sought to fend off complaints from key partners in the effort to end North Korea's nuclear programs that the Bush administration has not been sufficiently creative or willing to compromise in the negotiations.", 'label': 3, 'input_ids': [0, 1121, 1817, 6, 8274, 24951, 234, 4, 1101, 714, 6324, 5061, 574, 480, 1863, 9, 331, 8718, 226, 4, 8274, 2350, 2952, 7, 26885, 160, 4496, 31, 762, 2567, 11, 5, 1351, 7, 253, 369, 1101, 18, 1748, 1767, 14, 5, 3516, 942, 34, 45, 57, 21547, 3904, 50, 2882, 7, 7932, 11, 5, 3377, 4, 2, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]}.
11/15/2023 23:41:32 - WARNING - accelerate.utils.other - Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[INFO|trainer.py:755] 2023-11-15 23:41:34,227 >> The following columns in the training set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:1715] 2023-11-15 23:41:34,244 >> ***** Running training *****
[INFO|trainer.py:1716] 2023-11-15 23:41:34,245 >>   Num examples = 6,840
[INFO|trainer.py:1717] 2023-11-15 23:41:34,245 >>   Num Epochs = 5
[INFO|trainer.py:1718] 2023-11-15 23:41:34,245 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1721] 2023-11-15 23:41:34,246 >>   Total train batch size (w. parallel, distributed & accumulation) = 32
[INFO|trainer.py:1722] 2023-11-15 23:41:34,246 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:1723] 2023-11-15 23:41:34,246 >>   Total optimization steps = 1,070
[INFO|trainer.py:1724] 2023-11-15 23:41:34,247 >>   Number of trainable parameters = 1,488,196
[INFO|integration_utils.py:716] 2023-11-15 23:41:34,250 >> Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"
  0%|          | 0/1070 [00:00<?, ?it/s]  0%|          | 1/1070 [00:01<18:06,  1.02s/it]  0%|          | 3/1070 [00:01<05:30,  3.23it/s]  0%|          | 5/1070 [00:01<03:14,  5.47it/s]  1%|          | 7/1070 [00:01<02:19,  7.61it/s]  1%|          | 9/1070 [00:01<01:51,  9.51it/s]  1%|          | 11/1070 [00:01<01:35, 11.12it/s]  1%|          | 13/1070 [00:01<01:25, 12.40it/s]  1%|▏         | 15/1070 [00:01<01:18, 13.39it/s]  2%|▏         | 17/1070 [00:02<01:14, 14.13it/s]  2%|▏         | 19/1070 [00:02<01:11, 14.71it/s]  2%|▏         | 21/1070 [00:02<01:09, 15.11it/s]  2%|▏         | 23/1070 [00:02<01:08, 15.38it/s]  2%|▏         | 25/1070 [00:02<01:07, 15.55it/s]  3%|▎         | 27/1070 [00:02<01:06, 15.68it/s]  3%|▎         | 29/1070 [00:02<01:05, 15.80it/s]  3%|▎         | 31/1070 [00:02<01:05, 15.85it/s]  3%|▎         | 33/1070 [00:03<01:05, 15.88it/s]  3%|▎         | 35/1070 [00:03<01:04, 15.92it/s]  3%|▎         | 37/1070 [00:03<01:04, 16.00it/s]  4%|▎         | 39/1070 [00:03<01:04, 16.03it/s]  4%|▍         | 41/1070 [00:03<01:04, 16.04it/s]  4%|▍         | 43/1070 [00:03<01:04, 16.04it/s]  4%|▍         | 45/1070 [00:03<01:03, 16.03it/s]  4%|▍         | 47/1070 [00:03<01:03, 16.06it/s]  5%|▍         | 49/1070 [00:04<01:03, 16.07it/s]  5%|▍         | 51/1070 [00:04<01:03, 16.04it/s]  5%|▍         | 53/1070 [00:04<01:03, 16.07it/s]  5%|▌         | 55/1070 [00:04<01:03, 16.09it/s]  5%|▌         | 57/1070 [00:04<01:02, 16.09it/s]  6%|▌         | 59/1070 [00:04<01:02, 16.08it/s]  6%|▌         | 61/1070 [00:04<01:03, 15.95it/s]  6%|▌         | 63/1070 [00:04<01:03, 15.98it/s]  6%|▌         | 65/1070 [00:05<01:02, 15.96it/s]  6%|▋         | 67/1070 [00:05<01:02, 15.96it/s]  6%|▋         | 69/1070 [00:05<01:02, 15.96it/s]  7%|▋         | 71/1070 [00:05<01:02, 15.99it/s]  7%|▋         | 73/1070 [00:05<01:02, 16.01it/s]  7%|▋         | 75/1070 [00:05<01:02, 16.01it/s]  7%|▋         | 77/1070 [00:05<01:02, 16.02it/s]  7%|▋         | 79/1070 [00:05<01:01, 16.02it/s]  8%|▊         | 81/1070 [00:06<01:01, 16.03it/s]  8%|▊         | 83/1070 [00:06<01:01, 16.02it/s]  8%|▊         | 85/1070 [00:06<01:01, 16.03it/s]  8%|▊         | 87/1070 [00:06<01:02, 15.78it/s]  8%|▊         | 89/1070 [00:06<01:02, 15.61it/s]  9%|▊         | 91/1070 [00:06<01:02, 15.76it/s]  9%|▊         | 93/1070 [00:06<01:01, 15.83it/s]  9%|▉         | 95/1070 [00:06<01:01, 15.89it/s]  9%|▉         | 97/1070 [00:07<01:00, 15.95it/s]  9%|▉         | 99/1070 [00:07<01:00, 16.00it/s]  9%|▉         | 101/1070 [00:07<01:00, 15.99it/s] 10%|▉         | 103/1070 [00:07<01:00, 15.99it/s] 10%|▉         | 105/1070 [00:07<01:00, 15.99it/s] 10%|█         | 107/1070 [00:07<01:00, 15.99it/s] 10%|█         | 109/1070 [00:07<01:00, 15.99it/s] 10%|█         | 111/1070 [00:07<01:00, 15.96it/s] 11%|█         | 113/1070 [00:08<00:59, 16.03it/s] 11%|█         | 115/1070 [00:08<00:59, 16.09it/s] 11%|█         | 117/1070 [00:08<00:59, 16.11it/s] 11%|█         | 119/1070 [00:08<00:59, 16.12it/s] 11%|█▏        | 121/1070 [00:08<00:58, 16.11it/s] 11%|█▏        | 123/1070 [00:08<00:58, 16.12it/s] 12%|█▏        | 125/1070 [00:08<00:58, 16.13it/s] 12%|█▏        | 127/1070 [00:08<00:58, 16.14it/s] 12%|█▏        | 129/1070 [00:09<00:58, 16.13it/s] 12%|█▏        | 131/1070 [00:09<00:58, 16.11it/s] 12%|█▏        | 133/1070 [00:09<00:58, 16.11it/s] 13%|█▎        | 135/1070 [00:09<00:57, 16.15it/s] 13%|█▎        | 137/1070 [00:09<00:57, 16.17it/s] 13%|█▎        | 139/1070 [00:09<00:57, 16.16it/s] 13%|█▎        | 141/1070 [00:09<00:57, 16.14it/s] 13%|█▎        | 143/1070 [00:09<00:57, 16.13it/s] 14%|█▎        | 145/1070 [00:09<00:57, 16.10it/s] 14%|█▎        | 147/1070 [00:10<00:57, 15.99it/s] 14%|█▍        | 149/1070 [00:10<00:57, 15.89it/s] 14%|█▍        | 151/1070 [00:10<00:57, 15.87it/s] 14%|█▍        | 153/1070 [00:10<00:57, 15.83it/s] 14%|█▍        | 155/1070 [00:10<00:57, 15.80it/s] 15%|█▍        | 157/1070 [00:10<00:57, 15.87it/s] 15%|█▍        | 159/1070 [00:10<00:57, 15.84it/s] 15%|█▌        | 161/1070 [00:11<00:57, 15.86it/s] 15%|█▌        | 163/1070 [00:11<00:56, 15.94it/s] 15%|█▌        | 165/1070 [00:11<00:57, 15.84it/s] 16%|█▌        | 167/1070 [00:11<00:57, 15.71it/s] 16%|█▌        | 169/1070 [00:11<00:57, 15.68it/s] 16%|█▌        | 171/1070 [00:11<00:57, 15.62it/s] 16%|█▌        | 173/1070 [00:11<00:57, 15.60it/s] 16%|█▋        | 175/1070 [00:11<00:57, 15.51it/s] 17%|█▋        | 177/1070 [00:12<00:57, 15.46it/s] 17%|█▋        | 179/1070 [00:12<00:57, 15.51it/s] 17%|█▋        | 181/1070 [00:12<00:56, 15.65it/s] 17%|█▋        | 183/1070 [00:12<00:56, 15.80it/s] 17%|█▋        | 185/1070 [00:12<00:55, 15.90it/s] 17%|█▋        | 187/1070 [00:12<00:55, 15.95it/s] 18%|█▊        | 189/1070 [00:12<00:55, 15.99it/s] 18%|█▊        | 191/1070 [00:12<00:54, 16.02it/s] 18%|█▊        | 193/1070 [00:13<00:54, 16.08it/s] 18%|█▊        | 195/1070 [00:13<00:54, 16.10it/s] 18%|█▊        | 197/1070 [00:13<00:54, 16.05it/s] 19%|█▊        | 199/1070 [00:13<00:54, 16.05it/s] 19%|█▉        | 201/1070 [00:13<00:54, 16.09it/s] 19%|█▉        | 203/1070 [00:13<00:53, 16.13it/s] 19%|█▉        | 205/1070 [00:13<00:53, 16.08it/s] 19%|█▉        | 207/1070 [00:13<00:53, 16.11it/s] 20%|█▉        | 209/1070 [00:14<00:53, 16.08it/s] 20%|█▉        | 211/1070 [00:14<00:53, 16.12it/s] 20%|█▉        | 213/1070 [00:14<00:53, 16.14it/s]                                                   20%|██        | 214/1070 [00:14<00:53, 16.14it/s][INFO|trainer.py:755] 2023-11-15 23:41:48,576 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 23:41:48,578 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 23:41:48,578 >>   Num examples = 760
[INFO|trainer.py:3134] 2023-11-15 23:41:48,579 >>   Batch size = 8
{'loss': 0.4424, 'learning_rate': 0.0004, 'epoch': 1.0}

  0%|          | 0/95 [00:00<?, ?it/s][A
 11%|█         | 10/95 [00:00<00:00, 96.82it/s][A
 21%|██        | 20/95 [00:00<00:00, 90.66it/s][A
 32%|███▏      | 30/95 [00:00<00:00, 87.62it/s][A
 41%|████      | 39/95 [00:00<00:00, 82.72it/s][A
 51%|█████     | 48/95 [00:00<00:00, 83.67it/s][A
 60%|██████    | 57/95 [00:00<00:00, 84.45it/s][A
 69%|██████▉   | 66/95 [00:00<00:00, 85.08it/s][A
 79%|███████▉  | 75/95 [00:00<00:00, 85.20it/s][A
 88%|████████▊ | 84/95 [00:00<00:00, 83.48it/s][A
 98%|█████████▊| 93/95 [00:01<00:00, 84.60it/s][A                                                  
                                               [A 20%|██        | 214/1070 [00:15<00:53, 16.14it/s]
100%|██████████| 95/95 [00:01<00:00, 84.60it/s][A
                                               [A 20%|██        | 215/1070 [00:15<03:21,  4.24it/s] 20%|██        | 217/1070 [00:15<02:37,  5.42it/s] 20%|██        | 219/1070 [00:15<02:06,  6.75it/s] 21%|██        | 221/1070 [00:15<01:44,  8.14it/s] 21%|██        | 223/1070 [00:16<01:28,  9.52it/s] 21%|██        | 225/1070 [00:16<01:18, 10.81it/s] 21%|██        | 227/1070 [00:16<01:10, 11.91it/s] 21%|██▏       | 229/1070 [00:16<01:05, 12.81it/s] 22%|██▏       | 231/1070 [00:16<01:01, 13.58it/s] 22%|██▏       | 233/1070 [00:16<00:59, 14.14it/s] 22%|██▏       | 235/1070 [00:16<00:57, 14.59it/s] 22%|██▏       | 237/1070 [00:16<00:55, 14.93it/s] 22%|██▏       | 239/1070 [00:17<00:55, 15.02it/s] 23%|██▎       | 241/1070 [00:17<00:54, 15.12it/s] 23%|██▎       | 243/1070 [00:17<00:54, 15.20it/s] 23%|██▎       | 245/1070 [00:17<00:54, 15.14it/s] 23%|██▎       | 247/1070 [00:17<00:54, 15.16it/s] 23%|██▎       | 249/1070 [00:17<00:54, 15.18it/s] 23%|██▎       | 251/1070 [00:17<00:53, 15.25it/s] 24%|██▎       | 253/1070 [00:18<00:53, 15.28it/s] 24%|██▍       | 255/1070 [00:18<00:53, 15.35it/s] 24%|██▍       | 257/1070 [00:18<00:53, 15.30it/s] 24%|██▍       | 259/1070 [00:18<00:52, 15.36it/s] 24%|██▍       | 261/1070 [00:18<00:52, 15.47it/s] 25%|██▍       | 263/1070 [00:18<00:51, 15.58it/s] 25%|██▍       | 265/1070 [00:18<00:51, 15.70it/s] 25%|██▍       | 267/1070 [00:18<00:50, 15.80it/s] 25%|██▌       | 269/1070 [00:19<00:50, 15.86it/s] 25%|██▌       | 271/1070 [00:19<00:50, 15.93it/s] 26%|██▌       | 273/1070 [00:19<00:49, 15.97it/s] 26%|██▌       | 275/1070 [00:19<00:49, 15.99it/s] 26%|██▌       | 277/1070 [00:19<00:49, 15.99it/s] 26%|██▌       | 279/1070 [00:19<00:49, 16.00it/s] 26%|██▋       | 281/1070 [00:19<00:49, 16.03it/s] 26%|██▋       | 283/1070 [00:19<00:49, 16.05it/s] 27%|██▋       | 285/1070 [00:20<00:48, 16.03it/s] 27%|██▋       | 287/1070 [00:20<00:48, 16.03it/s] 27%|██▋       | 289/1070 [00:20<00:48, 16.05it/s] 27%|██▋       | 291/1070 [00:20<00:48, 16.05it/s] 27%|██▋       | 293/1070 [00:20<00:48, 16.01it/s] 28%|██▊       | 295/1070 [00:20<00:48, 15.99it/s] 28%|██▊       | 297/1070 [00:20<00:48, 16.02it/s] 28%|██▊       | 299/1070 [00:20<00:48, 16.04it/s] 28%|██▊       | 301/1070 [00:21<00:48, 16.01it/s] 28%|██▊       | 303/1070 [00:21<00:47, 16.02it/s] 29%|██▊       | 305/1070 [00:21<00:47, 16.06it/s] 29%|██▊       | 307/1070 [00:21<00:47, 16.08it/s] 29%|██▉       | 309/1070 [00:21<00:48, 15.83it/s] 29%|██▉       | 311/1070 [00:21<00:48, 15.79it/s] 29%|██▉       | 313/1070 [00:21<00:47, 15.86it/s] 29%|██▉       | 315/1070 [00:21<00:47, 15.90it/s] 30%|██▉       | 317/1070 [00:22<00:47, 15.94it/s] 30%|██▉       | 319/1070 [00:22<00:46, 16.00it/s] 30%|███       | 321/1070 [00:22<00:46, 16.03it/s] 30%|███       | 323/1070 [00:22<00:46, 16.04it/s] 30%|███       | 325/1070 [00:22<00:46, 16.01it/s] 31%|███       | 327/1070 [00:22<00:46, 16.04it/s] 31%|███       | 329/1070 [00:22<00:46, 16.04it/s] 31%|███       | 331/1070 [00:22<00:46, 16.01it/s] 31%|███       | 333/1070 [00:23<00:46, 16.00it/s] 31%|███▏      | 335/1070 [00:23<00:45, 16.03it/s] 31%|███▏      | 337/1070 [00:23<00:45, 16.05it/s] 32%|███▏      | 339/1070 [00:23<00:45, 16.06it/s] 32%|███▏      | 341/1070 [00:23<00:45, 16.02it/s] 32%|███▏      | 343/1070 [00:23<00:45, 16.01it/s] 32%|███▏      | 345/1070 [00:23<00:45, 16.02it/s] 32%|███▏      | 347/1070 [00:23<00:45, 15.99it/s] 33%|███▎      | 349/1070 [00:24<00:45, 15.99it/s] 33%|███▎      | 351/1070 [00:24<00:44, 16.00it/s] 33%|███▎      | 353/1070 [00:24<00:44, 16.02it/s] 33%|███▎      | 355/1070 [00:24<00:44, 16.04it/s] 33%|███▎      | 357/1070 [00:24<00:44, 16.02it/s] 34%|███▎      | 359/1070 [00:24<00:44, 16.03it/s] 34%|███▎      | 361/1070 [00:24<00:44, 16.05it/s] 34%|███▍      | 363/1070 [00:24<00:44, 16.04it/s] 34%|███▍      | 365/1070 [00:25<00:44, 16.02it/s] 34%|███▍      | 367/1070 [00:25<00:43, 16.01it/s] 34%|███▍      | 369/1070 [00:25<00:43, 16.05it/s] 35%|███▍      | 371/1070 [00:25<00:43, 16.04it/s] 35%|███▍      | 373/1070 [00:25<00:43, 16.02it/s] 35%|███▌      | 375/1070 [00:25<00:43, 16.01it/s] 35%|███▌      | 377/1070 [00:25<00:43, 15.98it/s] 35%|███▌      | 379/1070 [00:25<00:43, 15.75it/s] 36%|███▌      | 381/1070 [00:26<00:43, 15.68it/s] 36%|███▌      | 383/1070 [00:26<00:43, 15.62it/s] 36%|███▌      | 385/1070 [00:26<00:43, 15.61it/s] 36%|███▌      | 387/1070 [00:26<00:43, 15.67it/s] 36%|███▋      | 389/1070 [00:26<00:43, 15.74it/s] 37%|███▋      | 391/1070 [00:26<00:42, 15.82it/s] 37%|███▋      | 393/1070 [00:26<00:42, 15.90it/s] 37%|███▋      | 395/1070 [00:26<00:42, 15.95it/s] 37%|███▋      | 397/1070 [00:27<00:42, 15.97it/s] 37%|███▋      | 399/1070 [00:27<00:42, 15.97it/s] 37%|███▋      | 401/1070 [00:27<00:41, 16.01it/s] 38%|███▊      | 403/1070 [00:27<00:41, 16.02it/s] 38%|███▊      | 405/1070 [00:27<00:41, 16.02it/s] 38%|███▊      | 407/1070 [00:27<00:41, 16.01it/s] 38%|███▊      | 409/1070 [00:27<00:41, 15.93it/s] 38%|███▊      | 411/1070 [00:27<00:41, 15.87it/s] 39%|███▊      | 413/1070 [00:28<00:41, 15.82it/s] 39%|███▉      | 415/1070 [00:28<00:41, 15.69it/s] 39%|███▉      | 417/1070 [00:28<00:41, 15.61it/s] 39%|███▉      | 419/1070 [00:28<00:41, 15.56it/s] 39%|███▉      | 421/1070 [00:28<00:41, 15.48it/s] 40%|███▉      | 423/1070 [00:28<00:41, 15.49it/s] 40%|███▉      | 425/1070 [00:28<00:41, 15.40it/s] 40%|███▉      | 427/1070 [00:28<00:41, 15.56it/s]                                                   40%|████      | 428/1070 [00:28<00:41, 15.56it/s][INFO|trainer.py:755] 2023-11-15 23:42:03,252 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 23:42:03,254 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 23:42:03,254 >>   Num examples = 760
[INFO|trainer.py:3134] 2023-11-15 23:42:03,255 >>   Batch size = 8
{'eval_loss': 0.29698312282562256, 'eval_accuracy': 0.9052631578947369, 'eval_micro_f1': 0.9052631578947369, 'eval_macro_f1': 0.9016834315893272, 'eval_runtime': 1.1631, 'eval_samples_per_second': 653.435, 'eval_steps_per_second': 81.679, 'epoch': 1.0}
{'loss': 0.2602, 'learning_rate': 0.0003, 'epoch': 2.0}

  0%|          | 0/95 [00:00<?, ?it/s][A
 11%|█         | 10/95 [00:00<00:00, 96.43it/s][A
 21%|██        | 20/95 [00:00<00:00, 90.73it/s][A
 32%|███▏      | 30/95 [00:00<00:00, 88.95it/s][A
 41%|████      | 39/95 [00:00<00:00, 88.09it/s][A
 51%|█████     | 48/95 [00:00<00:00, 87.59it/s][A
 60%|██████    | 57/95 [00:00<00:00, 87.01it/s][A
 69%|██████▉   | 66/95 [00:00<00:00, 86.83it/s][A
 79%|███████▉  | 75/95 [00:00<00:00, 86.73it/s][A
 88%|████████▊ | 84/95 [00:00<00:00, 86.77it/s][A
 98%|█████████▊| 93/95 [00:01<00:00, 86.82it/s][A                                                  
                                               [A 40%|████      | 428/1070 [00:30<00:41, 15.56it/s]
100%|██████████| 95/95 [00:01<00:00, 86.82it/s][A
                                               [A 40%|████      | 429/1070 [00:30<02:29,  4.29it/s] 40%|████      | 431/1070 [00:30<01:56,  5.50it/s] 40%|████      | 433/1070 [00:30<01:32,  6.86it/s] 41%|████      | 435/1070 [00:30<01:16,  8.29it/s] 41%|████      | 437/1070 [00:30<01:05,  9.70it/s] 41%|████      | 439/1070 [00:30<00:57, 11.01it/s] 41%|████      | 441/1070 [00:30<00:51, 12.18it/s] 41%|████▏     | 443/1070 [00:31<00:47, 13.14it/s] 42%|████▏     | 445/1070 [00:31<00:44, 13.91it/s] 42%|████▏     | 447/1070 [00:31<00:43, 14.48it/s] 42%|████▏     | 449/1070 [00:31<00:41, 14.94it/s] 42%|████▏     | 451/1070 [00:31<00:40, 15.29it/s] 42%|████▏     | 453/1070 [00:31<00:39, 15.53it/s] 43%|████▎     | 455/1070 [00:31<00:39, 15.67it/s] 43%|████▎     | 457/1070 [00:31<00:38, 15.77it/s] 43%|████▎     | 459/1070 [00:32<00:38, 15.89it/s] 43%|████▎     | 461/1070 [00:32<00:38, 15.96it/s] 43%|████▎     | 463/1070 [00:32<00:37, 16.00it/s] 43%|████▎     | 465/1070 [00:32<00:37, 16.01it/s] 44%|████▎     | 467/1070 [00:32<00:37, 16.02it/s] 44%|████▍     | 469/1070 [00:32<00:37, 16.07it/s] 44%|████▍     | 471/1070 [00:32<00:37, 16.10it/s] 44%|████▍     | 473/1070 [00:32<00:37, 16.07it/s] 44%|████▍     | 475/1070 [00:33<00:37, 16.05it/s] 45%|████▍     | 477/1070 [00:33<00:36, 16.08it/s] 45%|████▍     | 479/1070 [00:33<00:36, 16.09it/s] 45%|████▍     | 481/1070 [00:33<00:36, 16.10it/s] 45%|████▌     | 483/1070 [00:33<00:36, 16.07it/s] 45%|████▌     | 485/1070 [00:33<00:36, 16.05it/s] 46%|████▌     | 487/1070 [00:33<00:36, 16.08it/s] 46%|████▌     | 489/1070 [00:33<00:36, 16.07it/s] 46%|████▌     | 491/1070 [00:34<00:36, 16.04it/s] 46%|████▌     | 493/1070 [00:34<00:36, 16.01it/s] 46%|████▋     | 495/1070 [00:34<00:35, 16.01it/s] 46%|████▋     | 497/1070 [00:34<00:35, 16.01it/s] 47%|████▋     | 499/1070 [00:34<00:35, 16.01it/s] 47%|████▋     | 501/1070 [00:34<00:35, 16.01it/s] 47%|████▋     | 503/1070 [00:34<00:35, 16.06it/s] 47%|████▋     | 505/1070 [00:34<00:35, 16.08it/s] 47%|████▋     | 507/1070 [00:35<00:35, 16.08it/s] 48%|████▊     | 509/1070 [00:35<00:34, 16.06it/s] 48%|████▊     | 511/1070 [00:35<00:34, 16.06it/s] 48%|████▊     | 513/1070 [00:35<00:34, 16.08it/s] 48%|████▊     | 515/1070 [00:35<00:34, 16.09it/s] 48%|████▊     | 517/1070 [00:35<00:34, 16.08it/s] 49%|████▊     | 519/1070 [00:35<00:34, 16.06it/s] 49%|████▊     | 521/1070 [00:35<00:34, 16.09it/s] 49%|████▉     | 523/1070 [00:36<00:33, 16.09it/s] 49%|████▉     | 525/1070 [00:36<00:33, 16.08it/s] 49%|████▉     | 527/1070 [00:36<00:33, 16.07it/s] 49%|████▉     | 529/1070 [00:36<00:33, 16.01it/s] 50%|████▉     | 531/1070 [00:36<00:33, 16.03it/s] 50%|████▉     | 533/1070 [00:36<00:33, 16.04it/s] 50%|█████     | 535/1070 [00:36<00:33, 16.03it/s] 50%|█████     | 537/1070 [00:36<00:33, 16.04it/s] 50%|█████     | 539/1070 [00:37<00:33, 16.07it/s] 51%|█████     | 541/1070 [00:37<00:32, 16.08it/s] 51%|█████     | 543/1070 [00:37<00:32, 16.08it/s] 51%|█████     | 545/1070 [00:37<00:32, 16.06it/s] 51%|█████     | 547/1070 [00:37<00:32, 16.09it/s] 51%|█████▏    | 549/1070 [00:37<00:32, 16.10it/s] 51%|█████▏    | 551/1070 [00:37<00:32, 16.10it/s] 52%|█████▏    | 553/1070 [00:37<00:32, 16.07it/s] 52%|█████▏    | 555/1070 [00:38<00:32, 16.06it/s] 52%|█████▏    | 557/1070 [00:38<00:31, 16.08it/s] 52%|█████▏    | 559/1070 [00:38<00:31, 16.08it/s] 52%|█████▏    | 561/1070 [00:38<00:31, 16.06it/s] 53%|█████▎    | 563/1070 [00:38<00:31, 16.05it/s] 53%|█████▎    | 565/1070 [00:38<00:31, 16.07it/s] 53%|█████▎    | 567/1070 [00:38<00:31, 16.08it/s] 53%|█████▎    | 569/1070 [00:38<00:31, 16.07it/s] 53%|█████▎    | 571/1070 [00:39<00:31, 16.06it/s] 54%|█████▎    | 573/1070 [00:39<00:30, 16.06it/s] 54%|█████▎    | 575/1070 [00:39<00:30, 16.08it/s] 54%|█████▍    | 577/1070 [00:39<00:30, 16.08it/s] 54%|█████▍    | 579/1070 [00:39<00:30, 16.05it/s] 54%|█████▍    | 581/1070 [00:39<00:30, 16.04it/s] 54%|█████▍    | 583/1070 [00:39<00:30, 16.07it/s] 55%|█████▍    | 585/1070 [00:39<00:30, 16.07it/s] 55%|█████▍    | 587/1070 [00:40<00:30, 16.05it/s] 55%|█████▌    | 589/1070 [00:40<00:29, 16.04it/s] 55%|█████▌    | 591/1070 [00:40<00:29, 16.00it/s] 55%|█████▌    | 593/1070 [00:40<00:29, 16.02it/s] 56%|█████▌    | 595/1070 [00:40<00:29, 15.99it/s] 56%|█████▌    | 597/1070 [00:40<00:29, 15.97it/s] 56%|█████▌    | 599/1070 [00:40<00:29, 16.02it/s] 56%|█████▌    | 601/1070 [00:40<00:29, 15.85it/s] 56%|█████▋    | 603/1070 [00:41<00:29, 15.83it/s] 57%|█████▋    | 605/1070 [00:41<00:29, 15.88it/s] 57%|█████▋    | 607/1070 [00:41<00:29, 15.94it/s] 57%|█████▋    | 609/1070 [00:41<00:28, 15.95it/s] 57%|█████▋    | 611/1070 [00:41<00:28, 15.97it/s] 57%|█████▋    | 613/1070 [00:41<00:28, 16.02it/s] 57%|█████▋    | 615/1070 [00:41<00:28, 16.04it/s] 58%|█████▊    | 617/1070 [00:41<00:28, 16.03it/s] 58%|█████▊    | 619/1070 [00:42<00:28, 16.02it/s] 58%|█████▊    | 621/1070 [00:42<00:28, 16.03it/s] 58%|█████▊    | 623/1070 [00:42<00:27, 16.06it/s] 58%|█████▊    | 625/1070 [00:42<00:27, 16.01it/s] 59%|█████▊    | 627/1070 [00:42<00:27, 15.99it/s] 59%|█████▉    | 629/1070 [00:42<00:27, 15.99it/s] 59%|█████▉    | 631/1070 [00:42<00:27, 16.00it/s] 59%|█████▉    | 633/1070 [00:42<00:27, 15.99it/s] 59%|█████▉    | 635/1070 [00:43<00:27, 15.98it/s] 60%|█████▉    | 637/1070 [00:43<00:27, 15.99it/s] 60%|█████▉    | 639/1070 [00:43<00:26, 15.97it/s] 60%|█████▉    | 641/1070 [00:43<00:26, 15.95it/s]                                                   60%|██████    | 642/1070 [00:43<00:26, 15.95it/s][INFO|trainer.py:755] 2023-11-15 23:42:17,716 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 23:42:17,718 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 23:42:17,718 >>   Num examples = 760
[INFO|trainer.py:3134] 2023-11-15 23:42:17,718 >>   Batch size = 8
{'eval_loss': 0.27912721037864685, 'eval_accuracy': 0.906578947368421, 'eval_micro_f1': 0.906578947368421, 'eval_macro_f1': 0.9034898672668839, 'eval_runtime': 1.1334, 'eval_samples_per_second': 670.549, 'eval_steps_per_second': 83.819, 'epoch': 2.0}
{'loss': 0.2057, 'learning_rate': 0.0002, 'epoch': 3.0}

  0%|          | 0/95 [00:00<?, ?it/s][A
 11%|█         | 10/95 [00:00<00:00, 96.48it/s][A
 21%|██        | 20/95 [00:00<00:00, 90.34it/s][A
 32%|███▏      | 30/95 [00:00<00:00, 88.51it/s][A
 41%|████      | 39/95 [00:00<00:00, 87.91it/s][A
 51%|█████     | 48/95 [00:00<00:00, 87.42it/s][A
 60%|██████    | 57/95 [00:00<00:00, 87.11it/s][A
 69%|██████▉   | 66/95 [00:00<00:00, 86.96it/s][A
 79%|███████▉  | 75/95 [00:00<00:00, 86.84it/s][A
 88%|████████▊ | 84/95 [00:00<00:00, 86.56it/s][A
 98%|█████████▊| 93/95 [00:01<00:00, 86.51it/s][A                                                  
                                               [A 60%|██████    | 642/1070 [00:44<00:26, 15.95it/s]
100%|██████████| 95/95 [00:01<00:00, 86.51it/s][A
                                               [A 60%|██████    | 643/1070 [00:44<01:38,  4.32it/s] 60%|██████    | 645/1070 [00:44<01:16,  5.53it/s] 60%|██████    | 647/1070 [00:44<01:01,  6.88it/s] 61%|██████    | 649/1070 [00:45<00:50,  8.29it/s] 61%|██████    | 651/1070 [00:45<00:43,  9.69it/s] 61%|██████    | 653/1070 [00:45<00:37, 10.98it/s] 61%|██████    | 655/1070 [00:45<00:34, 12.11it/s] 61%|██████▏   | 657/1070 [00:45<00:31, 13.06it/s] 62%|██████▏   | 659/1070 [00:45<00:29, 13.83it/s] 62%|██████▏   | 661/1070 [00:45<00:28, 14.40it/s] 62%|██████▏   | 663/1070 [00:45<00:27, 14.83it/s] 62%|██████▏   | 665/1070 [00:46<00:26, 15.09it/s] 62%|██████▏   | 667/1070 [00:46<00:26, 15.32it/s] 63%|██████▎   | 669/1070 [00:46<00:25, 15.50it/s] 63%|██████▎   | 671/1070 [00:46<00:25, 15.61it/s] 63%|██████▎   | 673/1070 [00:46<00:25, 15.71it/s] 63%|██████▎   | 675/1070 [00:46<00:25, 15.76it/s] 63%|██████▎   | 677/1070 [00:46<00:24, 15.81it/s] 63%|██████▎   | 679/1070 [00:46<00:24, 15.87it/s] 64%|██████▎   | 681/1070 [00:47<00:24, 15.90it/s] 64%|██████▍   | 683/1070 [00:47<00:24, 15.92it/s] 64%|██████▍   | 685/1070 [00:47<00:24, 15.93it/s] 64%|██████▍   | 687/1070 [00:47<00:24, 15.95it/s] 64%|██████▍   | 689/1070 [00:47<00:23, 15.95it/s] 65%|██████▍   | 691/1070 [00:47<00:23, 15.83it/s] 65%|██████▍   | 693/1070 [00:47<00:23, 15.84it/s] 65%|██████▍   | 695/1070 [00:47<00:23, 15.86it/s] 65%|██████▌   | 697/1070 [00:48<00:23, 15.88it/s] 65%|██████▌   | 699/1070 [00:48<00:23, 15.92it/s] 66%|██████▌   | 701/1070 [00:48<00:23, 15.94it/s] 66%|██████▌   | 703/1070 [00:48<00:23, 15.94it/s] 66%|██████▌   | 705/1070 [00:48<00:22, 15.93it/s] 66%|██████▌   | 707/1070 [00:48<00:22, 15.98it/s] 66%|██████▋   | 709/1070 [00:48<00:22, 15.96it/s] 66%|██████▋   | 711/1070 [00:48<00:22, 15.95it/s] 67%|██████▋   | 713/1070 [00:49<00:22, 15.95it/s] 67%|██████▋   | 715/1070 [00:49<00:22, 15.98it/s] 67%|██████▋   | 717/1070 [00:49<00:22, 15.98it/s] 67%|██████▋   | 719/1070 [00:49<00:21, 15.97it/s] 67%|██████▋   | 721/1070 [00:49<00:21, 15.98it/s] 68%|██████▊   | 723/1070 [00:49<00:21, 15.99it/s] 68%|██████▊   | 725/1070 [00:49<00:21, 15.97it/s] 68%|██████▊   | 727/1070 [00:49<00:21, 15.96it/s] 68%|██████▊   | 729/1070 [00:50<00:21, 15.97it/s] 68%|██████▊   | 731/1070 [00:50<00:21, 15.98it/s] 69%|██████▊   | 733/1070 [00:50<00:21, 15.96it/s] 69%|██████▊   | 735/1070 [00:50<00:21, 15.95it/s] 69%|██████▉   | 737/1070 [00:50<00:20, 15.96it/s] 69%|██████▉   | 739/1070 [00:50<00:20, 15.98it/s] 69%|██████▉   | 741/1070 [00:50<00:20, 15.95it/s] 69%|██████▉   | 743/1070 [00:50<00:20, 15.94it/s] 70%|██████▉   | 745/1070 [00:51<00:20, 15.96it/s] 70%|██████▉   | 747/1070 [00:51<00:20, 15.94it/s] 70%|███████   | 749/1070 [00:51<00:20, 15.91it/s] 70%|███████   | 751/1070 [00:51<00:20, 15.91it/s] 70%|███████   | 753/1070 [00:51<00:19, 15.91it/s] 71%|███████   | 755/1070 [00:51<00:19, 15.88it/s] 71%|███████   | 757/1070 [00:51<00:19, 15.91it/s] 71%|███████   | 759/1070 [00:51<00:19, 15.95it/s] 71%|███████   | 761/1070 [00:52<00:19, 15.95it/s] 71%|███████▏  | 763/1070 [00:52<00:19, 15.93it/s] 71%|███████▏  | 765/1070 [00:52<00:19, 15.93it/s] 72%|███████▏  | 767/1070 [00:52<00:19, 15.94it/s] 72%|███████▏  | 769/1070 [00:52<00:18, 15.95it/s] 72%|███████▏  | 771/1070 [00:52<00:18, 15.95it/s] 72%|███████▏  | 773/1070 [00:52<00:18, 15.86it/s] 72%|███████▏  | 775/1070 [00:52<00:18, 15.88it/s] 73%|███████▎  | 777/1070 [00:53<00:18, 15.89it/s] 73%|███████▎  | 779/1070 [00:53<00:18, 15.92it/s] 73%|███████▎  | 781/1070 [00:53<00:18, 15.94it/s] 73%|███████▎  | 783/1070 [00:53<00:17, 15.94it/s] 73%|███████▎  | 785/1070 [00:53<00:17, 15.93it/s] 74%|███████▎  | 787/1070 [00:53<00:17, 15.95it/s] 74%|███████▎  | 789/1070 [00:53<00:17, 15.97it/s] 74%|███████▍  | 791/1070 [00:53<00:17, 15.94it/s] 74%|███████▍  | 793/1070 [00:54<00:17, 15.95it/s] 74%|███████▍  | 795/1070 [00:54<00:17, 15.97it/s] 74%|███████▍  | 797/1070 [00:54<00:17, 15.95it/s] 75%|███████▍  | 799/1070 [00:54<00:16, 15.95it/s] 75%|███████▍  | 801/1070 [00:54<00:16, 15.95it/s] 75%|███████▌  | 803/1070 [00:54<00:16, 15.96it/s] 75%|███████▌  | 805/1070 [00:54<00:16, 15.95it/s] 75%|███████▌  | 807/1070 [00:54<00:16, 15.92it/s] 76%|███████▌  | 809/1070 [00:55<00:16, 15.94it/s] 76%|███████▌  | 811/1070 [00:55<00:16, 15.95it/s] 76%|███████▌  | 813/1070 [00:55<00:16, 15.93it/s] 76%|███████▌  | 815/1070 [00:55<00:16, 15.93it/s] 76%|███████▋  | 817/1070 [00:55<00:15, 15.96it/s] 77%|███████▋  | 819/1070 [00:55<00:15, 15.95it/s] 77%|███████▋  | 821/1070 [00:55<00:15, 15.92it/s] 77%|███████▋  | 823/1070 [00:55<00:15, 15.92it/s] 77%|███████▋  | 825/1070 [00:56<00:15, 15.74it/s] 77%|███████▋  | 827/1070 [00:56<00:15, 15.72it/s] 77%|███████▋  | 829/1070 [00:56<00:15, 15.80it/s] 78%|███████▊  | 831/1070 [00:56<00:15, 15.84it/s] 78%|███████▊  | 833/1070 [00:56<00:14, 15.83it/s] 78%|███████▊  | 835/1070 [00:56<00:14, 15.84it/s] 78%|███████▊  | 837/1070 [00:56<00:14, 15.85it/s] 78%|███████▊  | 839/1070 [00:56<00:14, 15.86it/s] 79%|███████▊  | 841/1070 [00:57<00:14, 15.90it/s] 79%|███████▉  | 843/1070 [00:57<00:14, 15.93it/s] 79%|███████▉  | 845/1070 [00:57<00:14, 15.93it/s] 79%|███████▉  | 847/1070 [00:57<00:14, 15.92it/s] 79%|███████▉  | 849/1070 [00:57<00:13, 15.80it/s] 80%|███████▉  | 851/1070 [00:57<00:13, 15.84it/s] 80%|███████▉  | 853/1070 [00:57<00:13, 15.86it/s] 80%|███████▉  | 855/1070 [00:57<00:13, 15.91it/s]                                                   80%|████████  | 856/1070 [00:58<00:13, 15.91it/s][INFO|trainer.py:755] 2023-11-15 23:42:32,280 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 23:42:32,282 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 23:42:32,282 >>   Num examples = 760
[INFO|trainer.py:3134] 2023-11-15 23:42:32,283 >>   Batch size = 8
{'eval_loss': 0.27378779649734497, 'eval_accuracy': 0.9105263157894737, 'eval_micro_f1': 0.9105263157894739, 'eval_macro_f1': 0.9080952665758159, 'eval_runtime': 1.1335, 'eval_samples_per_second': 670.478, 'eval_steps_per_second': 83.81, 'epoch': 3.0}
{'loss': 0.1665, 'learning_rate': 0.0001, 'epoch': 4.0}

  0%|          | 0/95 [00:00<?, ?it/s][A
 11%|█         | 10/95 [00:00<00:00, 96.68it/s][A
 21%|██        | 20/95 [00:00<00:00, 90.31it/s][A
 32%|███▏      | 30/95 [00:00<00:00, 88.62it/s][A
 41%|████      | 39/95 [00:00<00:00, 87.91it/s][A
 51%|█████     | 48/95 [00:00<00:00, 87.47it/s][A
 60%|██████    | 57/95 [00:00<00:00, 87.22it/s][A
 69%|██████▉   | 66/95 [00:00<00:00, 87.08it/s][A
 79%|███████▉  | 75/95 [00:00<00:00, 86.93it/s][A
 88%|████████▊ | 84/95 [00:00<00:00, 86.91it/s][A
 98%|█████████▊| 93/95 [00:01<00:00, 86.98it/s][A                                                  
                                               [A 80%|████████  | 856/1070 [00:59<00:13, 15.91it/s]
100%|██████████| 95/95 [00:01<00:00, 86.98it/s][A
                                               [A 80%|████████  | 857/1070 [00:59<00:49,  4.33it/s] 80%|████████  | 859/1070 [00:59<00:38,  5.54it/s] 80%|████████  | 861/1070 [00:59<00:30,  6.89it/s] 81%|████████  | 863/1070 [00:59<00:25,  8.26it/s] 81%|████████  | 865/1070 [00:59<00:21,  9.61it/s] 81%|████████  | 867/1070 [00:59<00:18, 10.92it/s] 81%|████████  | 869/1070 [00:59<00:16, 12.07it/s] 81%|████████▏ | 871/1070 [01:00<00:15, 13.01it/s] 82%|████████▏ | 873/1070 [01:00<00:14, 13.77it/s] 82%|████████▏ | 875/1070 [01:00<00:13, 14.38it/s] 82%|████████▏ | 877/1070 [01:00<00:13, 14.82it/s] 82%|████████▏ | 879/1070 [01:00<00:12, 15.13it/s] 82%|████████▏ | 881/1070 [01:00<00:12, 15.36it/s] 83%|████████▎ | 883/1070 [01:00<00:12, 15.55it/s] 83%|████████▎ | 885/1070 [01:00<00:11, 15.66it/s] 83%|████████▎ | 887/1070 [01:01<00:11, 15.73it/s] 83%|████████▎ | 889/1070 [01:01<00:11, 15.80it/s] 83%|████████▎ | 891/1070 [01:01<00:11, 15.85it/s] 83%|████████▎ | 893/1070 [01:01<00:11, 15.86it/s] 84%|████████▎ | 895/1070 [01:01<00:11, 15.87it/s] 84%|████████▍ | 897/1070 [01:01<00:10, 15.90it/s] 84%|████████▍ | 899/1070 [01:01<00:10, 15.89it/s] 84%|████████▍ | 901/1070 [01:01<00:10, 15.90it/s] 84%|████████▍ | 903/1070 [01:02<00:10, 15.88it/s] 85%|████████▍ | 905/1070 [01:02<00:10, 15.90it/s] 85%|████████▍ | 907/1070 [01:02<00:10, 15.86it/s] 85%|████████▍ | 909/1070 [01:02<00:10, 15.89it/s] 85%|████████▌ | 911/1070 [01:02<00:09, 15.92it/s] 85%|████████▌ | 913/1070 [01:02<00:09, 15.93it/s] 86%|████████▌ | 915/1070 [01:02<00:09, 15.92it/s] 86%|████████▌ | 917/1070 [01:02<00:09, 15.95it/s] 86%|████████▌ | 919/1070 [01:03<00:09, 15.96it/s] 86%|████████▌ | 921/1070 [01:03<00:09, 15.94it/s] 86%|████████▋ | 923/1070 [01:03<00:09, 15.93it/s] 86%|████████▋ | 925/1070 [01:03<00:09, 15.95it/s] 87%|████████▋ | 927/1070 [01:03<00:08, 15.95it/s] 87%|████████▋ | 929/1070 [01:03<00:08, 15.93it/s] 87%|████████▋ | 931/1070 [01:03<00:08, 15.93it/s] 87%|████████▋ | 933/1070 [01:03<00:08, 15.96it/s] 87%|████████▋ | 935/1070 [01:04<00:08, 15.95it/s] 88%|████████▊ | 937/1070 [01:04<00:08, 15.93it/s] 88%|████████▊ | 939/1070 [01:04<00:08, 15.96it/s] 88%|████████▊ | 941/1070 [01:04<00:08, 15.96it/s] 88%|████████▊ | 943/1070 [01:04<00:07, 15.95it/s] 88%|████████▊ | 945/1070 [01:04<00:07, 15.94it/s] 89%|████████▊ | 947/1070 [01:04<00:07, 15.96it/s] 89%|████████▊ | 949/1070 [01:05<00:07, 15.96it/s] 89%|████████▉ | 951/1070 [01:05<00:07, 15.94it/s] 89%|████████▉ | 953/1070 [01:05<00:07, 15.94it/s] 89%|████████▉ | 955/1070 [01:05<00:07, 15.96it/s] 89%|████████▉ | 957/1070 [01:05<00:07, 15.96it/s] 90%|████████▉ | 959/1070 [01:05<00:06, 15.94it/s] 90%|████████▉ | 961/1070 [01:05<00:06, 15.96it/s] 90%|█████████ | 963/1070 [01:05<00:06, 15.97it/s] 90%|█████████ | 965/1070 [01:06<00:06, 15.73it/s] 90%|█████████ | 967/1070 [01:06<00:06, 15.79it/s] 91%|█████████ | 969/1070 [01:06<00:06, 15.83it/s] 91%|█████████ | 971/1070 [01:06<00:06, 15.84it/s] 91%|█████████ | 973/1070 [01:06<00:06, 15.86it/s] 91%|█████████ | 975/1070 [01:06<00:05, 15.89it/s] 91%|█████████▏| 977/1070 [01:06<00:05, 15.87it/s] 91%|█████████▏| 979/1070 [01:06<00:05, 15.88it/s] 92%|█████████▏| 981/1070 [01:07<00:05, 15.91it/s] 92%|█████████▏| 983/1070 [01:07<00:05, 15.88it/s] 92%|█████████▏| 985/1070 [01:07<00:05, 15.85it/s] 92%|█████████▏| 987/1070 [01:07<00:05, 15.87it/s] 92%|█████████▏| 989/1070 [01:07<00:05, 15.87it/s] 93%|█████████▎| 991/1070 [01:07<00:04, 15.87it/s] 93%|█████████▎| 993/1070 [01:07<00:04, 15.57it/s] 93%|█████████▎| 995/1070 [01:07<00:04, 15.56it/s] 93%|█████████▎| 997/1070 [01:08<00:04, 15.69it/s] 93%|█████████▎| 999/1070 [01:08<00:04, 15.76it/s] 94%|█████████▎| 1001/1070 [01:08<00:04, 15.79it/s] 94%|█████████▎| 1003/1070 [01:08<00:04, 15.81it/s] 94%|█████████▍| 1005/1070 [01:08<00:04, 15.86it/s] 94%|█████████▍| 1007/1070 [01:08<00:03, 15.86it/s] 94%|█████████▍| 1009/1070 [01:08<00:03, 15.86it/s] 94%|█████████▍| 1011/1070 [01:08<00:03, 15.78it/s] 95%|█████████▍| 1013/1070 [01:09<00:03, 15.79it/s] 95%|█████████▍| 1015/1070 [01:09<00:03, 15.82it/s] 95%|█████████▌| 1017/1070 [01:09<00:03, 15.86it/s] 95%|█████████▌| 1019/1070 [01:09<00:03, 15.87it/s] 95%|█████████▌| 1021/1070 [01:09<00:03, 15.87it/s] 96%|█████████▌| 1023/1070 [01:09<00:02, 15.89it/s] 96%|█████████▌| 1025/1070 [01:09<00:02, 15.90it/s] 96%|█████████▌| 1027/1070 [01:09<00:02, 15.85it/s] 96%|█████████▌| 1029/1070 [01:10<00:02, 15.84it/s] 96%|█████████▋| 1031/1070 [01:10<00:02, 15.87it/s] 97%|█████████▋| 1033/1070 [01:10<00:02, 15.88it/s] 97%|█████████▋| 1035/1070 [01:10<00:02, 15.88it/s] 97%|█████████▋| 1037/1070 [01:10<00:02, 15.91it/s] 97%|█████████▋| 1039/1070 [01:10<00:01, 15.92it/s] 97%|█████████▋| 1041/1070 [01:10<00:01, 15.90it/s] 97%|█████████▋| 1043/1070 [01:10<00:01, 15.91it/s] 98%|█████████▊| 1045/1070 [01:11<00:01, 15.93it/s] 98%|█████████▊| 1047/1070 [01:11<00:01, 15.92it/s] 98%|█████████▊| 1049/1070 [01:11<00:01, 15.90it/s] 98%|█████████▊| 1051/1070 [01:11<00:01, 15.91it/s] 98%|█████████▊| 1053/1070 [01:11<00:01, 15.91it/s] 99%|█████████▊| 1055/1070 [01:11<00:00, 15.89it/s] 99%|█████████▉| 1057/1070 [01:11<00:00, 15.91it/s] 99%|█████████▉| 1059/1070 [01:11<00:00, 15.92it/s] 99%|█████████▉| 1061/1070 [01:12<00:00, 15.90it/s] 99%|█████████▉| 1063/1070 [01:12<00:00, 15.88it/s]100%|█████████▉| 1065/1070 [01:12<00:00, 15.88it/s]100%|█████████▉| 1067/1070 [01:12<00:00, 15.85it/s]100%|█████████▉| 1069/1070 [01:12<00:00, 15.85it/s]                                                   100%|██████████| 1070/1070 [01:12<00:00, 15.85it/s][INFO|trainer.py:755] 2023-11-15 23:42:46,873 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 23:42:46,875 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 23:42:46,875 >>   Num examples = 760
[INFO|trainer.py:3134] 2023-11-15 23:42:46,875 >>   Batch size = 8
{'eval_loss': 0.2884453237056732, 'eval_accuracy': 0.9144736842105263, 'eval_micro_f1': 0.9144736842105263, 'eval_macro_f1': 0.9117289701692189, 'eval_runtime': 1.1285, 'eval_samples_per_second': 673.487, 'eval_steps_per_second': 84.186, 'epoch': 4.0}
{'loss': 0.1319, 'learning_rate': 0.0, 'epoch': 5.0}

  0%|          | 0/95 [00:00<?, ?it/s][A
 11%|█         | 10/95 [00:00<00:00, 95.01it/s][A
 21%|██        | 20/95 [00:00<00:00, 89.39it/s][A
 31%|███       | 29/95 [00:00<00:00, 87.80it/s][A
 40%|████      | 38/95 [00:00<00:00, 87.46it/s][A
 49%|████▉     | 47/95 [00:00<00:00, 86.99it/s][A
 59%|█████▉    | 56/95 [00:00<00:00, 86.85it/s][A
 68%|██████▊   | 65/95 [00:00<00:00, 86.87it/s][A
 78%|███████▊  | 74/95 [00:00<00:00, 86.32it/s][A
 87%|████████▋ | 83/95 [00:00<00:00, 86.52it/s][A
 97%|█████████▋| 92/95 [00:01<00:00, 86.50it/s][A                                                   
                                               [A100%|██████████| 1070/1070 [01:13<00:00, 15.85it/s]
100%|██████████| 95/95 [00:01<00:00, 86.50it/s][A
                                               [A[INFO|trainer.py:1963] 2023-11-15 23:42:48,023 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                   100%|██████████| 1070/1070 [01:13<00:00, 15.85it/s]100%|██████████| 1070/1070 [01:13<00:00, 14.50it/s]
[INFO|trainer.py:2855] 2023-11-15 23:42:48,026 >> Saving model checkpoint to ./result/agnews_sup_roberta-base_seed3_adapter
[INFO|configuration_utils.py:460] 2023-11-15 23:42:48,029 >> Configuration saved in ./result/agnews_sup_roberta-base_seed3_adapter/config.json
[INFO|modeling_utils.py:1997] 2023-11-15 23:42:49,181 >> Model weights saved in ./result/agnews_sup_roberta-base_seed3_adapter/pytorch_model.bin
[INFO|tokenization_utils_base.py:2235] 2023-11-15 23:42:49,183 >> tokenizer config file saved in ./result/agnews_sup_roberta-base_seed3_adapter/tokenizer_config.json
[INFO|tokenization_utils_base.py:2242] 2023-11-15 23:42:49,186 >> Special tokens file saved in ./result/agnews_sup_roberta-base_seed3_adapter/special_tokens_map.json
{'eval_loss': 0.2965896427631378, 'eval_accuracy': 0.9184210526315789, 'eval_micro_f1': 0.9184210526315789, 'eval_macro_f1': 0.9161179057278801, 'eval_runtime': 1.1344, 'eval_samples_per_second': 669.965, 'eval_steps_per_second': 83.746, 'epoch': 5.0}
{'train_runtime': 73.7758, 'train_samples_per_second': 463.567, 'train_steps_per_second': 14.503, 'train_loss': 0.2413607124970338, 'epoch': 5.0}
***** train metrics *****
  epoch                    =        5.0
  train_loss               =     0.2414
  train_runtime            = 0:01:13.77
  train_samples            =       6840
  train_samples_per_second =    463.567
  train_steps_per_second   =     14.503
11/15/2023 23:42:49 - INFO - __main__ - *** Evaluate ***
[INFO|trainer.py:755] 2023-11-15 23:42:49,348 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 23:42:49,350 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 23:42:49,350 >>   Num examples = 760
[INFO|trainer.py:3134] 2023-11-15 23:42:49,350 >>   Batch size = 8
  0%|          | 0/95 [00:00<?, ?it/s]  9%|▉         | 9/95 [00:00<00:01, 82.04it/s] 19%|█▉        | 18/95 [00:00<00:00, 77.70it/s] 27%|██▋       | 26/95 [00:00<00:00, 76.26it/s] 36%|███▌      | 34/95 [00:00<00:00, 75.46it/s] 44%|████▍     | 42/95 [00:00<00:00, 75.10it/s] 53%|█████▎    | 50/95 [00:00<00:00, 74.76it/s] 61%|██████    | 58/95 [00:00<00:00, 74.08it/s] 69%|██████▉   | 66/95 [00:00<00:00, 72.88it/s] 78%|███████▊  | 74/95 [00:00<00:00, 73.35it/s] 86%|████████▋ | 82/95 [00:01<00:00, 72.68it/s] 95%|█████████▍| 90/95 [00:01<00:00, 73.35it/s]100%|██████████| 95/95 [00:01<00:00, 72.56it/s]
***** eval metrics *****
  epoch                   =        5.0
  eval_accuracy           =     0.9184
  eval_loss               =     0.2966
  eval_macro_f1           =     0.9161
  eval_micro_f1           =     0.9184
  eval_runtime            = 0:00:01.32
  eval_samples            =        760
  eval_samples_per_second =    572.832
  eval_steps_per_second   =     71.604
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                  eval/accuracy ▁▂▄▆██
wandb:                      eval/loss █▃▁▅██
wandb:                  eval/macro_f1 ▁▂▄▆██
wandb:                  eval/micro_f1 ▁▂▄▆██
wandb:                   eval/runtime ▂▁▁▁▁█
wandb:        eval/samples_per_second ▇████▁
wandb:          eval/steps_per_second ▇████▁
wandb:                    train/epoch ▁▁▃▃▅▅▆▆████
wandb:              train/global_step ▁▁▃▃▄▄▆▆████
wandb:            train/learning_rate █▆▅▃▁
wandb:                     train/loss █▄▃▂▁
wandb:               train/total_flos ▁
wandb:               train/train_loss ▁
wandb:            train/train_runtime ▁
wandb: train/train_samples_per_second ▁
wandb:   train/train_steps_per_second ▁
wandb: 
wandb: Run summary:
wandb:                  eval/accuracy 0.91842
wandb:                      eval/loss 0.29659
wandb:                  eval/macro_f1 0.91612
wandb:                  eval/micro_f1 0.91842
wandb:                   eval/runtime 1.3267
wandb:        eval/samples_per_second 572.832
wandb:          eval/steps_per_second 71.604
wandb:                    train/epoch 5.0
wandb:              train/global_step 1070
wandb:            train/learning_rate 0.0
wandb:                     train/loss 0.1319
wandb:               train/total_flos 1136567617228800.0
wandb:               train/train_loss 0.24136
wandb:            train/train_runtime 73.7758
wandb: train/train_samples_per_second 463.567
wandb:   train/train_steps_per_second 14.503
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /ceph/home/wangyifei/a3/wandb/offline-run-20231115_234016-ld76mt2t
wandb: Find logs at: ./wandb/offline-run-20231115_234016-ld76mt2t/logs
(ModelArguments(model_name_or_path='roberta-base', cache_dir='./cache', use_adapter=True, use_lora=False), DataTrainingArguments(dataset_name='restaurant', max_seq_length=64), TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.0005,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=./result/restaurant_roberta-base_seed4_adapter/runs/Nov15_23-43-01_node013,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.EPOCH,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=5.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
output_dir=./result/restaurant_roberta-base_seed4_adapter,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
resume_from_checkpoint=None,
run_name=./result/restaurant_roberta-base_seed4_adapter,
save_on_each_node=False,
save_safetensors=False,
save_steps=500,
save_strategy=IntervalStrategy.NO,
save_total_limit=None,
seed=555,
sharded_ddp=[],
skip_memory_metrics=True,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
))
11/15/2023 23:43:01 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False
11/15/2023 23:43:01 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.0005,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=./result/restaurant_roberta-base_seed4_adapter/runs/Nov15_23-43-00_node013,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.EPOCH,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=5.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
output_dir=./result/restaurant_roberta-base_seed4_adapter,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
resume_from_checkpoint=None,
run_name=./result/restaurant_roberta-base_seed4_adapter,
save_on_each_node=False,
save_safetensors=False,
save_steps=500,
save_strategy=IntervalStrategy.NO,
save_total_limit=None,
seed=555,
sharded_ddp=[],
skip_memory_metrics=True,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
wandb: Tracking run with wandb version 0.15.12
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
Map:   0%|          | 0/4722 [00:00<?, ? examples/s]Map:  87%|████████▋ | 4106/4722 [00:00<00:00, 39839.76 examples/s]Map: 100%|██████████| 4722/4722 [00:00<00:00, 38406.73 examples/s]
[INFO|configuration_utils.py:715] 2023-11-15 23:43:17,138 >> loading configuration file config.json from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/config.json
[INFO|configuration_utils.py:775] 2023-11-15 23:43:17,146 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.33.3",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_auto.py:535] 2023-11-15 23:43:27,162 >> Could not locate the tokenizer configuration file, will try to use the model config instead.
[INFO|configuration_utils.py:715] 2023-11-15 23:43:37,178 >> loading configuration file config.json from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/config.json
[INFO|configuration_utils.py:775] 2023-11-15 23:43:37,179 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.33.3",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:1852] 2023-11-15 23:43:57,225 >> loading file vocab.json from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/vocab.json
[INFO|tokenization_utils_base.py:1852] 2023-11-15 23:43:57,226 >> loading file merges.txt from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/merges.txt
[INFO|tokenization_utils_base.py:1852] 2023-11-15 23:43:57,226 >> loading file tokenizer.json from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/tokenizer.json
[INFO|tokenization_utils_base.py:1852] 2023-11-15 23:43:57,226 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1852] 2023-11-15 23:43:57,227 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1852] 2023-11-15 23:43:57,227 >> loading file tokenizer_config.json from cache at None
[INFO|configuration_utils.py:715] 2023-11-15 23:43:57,228 >> loading configuration file config.json from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/config.json
[INFO|configuration_utils.py:775] 2023-11-15 23:43:57,229 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.33.3",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:2862] 2023-11-15 23:44:17,401 >> loading weights file model.safetensors from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/model.safetensors
[INFO|modeling_utils.py:3638] 2023-11-15 23:44:18,125 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3650] 2023-11-15 23:44:18,126 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
parameters 1487427
model_max_length: 512
Running tokenizer on dataset:   0%|          | 0/3777 [00:00<?, ? examples/s]Running tokenizer on dataset: 100%|██████████| 3777/3777 [00:00<00:00, 23669.19 examples/s]Running tokenizer on dataset: 100%|██████████| 3777/3777 [00:00<00:00, 23298.71 examples/s]
Running tokenizer on dataset:   0%|          | 0/945 [00:00<?, ? examples/s]Running tokenizer on dataset: 100%|██████████| 945/945 [00:00<00:00, 27519.96 examples/s]
11/15/2023 23:44:19 - INFO - __main__ - Sample 791 of the training set: {'text': 'plain pizza <SEP> The plain pizza was soggy and the creative wild mushroom(third generation-Fornini) pizza we had was drenched with truffle oil in the middle( again making it soggy) and nothingon the rest.', 'label': 2, 'input_ids': [0, 21306, 9366, 28696, 3388, 510, 15698, 20, 10798, 9366, 21, 579, 2154, 4740, 8, 5, 3904, 3418, 30004, 1640, 12347, 2706, 12, 597, 4244, 2531, 43, 9366, 52, 56, 21, 385, 30388, 19, 2664, 15315, 681, 11, 5, 1692, 1640, 456, 442, 24, 579, 2154, 4740, 43, 8, 1085, 261, 5, 1079, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
11/15/2023 23:44:19 - INFO - __main__ - Sample 1124 of the training set: {'text': "lamb sausages <SEP> The dishes offered were unique, very tasty and fresh from the lamb sausages, sardines with biscuits, large whole shrimp to the amazing pistachio ice cream (the best and freshest I've ever had).", 'label': 0, 'input_ids': [0, 5112, 428, 2241, 687, 3443, 28696, 3388, 510, 15698, 20, 10230, 1661, 58, 2216, 6, 182, 22307, 8, 2310, 31, 5, 17988, 2241, 687, 3443, 6, 579, 1120, 3141, 19, 31729, 6, 739, 1086, 22126, 7, 5, 2770, 32617, 1488, 1020, 2480, 6353, 36, 627, 275, 8, 21862, 20921, 38, 348, 655, 56, 322, 2, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]}.
11/15/2023 23:44:19 - INFO - __main__ - Sample 659 of the training set: {'text': 'ingredients <SEP> Great value for the quality ingredients.', 'label': 0, 'input_ids': [0, 154, 48205, 28696, 3388, 510, 15698, 2860, 923, 13, 5, 1318, 7075, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
11/15/2023 23:44:19 - WARNING - accelerate.utils.other - Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[INFO|trainer.py:755] 2023-11-15 23:44:20,407 >> The following columns in the training set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:1715] 2023-11-15 23:44:20,418 >> ***** Running training *****
[INFO|trainer.py:1716] 2023-11-15 23:44:20,419 >>   Num examples = 3,777
[INFO|trainer.py:1717] 2023-11-15 23:44:20,419 >>   Num Epochs = 5
[INFO|trainer.py:1718] 2023-11-15 23:44:20,419 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1721] 2023-11-15 23:44:20,420 >>   Total train batch size (w. parallel, distributed & accumulation) = 32
[INFO|trainer.py:1722] 2023-11-15 23:44:20,420 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:1723] 2023-11-15 23:44:20,420 >>   Total optimization steps = 595
[INFO|trainer.py:1724] 2023-11-15 23:44:20,421 >>   Number of trainable parameters = 1,487,427
[INFO|integration_utils.py:716] 2023-11-15 23:44:20,422 >> Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"
  0%|          | 0/595 [00:00<?, ?it/s]  0%|          | 1/595 [00:01<11:05,  1.12s/it]  1%|          | 3/595 [00:01<03:20,  2.96it/s]  1%|          | 5/595 [00:01<01:56,  5.08it/s]  1%|          | 7/595 [00:01<01:22,  7.12it/s]  2%|▏         | 9/595 [00:01<01:05,  8.97it/s]  2%|▏         | 11/595 [00:01<00:55, 10.55it/s]  2%|▏         | 13/595 [00:01<00:49, 11.83it/s]  3%|▎         | 15/595 [00:02<00:45, 12.87it/s]  3%|▎         | 17/595 [00:02<00:42, 13.65it/s]  3%|▎         | 19/595 [00:02<00:40, 14.21it/s]  4%|▎         | 21/595 [00:02<00:39, 14.67it/s]  4%|▍         | 23/595 [00:02<00:38, 14.95it/s]  4%|▍         | 25/595 [00:02<00:37, 15.17it/s]  5%|▍         | 27/595 [00:02<00:37, 15.32it/s]  5%|▍         | 29/595 [00:02<00:36, 15.42it/s]  5%|▌         | 31/595 [00:03<00:36, 15.51it/s]  6%|▌         | 33/595 [00:03<00:36, 15.56it/s]  6%|▌         | 35/595 [00:03<00:35, 15.57it/s]  6%|▌         | 37/595 [00:03<00:35, 15.61it/s]  7%|▋         | 39/595 [00:03<00:35, 15.65it/s]  7%|▋         | 41/595 [00:03<00:35, 15.66it/s]  7%|▋         | 43/595 [00:03<00:35, 15.71it/s]  8%|▊         | 45/595 [00:03<00:34, 15.72it/s]  8%|▊         | 47/595 [00:04<00:34, 15.74it/s]  8%|▊         | 49/595 [00:04<00:34, 15.77it/s]  9%|▊         | 51/595 [00:04<00:34, 15.74it/s]  9%|▉         | 53/595 [00:04<00:34, 15.73it/s]  9%|▉         | 55/595 [00:04<00:34, 15.74it/s] 10%|▉         | 57/595 [00:04<00:34, 15.73it/s] 10%|▉         | 59/595 [00:04<00:34, 15.75it/s] 10%|█         | 61/595 [00:04<00:33, 15.72it/s] 11%|█         | 63/595 [00:05<00:33, 15.71it/s] 11%|█         | 65/595 [00:05<00:33, 15.75it/s] 11%|█▏        | 67/595 [00:05<00:33, 15.69it/s] 12%|█▏        | 69/595 [00:05<00:33, 15.71it/s] 12%|█▏        | 71/595 [00:05<00:33, 15.71it/s] 12%|█▏        | 73/595 [00:05<00:33, 15.68it/s] 13%|█▎        | 75/595 [00:05<00:33, 15.70it/s] 13%|█▎        | 77/595 [00:05<00:33, 15.70it/s] 13%|█▎        | 79/595 [00:06<00:32, 15.66it/s] 14%|█▎        | 81/595 [00:06<00:32, 15.70it/s] 14%|█▍        | 83/595 [00:06<00:32, 15.65it/s] 14%|█▍        | 85/595 [00:06<00:32, 15.63it/s] 15%|█▍        | 87/595 [00:06<00:32, 15.68it/s] 15%|█▍        | 89/595 [00:06<00:32, 15.64it/s] 15%|█▌        | 91/595 [00:06<00:32, 15.71it/s] 16%|█▌        | 93/595 [00:06<00:31, 15.70it/s] 16%|█▌        | 95/595 [00:07<00:31, 15.66it/s] 16%|█▋        | 97/595 [00:07<00:31, 15.71it/s] 17%|█▋        | 99/595 [00:07<00:31, 15.68it/s] 17%|█▋        | 101/595 [00:07<00:31, 15.70it/s] 17%|█▋        | 103/595 [00:07<00:31, 15.72it/s] 18%|█▊        | 105/595 [00:07<00:31, 15.70it/s] 18%|█▊        | 107/595 [00:07<00:31, 15.73it/s] 18%|█▊        | 109/595 [00:07<00:30, 15.75it/s] 19%|█▊        | 111/595 [00:08<00:30, 15.71it/s] 19%|█▉        | 113/595 [00:08<00:30, 15.76it/s] 19%|█▉        | 115/595 [00:08<00:30, 15.70it/s] 20%|█▉        | 117/595 [00:08<00:30, 15.70it/s]                                                  20%|██        | 119/595 [00:08<00:30, 15.70it/s][INFO|trainer.py:755] 2023-11-15 23:44:29,034 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 23:44:29,036 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 23:44:29,036 >>   Num examples = 945
[INFO|trainer.py:3134] 2023-11-15 23:44:29,036 >>   Batch size = 8
{'loss': 0.7302, 'learning_rate': 0.0004, 'epoch': 1.0}

  0%|          | 0/119 [00:00<?, ?it/s][A
  8%|▊         | 10/119 [00:00<00:01, 89.41it/s][A
 16%|█▌        | 19/119 [00:00<00:01, 83.89it/s][A
 24%|██▎       | 28/119 [00:00<00:01, 83.46it/s][A
 31%|███       | 37/119 [00:00<00:00, 82.61it/s][A
 39%|███▊      | 46/119 [00:00<00:00, 81.12it/s][A
 46%|████▌     | 55/119 [00:00<00:00, 81.14it/s][A
 54%|█████▍    | 64/119 [00:00<00:00, 80.45it/s][A
 61%|██████▏   | 73/119 [00:00<00:00, 79.89it/s][A
 68%|██████▊   | 81/119 [00:00<00:00, 79.40it/s][A
 76%|███████▌  | 90/119 [00:01<00:00, 79.84it/s][A
 82%|████████▏ | 98/119 [00:01<00:00, 78.82it/s][A
 90%|████████▉ | 107/119 [00:01<00:00, 79.23it/s][A
 97%|█████████▋| 115/119 [00:01<00:00, 78.81it/s][A                                                 
                                                 [A 20%|██        | 119/595 [00:10<00:30, 15.70it/s]
100%|██████████| 119/119 [00:01<00:00, 78.81it/s][A
                                                 [A 20%|██        | 120/595 [00:10<02:03,  3.84it/s] 21%|██        | 122/595 [00:10<01:38,  4.83it/s] 21%|██        | 124/595 [00:10<01:18,  5.98it/s] 21%|██        | 126/595 [00:10<01:04,  7.27it/s] 22%|██▏       | 128/595 [00:10<00:54,  8.61it/s] 22%|██▏       | 130/595 [00:10<00:46,  9.92it/s] 22%|██▏       | 132/595 [00:10<00:41, 11.13it/s] 23%|██▎       | 134/595 [00:11<00:37, 12.19it/s] 23%|██▎       | 136/595 [00:11<00:35, 13.06it/s] 23%|██▎       | 138/595 [00:11<00:33, 13.76it/s] 24%|██▎       | 140/595 [00:11<00:31, 14.28it/s] 24%|██▍       | 142/595 [00:11<00:30, 14.72it/s] 24%|██▍       | 144/595 [00:11<00:29, 15.05it/s] 25%|██▍       | 146/595 [00:11<00:29, 15.24it/s] 25%|██▍       | 148/595 [00:11<00:29, 15.39it/s] 25%|██▌       | 150/595 [00:12<00:28, 15.51it/s] 26%|██▌       | 152/595 [00:12<00:28, 15.57it/s] 26%|██▌       | 154/595 [00:12<00:28, 15.60it/s] 26%|██▌       | 156/595 [00:12<00:28, 15.66it/s] 27%|██▋       | 158/595 [00:12<00:27, 15.65it/s] 27%|██▋       | 160/595 [00:12<00:27, 15.74it/s] 27%|██▋       | 162/595 [00:12<00:27, 15.71it/s] 28%|██▊       | 164/595 [00:13<00:27, 15.70it/s] 28%|██▊       | 166/595 [00:13<00:27, 15.72it/s] 28%|██▊       | 168/595 [00:13<00:27, 15.69it/s] 29%|██▊       | 170/595 [00:13<00:27, 15.68it/s] 29%|██▉       | 172/595 [00:13<00:26, 15.74it/s] 29%|██▉       | 174/595 [00:13<00:26, 15.68it/s] 30%|██▉       | 176/595 [00:13<00:26, 15.71it/s] 30%|██▉       | 178/595 [00:13<00:26, 15.72it/s] 30%|███       | 180/595 [00:14<00:26, 15.70it/s] 31%|███       | 182/595 [00:14<00:26, 15.74it/s] 31%|███       | 184/595 [00:14<00:26, 15.74it/s] 31%|███▏      | 186/595 [00:14<00:26, 15.71it/s] 32%|███▏      | 188/595 [00:14<00:25, 15.74it/s] 32%|███▏      | 190/595 [00:14<00:25, 15.73it/s] 32%|███▏      | 192/595 [00:14<00:25, 15.73it/s] 33%|███▎      | 194/595 [00:14<00:25, 15.74it/s] 33%|███▎      | 196/595 [00:15<00:25, 15.68it/s] 33%|███▎      | 198/595 [00:15<00:25, 15.69it/s] 34%|███▎      | 200/595 [00:15<00:25, 15.67it/s] 34%|███▍      | 202/595 [00:15<00:25, 15.65it/s] 34%|███▍      | 204/595 [00:15<00:24, 15.71it/s] 35%|███▍      | 206/595 [00:15<00:24, 15.68it/s] 35%|███▍      | 208/595 [00:15<00:24, 15.70it/s] 35%|███▌      | 210/595 [00:15<00:24, 15.73it/s] 36%|███▌      | 212/595 [00:16<00:24, 15.73it/s] 36%|███▌      | 214/595 [00:16<00:24, 15.72it/s] 36%|███▋      | 216/595 [00:16<00:24, 15.73it/s] 37%|███▋      | 218/595 [00:16<00:23, 15.73it/s] 37%|███▋      | 220/595 [00:16<00:23, 15.76it/s] 37%|███▋      | 222/595 [00:16<00:23, 15.76it/s] 38%|███▊      | 224/595 [00:16<00:23, 15.71it/s] 38%|███▊      | 226/595 [00:16<00:23, 15.76it/s] 38%|███▊      | 228/595 [00:17<00:23, 15.74it/s] 39%|███▊      | 230/595 [00:17<00:23, 15.72it/s] 39%|███▉      | 232/595 [00:17<00:23, 15.75it/s] 39%|███▉      | 234/595 [00:17<00:22, 15.70it/s] 40%|███▉      | 236/595 [00:17<00:22, 15.65it/s]                                                  40%|████      | 238/595 [00:17<00:22, 15.65it/s][INFO|trainer.py:755] 2023-11-15 23:44:38,105 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 23:44:38,107 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 23:44:38,107 >>   Num examples = 945
[INFO|trainer.py:3134] 2023-11-15 23:44:38,108 >>   Batch size = 8
{'eval_loss': 0.5567183494567871, 'eval_accuracy': 0.7703703703703704, 'eval_micro_f1': 0.7703703703703704, 'eval_macro_f1': 0.6537000324978299, 'eval_runtime': 1.5323, 'eval_samples_per_second': 616.718, 'eval_steps_per_second': 77.661, 'epoch': 1.0}
{'loss': 0.5007, 'learning_rate': 0.0003, 'epoch': 2.0}

  0%|          | 0/119 [00:00<?, ?it/s][A
  8%|▊         | 9/119 [00:00<00:01, 89.39it/s][A
 15%|█▌        | 18/119 [00:00<00:01, 83.21it/s][A
 23%|██▎       | 27/119 [00:00<00:01, 81.63it/s][A
 30%|███       | 36/119 [00:00<00:01, 81.16it/s][A
 38%|███▊      | 45/119 [00:00<00:00, 81.02it/s][A
 45%|████▌     | 54/119 [00:00<00:00, 79.56it/s][A
 52%|█████▏    | 62/119 [00:00<00:00, 79.26it/s][A
 59%|█████▉    | 70/119 [00:00<00:00, 78.70it/s][A
 66%|██████▌   | 78/119 [00:00<00:00, 77.85it/s][A
 73%|███████▎  | 87/119 [00:01<00:00, 78.83it/s][A
 80%|███████▉  | 95/119 [00:01<00:00, 78.78it/s][A
 87%|████████▋ | 104/119 [00:01<00:00, 80.13it/s][A
 95%|█████████▍| 113/119 [00:01<00:00, 80.37it/s][A                                                 
                                                 [A 40%|████      | 238/595 [00:19<00:22, 15.65it/s]
100%|██████████| 119/119 [00:01<00:00, 80.37it/s][A
                                                 [A 40%|████      | 239/595 [00:19<01:32,  3.85it/s] 41%|████      | 241/595 [00:19<01:13,  4.84it/s] 41%|████      | 243/595 [00:19<00:58,  6.01it/s] 41%|████      | 245/595 [00:19<00:48,  7.29it/s] 42%|████▏     | 247/595 [00:19<00:40,  8.64it/s] 42%|████▏     | 249/595 [00:19<00:34,  9.95it/s] 42%|████▏     | 251/595 [00:20<00:30, 11.14it/s] 43%|████▎     | 253/595 [00:20<00:28, 12.18it/s] 43%|████▎     | 255/595 [00:20<00:26, 13.03it/s] 43%|████▎     | 257/595 [00:20<00:24, 13.75it/s] 44%|████▎     | 259/595 [00:20<00:23, 14.30it/s] 44%|████▍     | 261/595 [00:20<00:22, 14.68it/s] 44%|████▍     | 263/595 [00:20<00:22, 15.01it/s] 45%|████▍     | 265/595 [00:20<00:21, 15.20it/s] 45%|████▍     | 267/595 [00:21<00:21, 15.32it/s] 45%|████▌     | 269/595 [00:21<00:21, 15.46it/s] 46%|████▌     | 271/595 [00:21<00:20, 15.53it/s] 46%|████▌     | 273/595 [00:21<00:20, 15.63it/s] 46%|████▌     | 275/595 [00:21<00:20, 15.73it/s] 47%|████▋     | 277/595 [00:21<00:20, 15.75it/s] 47%|████▋     | 279/595 [00:21<00:20, 15.77it/s] 47%|████▋     | 281/595 [00:21<00:19, 15.80it/s] 48%|████▊     | 283/595 [00:22<00:19, 15.66it/s] 48%|████▊     | 285/595 [00:22<00:20, 15.43it/s] 48%|████▊     | 287/595 [00:22<00:19, 15.47it/s] 49%|████▊     | 289/595 [00:22<00:19, 15.56it/s] 49%|████▉     | 291/595 [00:22<00:19, 15.59it/s] 49%|████▉     | 293/595 [00:22<00:19, 15.60it/s] 50%|████▉     | 295/595 [00:22<00:19, 15.61it/s] 50%|████▉     | 297/595 [00:22<00:19, 15.60it/s] 50%|█████     | 299/595 [00:23<00:18, 15.64it/s] 51%|█████     | 301/595 [00:23<00:18, 15.61it/s] 51%|█████     | 303/595 [00:23<00:18, 15.63it/s] 51%|█████▏    | 305/595 [00:23<00:18, 15.67it/s] 52%|█████▏    | 307/595 [00:23<00:18, 15.66it/s] 52%|█████▏    | 309/595 [00:23<00:18, 15.66it/s] 52%|█████▏    | 311/595 [00:23<00:18, 15.66it/s] 53%|█████▎    | 313/595 [00:24<00:18, 15.64it/s] 53%|█████▎    | 315/595 [00:24<00:17, 15.68it/s] 53%|█████▎    | 317/595 [00:24<00:17, 15.66it/s] 54%|█████▎    | 319/595 [00:24<00:17, 15.67it/s] 54%|█████▍    | 321/595 [00:24<00:17, 15.70it/s] 54%|█████▍    | 323/595 [00:24<00:17, 15.64it/s] 55%|█████▍    | 325/595 [00:24<00:17, 15.66it/s] 55%|█████▍    | 327/595 [00:24<00:17, 15.62it/s] 55%|█████▌    | 329/595 [00:25<00:16, 15.69it/s] 56%|█████▌    | 331/595 [00:25<00:16, 15.84it/s] 56%|█████▌    | 333/595 [00:25<00:16, 15.92it/s] 56%|█████▋    | 335/595 [00:25<00:16, 15.95it/s] 57%|█████▋    | 337/595 [00:25<00:16, 15.94it/s] 57%|█████▋    | 339/595 [00:25<00:15, 16.01it/s] 57%|█████▋    | 341/595 [00:25<00:15, 16.02it/s] 58%|█████▊    | 343/595 [00:25<00:15, 16.00it/s] 58%|█████▊    | 345/595 [00:26<00:15, 15.94it/s] 58%|█████▊    | 347/595 [00:26<00:15, 15.90it/s] 59%|█████▊    | 349/595 [00:26<00:15, 15.81it/s] 59%|█████▉    | 351/595 [00:26<00:15, 15.73it/s] 59%|█████▉    | 353/595 [00:26<00:15, 15.75it/s] 60%|█████▉    | 355/595 [00:26<00:15, 15.73it/s]                                                  60%|██████    | 357/595 [00:26<00:15, 15.73it/s][INFO|trainer.py:755] 2023-11-15 23:44:47,177 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 23:44:47,179 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 23:44:47,179 >>   Num examples = 945
[INFO|trainer.py:3134] 2023-11-15 23:44:47,179 >>   Batch size = 8
{'eval_loss': 0.47241365909576416, 'eval_accuracy': 0.8084656084656084, 'eval_micro_f1': 0.8084656084656084, 'eval_macro_f1': 0.7264587780425426, 'eval_runtime': 1.5312, 'eval_samples_per_second': 617.16, 'eval_steps_per_second': 77.716, 'epoch': 2.0}
{'loss': 0.377, 'learning_rate': 0.0002, 'epoch': 3.0}

  0%|          | 0/119 [00:00<?, ?it/s][A
  8%|▊         | 10/119 [00:00<00:01, 92.28it/s][A
 17%|█▋        | 20/119 [00:00<00:01, 84.48it/s][A
 24%|██▍       | 29/119 [00:00<00:01, 82.14it/s][A
 32%|███▏      | 38/119 [00:00<00:00, 81.28it/s][A
 39%|███▉      | 47/119 [00:00<00:00, 80.07it/s][A
 47%|████▋     | 56/119 [00:00<00:00, 79.71it/s][A
 54%|█████▍    | 64/119 [00:00<00:00, 79.52it/s][A
 61%|██████    | 72/119 [00:00<00:00, 79.49it/s][A
 67%|██████▋   | 80/119 [00:00<00:00, 78.50it/s][A
 74%|███████▍  | 88/119 [00:01<00:00, 78.46it/s][A
 81%|████████  | 96/119 [00:01<00:00, 78.43it/s][A
 87%|████████▋ | 104/119 [00:01<00:00, 77.97it/s][A
 94%|█████████▍| 112/119 [00:01<00:00, 78.56it/s][A                                                 
                                                 [A 60%|██████    | 357/595 [00:28<00:15, 15.73it/s]
100%|██████████| 119/119 [00:01<00:00, 78.56it/s][A
                                                 [A 60%|██████    | 358/595 [00:28<01:01,  3.83it/s] 61%|██████    | 360/595 [00:28<00:48,  4.81it/s] 61%|██████    | 362/595 [00:28<00:39,  5.96it/s] 61%|██████    | 364/595 [00:28<00:31,  7.24it/s] 62%|██████▏   | 366/595 [00:28<00:26,  8.57it/s] 62%|██████▏   | 368/595 [00:29<00:22,  9.89it/s] 62%|██████▏   | 370/595 [00:29<00:20, 11.10it/s] 63%|██████▎   | 372/595 [00:29<00:18, 12.14it/s] 63%|██████▎   | 374/595 [00:29<00:16, 13.03it/s] 63%|██████▎   | 376/595 [00:29<00:15, 13.69it/s] 64%|██████▎   | 378/595 [00:29<00:15, 14.23it/s] 64%|██████▍   | 380/595 [00:29<00:14, 14.61it/s] 64%|██████▍   | 382/595 [00:29<00:14, 14.87it/s] 65%|██████▍   | 384/595 [00:30<00:13, 15.11it/s] 65%|██████▍   | 386/595 [00:30<00:13, 15.26it/s] 65%|██████▌   | 388/595 [00:30<00:13, 15.37it/s] 66%|██████▌   | 390/595 [00:30<00:13, 15.44it/s] 66%|██████▌   | 392/595 [00:30<00:13, 15.48it/s] 66%|██████▌   | 394/595 [00:30<00:12, 15.57it/s] 67%|██████▋   | 396/595 [00:30<00:12, 15.54it/s] 67%|██████▋   | 398/595 [00:30<00:12, 15.60it/s] 67%|██████▋   | 400/595 [00:31<00:12, 15.56it/s] 68%|██████▊   | 402/595 [00:31<00:12, 15.58it/s] 68%|██████▊   | 404/595 [00:31<00:12, 15.61it/s] 68%|██████▊   | 406/595 [00:31<00:12, 15.61it/s] 69%|██████▊   | 408/595 [00:31<00:11, 15.62it/s] 69%|██████▉   | 410/595 [00:31<00:11, 15.65it/s] 69%|██████▉   | 412/595 [00:31<00:11, 15.62it/s] 70%|██████▉   | 414/595 [00:31<00:11, 15.67it/s] 70%|██████▉   | 416/595 [00:32<00:11, 15.62it/s] 70%|███████   | 418/595 [00:32<00:11, 15.66it/s] 71%|███████   | 420/595 [00:32<00:11, 15.65it/s] 71%|███████   | 422/595 [00:32<00:11, 15.62it/s] 71%|███████▏  | 424/595 [00:32<00:10, 15.62it/s] 72%|███████▏  | 426/595 [00:32<00:10, 15.59it/s] 72%|███████▏  | 428/595 [00:32<00:10, 15.62it/s] 72%|███████▏  | 430/595 [00:32<00:10, 15.60it/s] 73%|███████▎  | 432/595 [00:33<00:10, 15.60it/s] 73%|███████▎  | 434/595 [00:33<00:10, 15.66it/s] 73%|███████▎  | 436/595 [00:33<00:10, 15.61it/s] 74%|███████▎  | 438/595 [00:33<00:10, 15.65it/s] 74%|███████▍  | 440/595 [00:33<00:09, 15.63it/s] 74%|███████▍  | 442/595 [00:33<00:09, 15.62it/s] 75%|███████▍  | 444/595 [00:33<00:09, 15.62it/s] 75%|███████▍  | 446/595 [00:33<00:09, 15.59it/s] 75%|███████▌  | 448/595 [00:34<00:09, 15.61it/s] 76%|███████▌  | 450/595 [00:34<00:09, 15.61it/s] 76%|███████▌  | 452/595 [00:34<00:09, 15.59it/s] 76%|███████▋  | 454/595 [00:34<00:09, 15.62it/s] 77%|███████▋  | 456/595 [00:34<00:08, 15.55it/s] 77%|███████▋  | 458/595 [00:34<00:08, 15.56it/s] 77%|███████▋  | 460/595 [00:34<00:08, 15.55it/s] 78%|███████▊  | 462/595 [00:35<00:08, 15.58it/s] 78%|███████▊  | 464/595 [00:35<00:08, 15.59it/s] 78%|███████▊  | 466/595 [00:35<00:08, 15.59it/s] 79%|███████▊  | 468/595 [00:35<00:08, 15.63it/s] 79%|███████▉  | 470/595 [00:35<00:08, 15.56it/s] 79%|███████▉  | 472/595 [00:35<00:07, 15.62it/s] 80%|███████▉  | 474/595 [00:35<00:07, 15.63it/s] 80%|████████  | 476/595 [00:35<00:07, 16.44it/s]                                                  80%|████████  | 476/595 [00:35<00:07, 16.44it/s][INFO|trainer.py:755] 2023-11-15 23:44:56,328 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 23:44:56,330 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 23:44:56,331 >>   Num examples = 945
[INFO|trainer.py:3134] 2023-11-15 23:44:56,331 >>   Batch size = 8
{'eval_loss': 0.4781709611415863, 'eval_accuracy': 0.8328042328042328, 'eval_micro_f1': 0.8328042328042328, 'eval_macro_f1': 0.7625059693829676, 'eval_runtime': 1.543, 'eval_samples_per_second': 612.448, 'eval_steps_per_second': 77.123, 'epoch': 3.0}
{'loss': 0.2936, 'learning_rate': 0.0001, 'epoch': 4.0}

  0%|          | 0/119 [00:00<?, ?it/s][A
  8%|▊         | 9/119 [00:00<00:01, 85.35it/s][A
 15%|█▌        | 18/119 [00:00<00:01, 80.51it/s][A
 23%|██▎       | 27/119 [00:00<00:01, 80.66it/s][A
 30%|███       | 36/119 [00:00<00:01, 78.69it/s][A
 38%|███▊      | 45/119 [00:00<00:00, 79.37it/s][A
 45%|████▌     | 54/119 [00:00<00:00, 79.73it/s][A
 53%|█████▎    | 63/119 [00:00<00:00, 81.00it/s][A
 61%|██████    | 72/119 [00:00<00:00, 81.11it/s][A
 68%|██████▊   | 81/119 [00:01<00:00, 81.25it/s][A
 76%|███████▌  | 90/119 [00:01<00:00, 81.57it/s][A
 83%|████████▎ | 99/119 [00:01<00:00, 80.75it/s][A
 91%|█████████ | 108/119 [00:01<00:00, 79.77it/s][A
 97%|█████████▋| 116/119 [00:01<00:00, 79.41it/s][A                                                 
                                                 [A 80%|████████  | 476/595 [00:37<00:07, 16.44it/s]
100%|██████████| 119/119 [00:01<00:00, 79.41it/s][A
                                                 [A 80%|████████  | 478/595 [00:37<00:34,  3.40it/s] 81%|████████  | 480/595 [00:37<00:25,  4.44it/s] 81%|████████  | 482/595 [00:37<00:19,  5.65it/s] 81%|████████▏ | 484/595 [00:37<00:15,  6.98it/s] 82%|████████▏ | 486/595 [00:38<00:13,  8.36it/s] 82%|████████▏ | 488/595 [00:38<00:11,  9.71it/s] 82%|████████▏ | 490/595 [00:38<00:09, 10.96it/s] 83%|████████▎ | 492/595 [00:38<00:08, 12.03it/s] 83%|████████▎ | 494/595 [00:38<00:07, 12.96it/s] 83%|████████▎ | 496/595 [00:38<00:07, 13.64it/s] 84%|████████▎ | 498/595 [00:38<00:06, 14.17it/s] 84%|████████▍ | 500/595 [00:38<00:06, 14.58it/s] 84%|████████▍ | 502/595 [00:39<00:06, 14.85it/s] 85%|████████▍ | 504/595 [00:39<00:06, 15.11it/s] 85%|████████▌ | 506/595 [00:39<00:05, 15.25it/s] 85%|████████▌ | 508/595 [00:39<00:05, 15.36it/s] 86%|████████▌ | 510/595 [00:39<00:05, 15.47it/s] 86%|████████▌ | 512/595 [00:39<00:05, 15.46it/s] 86%|████████▋ | 514/595 [00:39<00:05, 15.48it/s] 87%|████████▋ | 516/595 [00:40<00:05, 15.49it/s] 87%|████████▋ | 518/595 [00:40<00:04, 15.52it/s] 87%|████████▋ | 520/595 [00:40<00:04, 15.57it/s] 88%|████████▊ | 522/595 [00:40<00:04, 15.56it/s] 88%|████████▊ | 524/595 [00:40<00:04, 15.60it/s] 88%|████████▊ | 526/595 [00:40<00:04, 15.56it/s] 89%|████████▊ | 528/595 [00:40<00:04, 15.62it/s] 89%|████████▉ | 530/595 [00:40<00:04, 15.61it/s] 89%|████████▉ | 532/595 [00:41<00:04, 15.59it/s] 90%|████████▉ | 534/595 [00:41<00:03, 15.61it/s] 90%|█████████ | 536/595 [00:41<00:03, 15.56it/s] 90%|█████████ | 538/595 [00:41<00:03, 15.60it/s] 91%|█████████ | 540/595 [00:41<00:03, 15.59it/s] 91%|█████████ | 542/595 [00:41<00:03, 15.59it/s] 91%|█████████▏| 544/595 [00:41<00:03, 15.61it/s] 92%|█████████▏| 546/595 [00:41<00:03, 15.59it/s] 92%|█████████▏| 548/595 [00:42<00:03, 15.62it/s] 92%|█████████▏| 550/595 [00:42<00:02, 15.59it/s] 93%|█████████▎| 552/595 [00:42<00:02, 15.59it/s] 93%|█████████▎| 554/595 [00:42<00:02, 15.59it/s] 93%|█████████▎| 556/595 [00:42<00:02, 15.55it/s] 94%|█████████▍| 558/595 [00:42<00:02, 15.57it/s] 94%|█████████▍| 560/595 [00:42<00:02, 15.57it/s] 94%|█████████▍| 562/595 [00:42<00:02, 15.61it/s] 95%|█████████▍| 564/595 [00:43<00:01, 15.57it/s] 95%|█████████▌| 566/595 [00:43<00:01, 15.59it/s] 95%|█████████▌| 568/595 [00:43<00:01, 15.61it/s] 96%|█████████▌| 570/595 [00:43<00:01, 15.60it/s] 96%|█████████▌| 572/595 [00:43<00:01, 15.64it/s] 96%|█████████▋| 574/595 [00:43<00:01, 15.61it/s] 97%|█████████▋| 576/595 [00:43<00:01, 15.60it/s] 97%|█████████▋| 578/595 [00:43<00:01, 15.59it/s] 97%|█████████▋| 580/595 [00:44<00:00, 15.59it/s] 98%|█████████▊| 582/595 [00:44<00:00, 15.62it/s] 98%|█████████▊| 584/595 [00:44<00:00, 15.61it/s] 98%|█████████▊| 586/595 [00:44<00:00, 15.63it/s] 99%|█████████▉| 588/595 [00:44<00:00, 15.64it/s] 99%|█████████▉| 590/595 [00:44<00:00, 15.62it/s] 99%|█████████▉| 592/595 [00:44<00:00, 15.64it/s]100%|█████████▉| 594/595 [00:45<00:00, 15.69it/s]                                                 100%|██████████| 595/595 [00:45<00:00, 15.69it/s][INFO|trainer.py:755] 2023-11-15 23:45:05,465 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 23:45:05,467 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 23:45:05,467 >>   Num examples = 945
[INFO|trainer.py:3134] 2023-11-15 23:45:05,467 >>   Batch size = 8
{'eval_loss': 0.4471427798271179, 'eval_accuracy': 0.8486772486772487, 'eval_micro_f1': 0.8486772486772488, 'eval_macro_f1': 0.7917446782856764, 'eval_runtime': 1.5378, 'eval_samples_per_second': 614.508, 'eval_steps_per_second': 77.382, 'epoch': 4.0}
{'loss': 0.2751, 'learning_rate': 0.0, 'epoch': 5.0}

  0%|          | 0/119 [00:00<?, ?it/s][A
  8%|▊         | 9/119 [00:00<00:01, 88.41it/s][A
 15%|█▌        | 18/119 [00:00<00:01, 83.00it/s][A
 23%|██▎       | 27/119 [00:00<00:01, 80.86it/s][A
 30%|███       | 36/119 [00:00<00:01, 79.90it/s][A
 38%|███▊      | 45/119 [00:00<00:00, 78.39it/s][A
 45%|████▍     | 53/119 [00:00<00:00, 78.71it/s][A
 51%|█████▏    | 61/119 [00:00<00:00, 78.82it/s][A
 58%|█████▊    | 69/119 [00:00<00:00, 77.89it/s][A
 66%|██████▌   | 78/119 [00:00<00:00, 79.21it/s][A
 73%|███████▎  | 87/119 [00:01<00:00, 79.75it/s][A
 80%|███████▉  | 95/119 [00:01<00:00, 79.05it/s][A
 87%|████████▋ | 103/119 [00:01<00:00, 79.00it/s][A
 93%|█████████▎| 111/119 [00:01<00:00, 78.31it/s][A                                                 
                                                 [A100%|██████████| 595/595 [00:46<00:00, 15.69it/s]
100%|██████████| 119/119 [00:01<00:00, 78.31it/s][A
                                                 [A[INFO|trainer.py:1963] 2023-11-15 23:45:07,016 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                 100%|██████████| 595/595 [00:46<00:00, 15.69it/s]100%|██████████| 595/595 [00:46<00:00, 12.77it/s]
[INFO|trainer.py:2855] 2023-11-15 23:45:07,019 >> Saving model checkpoint to ./result/restaurant_roberta-base_seed4_adapter
[INFO|configuration_utils.py:460] 2023-11-15 23:45:07,022 >> Configuration saved in ./result/restaurant_roberta-base_seed4_adapter/config.json
[INFO|modeling_utils.py:1997] 2023-11-15 23:45:08,149 >> Model weights saved in ./result/restaurant_roberta-base_seed4_adapter/pytorch_model.bin
[INFO|tokenization_utils_base.py:2235] 2023-11-15 23:45:08,152 >> tokenizer config file saved in ./result/restaurant_roberta-base_seed4_adapter/tokenizer_config.json
[INFO|tokenization_utils_base.py:2242] 2023-11-15 23:45:08,154 >> Special tokens file saved in ./result/restaurant_roberta-base_seed4_adapter/special_tokens_map.json
{'eval_loss': 0.464079886674881, 'eval_accuracy': 0.8486772486772487, 'eval_micro_f1': 0.8486772486772488, 'eval_macro_f1': 0.7913223511192321, 'eval_runtime': 1.5451, 'eval_samples_per_second': 611.615, 'eval_steps_per_second': 77.018, 'epoch': 5.0}
{'train_runtime': 46.5946, 'train_samples_per_second': 405.304, 'train_steps_per_second': 12.77, 'train_loss': 0.43531759566619616, 'epoch': 5.0}
***** train metrics *****
  epoch                    =        5.0
  train_loss               =     0.4353
  train_runtime            = 0:00:46.59
  train_samples            =       3777
  train_samples_per_second =    405.304
  train_steps_per_second   =      12.77
11/15/2023 23:45:08 - INFO - __main__ - *** Evaluate ***
[INFO|trainer.py:755] 2023-11-15 23:45:08,301 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 23:45:08,302 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 23:45:08,303 >>   Num examples = 945
[INFO|trainer.py:3134] 2023-11-15 23:45:08,303 >>   Batch size = 8
  0%|          | 0/119 [00:00<?, ?it/s]  8%|▊         | 9/119 [00:00<00:01, 89.48it/s] 15%|█▌        | 18/119 [00:00<00:01, 84.69it/s] 23%|██▎       | 27/119 [00:00<00:01, 82.26it/s] 30%|███       | 36/119 [00:00<00:01, 81.61it/s] 38%|███▊      | 45/119 [00:00<00:00, 79.91it/s] 45%|████▌     | 54/119 [00:00<00:00, 80.08it/s] 53%|█████▎    | 63/119 [00:00<00:00, 79.64it/s] 60%|█████▉    | 71/119 [00:00<00:00, 79.30it/s] 66%|██████▋   | 79/119 [00:00<00:00, 79.51it/s] 73%|███████▎  | 87/119 [00:01<00:00, 78.94it/s] 81%|████████  | 96/119 [00:01<00:00, 79.58it/s] 88%|████████▊ | 105/119 [00:01<00:00, 79.67it/s] 96%|█████████▌| 114/119 [00:01<00:00, 80.40it/s]100%|██████████| 119/119 [00:01<00:00, 78.90it/s]
***** eval metrics *****
  epoch                   =        5.0
  eval_accuracy           =     0.8487
  eval_loss               =     0.4641
  eval_macro_f1           =     0.7913
  eval_micro_f1           =     0.8487
  eval_runtime            = 0:00:01.52
  eval_samples            =        945
  eval_samples_per_second =    619.743
  eval_steps_per_second   =     78.042
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                  eval/accuracy ▁▄▇███
wandb:                      eval/loss █▃▃▁▂▂
wandb:                  eval/macro_f1 ▁▅▇███
wandb:                  eval/micro_f1 ▁▄▇███
wandb:                   eval/runtime ▄▃▇▅█▁
wandb:        eval/samples_per_second ▅▆▂▃▁█
wandb:          eval/steps_per_second ▅▆▂▃▁█
wandb:                    train/epoch ▁▁▃▃▅▅▆▆████
wandb:              train/global_step ▁▁▃▃▅▅▆▆████
wandb:            train/learning_rate █▆▅▃▁
wandb:                     train/loss █▄▃▁▁
wandb:               train/total_flos ▁
wandb:               train/train_loss ▁
wandb:            train/train_runtime ▁
wandb: train/train_samples_per_second ▁
wandb:   train/train_steps_per_second ▁
wandb: 
wandb: Run summary:
wandb:                  eval/accuracy 0.84868
wandb:                      eval/loss 0.46408
wandb:                  eval/macro_f1 0.79132
wandb:                  eval/micro_f1 0.84868
wandb:                   eval/runtime 1.5248
wandb:        eval/samples_per_second 619.743
wandb:          eval/steps_per_second 78.042
wandb:                    train/epoch 5.0
wandb:              train/global_step 595
wandb:            train/learning_rate 0.0
wandb:                     train/loss 0.2751
wandb:               train/total_flos 627599085655680.0
wandb:               train/train_loss 0.43532
wandb:            train/train_runtime 46.5946
wandb: train/train_samples_per_second 405.304
wandb:   train/train_steps_per_second 12.77
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /ceph/home/wangyifei/a3/wandb/offline-run-20231115_234302-8l0vt5q0
wandb: Find logs at: ./wandb/offline-run-20231115_234302-8l0vt5q0/logs
(ModelArguments(model_name_or_path='roberta-base', cache_dir='./cache', use_adapter=True, use_lora=False), DataTrainingArguments(dataset_name='acl', max_seq_length=64), TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.0005,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=./result/acl_roberta-base_seed4_adapter/runs/Nov15_23-45-21_node013,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.EPOCH,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=5.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
output_dir=./result/acl_roberta-base_seed4_adapter,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
resume_from_checkpoint=None,
run_name=./result/acl_roberta-base_seed4_adapter,
save_on_each_node=False,
save_safetensors=False,
save_steps=500,
save_strategy=IntervalStrategy.NO,
save_total_limit=None,
seed=555,
sharded_ddp=[],
skip_memory_metrics=True,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
))
11/15/2023 23:45:21 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False
11/15/2023 23:45:21 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.0005,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=./result/acl_roberta-base_seed4_adapter/runs/Nov15_23-45-20_node013,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.EPOCH,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=5.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
output_dir=./result/acl_roberta-base_seed4_adapter,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
resume_from_checkpoint=None,
run_name=./result/acl_roberta-base_seed4_adapter,
save_on_each_node=False,
save_safetensors=False,
save_steps=500,
save_strategy=IntervalStrategy.NO,
save_total_limit=None,
seed=555,
sharded_ddp=[],
skip_memory_metrics=True,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
wandb: Tracking run with wandb version 0.15.12
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
Map:   0%|          | 0/11020 [00:00<?, ? examples/s]Map:  35%|███▍      | 3848/11020 [00:00<00:00, 38262.65 examples/s]Map:  71%|███████   | 7840/11020 [00:00<00:00, 39229.60 examples/s]Map: 100%|██████████| 11020/11020 [00:00<00:00, 38147.44 examples/s]
[INFO|configuration_utils.py:715] 2023-11-15 23:45:37,314 >> loading configuration file config.json from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/config.json
[INFO|configuration_utils.py:775] 2023-11-15 23:45:37,323 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.33.3",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_auto.py:535] 2023-11-15 23:45:47,339 >> Could not locate the tokenizer configuration file, will try to use the model config instead.
[INFO|configuration_utils.py:715] 2023-11-15 23:45:57,356 >> loading configuration file config.json from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/config.json
[INFO|configuration_utils.py:775] 2023-11-15 23:45:57,357 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.33.3",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:1852] 2023-11-15 23:46:17,408 >> loading file vocab.json from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/vocab.json
[INFO|tokenization_utils_base.py:1852] 2023-11-15 23:46:17,408 >> loading file merges.txt from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/merges.txt
[INFO|tokenization_utils_base.py:1852] 2023-11-15 23:46:17,409 >> loading file tokenizer.json from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/tokenizer.json
[INFO|tokenization_utils_base.py:1852] 2023-11-15 23:46:17,409 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1852] 2023-11-15 23:46:17,409 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1852] 2023-11-15 23:46:17,409 >> loading file tokenizer_config.json from cache at None
[INFO|configuration_utils.py:715] 2023-11-15 23:46:17,410 >> loading configuration file config.json from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/config.json
[INFO|configuration_utils.py:775] 2023-11-15 23:46:17,411 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.33.3",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:2862] 2023-11-15 23:46:37,578 >> loading weights file model.safetensors from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/model.safetensors
[INFO|modeling_utils.py:3638] 2023-11-15 23:46:38,282 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3650] 2023-11-15 23:46:38,283 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
parameters 1487427
model_max_length: 512
Running tokenizer on dataset:   0%|          | 0/8816 [00:00<?, ? examples/s]Running tokenizer on dataset:  34%|███▍      | 3000/8816 [00:00<00:00, 19741.59 examples/s]Running tokenizer on dataset:  79%|███████▉  | 7000/8816 [00:00<00:00, 20496.38 examples/s]Running tokenizer on dataset: 100%|██████████| 8816/8816 [00:00<00:00, 20523.18 examples/s]
Running tokenizer on dataset:   0%|          | 0/2204 [00:00<?, ? examples/s]Running tokenizer on dataset: 100%|██████████| 2204/2204 [00:00<00:00, 22493.34 examples/s]
11/15/2023 23:46:39 - INFO - __main__ - Sample 3167 of the training set: {'text': 'Other studies showed that ACR could affect the cellular energy generation and the deficiency of energy induced the neurotoxicity [7, 8].', 'label': 0, 'input_ids': [0, 24989, 3218, 969, 14, 7224, 500, 115, 3327, 5, 19729, 1007, 2706, 8, 5, 30367, 9, 1007, 26914, 5, 44978, 46513, 646, 406, 6, 290, 8174, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
11/15/2023 23:46:39 - INFO - __main__ - Sample 4498 of the training set: {'text': 'Left renal glucose utilization and splanchnic glucose utilization (utilization) were calculated using the formula\n utilization 5 FEGlc 3 3Glc4a 3 R1H2PF (4)\n where R(H)PF equals either unilateral renal plasma flow or hepatic plasma flow.', 'label': 1, 'input_ids': [0, 39961, 39729, 26071, 21429, 8, 11743, 260, 13212, 636, 26071, 21429, 36, 32843, 1938, 43, 58, 9658, 634, 5, 9288, 50118, 21429, 195, 274, 7170, 45071, 155, 155, 16389, 438, 306, 102, 155, 248, 134, 725, 176, 16088, 36, 306, 43, 50118, 147, 248, 1640, 725, 43, 16088, 27601, 1169, 23077, 39729, 29051, 3041, 50, 45441, 5183, 29051, 3041, 4, 2, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]}.
11/15/2023 23:46:39 - INFO - __main__ - Sample 2638 of the training set: {'text': 'The randomized controlled trials (RCTs) which evaluated the efficacy of PEG for mechanical bowel preparation in prevention of postoperative complications in colorectal surgery were considered for inclusion.', 'label': 1, 'input_ids': [0, 133, 36861, 4875, 7341, 36, 500, 7164, 29, 43, 61, 15423, 5, 22081, 9, 221, 7170, 13, 12418, 29928, 7094, 11, 8555, 9, 618, 23655, 12385, 11, 11311, 1688, 3894, 337, 3012, 58, 1687, 13, 9290, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
11/15/2023 23:46:39 - WARNING - accelerate.utils.other - Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[INFO|trainer.py:755] 2023-11-15 23:46:40,739 >> The following columns in the training set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:1715] 2023-11-15 23:46:40,755 >> ***** Running training *****
[INFO|trainer.py:1716] 2023-11-15 23:46:40,755 >>   Num examples = 8,816
[INFO|trainer.py:1717] 2023-11-15 23:46:40,756 >>   Num Epochs = 5
[INFO|trainer.py:1718] 2023-11-15 23:46:40,756 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1721] 2023-11-15 23:46:40,756 >>   Total train batch size (w. parallel, distributed & accumulation) = 32
[INFO|trainer.py:1722] 2023-11-15 23:46:40,756 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:1723] 2023-11-15 23:46:40,757 >>   Total optimization steps = 1,380
[INFO|trainer.py:1724] 2023-11-15 23:46:40,758 >>   Number of trainable parameters = 1,487,427
[INFO|integration_utils.py:716] 2023-11-15 23:46:40,759 >> Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"
  0%|          | 0/1380 [00:00<?, ?it/s]  0%|          | 1/1380 [00:01<24:08,  1.05s/it]  0%|          | 3/1380 [00:01<07:19,  3.14it/s]  0%|          | 5/1380 [00:01<04:16,  5.35it/s]  1%|          | 7/1380 [00:01<03:04,  7.44it/s]  1%|          | 9/1380 [00:01<02:27,  9.32it/s]  1%|          | 11/1380 [00:01<02:05, 10.91it/s]  1%|          | 13/1380 [00:01<01:52, 12.16it/s]  1%|          | 15/1380 [00:01<01:44, 13.07it/s]  1%|          | 17/1380 [00:02<01:39, 13.63it/s]  1%|▏         | 19/1380 [00:02<01:35, 14.22it/s]  2%|▏         | 21/1380 [00:02<01:33, 14.57it/s]  2%|▏         | 23/1380 [00:02<01:30, 14.92it/s]  2%|▏         | 25/1380 [00:02<01:28, 15.28it/s]  2%|▏         | 27/1380 [00:02<01:27, 15.54it/s]  2%|▏         | 29/1380 [00:02<01:25, 15.72it/s]  2%|▏         | 31/1380 [00:02<01:25, 15.82it/s]  2%|▏         | 33/1380 [00:03<01:24, 15.91it/s]  3%|▎         | 35/1380 [00:03<01:24, 16.00it/s]  3%|▎         | 37/1380 [00:03<01:23, 16.06it/s]  3%|▎         | 39/1380 [00:03<01:23, 16.10it/s]  3%|▎         | 41/1380 [00:03<01:23, 16.11it/s]  3%|▎         | 43/1380 [00:03<01:23, 16.09it/s]  3%|▎         | 45/1380 [00:03<01:22, 16.13it/s]  3%|▎         | 47/1380 [00:03<01:22, 16.16it/s]  4%|▎         | 49/1380 [00:04<01:22, 16.08it/s]  4%|▎         | 51/1380 [00:04<01:22, 16.09it/s]  4%|▍         | 53/1380 [00:04<01:22, 16.09it/s]  4%|▍         | 55/1380 [00:04<01:22, 16.11it/s]  4%|▍         | 57/1380 [00:04<01:22, 16.13it/s]  4%|▍         | 59/1380 [00:04<01:21, 16.13it/s]  4%|▍         | 61/1380 [00:04<01:21, 16.11it/s]  5%|▍         | 63/1380 [00:04<01:22, 16.06it/s]  5%|▍         | 65/1380 [00:05<01:22, 15.98it/s]  5%|▍         | 67/1380 [00:05<01:22, 15.88it/s]  5%|▌         | 69/1380 [00:05<01:22, 15.84it/s]  5%|▌         | 71/1380 [00:05<01:23, 15.75it/s]  5%|▌         | 73/1380 [00:05<01:23, 15.72it/s]  5%|▌         | 75/1380 [00:05<01:22, 15.79it/s]  6%|▌         | 77/1380 [00:05<01:22, 15.87it/s]  6%|▌         | 79/1380 [00:05<01:21, 15.89it/s]  6%|▌         | 81/1380 [00:06<01:21, 15.93it/s]  6%|▌         | 83/1380 [00:06<01:21, 15.95it/s]  6%|▌         | 85/1380 [00:06<01:21, 15.89it/s]  6%|▋         | 87/1380 [00:06<01:21, 15.85it/s]  6%|▋         | 89/1380 [00:06<01:21, 15.93it/s]  7%|▋         | 91/1380 [00:06<01:20, 15.95it/s]  7%|▋         | 93/1380 [00:06<01:20, 15.94it/s]  7%|▋         | 95/1380 [00:06<01:20, 15.96it/s]  7%|▋         | 97/1380 [00:07<01:20, 15.92it/s]  7%|▋         | 99/1380 [00:07<01:20, 15.92it/s]  7%|▋         | 101/1380 [00:07<01:20, 15.93it/s]  7%|▋         | 103/1380 [00:07<01:19, 15.97it/s]  8%|▊         | 105/1380 [00:07<01:20, 15.89it/s]  8%|▊         | 107/1380 [00:07<01:21, 15.72it/s]  8%|▊         | 109/1380 [00:07<01:21, 15.65it/s]  8%|▊         | 111/1380 [00:07<01:21, 15.58it/s]  8%|▊         | 113/1380 [00:08<01:20, 15.70it/s]  8%|▊         | 115/1380 [00:08<01:19, 15.83it/s]  8%|▊         | 117/1380 [00:08<01:19, 15.93it/s]  9%|▊         | 119/1380 [00:08<01:18, 15.97it/s]  9%|▉         | 121/1380 [00:08<01:18, 16.01it/s]  9%|▉         | 123/1380 [00:08<01:18, 16.07it/s]  9%|▉         | 125/1380 [00:08<01:18, 16.08it/s]  9%|▉         | 127/1380 [00:08<01:17, 16.06it/s]  9%|▉         | 129/1380 [00:09<01:17, 16.07it/s]  9%|▉         | 131/1380 [00:09<01:17, 16.08it/s] 10%|▉         | 133/1380 [00:09<01:17, 16.13it/s] 10%|▉         | 135/1380 [00:09<01:17, 16.14it/s] 10%|▉         | 137/1380 [00:09<01:17, 16.13it/s] 10%|█         | 139/1380 [00:09<01:17, 16.12it/s] 10%|█         | 141/1380 [00:09<01:16, 16.11it/s] 10%|█         | 143/1380 [00:09<01:16, 16.12it/s] 11%|█         | 145/1380 [00:10<01:16, 16.17it/s] 11%|█         | 147/1380 [00:10<01:16, 16.17it/s] 11%|█         | 149/1380 [00:10<01:16, 16.14it/s] 11%|█         | 151/1380 [00:10<01:16, 16.14it/s] 11%|█         | 153/1380 [00:10<01:16, 16.14it/s] 11%|█         | 155/1380 [00:10<01:16, 16.02it/s] 11%|█▏        | 157/1380 [00:10<01:16, 15.95it/s] 12%|█▏        | 159/1380 [00:10<01:16, 15.91it/s] 12%|█▏        | 161/1380 [00:11<01:16, 15.86it/s] 12%|█▏        | 163/1380 [00:11<01:16, 15.85it/s] 12%|█▏        | 165/1380 [00:11<01:16, 15.81it/s] 12%|█▏        | 167/1380 [00:11<01:16, 15.90it/s] 12%|█▏        | 169/1380 [00:11<01:16, 15.91it/s] 12%|█▏        | 171/1380 [00:11<01:15, 15.94it/s] 13%|█▎        | 173/1380 [00:11<01:15, 16.02it/s] 13%|█▎        | 175/1380 [00:11<01:15, 16.04it/s] 13%|█▎        | 177/1380 [00:12<01:15, 15.98it/s] 13%|█▎        | 179/1380 [00:12<01:15, 15.98it/s] 13%|█▎        | 181/1380 [00:12<01:15, 15.98it/s] 13%|█▎        | 183/1380 [00:12<01:14, 15.98it/s] 13%|█▎        | 185/1380 [00:12<01:14, 16.01it/s] 14%|█▎        | 187/1380 [00:12<01:14, 16.00it/s] 14%|█▎        | 189/1380 [00:12<01:14, 16.00it/s] 14%|█▍        | 191/1380 [00:12<01:14, 16.04it/s] 14%|█▍        | 193/1380 [00:13<01:14, 16.04it/s] 14%|█▍        | 195/1380 [00:13<01:14, 15.95it/s] 14%|█▍        | 197/1380 [00:13<01:14, 15.92it/s] 14%|█▍        | 199/1380 [00:13<01:14, 15.91it/s] 15%|█▍        | 201/1380 [00:13<01:14, 15.88it/s] 15%|█▍        | 203/1380 [00:13<01:13, 15.92it/s] 15%|█▍        | 205/1380 [00:13<01:13, 16.02it/s] 15%|█▌        | 207/1380 [00:13<01:13, 16.05it/s] 15%|█▌        | 209/1380 [00:14<01:12, 16.08it/s] 15%|█▌        | 211/1380 [00:14<01:12, 16.08it/s] 15%|█▌        | 213/1380 [00:14<01:12, 16.09it/s] 16%|█▌        | 215/1380 [00:14<01:12, 16.14it/s] 16%|█▌        | 217/1380 [00:14<01:11, 16.18it/s] 16%|█▌        | 219/1380 [00:14<01:11, 16.18it/s] 16%|█▌        | 221/1380 [00:14<01:11, 16.16it/s] 16%|█▌        | 223/1380 [00:14<01:11, 16.17it/s] 16%|█▋        | 225/1380 [00:15<01:11, 16.21it/s] 16%|█▋        | 227/1380 [00:15<01:11, 16.22it/s] 17%|█▋        | 229/1380 [00:15<01:10, 16.21it/s] 17%|█▋        | 231/1380 [00:15<01:10, 16.20it/s] 17%|█▋        | 233/1380 [00:15<01:10, 16.20it/s] 17%|█▋        | 235/1380 [00:15<01:10, 16.16it/s] 17%|█▋        | 237/1380 [00:15<01:10, 16.21it/s] 17%|█▋        | 239/1380 [00:15<01:10, 16.21it/s] 17%|█▋        | 241/1380 [00:16<01:10, 16.20it/s] 18%|█▊        | 243/1380 [00:16<01:10, 16.13it/s] 18%|█▊        | 245/1380 [00:16<01:10, 16.02it/s] 18%|█▊        | 247/1380 [00:16<01:10, 15.97it/s] 18%|█▊        | 249/1380 [00:16<01:11, 15.90it/s] 18%|█▊        | 251/1380 [00:16<01:11, 15.88it/s] 18%|█▊        | 253/1380 [00:16<01:11, 15.84it/s] 18%|█▊        | 255/1380 [00:16<01:10, 15.86it/s] 19%|█▊        | 257/1380 [00:17<01:10, 15.88it/s] 19%|█▉        | 259/1380 [00:17<01:10, 15.97it/s] 19%|█▉        | 261/1380 [00:17<01:09, 15.99it/s] 19%|█▉        | 263/1380 [00:17<01:09, 15.99it/s] 19%|█▉        | 265/1380 [00:17<01:09, 15.93it/s] 19%|█▉        | 267/1380 [00:17<01:09, 15.92it/s] 19%|█▉        | 269/1380 [00:17<01:09, 15.91it/s] 20%|█▉        | 271/1380 [00:17<01:09, 15.93it/s] 20%|█▉        | 273/1380 [00:18<01:09, 16.00it/s] 20%|█▉        | 275/1380 [00:18<01:08, 16.02it/s]                                                   20%|██        | 276/1380 [00:18<01:08, 16.02it/s][INFO|trainer.py:755] 2023-11-15 23:46:58,992 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 23:46:58,994 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 23:46:58,994 >>   Num examples = 2204
[INFO|trainer.py:3134] 2023-11-15 23:46:58,994 >>   Batch size = 8
{'loss': 0.4874, 'learning_rate': 0.0004, 'epoch': 1.0}

  0%|          | 0/276 [00:00<?, ?it/s][A
  4%|▎         | 10/276 [00:00<00:02, 95.12it/s][A
  7%|▋         | 20/276 [00:00<00:02, 88.83it/s][A
 11%|█         | 29/276 [00:00<00:02, 87.28it/s][A
 14%|█▍        | 38/276 [00:00<00:02, 85.51it/s][A
 17%|█▋        | 47/276 [00:00<00:02, 81.58it/s][A
 20%|██        | 56/276 [00:00<00:02, 79.70it/s][A
 23%|██▎       | 64/276 [00:00<00:02, 78.67it/s][A
 26%|██▌       | 72/276 [00:00<00:02, 77.49it/s][A
 29%|██▉       | 80/276 [00:00<00:02, 76.60it/s][A
 32%|███▏      | 89/276 [00:01<00:02, 79.77it/s][A
 36%|███▌      | 98/276 [00:01<00:02, 81.49it/s][A
 39%|███▉      | 107/276 [00:01<00:02, 82.71it/s][A
 42%|████▏     | 116/276 [00:01<00:01, 84.07it/s][A
 45%|████▌     | 125/276 [00:01<00:01, 84.38it/s][A
 49%|████▊     | 134/276 [00:01<00:01, 85.02it/s][A
 52%|█████▏    | 143/276 [00:01<00:01, 85.80it/s][A
 55%|█████▌    | 152/276 [00:01<00:01, 86.00it/s][A
 58%|█████▊    | 161/276 [00:01<00:01, 86.09it/s][A
 62%|██████▏   | 170/276 [00:02<00:01, 85.69it/s][A
 65%|██████▍   | 179/276 [00:02<00:01, 86.14it/s][A
 68%|██████▊   | 188/276 [00:02<00:01, 86.43it/s][A
 71%|███████▏  | 197/276 [00:02<00:00, 86.48it/s][A
 75%|███████▍  | 206/276 [00:02<00:00, 85.85it/s][A
 78%|███████▊  | 215/276 [00:02<00:00, 85.59it/s][A
 81%|████████  | 224/276 [00:02<00:00, 86.15it/s][A
 84%|████████▍ | 233/276 [00:02<00:00, 86.51it/s][A
 88%|████████▊ | 242/276 [00:02<00:00, 86.72it/s][A
 91%|█████████ | 251/276 [00:02<00:00, 86.47it/s][A
 94%|█████████▍| 260/276 [00:03<00:00, 86.39it/s][A
 97%|█████████▋| 269/276 [00:03<00:00, 86.12it/s][A                                                  
                                                 [A 20%|██        | 276/1380 [00:21<01:08, 16.02it/s]
100%|██████████| 276/276 [00:03<00:00, 86.12it/s][A
                                                 [A 20%|██        | 277/1380 [00:21<10:15,  1.79it/s] 20%|██        | 279/1380 [00:21<07:30,  2.44it/s] 20%|██        | 281/1380 [00:21<05:35,  3.27it/s] 21%|██        | 283/1380 [00:21<04:15,  4.29it/s] 21%|██        | 285/1380 [00:22<03:19,  5.50it/s] 21%|██        | 287/1380 [00:22<02:39,  6.83it/s] 21%|██        | 289/1380 [00:22<02:12,  8.22it/s] 21%|██        | 291/1380 [00:22<01:53,  9.61it/s] 21%|██        | 293/1380 [00:22<01:39, 10.90it/s] 21%|██▏       | 295/1380 [00:22<01:30, 12.02it/s] 22%|██▏       | 297/1380 [00:22<01:23, 12.98it/s] 22%|██▏       | 299/1380 [00:23<01:18, 13.76it/s] 22%|██▏       | 301/1380 [00:23<01:15, 14.32it/s] 22%|██▏       | 303/1380 [00:23<01:13, 14.71it/s] 22%|██▏       | 305/1380 [00:23<01:11, 15.09it/s] 22%|██▏       | 307/1380 [00:23<01:09, 15.34it/s] 22%|██▏       | 309/1380 [00:23<01:09, 15.47it/s] 23%|██▎       | 311/1380 [00:23<01:08, 15.63it/s] 23%|██▎       | 313/1380 [00:23<01:07, 15.74it/s] 23%|██▎       | 315/1380 [00:24<01:07, 15.76it/s] 23%|██▎       | 317/1380 [00:24<01:07, 15.80it/s] 23%|██▎       | 319/1380 [00:24<01:06, 15.88it/s] 23%|██▎       | 321/1380 [00:24<01:06, 15.81it/s] 23%|██▎       | 323/1380 [00:24<01:07, 15.73it/s] 24%|██▎       | 325/1380 [00:24<01:07, 15.59it/s] 24%|██▎       | 327/1380 [00:24<01:07, 15.53it/s] 24%|██▍       | 329/1380 [00:24<01:06, 15.71it/s] 24%|██▍       | 331/1380 [00:25<01:06, 15.82it/s] 24%|██▍       | 333/1380 [00:25<01:05, 15.89it/s] 24%|██▍       | 335/1380 [00:25<01:05, 15.93it/s] 24%|██▍       | 337/1380 [00:25<01:05, 15.98it/s] 25%|██▍       | 339/1380 [00:25<01:04, 16.04it/s] 25%|██▍       | 341/1380 [00:25<01:04, 16.07it/s] 25%|██▍       | 343/1380 [00:25<01:04, 16.07it/s] 25%|██▌       | 345/1380 [00:25<01:04, 16.03it/s] 25%|██▌       | 347/1380 [00:26<01:04, 16.06it/s] 25%|██▌       | 349/1380 [00:26<01:04, 16.03it/s] 25%|██▌       | 351/1380 [00:26<01:04, 16.05it/s] 26%|██▌       | 353/1380 [00:26<01:04, 16.04it/s] 26%|██▌       | 355/1380 [00:26<01:03, 16.07it/s] 26%|██▌       | 357/1380 [00:26<01:03, 16.10it/s] 26%|██▌       | 359/1380 [00:26<01:03, 16.08it/s] 26%|██▌       | 361/1380 [00:26<01:03, 16.06it/s] 26%|██▋       | 363/1380 [00:27<01:03, 16.05it/s] 26%|██▋       | 365/1380 [00:27<01:03, 16.01it/s] 27%|██▋       | 367/1380 [00:27<01:03, 15.88it/s] 27%|██▋       | 369/1380 [00:27<01:03, 15.84it/s] 27%|██▋       | 371/1380 [00:27<01:03, 15.82it/s] 27%|██▋       | 373/1380 [00:27<01:03, 15.75it/s] 27%|██▋       | 375/1380 [00:27<01:04, 15.69it/s] 27%|██▋       | 377/1380 [00:27<01:04, 15.66it/s] 27%|██▋       | 379/1380 [00:28<01:03, 15.69it/s] 28%|██▊       | 381/1380 [00:28<01:03, 15.74it/s] 28%|██▊       | 383/1380 [00:28<01:03, 15.81it/s] 28%|██▊       | 385/1380 [00:28<01:03, 15.77it/s] 28%|██▊       | 387/1380 [00:28<01:02, 15.77it/s] 28%|██▊       | 389/1380 [00:28<01:02, 15.80it/s] 28%|██▊       | 391/1380 [00:28<01:02, 15.73it/s] 28%|██▊       | 393/1380 [00:28<01:02, 15.74it/s] 29%|██▊       | 395/1380 [00:29<01:02, 15.81it/s] 29%|██▉       | 397/1380 [00:29<01:02, 15.84it/s] 29%|██▉       | 399/1380 [00:29<01:01, 15.92it/s] 29%|██▉       | 401/1380 [00:29<01:01, 15.84it/s] 29%|██▉       | 403/1380 [00:29<01:01, 15.80it/s] 29%|██▉       | 405/1380 [00:29<01:01, 15.87it/s] 29%|██▉       | 407/1380 [00:29<01:01, 15.81it/s] 30%|██▉       | 409/1380 [00:29<01:02, 15.57it/s] 30%|██▉       | 411/1380 [00:30<01:02, 15.46it/s] 30%|██▉       | 413/1380 [00:30<01:02, 15.41it/s] 30%|███       | 415/1380 [00:30<01:02, 15.48it/s] 30%|███       | 417/1380 [00:30<01:01, 15.64it/s] 30%|███       | 419/1380 [00:30<01:00, 15.79it/s] 31%|███       | 421/1380 [00:30<01:00, 15.89it/s] 31%|███       | 423/1380 [00:30<01:00, 15.95it/s] 31%|███       | 425/1380 [00:30<00:59, 15.96it/s] 31%|███       | 427/1380 [00:31<00:59, 16.00it/s] 31%|███       | 429/1380 [00:31<00:59, 16.06it/s] 31%|███       | 431/1380 [00:31<00:59, 16.08it/s] 31%|███▏      | 433/1380 [00:31<00:58, 16.07it/s] 32%|███▏      | 435/1380 [00:31<00:58, 16.04it/s] 32%|███▏      | 437/1380 [00:31<00:58, 16.08it/s] 32%|███▏      | 439/1380 [00:31<00:58, 16.11it/s] 32%|███▏      | 441/1380 [00:31<00:58, 16.11it/s] 32%|███▏      | 443/1380 [00:32<00:58, 16.08it/s] 32%|███▏      | 445/1380 [00:32<00:58, 16.07it/s] 32%|███▏      | 447/1380 [00:32<00:57, 16.09it/s] 33%|███▎      | 449/1380 [00:32<00:57, 16.10it/s] 33%|███▎      | 451/1380 [00:32<00:57, 16.09it/s] 33%|███▎      | 453/1380 [00:32<00:57, 16.06it/s] 33%|███▎      | 455/1380 [00:32<00:57, 16.00it/s] 33%|███▎      | 457/1380 [00:32<00:57, 15.95it/s] 33%|███▎      | 459/1380 [00:33<00:57, 15.88it/s] 33%|███▎      | 461/1380 [00:33<00:57, 15.86it/s] 34%|███▎      | 463/1380 [00:33<00:57, 15.82it/s] 34%|███▎      | 465/1380 [00:33<00:58, 15.76it/s] 34%|███▍      | 467/1380 [00:33<00:57, 15.74it/s] 34%|███▍      | 469/1380 [00:33<00:57, 15.79it/s] 34%|███▍      | 471/1380 [00:33<00:57, 15.74it/s] 34%|███▍      | 473/1380 [00:33<00:57, 15.79it/s] 34%|███▍      | 475/1380 [00:34<00:57, 15.78it/s] 35%|███▍      | 477/1380 [00:34<00:57, 15.73it/s] 35%|███▍      | 479/1380 [00:34<00:57, 15.78it/s] 35%|███▍      | 481/1380 [00:34<00:57, 15.66it/s] 35%|███▌      | 483/1380 [00:34<00:57, 15.72it/s] 35%|███▌      | 485/1380 [00:34<00:56, 15.80it/s] 35%|███▌      | 487/1380 [00:34<00:56, 15.83it/s] 35%|███▌      | 489/1380 [00:34<00:56, 15.78it/s] 36%|███▌      | 491/1380 [00:35<00:56, 15.84it/s] 36%|███▌      | 493/1380 [00:35<00:55, 15.84it/s] 36%|███▌      | 495/1380 [00:35<00:56, 15.79it/s] 36%|███▌      | 497/1380 [00:35<00:56, 15.61it/s] 36%|███▌      | 499/1380 [00:35<00:56, 15.57it/s] 36%|███▋      | 501/1380 [00:35<00:56, 15.52it/s] 36%|███▋      | 503/1380 [00:35<00:56, 15.52it/s] 37%|███▋      | 505/1380 [00:36<00:55, 15.68it/s] 37%|███▋      | 507/1380 [00:36<00:55, 15.81it/s] 37%|███▋      | 509/1380 [00:36<00:54, 15.87it/s] 37%|███▋      | 511/1380 [00:36<00:54, 15.91it/s] 37%|███▋      | 513/1380 [00:36<00:54, 15.98it/s] 37%|███▋      | 515/1380 [00:36<00:53, 16.03it/s] 37%|███▋      | 517/1380 [00:36<00:53, 16.06it/s] 38%|███▊      | 519/1380 [00:36<00:53, 16.05it/s] 38%|███▊      | 521/1380 [00:36<00:53, 16.04it/s] 38%|███▊      | 523/1380 [00:37<00:53, 16.05it/s] 38%|███▊      | 525/1380 [00:37<00:53, 16.07it/s] 38%|███▊      | 527/1380 [00:37<00:53, 16.06it/s] 38%|███▊      | 529/1380 [00:37<00:53, 16.05it/s] 38%|███▊      | 531/1380 [00:37<00:52, 16.08it/s] 39%|███▊      | 533/1380 [00:37<00:52, 16.10it/s] 39%|███▉      | 535/1380 [00:37<00:52, 16.09it/s] 39%|███▉      | 537/1380 [00:37<00:52, 16.07it/s] 39%|███▉      | 539/1380 [00:38<00:52, 16.03it/s] 39%|███▉      | 541/1380 [00:38<00:52, 16.06it/s] 39%|███▉      | 543/1380 [00:38<00:52, 16.00it/s] 39%|███▉      | 545/1380 [00:38<00:52, 15.89it/s] 40%|███▉      | 547/1380 [00:38<00:52, 15.81it/s] 40%|███▉      | 549/1380 [00:38<00:52, 15.76it/s] 40%|███▉      | 551/1380 [00:38<00:52, 15.77it/s]                                                   40%|████      | 552/1380 [00:38<00:52, 15.77it/s][INFO|trainer.py:755] 2023-11-15 23:47:19,679 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 23:47:19,681 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 23:47:19,682 >>   Num examples = 2204
[INFO|trainer.py:3134] 2023-11-15 23:47:19,682 >>   Batch size = 8
{'eval_loss': 0.4348752796649933, 'eval_accuracy': 0.8339382940108893, 'eval_micro_f1': 0.8339382940108893, 'eval_macro_f1': 0.8270310194471163, 'eval_runtime': 3.3215, 'eval_samples_per_second': 663.553, 'eval_steps_per_second': 83.095, 'epoch': 1.0}
{'loss': 0.359, 'learning_rate': 0.0003, 'epoch': 2.0}

  0%|          | 0/276 [00:00<?, ?it/s][A
  4%|▎         | 10/276 [00:00<00:02, 95.00it/s][A
  7%|▋         | 20/276 [00:00<00:02, 89.83it/s][A
 11%|█         | 30/276 [00:00<00:02, 88.17it/s][A
 14%|█▍        | 39/276 [00:00<00:02, 87.70it/s][A
 17%|█▋        | 48/276 [00:00<00:02, 87.62it/s][A
 21%|██        | 57/276 [00:00<00:02, 87.46it/s][A
 24%|██▍       | 66/276 [00:00<00:02, 86.76it/s][A
 27%|██▋       | 75/276 [00:00<00:02, 86.94it/s][A
 30%|███       | 84/276 [00:00<00:02, 86.98it/s][A
 34%|███▎      | 93/276 [00:01<00:02, 87.40it/s][A
 37%|███▋      | 102/276 [00:01<00:01, 87.12it/s][A
 40%|████      | 111/276 [00:01<00:01, 86.66it/s][A
 43%|████▎     | 120/276 [00:01<00:01, 86.38it/s][A
 47%|████▋     | 129/276 [00:01<00:01, 86.23it/s][A
 50%|█████     | 138/276 [00:01<00:01, 86.52it/s][A
 53%|█████▎    | 147/276 [00:01<00:01, 86.75it/s][A
 57%|█████▋    | 156/276 [00:01<00:01, 86.72it/s][A
 60%|█████▉    | 165/276 [00:01<00:01, 86.04it/s][A
 63%|██████▎   | 174/276 [00:02<00:01, 84.54it/s][A
 66%|██████▋   | 183/276 [00:02<00:01, 81.35it/s][A
 70%|██████▉   | 192/276 [00:02<00:01, 80.95it/s][A
 73%|███████▎  | 201/276 [00:02<00:00, 79.71it/s][A
 76%|███████▌  | 209/276 [00:02<00:00, 79.10it/s][A
 79%|███████▊  | 217/276 [00:02<00:00, 78.79it/s][A
 82%|████████▏ | 226/276 [00:02<00:00, 81.65it/s][A
 85%|████████▌ | 235/276 [00:02<00:00, 83.95it/s][A
 88%|████████▊ | 244/276 [00:02<00:00, 85.56it/s][A
 92%|█████████▏| 253/276 [00:02<00:00, 85.79it/s][A
 95%|█████████▍| 262/276 [00:03<00:00, 86.72it/s][A
 98%|█████████▊| 271/276 [00:03<00:00, 86.51it/s][A                                                  
                                                 [A 40%|████      | 552/1380 [00:42<00:52, 15.77it/s]
100%|██████████| 276/276 [00:03<00:00, 86.51it/s][A
                                                 [A 40%|████      | 553/1380 [00:42<07:37,  1.81it/s] 40%|████      | 555/1380 [00:42<05:35,  2.46it/s] 40%|████      | 557/1380 [00:42<04:09,  3.30it/s] 41%|████      | 559/1380 [00:42<03:09,  4.34it/s] 41%|████      | 561/1380 [00:42<02:27,  5.55it/s] 41%|████      | 563/1380 [00:42<01:58,  6.90it/s] 41%|████      | 565/1380 [00:43<01:37,  8.32it/s] 41%|████      | 567/1380 [00:43<01:23,  9.74it/s] 41%|████      | 569/1380 [00:43<01:13, 11.02it/s] 41%|████▏     | 571/1380 [00:43<01:06, 12.16it/s] 42%|████▏     | 573/1380 [00:43<01:01, 13.14it/s] 42%|████▏     | 575/1380 [00:43<00:57, 13.88it/s] 42%|████▏     | 577/1380 [00:43<00:55, 14.47it/s] 42%|████▏     | 579/1380 [00:43<00:53, 14.89it/s] 42%|████▏     | 581/1380 [00:44<00:52, 15.20it/s] 42%|████▏     | 583/1380 [00:44<00:51, 15.38it/s] 42%|████▏     | 585/1380 [00:44<00:51, 15.44it/s] 43%|████▎     | 587/1380 [00:44<00:51, 15.53it/s] 43%|████▎     | 589/1380 [00:44<00:50, 15.55it/s] 43%|████▎     | 591/1380 [00:44<00:50, 15.59it/s] 43%|████▎     | 593/1380 [00:44<00:50, 15.67it/s] 43%|████▎     | 595/1380 [00:44<00:49, 15.73it/s] 43%|████▎     | 597/1380 [00:45<00:49, 15.79it/s] 43%|████▎     | 599/1380 [00:45<00:49, 15.85it/s] 44%|████▎     | 601/1380 [00:45<00:48, 15.91it/s] 44%|████▎     | 603/1380 [00:45<00:48, 15.90it/s] 44%|████▍     | 605/1380 [00:45<00:48, 15.83it/s] 44%|████▍     | 607/1380 [00:45<00:48, 15.85it/s] 44%|████▍     | 609/1380 [00:45<00:48, 15.83it/s] 44%|████▍     | 611/1380 [00:45<00:48, 15.85it/s] 44%|████▍     | 613/1380 [00:46<00:48, 15.89it/s] 45%|████▍     | 615/1380 [00:46<00:48, 15.93it/s] 45%|████▍     | 617/1380 [00:46<00:48, 15.85it/s] 45%|████▍     | 619/1380 [00:46<00:47, 15.88it/s] 45%|████▌     | 621/1380 [00:46<00:47, 15.88it/s] 45%|████▌     | 623/1380 [00:46<00:47, 15.78it/s] 45%|████▌     | 625/1380 [00:46<00:47, 15.80it/s] 45%|████▌     | 627/1380 [00:46<00:47, 15.78it/s] 46%|████▌     | 629/1380 [00:47<00:47, 15.75it/s] 46%|████▌     | 631/1380 [00:47<00:47, 15.86it/s] 46%|████▌     | 633/1380 [00:47<00:46, 15.91it/s] 46%|████▌     | 635/1380 [00:47<00:46, 15.93it/s] 46%|████▌     | 637/1380 [00:47<00:46, 15.95it/s] 46%|████▋     | 639/1380 [00:47<00:46, 16.01it/s] 46%|████▋     | 641/1380 [00:47<00:46, 16.04it/s] 47%|████▋     | 643/1380 [00:47<00:45, 16.05it/s] 47%|████▋     | 645/1380 [00:48<00:45, 16.03it/s] 47%|████▋     | 647/1380 [00:48<00:45, 16.04it/s] 47%|████▋     | 649/1380 [00:48<00:45, 16.06it/s] 47%|████▋     | 651/1380 [00:48<00:45, 16.07it/s] 47%|████▋     | 653/1380 [00:48<00:45, 16.05it/s] 47%|████▋     | 655/1380 [00:48<00:45, 16.04it/s] 48%|████▊     | 657/1380 [00:48<00:44, 16.09it/s] 48%|████▊     | 659/1380 [00:48<00:44, 16.10it/s] 48%|████▊     | 661/1380 [00:49<00:44, 16.07it/s] 48%|████▊     | 663/1380 [00:49<00:44, 16.04it/s] 48%|████▊     | 665/1380 [00:49<00:44, 16.06it/s] 48%|████▊     | 667/1380 [00:49<00:44, 16.09it/s] 48%|████▊     | 669/1380 [00:49<00:44, 16.03it/s] 49%|████▊     | 671/1380 [00:49<00:44, 15.90it/s] 49%|████▉     | 673/1380 [00:49<00:44, 15.88it/s] 49%|████▉     | 675/1380 [00:49<00:44, 15.81it/s] 49%|████▉     | 677/1380 [00:50<00:44, 15.77it/s] 49%|████▉     | 679/1380 [00:50<00:44, 15.78it/s] 49%|████▉     | 681/1380 [00:50<00:44, 15.76it/s] 49%|████▉     | 683/1380 [00:50<00:44, 15.76it/s] 50%|████▉     | 685/1380 [00:50<00:44, 15.77it/s] 50%|████▉     | 687/1380 [00:50<00:43, 15.79it/s] 50%|████▉     | 689/1380 [00:50<00:43, 15.80it/s] 50%|█████     | 691/1380 [00:50<00:43, 15.89it/s] 50%|█████     | 693/1380 [00:51<00:43, 15.90it/s] 50%|█████     | 695/1380 [00:51<00:43, 15.80it/s] 51%|█████     | 697/1380 [00:51<00:43, 15.84it/s] 51%|█████     | 699/1380 [00:51<00:42, 15.85it/s] 51%|█████     | 701/1380 [00:51<00:42, 15.86it/s] 51%|█████     | 703/1380 [00:51<00:42, 15.89it/s] 51%|█████     | 705/1380 [00:51<00:42, 15.90it/s] 51%|█████     | 707/1380 [00:51<00:42, 15.81it/s] 51%|█████▏    | 709/1380 [00:52<00:42, 15.77it/s] 52%|█████▏    | 711/1380 [00:52<00:42, 15.72it/s] 52%|█████▏    | 713/1380 [00:52<00:42, 15.66it/s] 52%|█████▏    | 715/1380 [00:52<00:42, 15.70it/s] 52%|█████▏    | 717/1380 [00:52<00:42, 15.69it/s] 52%|█████▏    | 719/1380 [00:52<00:41, 15.78it/s] 52%|█████▏    | 721/1380 [00:52<00:41, 15.87it/s] 52%|█████▏    | 723/1380 [00:52<00:41, 15.92it/s] 53%|█████▎    | 725/1380 [00:53<00:41, 15.95it/s] 53%|█████▎    | 727/1380 [00:53<00:40, 15.97it/s] 53%|█████▎    | 729/1380 [00:53<00:40, 16.01it/s] 53%|█████▎    | 731/1380 [00:53<00:40, 16.01it/s] 53%|█████▎    | 733/1380 [00:53<00:40, 16.00it/s] 53%|█████▎    | 735/1380 [00:53<00:40, 15.99it/s] 53%|█████▎    | 737/1380 [00:53<00:40, 16.04it/s] 54%|█████▎    | 739/1380 [00:53<00:39, 16.06it/s] 54%|█████▎    | 741/1380 [00:54<00:39, 16.04it/s] 54%|█████▍    | 743/1380 [00:54<00:39, 16.03it/s] 54%|█████▍    | 745/1380 [00:54<00:39, 16.04it/s] 54%|█████▍    | 747/1380 [00:54<00:39, 16.01it/s] 54%|█████▍    | 749/1380 [00:54<00:39, 16.00it/s] 54%|█████▍    | 751/1380 [00:54<00:39, 15.99it/s] 55%|█████▍    | 753/1380 [00:54<00:39, 16.05it/s] 55%|█████▍    | 755/1380 [00:54<00:38, 16.05it/s] 55%|█████▍    | 757/1380 [00:55<00:38, 16.01it/s] 55%|█████▌    | 759/1380 [00:55<00:39, 15.91it/s] 55%|█████▌    | 761/1380 [00:55<00:38, 15.90it/s] 55%|█████▌    | 763/1380 [00:55<00:39, 15.81it/s] 55%|█████▌    | 765/1380 [00:55<00:39, 15.76it/s] 56%|█████▌    | 767/1380 [00:55<00:38, 15.74it/s] 56%|█████▌    | 769/1380 [00:55<00:38, 15.74it/s] 56%|█████▌    | 771/1380 [00:55<00:38, 15.74it/s] 56%|█████▌    | 773/1380 [00:56<00:38, 15.79it/s] 56%|█████▌    | 775/1380 [00:56<00:38, 15.81it/s] 56%|█████▋    | 777/1380 [00:56<00:38, 15.84it/s] 56%|█████▋    | 779/1380 [00:56<00:37, 15.90it/s] 57%|█████▋    | 781/1380 [00:56<00:37, 15.93it/s] 57%|█████▋    | 783/1380 [00:56<00:37, 15.87it/s] 57%|█████▋    | 785/1380 [00:56<00:37, 15.86it/s] 57%|█████▋    | 787/1380 [00:56<00:37, 15.88it/s] 57%|█████▋    | 789/1380 [00:57<00:37, 15.89it/s] 57%|█████▋    | 791/1380 [00:57<00:37, 15.87it/s] 57%|█████▋    | 793/1380 [00:57<00:36, 15.91it/s] 58%|█████▊    | 795/1380 [00:57<00:36, 15.91it/s] 58%|█████▊    | 797/1380 [00:57<00:36, 15.83it/s] 58%|█████▊    | 799/1380 [00:57<00:36, 15.78it/s] 58%|█████▊    | 801/1380 [00:57<00:36, 15.71it/s] 58%|█████▊    | 803/1380 [00:57<00:36, 15.71it/s] 58%|█████▊    | 805/1380 [00:58<00:36, 15.71it/s] 58%|█████▊    | 807/1380 [00:58<00:36, 15.77it/s] 59%|█████▊    | 809/1380 [00:58<00:36, 15.86it/s] 59%|█████▉    | 811/1380 [00:58<00:35, 15.94it/s] 59%|█████▉    | 813/1380 [00:58<00:35, 15.95it/s] 59%|█████▉    | 815/1380 [00:58<00:35, 15.87it/s] 59%|█████▉    | 817/1380 [00:58<00:35, 15.94it/s] 59%|█████▉    | 819/1380 [00:58<00:35, 15.98it/s] 59%|█████▉    | 821/1380 [00:59<00:34, 15.99it/s] 60%|█████▉    | 823/1380 [00:59<00:34, 15.99it/s] 60%|█████▉    | 825/1380 [00:59<00:34, 16.03it/s] 60%|█████▉    | 827/1380 [00:59<00:34, 16.08it/s]                                                   60%|██████    | 828/1380 [00:59<00:34, 16.08it/s][INFO|trainer.py:755] 2023-11-15 23:47:40,295 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 23:47:40,296 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 23:47:40,297 >>   Num examples = 2204
[INFO|trainer.py:3134] 2023-11-15 23:47:40,297 >>   Batch size = 8
{'eval_loss': 0.3850655257701874, 'eval_accuracy': 0.8498185117967332, 'eval_micro_f1': 0.8498185117967332, 'eval_macro_f1': 0.8438500880258196, 'eval_runtime': 3.2865, 'eval_samples_per_second': 670.618, 'eval_steps_per_second': 83.979, 'epoch': 2.0}
{'loss': 0.3149, 'learning_rate': 0.0002, 'epoch': 3.0}

  0%|          | 0/276 [00:00<?, ?it/s][A
  4%|▎         | 10/276 [00:00<00:02, 98.46it/s][A
  7%|▋         | 20/276 [00:00<00:02, 93.04it/s][A
 11%|█         | 30/276 [00:00<00:02, 91.22it/s][A
 14%|█▍        | 40/276 [00:00<00:02, 90.31it/s][A
 18%|█▊        | 50/276 [00:00<00:02, 90.09it/s][A
 22%|██▏       | 60/276 [00:00<00:02, 88.96it/s][A
 25%|██▌       | 69/276 [00:00<00:02, 88.54it/s][A
 28%|██▊       | 78/276 [00:00<00:02, 88.57it/s][A
 32%|███▏      | 87/276 [00:00<00:02, 88.65it/s][A
 35%|███▍      | 96/276 [00:01<00:02, 88.68it/s][A
 38%|███▊      | 105/276 [00:01<00:01, 87.72it/s][A
 41%|████▏     | 114/276 [00:01<00:01, 86.99it/s][A
 45%|████▍     | 123/276 [00:01<00:01, 87.15it/s][A
 48%|████▊     | 132/276 [00:01<00:01, 86.75it/s][A
 51%|█████     | 141/276 [00:01<00:01, 85.70it/s][A
 54%|█████▍    | 150/276 [00:01<00:01, 85.56it/s][A
 58%|█████▊    | 159/276 [00:01<00:01, 85.85it/s][A
 61%|██████    | 168/276 [00:01<00:01, 85.47it/s][A
 64%|██████▍   | 177/276 [00:02<00:01, 85.89it/s][A
 67%|██████▋   | 186/276 [00:02<00:01, 85.68it/s][A
 71%|███████   | 195/276 [00:02<00:00, 85.54it/s][A
 74%|███████▍  | 204/276 [00:02<00:00, 85.79it/s][A
 77%|███████▋  | 213/276 [00:02<00:00, 86.08it/s][A
 80%|████████  | 222/276 [00:02<00:00, 86.72it/s][A
 84%|████████▎ | 231/276 [00:02<00:00, 86.71it/s][A
 87%|████████▋ | 240/276 [00:02<00:00, 86.41it/s][A
 90%|█████████ | 249/276 [00:02<00:00, 86.65it/s][A
 93%|█████████▎| 258/276 [00:02<00:00, 86.97it/s][A
 97%|█████████▋| 267/276 [00:03<00:00, 86.86it/s][A
100%|██████████| 276/276 [00:03<00:00, 87.31it/s][A                                                  
                                                 [A 60%|██████    | 828/1380 [01:02<00:34, 16.08it/s]
100%|██████████| 276/276 [00:03<00:00, 87.31it/s][A
                                                 [A 60%|██████    | 829/1380 [01:02<04:58,  1.85it/s] 60%|██████    | 831/1380 [01:02<03:38,  2.51it/s] 60%|██████    | 833/1380 [01:03<02:42,  3.36it/s] 61%|██████    | 835/1380 [01:03<02:04,  4.38it/s] 61%|██████    | 837/1380 [01:03<01:37,  5.58it/s] 61%|██████    | 839/1380 [01:03<01:18,  6.88it/s] 61%|██████    | 841/1380 [01:03<01:05,  8.25it/s] 61%|██████    | 843/1380 [01:03<00:55,  9.65it/s] 61%|██████    | 845/1380 [01:03<00:48, 10.95it/s] 61%|██████▏   | 847/1380 [01:03<00:44, 12.11it/s] 62%|██████▏   | 849/1380 [01:04<00:40, 13.07it/s] 62%|██████▏   | 851/1380 [01:04<00:38, 13.81it/s] 62%|██████▏   | 853/1380 [01:04<00:36, 14.39it/s] 62%|██████▏   | 855/1380 [01:04<00:35, 14.85it/s] 62%|██████▏   | 857/1380 [01:04<00:34, 15.19it/s] 62%|██████▏   | 859/1380 [01:04<00:33, 15.42it/s] 62%|██████▏   | 861/1380 [01:04<00:33, 15.58it/s] 63%|██████▎   | 863/1380 [01:04<00:32, 15.69it/s] 63%|██████▎   | 865/1380 [01:05<00:32, 15.80it/s] 63%|██████▎   | 867/1380 [01:05<00:32, 15.84it/s] 63%|██████▎   | 869/1380 [01:05<00:32, 15.88it/s] 63%|██████▎   | 871/1380 [01:05<00:31, 15.95it/s] 63%|██████▎   | 873/1380 [01:05<00:31, 15.98it/s] 63%|██████▎   | 875/1380 [01:05<00:31, 15.97it/s] 64%|██████▎   | 877/1380 [01:05<00:31, 15.93it/s] 64%|██████▎   | 879/1380 [01:05<00:31, 15.94it/s] 64%|██████▍   | 881/1380 [01:06<00:31, 15.90it/s] 64%|██████▍   | 883/1380 [01:06<00:31, 15.79it/s] 64%|██████▍   | 885/1380 [01:06<00:31, 15.78it/s] 64%|██████▍   | 887/1380 [01:06<00:31, 15.73it/s] 64%|██████▍   | 889/1380 [01:06<00:31, 15.71it/s] 65%|██████▍   | 891/1380 [01:06<00:31, 15.73it/s] 65%|██████▍   | 893/1380 [01:06<00:30, 15.76it/s] 65%|██████▍   | 895/1380 [01:06<00:30, 15.73it/s] 65%|██████▌   | 897/1380 [01:07<00:30, 15.72it/s] 65%|██████▌   | 899/1380 [01:07<00:30, 15.75it/s] 65%|██████▌   | 901/1380 [01:07<00:30, 15.76it/s] 65%|██████▌   | 903/1380 [01:07<00:30, 15.78it/s] 66%|██████▌   | 905/1380 [01:07<00:30, 15.79it/s] 66%|██████▌   | 907/1380 [01:07<00:29, 15.77it/s] 66%|██████▌   | 909/1380 [01:07<00:29, 15.78it/s] 66%|██████▌   | 911/1380 [01:08<00:29, 15.78it/s] 66%|██████▌   | 913/1380 [01:08<00:29, 15.74it/s] 66%|██████▋   | 915/1380 [01:08<00:29, 15.79it/s] 66%|██████▋   | 917/1380 [01:08<00:29, 15.81it/s] 67%|██████▋   | 919/1380 [01:08<00:29, 15.60it/s] 67%|██████▋   | 921/1380 [01:08<00:29, 15.59it/s] 67%|██████▋   | 923/1380 [01:08<00:29, 15.63it/s] 67%|██████▋   | 925/1380 [01:08<00:29, 15.64it/s] 67%|██████▋   | 927/1380 [01:09<00:28, 15.66it/s] 67%|██████▋   | 929/1380 [01:09<00:28, 15.78it/s] 67%|██████▋   | 931/1380 [01:09<00:28, 15.86it/s] 68%|██████▊   | 933/1380 [01:09<00:28, 15.89it/s] 68%|██████▊   | 935/1380 [01:09<00:27, 15.92it/s] 68%|██████▊   | 937/1380 [01:09<00:27, 15.97it/s] 68%|██████▊   | 939/1380 [01:09<00:27, 15.98it/s] 68%|██████▊   | 941/1380 [01:09<00:27, 15.98it/s] 68%|██████▊   | 943/1380 [01:10<00:27, 15.94it/s] 68%|██████▊   | 945/1380 [01:10<00:27, 15.99it/s] 69%|██████▊   | 947/1380 [01:10<00:27, 15.97it/s] 69%|██████▉   | 949/1380 [01:10<00:27, 15.94it/s] 69%|██████▉   | 951/1380 [01:10<00:26, 15.97it/s] 69%|██████▉   | 953/1380 [01:10<00:26, 16.00it/s] 69%|██████▉   | 955/1380 [01:10<00:26, 15.98it/s] 69%|██████▉   | 957/1380 [01:10<00:26, 15.95it/s] 69%|██████▉   | 959/1380 [01:11<00:26, 15.91it/s] 70%|██████▉   | 961/1380 [01:11<00:26, 15.86it/s] 70%|██████▉   | 963/1380 [01:11<00:26, 15.88it/s] 70%|██████▉   | 965/1380 [01:11<00:26, 15.50it/s] 70%|███████   | 967/1380 [01:11<00:26, 15.61it/s] 70%|███████   | 969/1380 [01:11<00:26, 15.55it/s] 70%|███████   | 971/1380 [01:11<00:26, 15.55it/s] 71%|███████   | 973/1380 [01:11<00:26, 15.58it/s] 71%|███████   | 975/1380 [01:12<00:26, 15.54it/s] 71%|███████   | 977/1380 [01:12<00:25, 15.54it/s] 71%|███████   | 979/1380 [01:12<00:25, 15.58it/s] 71%|███████   | 981/1380 [01:12<00:25, 15.62it/s] 71%|███████   | 983/1380 [01:12<00:25, 15.65it/s] 71%|███████▏  | 985/1380 [01:12<00:25, 15.68it/s] 72%|███████▏  | 987/1380 [01:12<00:25, 15.70it/s] 72%|███████▏  | 989/1380 [01:12<00:24, 15.78it/s] 72%|███████▏  | 991/1380 [01:13<00:24, 15.80it/s] 72%|███████▏  | 993/1380 [01:13<00:24, 15.81it/s] 72%|███████▏  | 995/1380 [01:13<00:24, 15.81it/s] 72%|███████▏  | 997/1380 [01:13<00:24, 15.77it/s] 72%|███████▏  | 999/1380 [01:13<00:24, 15.77it/s] 73%|███████▎  | 1001/1380 [01:13<00:23, 15.81it/s] 73%|███████▎  | 1003/1380 [01:13<00:23, 15.85it/s] 73%|███████▎  | 1005/1380 [01:13<00:23, 15.83it/s] 73%|███████▎  | 1007/1380 [01:14<00:23, 15.82it/s] 73%|███████▎  | 1009/1380 [01:14<00:23, 15.75it/s] 73%|███████▎  | 1011/1380 [01:14<00:23, 15.67it/s] 73%|███████▎  | 1013/1380 [01:14<00:23, 15.67it/s] 74%|███████▎  | 1015/1380 [01:14<00:23, 15.65it/s] 74%|███████▎  | 1017/1380 [01:14<00:23, 15.72it/s] 74%|███████▍  | 1019/1380 [01:14<00:22, 15.83it/s] 74%|███████▍  | 1021/1380 [01:14<00:22, 15.84it/s] 74%|███████▍  | 1023/1380 [01:15<00:22, 15.88it/s] 74%|███████▍  | 1025/1380 [01:15<00:22, 15.94it/s] 74%|███████▍  | 1027/1380 [01:15<00:22, 15.98it/s] 75%|███████▍  | 1029/1380 [01:15<00:21, 15.98it/s] 75%|███████▍  | 1031/1380 [01:15<00:21, 15.98it/s] 75%|███████▍  | 1033/1380 [01:15<00:21, 16.00it/s] 75%|███████▌  | 1035/1380 [01:15<00:21, 16.02it/s] 75%|███████▌  | 1037/1380 [01:15<00:21, 16.00it/s] 75%|███████▌  | 1039/1380 [01:16<00:21, 15.99it/s] 75%|███████▌  | 1041/1380 [01:16<00:21, 15.92it/s] 76%|███████▌  | 1043/1380 [01:16<00:21, 15.88it/s] 76%|███████▌  | 1045/1380 [01:16<00:21, 15.91it/s] 76%|███████▌  | 1047/1380 [01:16<00:20, 15.93it/s] 76%|███████▌  | 1049/1380 [01:16<00:20, 15.97it/s] 76%|███████▌  | 1051/1380 [01:16<00:20, 15.97it/s] 76%|███████▋  | 1053/1380 [01:16<00:20, 15.95it/s] 76%|███████▋  | 1055/1380 [01:17<00:20, 15.98it/s] 77%|███████▋  | 1057/1380 [01:17<00:20, 15.98it/s] 77%|███████▋  | 1059/1380 [01:17<00:20, 15.85it/s] 77%|███████▋  | 1061/1380 [01:17<00:20, 15.80it/s] 77%|███████▋  | 1063/1380 [01:17<00:20, 15.78it/s] 77%|███████▋  | 1065/1380 [01:17<00:20, 15.66it/s] 77%|███████▋  | 1067/1380 [01:17<00:19, 15.71it/s] 77%|███████▋  | 1069/1380 [01:17<00:19, 15.72it/s] 78%|███████▊  | 1071/1380 [01:18<00:19, 15.72it/s] 78%|███████▊  | 1073/1380 [01:18<00:19, 15.73it/s] 78%|███████▊  | 1075/1380 [01:18<00:19, 15.77it/s] 78%|███████▊  | 1077/1380 [01:18<00:19, 15.79it/s] 78%|███████▊  | 1079/1380 [01:18<00:18, 15.86it/s] 78%|███████▊  | 1081/1380 [01:18<00:18, 15.86it/s] 78%|███████▊  | 1083/1380 [01:18<00:18, 15.85it/s] 79%|███████▊  | 1085/1380 [01:19<00:18, 15.80it/s] 79%|███████▉  | 1087/1380 [01:19<00:18, 15.80it/s] 79%|███████▉  | 1089/1380 [01:19<00:18, 15.77it/s] 79%|███████▉  | 1091/1380 [01:19<00:18, 15.84it/s] 79%|███████▉  | 1093/1380 [01:19<00:18, 15.84it/s] 79%|███████▉  | 1095/1380 [01:19<00:17, 15.83it/s] 79%|███████▉  | 1097/1380 [01:19<00:18, 15.72it/s] 80%|███████▉  | 1099/1380 [01:19<00:17, 15.63it/s] 80%|███████▉  | 1101/1380 [01:20<00:17, 15.53it/s] 80%|███████▉  | 1103/1380 [01:20<00:17, 15.45it/s]                                                    80%|████████  | 1104/1380 [01:20<00:17, 15.45it/s][INFO|trainer.py:755] 2023-11-15 23:48:00,961 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 23:48:00,962 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 23:48:00,963 >>   Num examples = 2204
[INFO|trainer.py:3134] 2023-11-15 23:48:00,963 >>   Batch size = 8
{'eval_loss': 0.3899447023868561, 'eval_accuracy': 0.8539019963702359, 'eval_micro_f1': 0.8539019963702359, 'eval_macro_f1': 0.8432550584895392, 'eval_runtime': 3.2172, 'eval_samples_per_second': 685.063, 'eval_steps_per_second': 85.788, 'epoch': 3.0}
{'loss': 0.2847, 'learning_rate': 0.0001, 'epoch': 4.0}

  0%|          | 0/276 [00:00<?, ?it/s][A
  4%|▎         | 10/276 [00:00<00:02, 97.21it/s][A
  7%|▋         | 20/276 [00:00<00:02, 91.98it/s][A
 11%|█         | 30/276 [00:00<00:02, 91.01it/s][A
 14%|█▍        | 40/276 [00:00<00:02, 89.44it/s][A
 18%|█▊        | 49/276 [00:00<00:02, 89.25it/s][A
 21%|██        | 58/276 [00:00<00:02, 88.55it/s][A
 24%|██▍       | 67/276 [00:00<00:02, 88.32it/s][A
 28%|██▊       | 76/276 [00:00<00:02, 88.64it/s][A
 31%|███       | 85/276 [00:00<00:02, 88.96it/s][A
 34%|███▍      | 94/276 [00:01<00:02, 88.58it/s][A
 37%|███▋      | 103/276 [00:01<00:01, 88.43it/s][A
 41%|████      | 112/276 [00:01<00:01, 88.10it/s][A
 44%|████▍     | 121/276 [00:01<00:01, 88.17it/s][A
 47%|████▋     | 130/276 [00:01<00:01, 88.29it/s][A
 50%|█████     | 139/276 [00:01<00:01, 88.35it/s][A
 54%|█████▎    | 148/276 [00:01<00:01, 88.00it/s][A
 57%|█████▋    | 157/276 [00:01<00:01, 88.00it/s][A
 60%|██████    | 166/276 [00:01<00:01, 88.03it/s][A
 63%|██████▎   | 175/276 [00:01<00:01, 88.01it/s][A
 67%|██████▋   | 184/276 [00:02<00:01, 88.39it/s][A
 70%|██████▉   | 193/276 [00:02<00:00, 87.75it/s][A
 73%|███████▎  | 202/276 [00:02<00:00, 87.84it/s][A
 76%|███████▋  | 211/276 [00:02<00:00, 88.17it/s][A
 80%|███████▉  | 220/276 [00:02<00:00, 87.84it/s][A
 83%|████████▎ | 229/276 [00:02<00:00, 87.47it/s][A
 86%|████████▌ | 238/276 [00:02<00:00, 87.31it/s][A
 89%|████████▉ | 247/276 [00:02<00:00, 86.20it/s][A
 93%|█████████▎| 256/276 [00:02<00:00, 85.81it/s][A
 96%|█████████▌| 265/276 [00:03<00:00, 85.70it/s][A
 99%|█████████▉| 274/276 [00:03<00:00, 84.42it/s][A                                                   
                                                 [A 80%|████████  | 1104/1380 [01:23<00:17, 15.45it/s]
100%|██████████| 276/276 [00:03<00:00, 84.42it/s][A
                                                 [A 80%|████████  | 1105/1380 [01:23<02:28,  1.85it/s] 80%|████████  | 1107/1380 [01:23<01:48,  2.51it/s] 80%|████████  | 1109/1380 [01:23<01:20,  3.36it/s] 81%|████████  | 1111/1380 [01:23<01:01,  4.40it/s] 81%|████████  | 1113/1380 [01:23<00:47,  5.61it/s] 81%|████████  | 1115/1380 [01:24<00:38,  6.96it/s] 81%|████████  | 1117/1380 [01:24<00:31,  8.37it/s] 81%|████████  | 1119/1380 [01:24<00:26,  9.75it/s] 81%|████████  | 1121/1380 [01:24<00:23, 10.99it/s] 81%|████████▏ | 1123/1380 [01:24<00:21, 12.08it/s] 82%|████████▏ | 1125/1380 [01:24<00:19, 13.00it/s] 82%|████████▏ | 1127/1380 [01:24<00:18, 13.72it/s] 82%|████████▏ | 1129/1380 [01:24<00:17, 14.32it/s] 82%|████████▏ | 1131/1380 [01:25<00:16, 14.75it/s] 82%|████████▏ | 1133/1380 [01:25<00:16, 15.02it/s] 82%|████████▏ | 1135/1380 [01:25<00:16, 15.19it/s] 82%|████████▏ | 1137/1380 [01:25<00:15, 15.28it/s] 83%|████████▎ | 1139/1380 [01:25<00:15, 15.40it/s] 83%|████████▎ | 1141/1380 [01:25<00:15, 15.51it/s] 83%|████████▎ | 1143/1380 [01:25<00:15, 15.64it/s] 83%|████████▎ | 1145/1380 [01:26<00:14, 15.77it/s] 83%|████████▎ | 1147/1380 [01:26<00:14, 15.81it/s] 83%|████████▎ | 1149/1380 [01:26<00:14, 15.86it/s] 83%|████████▎ | 1151/1380 [01:26<00:14, 15.88it/s] 84%|████████▎ | 1153/1380 [01:26<00:14, 15.94it/s] 84%|████████▎ | 1155/1380 [01:26<00:14, 15.96it/s] 84%|████████▍ | 1157/1380 [01:26<00:13, 15.95it/s] 84%|████████▍ | 1159/1380 [01:26<00:13, 15.95it/s] 84%|████████▍ | 1161/1380 [01:27<00:13, 15.98it/s] 84%|████████▍ | 1163/1380 [01:27<00:13, 15.99it/s] 84%|████████▍ | 1165/1380 [01:27<00:13, 15.97it/s] 85%|████████▍ | 1167/1380 [01:27<00:13, 15.96it/s] 85%|████████▍ | 1169/1380 [01:27<00:13, 16.00it/s] 85%|████████▍ | 1171/1380 [01:27<00:13, 15.95it/s] 85%|████████▌ | 1173/1380 [01:27<00:13, 15.91it/s] 85%|████████▌ | 1175/1380 [01:27<00:12, 15.95it/s] 85%|████████▌ | 1177/1380 [01:28<00:12, 15.97it/s] 85%|████████▌ | 1179/1380 [01:28<00:12, 15.89it/s] 86%|████████▌ | 1181/1380 [01:28<00:12, 15.83it/s] 86%|████████▌ | 1183/1380 [01:28<00:12, 15.80it/s] 86%|████████▌ | 1185/1380 [01:28<00:12, 15.72it/s] 86%|████████▌ | 1187/1380 [01:28<00:12, 15.72it/s] 86%|████████▌ | 1189/1380 [01:28<00:12, 15.67it/s] 86%|████████▋ | 1191/1380 [01:28<00:12, 15.65it/s] 86%|████████▋ | 1193/1380 [01:29<00:11, 15.68it/s] 87%|████████▋ | 1195/1380 [01:29<00:11, 15.70it/s] 87%|████████▋ | 1197/1380 [01:29<00:11, 15.70it/s] 87%|████████▋ | 1199/1380 [01:29<00:11, 15.72it/s] 87%|████████▋ | 1201/1380 [01:29<00:11, 15.72it/s] 87%|████████▋ | 1203/1380 [01:29<00:11, 15.77it/s] 87%|████████▋ | 1205/1380 [01:29<00:11, 15.80it/s] 87%|████████▋ | 1207/1380 [01:29<00:10, 15.73it/s] 88%|████████▊ | 1209/1380 [01:30<00:10, 15.73it/s] 88%|████████▊ | 1211/1380 [01:30<00:10, 15.71it/s] 88%|████████▊ | 1213/1380 [01:30<00:10, 15.69it/s] 88%|████████▊ | 1215/1380 [01:30<00:10, 15.75it/s] 88%|████████▊ | 1217/1380 [01:30<00:10, 15.78it/s] 88%|████████▊ | 1219/1380 [01:30<00:10, 15.77it/s] 88%|████████▊ | 1221/1380 [01:30<00:10, 15.72it/s] 89%|████████▊ | 1223/1380 [01:30<00:10, 15.62it/s] 89%|████████▉ | 1225/1380 [01:31<00:09, 15.62it/s] 89%|████████▉ | 1227/1380 [01:31<00:09, 15.58it/s] 89%|████████▉ | 1229/1380 [01:31<00:09, 15.67it/s] 89%|████████▉ | 1231/1380 [01:31<00:09, 15.72it/s] 89%|████████▉ | 1233/1380 [01:31<00:09, 15.77it/s] 89%|████████▉ | 1235/1380 [01:31<00:09, 15.81it/s] 90%|████████▉ | 1237/1380 [01:31<00:09, 15.88it/s] 90%|████████▉ | 1239/1380 [01:31<00:08, 15.90it/s] 90%|████████▉ | 1241/1380 [01:32<00:08, 15.90it/s] 90%|█████████ | 1243/1380 [01:32<00:08, 15.91it/s] 90%|█████████ | 1245/1380 [01:32<00:08, 15.91it/s] 90%|█████████ | 1247/1380 [01:32<00:08, 15.91it/s] 91%|█████████ | 1249/1380 [01:32<00:08, 15.93it/s] 91%|█████████ | 1251/1380 [01:32<00:08, 15.95it/s] 91%|█████████ | 1253/1380 [01:32<00:07, 15.96it/s] 91%|█████████ | 1255/1380 [01:32<00:07, 15.95it/s] 91%|█████████ | 1257/1380 [01:33<00:07, 15.94it/s] 91%|█████████ | 1259/1380 [01:33<00:07, 15.97it/s] 91%|█████████▏| 1261/1380 [01:33<00:07, 15.94it/s] 92%|█████████▏| 1263/1380 [01:33<00:07, 15.94it/s] 92%|█████████▏| 1265/1380 [01:33<00:07, 15.94it/s] 92%|█████████▏| 1267/1380 [01:33<00:07, 15.89it/s] 92%|█████████▏| 1269/1380 [01:33<00:07, 15.74it/s] 92%|█████████▏| 1271/1380 [01:33<00:06, 15.69it/s] 92%|█████████▏| 1273/1380 [01:34<00:06, 15.64it/s] 92%|█████████▏| 1275/1380 [01:34<00:06, 15.62it/s] 93%|█████████▎| 1277/1380 [01:34<00:06, 15.64it/s] 93%|█████████▎| 1279/1380 [01:34<00:06, 15.60it/s] 93%|█████████▎| 1281/1380 [01:34<00:06, 15.68it/s] 93%|█████████▎| 1283/1380 [01:34<00:06, 15.69it/s] 93%|█████████▎| 1285/1380 [01:34<00:06, 15.62it/s] 93%|█████████▎| 1287/1380 [01:34<00:05, 15.70it/s] 93%|█████████▎| 1289/1380 [01:35<00:05, 15.73it/s] 94%|█████████▎| 1291/1380 [01:35<00:05, 15.77it/s] 94%|█████████▎| 1293/1380 [01:35<00:05, 15.81it/s] 94%|█████████▍| 1295/1380 [01:35<00:05, 15.81it/s] 94%|█████████▍| 1297/1380 [01:35<00:05, 15.75it/s] 94%|█████████▍| 1299/1380 [01:35<00:05, 15.77it/s] 94%|█████████▍| 1301/1380 [01:35<00:05, 15.70it/s] 94%|█████████▍| 1303/1380 [01:36<00:04, 15.75it/s] 95%|█████████▍| 1305/1380 [01:36<00:04, 15.77it/s] 95%|█████████▍| 1307/1380 [01:36<00:04, 15.67it/s] 95%|█████████▍| 1309/1380 [01:36<00:04, 15.48it/s] 95%|█████████▌| 1311/1380 [01:36<00:04, 15.43it/s] 95%|█████████▌| 1313/1380 [01:36<00:04, 15.32it/s] 95%|█████████▌| 1315/1380 [01:36<00:04, 15.43it/s] 95%|█████████▌| 1317/1380 [01:36<00:04, 15.57it/s] 96%|█████████▌| 1319/1380 [01:37<00:03, 15.66it/s] 96%|█████████▌| 1321/1380 [01:37<00:03, 15.75it/s] 96%|█████████▌| 1323/1380 [01:37<00:03, 15.84it/s] 96%|█████████▌| 1325/1380 [01:37<00:03, 15.86it/s] 96%|█████████▌| 1327/1380 [01:37<00:03, 15.84it/s] 96%|█████████▋| 1329/1380 [01:37<00:03, 15.84it/s] 96%|█████████▋| 1331/1380 [01:37<00:03, 15.82it/s] 97%|█████████▋| 1333/1380 [01:37<00:02, 15.81it/s] 97%|█████████▋| 1335/1380 [01:38<00:02, 15.87it/s] 97%|█████████▋| 1337/1380 [01:38<00:02, 15.90it/s] 97%|█████████▋| 1339/1380 [01:38<00:02, 15.91it/s] 97%|█████████▋| 1341/1380 [01:38<00:02, 15.91it/s] 97%|█████████▋| 1343/1380 [01:38<00:02, 15.95it/s] 97%|█████████▋| 1345/1380 [01:38<00:02, 15.94it/s] 98%|█████████▊| 1347/1380 [01:38<00:02, 15.92it/s] 98%|█████████▊| 1349/1380 [01:38<00:01, 15.92it/s] 98%|█████████▊| 1351/1380 [01:39<00:01, 15.95it/s] 98%|█████████▊| 1353/1380 [01:39<00:01, 15.87it/s] 98%|█████████▊| 1355/1380 [01:39<00:01, 15.79it/s] 98%|█████████▊| 1357/1380 [01:39<00:01, 15.68it/s] 98%|█████████▊| 1359/1380 [01:39<00:01, 15.66it/s] 99%|█████████▊| 1361/1380 [01:39<00:01, 15.63it/s] 99%|█████████▉| 1363/1380 [01:39<00:01, 15.66it/s] 99%|█████████▉| 1365/1380 [01:39<00:00, 15.66it/s] 99%|█████████▉| 1367/1380 [01:40<00:00, 15.73it/s] 99%|█████████▉| 1369/1380 [01:40<00:00, 15.76it/s] 99%|█████████▉| 1371/1380 [01:40<00:00, 15.75it/s] 99%|█████████▉| 1373/1380 [01:40<00:00, 15.73it/s]100%|█████████▉| 1375/1380 [01:40<00:00, 15.75it/s]100%|█████████▉| 1377/1380 [01:40<00:00, 15.79it/s]100%|█████████▉| 1379/1380 [01:40<00:00, 15.78it/s]                                                   100%|██████████| 1380/1380 [01:40<00:00, 15.78it/s][INFO|trainer.py:755] 2023-11-15 23:48:21,627 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 23:48:21,629 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 23:48:21,629 >>   Num examples = 2204
[INFO|trainer.py:3134] 2023-11-15 23:48:21,629 >>   Batch size = 8
{'eval_loss': 0.39881056547164917, 'eval_accuracy': 0.852087114337568, 'eval_micro_f1': 0.852087114337568, 'eval_macro_f1': 0.8418641556507511, 'eval_runtime': 3.2015, 'eval_samples_per_second': 688.424, 'eval_steps_per_second': 86.209, 'epoch': 4.0}
{'loss': 0.2513, 'learning_rate': 0.0, 'epoch': 5.0}

  0%|          | 0/276 [00:00<?, ?it/s][A
  4%|▎         | 10/276 [00:00<00:02, 96.91it/s][A
  7%|▋         | 20/276 [00:00<00:02, 90.83it/s][A
 11%|█         | 30/276 [00:00<00:02, 88.74it/s][A
 14%|█▍        | 39/276 [00:00<00:02, 87.36it/s][A
 17%|█▋        | 48/276 [00:00<00:02, 86.12it/s][A
 21%|██        | 57/276 [00:00<00:02, 85.84it/s][A
 24%|██▍       | 66/276 [00:00<00:02, 85.45it/s][A
 27%|██▋       | 75/276 [00:00<00:02, 82.16it/s][A
 30%|███       | 84/276 [00:01<00:02, 75.64it/s][A
 33%|███▎      | 92/276 [00:01<00:02, 75.74it/s][A
 36%|███▌      | 100/276 [00:01<00:02, 74.96it/s][A
 39%|███▉      | 108/276 [00:01<00:02, 74.95it/s][A
 42%|████▏     | 117/276 [00:01<00:02, 77.48it/s][A
 46%|████▌     | 126/276 [00:01<00:01, 80.53it/s][A
 49%|████▉     | 135/276 [00:01<00:01, 80.13it/s][A
 52%|█████▏    | 144/276 [00:01<00:01, 73.92it/s][A
 55%|█████▌    | 153/276 [00:01<00:01, 77.28it/s][A
 59%|█████▊    | 162/276 [00:02<00:01, 79.96it/s][A
 62%|██████▏   | 171/276 [00:02<00:01, 81.95it/s][A
 65%|██████▌   | 180/276 [00:02<00:01, 83.79it/s][A
 68%|██████▊   | 189/276 [00:02<00:01, 84.82it/s][A
 72%|███████▏  | 198/276 [00:02<00:00, 85.08it/s][A
 75%|███████▌  | 207/276 [00:02<00:00, 85.78it/s][A
 78%|███████▊  | 216/276 [00:02<00:00, 85.69it/s][A
 82%|████████▏ | 225/276 [00:02<00:00, 86.55it/s][A
 85%|████████▍ | 234/276 [00:02<00:00, 86.87it/s][A
 88%|████████▊ | 243/276 [00:02<00:00, 86.77it/s][A
 91%|█████████▏| 252/276 [00:03<00:00, 86.85it/s][A
 95%|█████████▍| 261/276 [00:03<00:00, 86.87it/s][A
 98%|█████████▊| 270/276 [00:03<00:00, 87.35it/s][A                                                   
                                                 [A100%|██████████| 1380/1380 [01:44<00:00, 15.78it/s]
100%|██████████| 276/276 [00:03<00:00, 87.35it/s][A
                                                 [A[INFO|trainer.py:1963] 2023-11-15 23:48:25,006 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                   100%|██████████| 1380/1380 [01:44<00:00, 15.78it/s]100%|██████████| 1380/1380 [01:44<00:00, 13.24it/s]
[INFO|trainer.py:2855] 2023-11-15 23:48:25,009 >> Saving model checkpoint to ./result/acl_roberta-base_seed4_adapter
[INFO|configuration_utils.py:460] 2023-11-15 23:48:25,011 >> Configuration saved in ./result/acl_roberta-base_seed4_adapter/config.json
[INFO|modeling_utils.py:1997] 2023-11-15 23:48:26,093 >> Model weights saved in ./result/acl_roberta-base_seed4_adapter/pytorch_model.bin
[INFO|tokenization_utils_base.py:2235] 2023-11-15 23:48:26,096 >> tokenizer config file saved in ./result/acl_roberta-base_seed4_adapter/tokenizer_config.json
[INFO|tokenization_utils_base.py:2242] 2023-11-15 23:48:26,098 >> Special tokens file saved in ./result/acl_roberta-base_seed4_adapter/special_tokens_map.json
{'eval_loss': 0.431620329618454, 'eval_accuracy': 0.8507259528130672, 'eval_micro_f1': 0.8507259528130672, 'eval_macro_f1': 0.8426018997206727, 'eval_runtime': 3.3741, 'eval_samples_per_second': 653.213, 'eval_steps_per_second': 81.8, 'epoch': 5.0}
{'train_runtime': 104.2486, 'train_samples_per_second': 422.835, 'train_steps_per_second': 13.238, 'train_loss': 0.33946035081061765, 'epoch': 5.0}
***** train metrics *****
  epoch                    =        5.0
  train_loss               =     0.3395
  train_runtime            = 0:01:44.24
  train_samples            =       8816
  train_samples_per_second =    422.835
  train_steps_per_second   =     13.238
11/15/2023 23:48:26 - INFO - __main__ - *** Evaluate ***
[INFO|trainer.py:755] 2023-11-15 23:48:26,228 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 23:48:26,229 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 23:48:26,229 >>   Num examples = 2204
[INFO|trainer.py:3134] 2023-11-15 23:48:26,230 >>   Batch size = 8
  0%|          | 0/276 [00:00<?, ?it/s]  4%|▎         | 10/276 [00:00<00:02, 96.48it/s]  7%|▋         | 20/276 [00:00<00:02, 90.38it/s] 11%|█         | 30/276 [00:00<00:02, 87.51it/s] 14%|█▍        | 39/276 [00:00<00:02, 85.20it/s] 17%|█▋        | 48/276 [00:00<00:02, 85.84it/s] 21%|██        | 57/276 [00:00<00:02, 85.57it/s] 24%|██▍       | 66/276 [00:00<00:02, 85.85it/s] 27%|██▋       | 75/276 [00:00<00:02, 85.55it/s] 30%|███       | 84/276 [00:00<00:02, 86.04it/s] 34%|███▎      | 93/276 [00:01<00:02, 85.46it/s] 37%|███▋      | 102/276 [00:01<00:02, 85.16it/s] 40%|████      | 111/276 [00:01<00:01, 85.63it/s] 43%|████▎     | 120/276 [00:01<00:01, 85.83it/s] 47%|████▋     | 129/276 [00:01<00:01, 85.79it/s] 50%|█████     | 138/276 [00:01<00:01, 86.09it/s] 53%|█████▎    | 147/276 [00:01<00:01, 85.91it/s] 57%|█████▋    | 156/276 [00:01<00:01, 85.26it/s] 60%|█████▉    | 165/276 [00:01<00:01, 83.42it/s] 63%|██████▎   | 174/276 [00:02<00:01, 80.58it/s] 66%|██████▋   | 183/276 [00:02<00:01, 80.35it/s] 70%|██████▉   | 192/276 [00:02<00:01, 78.70it/s] 72%|███████▏  | 200/276 [00:02<00:00, 78.32it/s] 76%|███████▌  | 209/276 [00:02<00:00, 80.65it/s] 79%|███████▉  | 218/276 [00:02<00:00, 82.85it/s] 82%|████████▏ | 227/276 [00:02<00:00, 84.12it/s] 86%|████████▌ | 236/276 [00:02<00:00, 85.01it/s] 89%|████████▉ | 245/276 [00:02<00:00, 85.98it/s] 92%|█████████▏| 254/276 [00:03<00:00, 86.62it/s] 95%|█████████▌| 263/276 [00:03<00:00, 87.28it/s] 99%|█████████▊| 272/276 [00:03<00:00, 87.07it/s]100%|██████████| 276/276 [00:03<00:00, 83.87it/s]
***** eval metrics *****
  epoch                   =        5.0
  eval_accuracy           =     0.8507
  eval_loss               =     0.4316
  eval_macro_f1           =     0.8426
  eval_micro_f1           =     0.8507
  eval_runtime            = 0:00:03.30
  eval_samples            =       2204
  eval_samples_per_second =    666.548
  eval_steps_per_second   =      83.47
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                  eval/accuracy ▁▇█▇▇▇
wandb:                      eval/loss █▁▂▃██
wandb:                  eval/macro_f1 ▁██▇▇▇
wandb:                  eval/micro_f1 ▁▇█▇▇▇
wandb:                   eval/runtime ▆▄▂▁█▅
wandb:        eval/samples_per_second ▃▄▇█▁▄
wandb:          eval/steps_per_second ▃▄▇█▁▄
wandb:                    train/epoch ▁▁▃▃▅▅▆▆████
wandb:              train/global_step ▁▁▃▃▅▅▆▆████
wandb:            train/learning_rate █▆▅▃▁
wandb:                     train/loss █▄▃▂▁
wandb:               train/total_flos ▁
wandb:               train/train_loss ▁
wandb:            train/train_runtime ▁
wandb: train/train_samples_per_second ▁
wandb:   train/train_steps_per_second ▁
wandb: 
wandb: Run summary:
wandb:                  eval/accuracy 0.85073
wandb:                      eval/loss 0.43162
wandb:                  eval/macro_f1 0.8426
wandb:                  eval/micro_f1 0.85073
wandb:                   eval/runtime 3.3066
wandb:        eval/samples_per_second 666.548
wandb:          eval/steps_per_second 83.47
wandb:                    train/epoch 5.0
wandb:              train/global_step 1380
wandb:            train/learning_rate 0.0
wandb:                     train/loss 0.2513
wandb:               train/total_flos 1464896356669440.0
wandb:               train/train_loss 0.33946
wandb:            train/train_runtime 104.2486
wandb: train/train_samples_per_second 422.835
wandb:   train/train_steps_per_second 13.238
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /ceph/home/wangyifei/a3/wandb/offline-run-20231115_234522-4d5zeck8
wandb: Find logs at: ./wandb/offline-run-20231115_234522-4d5zeck8/logs
(ModelArguments(model_name_or_path='roberta-base', cache_dir='./cache', use_adapter=True, use_lora=False), DataTrainingArguments(dataset_name='agnews_sup', max_seq_length=64), TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.0005,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=./result/agnews_sup_roberta-base_seed4_adapter/runs/Nov15_23-48-40_node013,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.EPOCH,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=5.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
output_dir=./result/agnews_sup_roberta-base_seed4_adapter,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
resume_from_checkpoint=None,
run_name=./result/agnews_sup_roberta-base_seed4_adapter,
save_on_each_node=False,
save_safetensors=False,
save_steps=500,
save_strategy=IntervalStrategy.NO,
save_total_limit=None,
seed=555,
sharded_ddp=[],
skip_memory_metrics=True,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
))
11/15/2023 23:48:40 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False
11/15/2023 23:48:40 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.0005,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=./result/agnews_sup_roberta-base_seed4_adapter/runs/Nov15_23-48-39_node013,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.EPOCH,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=5.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
output_dir=./result/agnews_sup_roberta-base_seed4_adapter,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
resume_from_checkpoint=None,
run_name=./result/agnews_sup_roberta-base_seed4_adapter,
save_on_each_node=False,
save_safetensors=False,
save_steps=500,
save_strategy=IntervalStrategy.NO,
save_total_limit=None,
seed=555,
sharded_ddp=[],
skip_memory_metrics=True,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
wandb: Tracking run with wandb version 0.15.12
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[INFO|configuration_utils.py:715] 2023-11-15 23:48:55,778 >> loading configuration file config.json from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/config.json
[INFO|configuration_utils.py:775] 2023-11-15 23:48:55,787 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.33.3",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_auto.py:535] 2023-11-15 23:49:05,803 >> Could not locate the tokenizer configuration file, will try to use the model config instead.
[INFO|configuration_utils.py:715] 2023-11-15 23:49:15,822 >> loading configuration file config.json from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/config.json
[INFO|configuration_utils.py:775] 2023-11-15 23:49:15,823 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.33.3",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:1852] 2023-11-15 23:49:35,870 >> loading file vocab.json from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/vocab.json
[INFO|tokenization_utils_base.py:1852] 2023-11-15 23:49:35,871 >> loading file merges.txt from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/merges.txt
[INFO|tokenization_utils_base.py:1852] 2023-11-15 23:49:35,871 >> loading file tokenizer.json from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/tokenizer.json
[INFO|tokenization_utils_base.py:1852] 2023-11-15 23:49:35,871 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1852] 2023-11-15 23:49:35,871 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1852] 2023-11-15 23:49:35,872 >> loading file tokenizer_config.json from cache at None
[INFO|configuration_utils.py:715] 2023-11-15 23:49:35,873 >> loading configuration file config.json from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/config.json
[INFO|configuration_utils.py:775] 2023-11-15 23:49:35,873 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.33.3",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:2862] 2023-11-15 23:49:56,040 >> loading weights file model.safetensors from cache at ./cache/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/model.safetensors
[INFO|modeling_utils.py:3638] 2023-11-15 23:49:56,789 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3650] 2023-11-15 23:49:56,790 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
parameters 1488196
model_max_length: 512
Running tokenizer on dataset:   0%|          | 0/6840 [00:00<?, ? examples/s]Running tokenizer on dataset:  29%|██▉       | 2000/6840 [00:00<00:00, 18835.31 examples/s]Running tokenizer on dataset:  88%|████████▊ | 6000/6840 [00:00<00:00, 20021.46 examples/s]Running tokenizer on dataset: 100%|██████████| 6840/6840 [00:00<00:00, 19927.36 examples/s]
Running tokenizer on dataset:   0%|          | 0/760 [00:00<?, ? examples/s]Running tokenizer on dataset: 100%|██████████| 760/760 [00:00<00:00, 21390.46 examples/s]
11/15/2023 23:49:58 - INFO - __main__ - Sample 1583 of the training set: {'text': 'SA-India Test: South Africa declare at 510 for 9 : Sports India: Cricket  gt; Kanpur, Nov 22 : South Africa declared their first innings at 510 for nine on the third day of the first cricket Test against India here today.', 'label': 0, 'input_ids': [0, 3603, 12, 11015, 4500, 35, 391, 1327, 10152, 23, 30703, 13, 361, 4832, 1847, 666, 35, 10424, 1437, 821, 90, 131, 7542, 7748, 6, 1442, 820, 4832, 391, 1327, 2998, 49, 78, 2699, 23, 30703, 13, 1117, 15, 5, 371, 183, 9, 5, 78, 5630, 4500, 136, 666, 259, 452, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
11/15/2023 23:49:58 - INFO - __main__ - Sample 2249 of the training set: {'text': 'FTSE 100 lifted by bright Dow The FTSE 100 has climbed as a surge by US shares gives a boost to European markets. Shire Pharmaceuticals SHP.L jumped after winning approval for a key drug and consumer goods giant Unilever ULVR.', 'label': 1, 'input_ids': [0, 597, 2685, 717, 727, 4639, 30, 4520, 4614, 20, 274, 2685, 717, 727, 34, 7334, 25, 10, 6564, 30, 382, 327, 2029, 10, 2501, 7, 796, 1048, 4, 840, 1885, 7937, 29, 4584, 510, 4, 574, 4262, 71, 1298, 2846, 13, 10, 762, 1262, 8, 2267, 3057, 3065, 1890, 1848, 2802, 33987, 13055, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]}.
11/15/2023 23:49:58 - INFO - __main__ - Sample 1319 of the training set: {'text': 'Language of goals what counts for tongue-tied Ronnie and Michael England striker Michael Owen said his lack of Spanish and Ronaldo #39;s lack of English did not hinder celebrations of the Brazilian #39;s matchwinner for Real Madrid in Sunday #39;s 1-0 win at Mallorca.', 'label': 0, 'input_ids': [0, 46969, 9, 1175, 99, 3948, 13, 15686, 12, 90, 2550, 21127, 8, 988, 1156, 5955, 988, 12212, 26, 39, 1762, 9, 3453, 8, 7991, 849, 3416, 131, 29, 1762, 9, 2370, 222, 45, 26679, 9570, 9, 5, 6606, 849, 3416, 131, 29, 914, 20547, 13, 2822, 3622, 11, 395, 849, 3416, 131, 29, 112, 12, 288, 339, 23, 6633, 368, 3245, 4, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.
11/15/2023 23:49:58 - WARNING - accelerate.utils.other - Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[INFO|trainer.py:755] 2023-11-15 23:49:59,270 >> The following columns in the training set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:1715] 2023-11-15 23:49:59,280 >> ***** Running training *****
[INFO|trainer.py:1716] 2023-11-15 23:49:59,280 >>   Num examples = 6,840
[INFO|trainer.py:1717] 2023-11-15 23:49:59,281 >>   Num Epochs = 5
[INFO|trainer.py:1718] 2023-11-15 23:49:59,281 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1721] 2023-11-15 23:49:59,281 >>   Total train batch size (w. parallel, distributed & accumulation) = 32
[INFO|trainer.py:1722] 2023-11-15 23:49:59,281 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:1723] 2023-11-15 23:49:59,282 >>   Total optimization steps = 1,070
[INFO|trainer.py:1724] 2023-11-15 23:49:59,282 >>   Number of trainable parameters = 1,488,196
[INFO|integration_utils.py:716] 2023-11-15 23:49:59,284 >> Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"
  0%|          | 0/1070 [00:00<?, ?it/s]  0%|          | 1/1070 [00:01<18:03,  1.01s/it]  0%|          | 3/1070 [00:01<05:30,  3.23it/s]  0%|          | 5/1070 [00:01<03:14,  5.47it/s]  1%|          | 7/1070 [00:01<02:20,  7.54it/s]  1%|          | 9/1070 [00:01<01:53,  9.39it/s]  1%|          | 11/1070 [00:01<01:36, 10.98it/s]  1%|          | 13/1070 [00:01<01:26, 12.26it/s]  1%|▏         | 15/1070 [00:01<01:19, 13.27it/s]  2%|▏         | 17/1070 [00:02<01:15, 14.01it/s]  2%|▏         | 19/1070 [00:02<01:12, 14.57it/s]  2%|▏         | 21/1070 [00:02<01:09, 15.02it/s]  2%|▏         | 23/1070 [00:02<01:08, 15.25it/s]  2%|▏         | 25/1070 [00:02<01:07, 15.43it/s]  3%|▎         | 27/1070 [00:02<01:06, 15.64it/s]  3%|▎         | 29/1070 [00:02<01:06, 15.74it/s]  3%|▎         | 31/1070 [00:02<01:05, 15.81it/s]  3%|▎         | 33/1070 [00:03<01:05, 15.85it/s]  3%|▎         | 35/1070 [00:03<01:04, 15.95it/s]  3%|▎         | 37/1070 [00:03<01:04, 15.96it/s]  4%|▎         | 39/1070 [00:03<01:05, 15.76it/s]  4%|▍         | 41/1070 [00:03<01:05, 15.77it/s]  4%|▍         | 43/1070 [00:03<01:05, 15.77it/s]  4%|▍         | 45/1070 [00:03<01:05, 15.69it/s]  4%|▍         | 47/1070 [00:03<01:04, 15.77it/s]  5%|▍         | 49/1070 [00:04<01:04, 15.85it/s]  5%|▍         | 51/1070 [00:04<01:03, 15.93it/s]  5%|▍         | 53/1070 [00:04<01:03, 15.97it/s]  5%|▌         | 55/1070 [00:04<01:03, 16.04it/s]  5%|▌         | 57/1070 [00:04<01:02, 16.08it/s]  6%|▌         | 59/1070 [00:04<01:02, 16.07it/s]  6%|▌         | 61/1070 [00:04<01:02, 16.07it/s]  6%|▌         | 63/1070 [00:04<01:02, 16.06it/s]  6%|▌         | 65/1070 [00:05<01:02, 16.11it/s]  6%|▋         | 67/1070 [00:05<01:02, 16.14it/s]  6%|▋         | 69/1070 [00:05<01:02, 16.12it/s]  7%|▋         | 71/1070 [00:05<01:02, 16.11it/s]  7%|▋         | 73/1070 [00:05<01:02, 16.07it/s]  7%|▋         | 75/1070 [00:05<01:01, 16.11it/s]  7%|▋         | 77/1070 [00:05<01:01, 16.12it/s]  7%|▋         | 79/1070 [00:05<01:01, 16.10it/s]  8%|▊         | 81/1070 [00:06<01:01, 16.11it/s]  8%|▊         | 83/1070 [00:06<01:01, 16.15it/s]  8%|▊         | 85/1070 [00:06<01:00, 16.17it/s]  8%|▊         | 87/1070 [00:06<01:01, 16.01it/s]  8%|▊         | 89/1070 [00:06<01:01, 15.98it/s]  9%|▊         | 91/1070 [00:06<01:01, 15.98it/s]  9%|▊         | 93/1070 [00:06<01:01, 15.96it/s]  9%|▉         | 95/1070 [00:06<01:01, 15.92it/s]  9%|▉         | 97/1070 [00:07<01:00, 15.95it/s]  9%|▉         | 99/1070 [00:07<01:00, 15.97it/s]  9%|▉         | 101/1070 [00:07<01:00, 15.97it/s] 10%|▉         | 103/1070 [00:07<01:00, 15.96it/s] 10%|▉         | 105/1070 [00:07<01:00, 15.94it/s] 10%|█         | 107/1070 [00:07<01:00, 15.97it/s] 10%|█         | 109/1070 [00:07<01:00, 16.01it/s] 10%|█         | 111/1070 [00:07<00:59, 16.03it/s] 11%|█         | 113/1070 [00:08<00:59, 16.03it/s] 11%|█         | 115/1070 [00:08<00:59, 16.00it/s] 11%|█         | 117/1070 [00:08<00:59, 16.03it/s] 11%|█         | 119/1070 [00:08<00:59, 16.03it/s] 11%|█▏        | 121/1070 [00:08<00:59, 16.04it/s] 11%|█▏        | 123/1070 [00:08<00:58, 16.10it/s] 12%|█▏        | 125/1070 [00:08<00:58, 16.08it/s] 12%|█▏        | 127/1070 [00:08<00:59, 15.96it/s] 12%|█▏        | 129/1070 [00:09<00:59, 15.91it/s] 12%|█▏        | 131/1070 [00:09<00:59, 15.89it/s] 12%|█▏        | 133/1070 [00:09<00:59, 15.87it/s] 13%|█▎        | 135/1070 [00:09<00:58, 15.95it/s] 13%|█▎        | 137/1070 [00:09<00:58, 16.06it/s] 13%|█▎        | 139/1070 [00:09<00:57, 16.13it/s] 13%|█▎        | 141/1070 [00:09<00:57, 16.17it/s] 13%|█▎        | 143/1070 [00:09<00:57, 16.19it/s] 14%|█▎        | 145/1070 [00:10<00:57, 16.19it/s] 14%|█▎        | 147/1070 [00:10<00:57, 16.19it/s] 14%|█▍        | 149/1070 [00:10<00:56, 16.20it/s] 14%|█▍        | 151/1070 [00:10<00:56, 16.23it/s] 14%|█▍        | 153/1070 [00:10<00:56, 16.11it/s] 14%|█▍        | 155/1070 [00:10<00:56, 16.10it/s] 15%|█▍        | 157/1070 [00:10<00:56, 16.12it/s] 15%|█▍        | 159/1070 [00:10<00:56, 16.18it/s] 15%|█▌        | 161/1070 [00:11<00:56, 16.23it/s] 15%|█▌        | 163/1070 [00:11<00:55, 16.22it/s] 15%|█▌        | 165/1070 [00:11<00:55, 16.23it/s] 16%|█▌        | 167/1070 [00:11<00:55, 16.19it/s] 16%|█▌        | 169/1070 [00:11<00:55, 16.18it/s] 16%|█▌        | 171/1070 [00:11<00:55, 16.21it/s] 16%|█▌        | 173/1070 [00:11<00:55, 16.22it/s] 16%|█▋        | 175/1070 [00:11<00:55, 16.18it/s] 17%|█▋        | 177/1070 [00:11<00:55, 16.07it/s] 17%|█▋        | 179/1070 [00:12<00:55, 15.98it/s] 17%|█▋        | 181/1070 [00:12<00:55, 15.94it/s] 17%|█▋        | 183/1070 [00:12<00:55, 15.88it/s] 17%|█▋        | 185/1070 [00:12<00:55, 15.89it/s] 17%|█▋        | 187/1070 [00:12<00:55, 15.91it/s] 18%|█▊        | 189/1070 [00:12<00:55, 15.89it/s] 18%|█▊        | 191/1070 [00:12<00:55, 15.90it/s] 18%|█▊        | 193/1070 [00:13<00:54, 15.95it/s] 18%|█▊        | 195/1070 [00:13<00:54, 15.97it/s] 18%|█▊        | 197/1070 [00:13<00:54, 15.99it/s] 19%|█▊        | 199/1070 [00:13<00:54, 16.03it/s] 19%|█▉        | 201/1070 [00:13<00:54, 16.04it/s] 19%|█▉        | 203/1070 [00:13<00:54, 16.00it/s] 19%|█▉        | 205/1070 [00:13<00:54, 15.96it/s] 19%|█▉        | 207/1070 [00:13<00:53, 15.99it/s] 20%|█▉        | 209/1070 [00:14<00:53, 16.00it/s] 20%|█▉        | 211/1070 [00:14<00:53, 16.03it/s] 20%|█▉        | 213/1070 [00:14<00:53, 16.02it/s]                                                   20%|██        | 214/1070 [00:14<00:53, 16.02it/s][INFO|trainer.py:755] 2023-11-15 23:50:13,587 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 23:50:13,589 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 23:50:13,589 >>   Num examples = 760
[INFO|trainer.py:3134] 2023-11-15 23:50:13,590 >>   Batch size = 8
{'loss': 0.4253, 'learning_rate': 0.0004, 'epoch': 1.0}

  0%|          | 0/95 [00:00<?, ?it/s][A
  9%|▉         | 9/95 [00:00<00:00, 86.66it/s][A
 19%|█▉        | 18/95 [00:00<00:00, 80.24it/s][A
 28%|██▊       | 27/95 [00:00<00:00, 77.61it/s][A
 38%|███▊      | 36/95 [00:00<00:00, 79.10it/s][A
 47%|████▋     | 45/95 [00:00<00:00, 81.61it/s][A
 57%|█████▋    | 54/95 [00:00<00:00, 83.02it/s][A
 66%|██████▋   | 63/95 [00:00<00:00, 84.67it/s][A
 76%|███████▌  | 72/95 [00:00<00:00, 84.94it/s][A
 85%|████████▌ | 81/95 [00:00<00:00, 86.23it/s][A
 95%|█████████▍| 90/95 [00:01<00:00, 87.02it/s][A                                                  
                                               [A 20%|██        | 214/1070 [00:15<00:53, 16.02it/s]
100%|██████████| 95/95 [00:01<00:00, 87.02it/s][A
                                               [A 20%|██        | 215/1070 [00:15<03:23,  4.20it/s] 20%|██        | 217/1070 [00:15<02:38,  5.39it/s] 20%|██        | 219/1070 [00:15<02:06,  6.74it/s] 21%|██        | 221/1070 [00:15<01:43,  8.17it/s] 21%|██        | 223/1070 [00:16<01:28,  9.60it/s] 21%|██        | 225/1070 [00:16<01:17, 10.94it/s] 21%|██        | 227/1070 [00:16<01:09, 12.07it/s] 21%|██▏       | 229/1070 [00:16<01:04, 13.02it/s] 22%|██▏       | 231/1070 [00:16<01:00, 13.83it/s] 22%|██▏       | 233/1070 [00:16<00:57, 14.47it/s] 22%|██▏       | 235/1070 [00:16<00:55, 14.96it/s] 22%|██▏       | 237/1070 [00:16<00:54, 15.30it/s] 22%|██▏       | 239/1070 [00:17<00:53, 15.54it/s] 23%|██▎       | 241/1070 [00:17<00:52, 15.69it/s] 23%|██▎       | 243/1070 [00:17<00:52, 15.84it/s] 23%|██▎       | 245/1070 [00:17<00:52, 15.86it/s] 23%|██▎       | 247/1070 [00:17<00:52, 15.83it/s] 23%|██▎       | 249/1070 [00:17<00:51, 15.85it/s] 23%|██▎       | 251/1070 [00:17<00:51, 15.81it/s] 24%|██▎       | 253/1070 [00:17<00:51, 15.72it/s] 24%|██▍       | 255/1070 [00:18<00:51, 15.80it/s] 24%|██▍       | 257/1070 [00:18<00:51, 15.85it/s] 24%|██▍       | 259/1070 [00:18<00:51, 15.78it/s] 24%|██▍       | 261/1070 [00:18<00:51, 15.85it/s] 25%|██▍       | 263/1070 [00:18<00:50, 15.90it/s] 25%|██▍       | 265/1070 [00:18<00:50, 15.90it/s] 25%|██▍       | 267/1070 [00:18<00:50, 15.93it/s] 25%|██▌       | 269/1070 [00:18<00:50, 15.93it/s] 25%|██▌       | 271/1070 [00:19<00:50, 15.89it/s] 26%|██▌       | 273/1070 [00:19<00:50, 15.87it/s] 26%|██▌       | 275/1070 [00:19<00:49, 15.93it/s] 26%|██▌       | 277/1070 [00:19<00:49, 15.94it/s] 26%|██▌       | 279/1070 [00:19<00:49, 15.96it/s] 26%|██▋       | 281/1070 [00:19<00:49, 15.96it/s] 26%|██▋       | 283/1070 [00:19<00:49, 15.96it/s] 27%|██▋       | 285/1070 [00:19<00:49, 15.91it/s] 27%|██▋       | 287/1070 [00:20<00:49, 15.85it/s] 27%|██▋       | 289/1070 [00:20<00:49, 15.84it/s] 27%|██▋       | 291/1070 [00:20<00:49, 15.83it/s] 27%|██▋       | 293/1070 [00:20<00:48, 15.89it/s] 28%|██▊       | 295/1070 [00:20<00:48, 15.92it/s] 28%|██▊       | 297/1070 [00:20<00:48, 15.89it/s] 28%|██▊       | 299/1070 [00:20<00:48, 15.89it/s] 28%|██▊       | 301/1070 [00:20<00:48, 15.92it/s] 28%|██▊       | 303/1070 [00:21<00:47, 15.99it/s] 29%|██▊       | 305/1070 [00:21<00:47, 16.07it/s] 29%|██▊       | 307/1070 [00:21<00:47, 16.09it/s] 29%|██▉       | 309/1070 [00:21<00:47, 16.09it/s] 29%|██▉       | 311/1070 [00:21<00:47, 16.10it/s] 29%|██▉       | 313/1070 [00:21<00:46, 16.11it/s] 29%|██▉       | 315/1070 [00:21<00:46, 16.13it/s] 30%|██▉       | 317/1070 [00:21<00:46, 16.11it/s] 30%|██▉       | 319/1070 [00:22<00:46, 16.10it/s] 30%|███       | 321/1070 [00:22<00:46, 16.08it/s] 30%|███       | 323/1070 [00:22<00:46, 16.12it/s] 30%|███       | 325/1070 [00:22<00:46, 16.15it/s] 31%|███       | 327/1070 [00:22<00:46, 16.14it/s] 31%|███       | 329/1070 [00:22<00:45, 16.13it/s] 31%|███       | 331/1070 [00:22<00:45, 16.10it/s] 31%|███       | 333/1070 [00:22<00:46, 16.01it/s] 31%|███▏      | 335/1070 [00:23<00:46, 15.94it/s] 31%|███▏      | 337/1070 [00:23<00:46, 15.86it/s] 32%|███▏      | 339/1070 [00:23<00:46, 15.84it/s] 32%|███▏      | 341/1070 [00:23<00:46, 15.79it/s] 32%|███▏      | 343/1070 [00:23<00:46, 15.78it/s] 32%|███▏      | 345/1070 [00:23<00:45, 15.80it/s] 32%|███▏      | 347/1070 [00:23<00:45, 15.74it/s] 33%|███▎      | 349/1070 [00:23<00:45, 15.79it/s] 33%|███▎      | 351/1070 [00:24<00:45, 15.86it/s] 33%|███▎      | 353/1070 [00:24<00:45, 15.91it/s] 33%|███▎      | 355/1070 [00:24<00:45, 15.88it/s] 33%|███▎      | 357/1070 [00:24<00:44, 15.93it/s] 34%|███▎      | 359/1070 [00:24<00:44, 15.94it/s] 34%|███▎      | 361/1070 [00:24<00:44, 15.91it/s] 34%|███▍      | 363/1070 [00:24<00:44, 15.91it/s] 34%|███▍      | 365/1070 [00:24<00:44, 15.96it/s] 34%|███▍      | 367/1070 [00:25<00:43, 15.98it/s] 34%|███▍      | 369/1070 [00:25<00:43, 15.99it/s] 35%|███▍      | 371/1070 [00:25<00:43, 15.96it/s] 35%|███▍      | 373/1070 [00:25<00:44, 15.83it/s] 35%|███▌      | 375/1070 [00:25<00:44, 15.64it/s] 35%|███▌      | 377/1070 [00:25<00:44, 15.57it/s] 35%|███▌      | 379/1070 [00:25<00:44, 15.60it/s] 36%|███▌      | 381/1070 [00:25<00:43, 15.72it/s] 36%|███▌      | 383/1070 [00:26<00:43, 15.83it/s] 36%|███▌      | 385/1070 [00:26<00:43, 15.92it/s] 36%|███▌      | 387/1070 [00:26<00:42, 15.96it/s] 36%|███▋      | 389/1070 [00:26<00:42, 15.98it/s] 37%|███▋      | 391/1070 [00:26<00:42, 16.04it/s] 37%|███▋      | 393/1070 [00:26<00:42, 16.08it/s] 37%|███▋      | 395/1070 [00:26<00:41, 16.08it/s] 37%|███▋      | 397/1070 [00:26<00:41, 16.05it/s] 37%|███▋      | 399/1070 [00:27<00:41, 16.06it/s] 37%|███▋      | 401/1070 [00:27<00:41, 16.08it/s] 38%|███▊      | 403/1070 [00:27<00:41, 16.07it/s] 38%|███▊      | 405/1070 [00:27<00:41, 16.03it/s] 38%|███▊      | 407/1070 [00:27<00:41, 16.02it/s] 38%|███▊      | 409/1070 [00:27<00:41, 16.07it/s] 38%|███▊      | 411/1070 [00:27<00:40, 16.08it/s] 39%|███▊      | 413/1070 [00:27<00:41, 15.99it/s] 39%|███▉      | 415/1070 [00:28<00:41, 15.93it/s] 39%|███▉      | 417/1070 [00:28<00:40, 15.98it/s] 39%|███▉      | 419/1070 [00:28<00:40, 16.00it/s] 39%|███▉      | 421/1070 [00:28<00:40, 15.91it/s] 40%|███▉      | 423/1070 [00:28<00:40, 15.88it/s] 40%|███▉      | 425/1070 [00:28<00:40, 15.87it/s] 40%|███▉      | 427/1070 [00:28<00:40, 15.81it/s]                                                   40%|████      | 428/1070 [00:28<00:40, 15.81it/s][INFO|trainer.py:755] 2023-11-15 23:50:28,170 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 23:50:28,171 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 23:50:28,172 >>   Num examples = 760
[INFO|trainer.py:3134] 2023-11-15 23:50:28,172 >>   Batch size = 8
{'eval_loss': 0.30013507604599, 'eval_accuracy': 0.906578947368421, 'eval_micro_f1': 0.906578947368421, 'eval_macro_f1': 0.9037528762329118, 'eval_runtime': 1.177, 'eval_samples_per_second': 645.711, 'eval_steps_per_second': 80.714, 'epoch': 1.0}
{'loss': 0.2567, 'learning_rate': 0.0003, 'epoch': 2.0}

  0%|          | 0/95 [00:00<?, ?it/s][A
 11%|█         | 10/95 [00:00<00:00, 88.33it/s][A
 20%|██        | 19/95 [00:00<00:00, 85.06it/s][A
 29%|██▉       | 28/95 [00:00<00:00, 84.19it/s][A
 39%|███▉      | 37/95 [00:00<00:00, 84.17it/s][A
 48%|████▊     | 46/95 [00:00<00:00, 83.24it/s][A
 58%|█████▊    | 55/95 [00:00<00:00, 83.46it/s][A
 67%|██████▋   | 64/95 [00:00<00:00, 84.13it/s][A
 77%|███████▋  | 73/95 [00:00<00:00, 83.38it/s][A
 86%|████████▋ | 82/95 [00:00<00:00, 83.39it/s][A
 96%|█████████▌| 91/95 [00:01<00:00, 83.23it/s][A                                                  
                                               [A 40%|████      | 428/1070 [00:30<00:40, 15.81it/s]
100%|██████████| 95/95 [00:01<00:00, 83.23it/s][A
                                               [A 40%|████      | 429/1070 [00:30<02:33,  4.18it/s] 40%|████      | 431/1070 [00:30<01:59,  5.36it/s] 40%|████      | 433/1070 [00:30<01:35,  6.69it/s] 41%|████      | 435/1070 [00:30<01:18,  8.10it/s] 41%|████      | 437/1070 [00:30<01:06,  9.49it/s] 41%|████      | 439/1070 [00:30<00:58, 10.75it/s] 41%|████      | 441/1070 [00:30<00:53, 11.79it/s] 41%|████▏     | 443/1070 [00:31<00:49, 12.63it/s] 42%|████▏     | 445/1070 [00:31<00:46, 13.41it/s] 42%|████▏     | 447/1070 [00:31<00:44, 14.04it/s] 42%|████▏     | 449/1070 [00:31<00:42, 14.53it/s] 42%|████▏     | 451/1070 [00:31<00:41, 14.96it/s] 42%|████▏     | 453/1070 [00:31<00:40, 15.30it/s] 43%|████▎     | 455/1070 [00:31<00:39, 15.52it/s] 43%|████▎     | 457/1070 [00:31<00:39, 15.64it/s] 43%|████▎     | 459/1070 [00:32<00:38, 15.76it/s] 43%|████▎     | 461/1070 [00:32<00:38, 15.86it/s] 43%|████▎     | 463/1070 [00:32<00:38, 15.93it/s] 43%|████▎     | 465/1070 [00:32<00:37, 15.97it/s] 44%|████▎     | 467/1070 [00:32<00:37, 16.00it/s] 44%|████▍     | 469/1070 [00:32<00:37, 16.04it/s] 44%|████▍     | 471/1070 [00:32<00:37, 16.07it/s] 44%|████▍     | 473/1070 [00:32<00:37, 16.02it/s] 44%|████▍     | 475/1070 [00:33<00:37, 15.98it/s] 45%|████▍     | 477/1070 [00:33<00:37, 16.02it/s] 45%|████▍     | 479/1070 [00:33<00:36, 16.05it/s] 45%|████▍     | 481/1070 [00:33<00:36, 16.06it/s] 45%|████▌     | 483/1070 [00:33<00:36, 16.03it/s] 45%|████▌     | 485/1070 [00:33<00:36, 16.04it/s] 46%|████▌     | 487/1070 [00:33<00:36, 16.07it/s] 46%|████▌     | 489/1070 [00:33<00:36, 16.00it/s] 46%|████▌     | 491/1070 [00:34<00:36, 15.88it/s] 46%|████▌     | 493/1070 [00:34<00:36, 15.85it/s] 46%|████▋     | 495/1070 [00:34<00:36, 15.84it/s] 46%|████▋     | 497/1070 [00:34<00:36, 15.80it/s] 47%|████▋     | 499/1070 [00:34<00:36, 15.82it/s] 47%|████▋     | 501/1070 [00:34<00:35, 15.82it/s] 47%|████▋     | 503/1070 [00:34<00:35, 15.85it/s] 47%|████▋     | 505/1070 [00:34<00:35, 15.84it/s] 47%|████▋     | 507/1070 [00:35<00:35, 15.84it/s] 48%|████▊     | 509/1070 [00:35<00:35, 15.86it/s] 48%|████▊     | 511/1070 [00:35<00:35, 15.85it/s] 48%|████▊     | 513/1070 [00:35<00:35, 15.90it/s] 48%|████▊     | 515/1070 [00:35<00:34, 15.86it/s] 48%|████▊     | 517/1070 [00:35<00:34, 15.84it/s] 49%|████▊     | 519/1070 [00:35<00:34, 15.85it/s] 49%|████▊     | 521/1070 [00:35<00:34, 15.85it/s] 49%|████▉     | 523/1070 [00:36<00:34, 15.88it/s] 49%|████▉     | 525/1070 [00:36<00:34, 15.92it/s] 49%|████▉     | 527/1070 [00:36<00:34, 15.96it/s] 49%|████▉     | 529/1070 [00:36<00:34, 15.89it/s] 50%|████▉     | 531/1070 [00:36<00:34, 15.81it/s] 50%|████▉     | 533/1070 [00:36<00:33, 15.80it/s] 50%|█████     | 535/1070 [00:36<00:33, 15.76it/s] 50%|█████     | 537/1070 [00:36<00:33, 15.77it/s] 50%|█████     | 539/1070 [00:37<00:33, 15.85it/s] 51%|█████     | 541/1070 [00:37<00:33, 15.86it/s] 51%|█████     | 543/1070 [00:37<00:33, 15.91it/s] 51%|█████     | 545/1070 [00:37<00:32, 15.99it/s] 51%|█████     | 547/1070 [00:37<00:32, 16.04it/s] 51%|█████▏    | 549/1070 [00:37<00:32, 16.04it/s] 51%|█████▏    | 551/1070 [00:37<00:32, 15.99it/s] 52%|█████▏    | 553/1070 [00:37<00:32, 16.04it/s] 52%|█████▏    | 555/1070 [00:38<00:32, 16.08it/s] 52%|█████▏    | 557/1070 [00:38<00:31, 16.09it/s] 52%|█████▏    | 559/1070 [00:38<00:31, 16.07it/s] 52%|█████▏    | 561/1070 [00:38<00:31, 16.06it/s] 53%|█████▎    | 563/1070 [00:38<00:31, 16.10it/s] 53%|█████▎    | 565/1070 [00:38<00:31, 16.12it/s] 53%|█████▎    | 567/1070 [00:38<00:31, 16.10it/s] 53%|█████▎    | 569/1070 [00:38<00:31, 16.04it/s] 53%|█████▎    | 571/1070 [00:39<00:31, 16.04it/s] 54%|█████▎    | 573/1070 [00:39<00:30, 16.04it/s] 54%|█████▎    | 575/1070 [00:39<00:30, 16.03it/s] 54%|█████▍    | 577/1070 [00:39<00:30, 15.94it/s] 54%|█████▍    | 579/1070 [00:39<00:30, 15.89it/s] 54%|█████▍    | 581/1070 [00:39<00:30, 15.82it/s] 54%|█████▍    | 583/1070 [00:39<00:30, 15.73it/s] 55%|█████▍    | 585/1070 [00:39<00:30, 15.74it/s] 55%|█████▍    | 587/1070 [00:40<00:30, 15.72it/s] 55%|█████▌    | 589/1070 [00:40<00:30, 15.76it/s] 55%|█████▌    | 591/1070 [00:40<00:30, 15.84it/s] 55%|█████▌    | 593/1070 [00:40<00:30, 15.85it/s] 56%|█████▌    | 595/1070 [00:40<00:30, 15.81it/s] 56%|█████▌    | 597/1070 [00:40<00:29, 15.86it/s] 56%|█████▌    | 599/1070 [00:40<00:29, 15.89it/s] 56%|█████▌    | 601/1070 [00:40<00:29, 15.90it/s] 56%|█████▋    | 603/1070 [00:41<00:29, 15.92it/s] 57%|█████▋    | 605/1070 [00:41<00:29, 15.95it/s] 57%|█████▋    | 607/1070 [00:41<00:29, 15.88it/s] 57%|█████▋    | 609/1070 [00:41<00:29, 15.86it/s] 57%|█████▋    | 611/1070 [00:41<00:28, 15.92it/s] 57%|█████▋    | 613/1070 [00:41<00:28, 15.91it/s] 57%|█████▋    | 615/1070 [00:41<00:28, 15.87it/s] 58%|█████▊    | 617/1070 [00:41<00:28, 15.89it/s] 58%|█████▊    | 619/1070 [00:42<00:28, 15.82it/s] 58%|█████▊    | 621/1070 [00:42<00:28, 15.77it/s] 58%|█████▊    | 623/1070 [00:42<00:28, 15.79it/s] 58%|█████▊    | 625/1070 [00:42<00:28, 15.75it/s] 59%|█████▊    | 627/1070 [00:42<00:28, 15.80it/s] 59%|█████▉    | 629/1070 [00:42<00:27, 15.90it/s] 59%|█████▉    | 631/1070 [00:42<00:27, 15.96it/s] 59%|█████▉    | 633/1070 [00:42<00:27, 15.97it/s] 59%|█████▉    | 635/1070 [00:43<00:27, 15.98it/s] 60%|█████▉    | 637/1070 [00:43<00:27, 16.02it/s] 60%|█████▉    | 639/1070 [00:43<00:26, 16.04it/s] 60%|█████▉    | 641/1070 [00:43<00:26, 16.05it/s]                                                   60%|██████    | 642/1070 [00:43<00:26, 16.05it/s][INFO|trainer.py:755] 2023-11-15 23:50:42,796 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 23:50:42,798 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 23:50:42,798 >>   Num examples = 760
[INFO|trainer.py:3134] 2023-11-15 23:50:42,798 >>   Batch size = 8
{'eval_loss': 0.30120062828063965, 'eval_accuracy': 0.9039473684210526, 'eval_micro_f1': 0.9039473684210526, 'eval_macro_f1': 0.9016103251365802, 'eval_runtime': 1.1795, 'eval_samples_per_second': 644.35, 'eval_steps_per_second': 80.544, 'epoch': 2.0}
{'loss': 0.2026, 'learning_rate': 0.0002, 'epoch': 3.0}

  0%|          | 0/95 [00:00<?, ?it/s][A
 11%|█         | 10/95 [00:00<00:00, 97.79it/s][A
 21%|██        | 20/95 [00:00<00:00, 91.84it/s][A
 32%|███▏      | 30/95 [00:00<00:00, 89.95it/s][A
 42%|████▏     | 40/95 [00:00<00:00, 89.02it/s][A
 52%|█████▏    | 49/95 [00:00<00:00, 88.78it/s][A
 61%|██████    | 58/95 [00:00<00:00, 88.36it/s][A
 71%|███████   | 67/95 [00:00<00:00, 88.19it/s][A
 80%|████████  | 76/95 [00:00<00:00, 88.46it/s][A
 89%|████████▉ | 85/95 [00:00<00:00, 88.27it/s][A
 99%|█████████▉| 94/95 [00:01<00:00, 88.16it/s][A                                                  
                                               [A 60%|██████    | 642/1070 [00:44<00:26, 16.05it/s]
100%|██████████| 95/95 [00:01<00:00, 88.16it/s][A
                                               [A 60%|██████    | 643/1070 [00:44<01:37,  4.38it/s] 60%|██████    | 645/1070 [00:44<01:15,  5.60it/s] 60%|██████    | 647/1070 [00:44<01:00,  6.95it/s] 61%|██████    | 649/1070 [00:45<00:50,  8.34it/s] 61%|██████    | 651/1070 [00:45<00:43,  9.69it/s] 61%|██████    | 653/1070 [00:45<00:38, 10.94it/s] 61%|██████    | 655/1070 [00:45<00:34, 12.05it/s] 61%|██████▏   | 657/1070 [00:45<00:31, 12.99it/s] 62%|██████▏   | 659/1070 [00:45<00:29, 13.70it/s] 62%|██████▏   | 661/1070 [00:45<00:28, 14.29it/s] 62%|██████▏   | 663/1070 [00:45<00:27, 14.74it/s] 62%|██████▏   | 665/1070 [00:46<00:26, 15.04it/s] 62%|██████▏   | 667/1070 [00:46<00:26, 15.25it/s] 63%|██████▎   | 669/1070 [00:46<00:25, 15.43it/s] 63%|██████▎   | 671/1070 [00:46<00:25, 15.57it/s] 63%|██████▎   | 673/1070 [00:46<00:25, 15.67it/s] 63%|██████▎   | 675/1070 [00:46<00:25, 15.73it/s] 63%|██████▎   | 677/1070 [00:46<00:24, 15.76it/s] 63%|██████▎   | 679/1070 [00:46<00:24, 15.72it/s] 64%|██████▎   | 681/1070 [00:47<00:24, 15.80it/s] 64%|██████▍   | 683/1070 [00:47<00:24, 15.80it/s] 64%|██████▍   | 685/1070 [00:47<00:24, 15.84it/s] 64%|██████▍   | 687/1070 [00:47<00:24, 15.88it/s] 64%|██████▍   | 689/1070 [00:47<00:24, 15.83it/s] 65%|██████▍   | 691/1070 [00:47<00:24, 15.72it/s] 65%|██████▍   | 693/1070 [00:47<00:23, 15.73it/s] 65%|██████▍   | 695/1070 [00:47<00:23, 15.69it/s] 65%|██████▌   | 697/1070 [00:48<00:23, 15.72it/s] 65%|██████▌   | 699/1070 [00:48<00:23, 15.83it/s] 66%|██████▌   | 701/1070 [00:48<00:23, 15.91it/s] 66%|██████▌   | 703/1070 [00:48<00:23, 15.94it/s] 66%|██████▌   | 705/1070 [00:48<00:22, 15.93it/s] 66%|██████▌   | 707/1070 [00:48<00:22, 16.00it/s] 66%|██████▋   | 709/1070 [00:48<00:22, 16.02it/s] 66%|██████▋   | 711/1070 [00:48<00:22, 16.01it/s] 67%|██████▋   | 713/1070 [00:49<00:22, 16.01it/s] 67%|██████▋   | 715/1070 [00:49<00:22, 16.05it/s] 67%|██████▋   | 717/1070 [00:49<00:21, 16.07it/s] 67%|██████▋   | 719/1070 [00:49<00:21, 16.05it/s] 67%|██████▋   | 721/1070 [00:49<00:21, 16.04it/s] 68%|██████▊   | 723/1070 [00:49<00:21, 16.03it/s] 68%|██████▊   | 725/1070 [00:49<00:21, 16.05it/s] 68%|██████▊   | 727/1070 [00:49<00:21, 16.05it/s] 68%|██████▊   | 729/1070 [00:50<00:21, 16.04it/s] 68%|██████▊   | 731/1070 [00:50<00:21, 16.04it/s] 69%|██████▊   | 733/1070 [00:50<00:21, 16.03it/s] 69%|██████▊   | 735/1070 [00:50<00:20, 16.01it/s] 69%|██████▉   | 737/1070 [00:50<00:20, 15.93it/s] 69%|██████▉   | 739/1070 [00:50<00:20, 15.90it/s] 69%|██████▉   | 741/1070 [00:50<00:20, 15.85it/s] 69%|██████▉   | 743/1070 [00:50<00:20, 15.77it/s] 70%|██████▉   | 745/1070 [00:51<00:20, 15.80it/s] 70%|██████▉   | 747/1070 [00:51<00:20, 15.76it/s] 70%|███████   | 749/1070 [00:51<00:20, 15.77it/s] 70%|███████   | 751/1070 [00:51<00:20, 15.82it/s] 70%|███████   | 753/1070 [00:51<00:20, 15.84it/s] 71%|███████   | 755/1070 [00:51<00:19, 15.77it/s] 71%|███████   | 757/1070 [00:51<00:19, 15.78it/s] 71%|███████   | 759/1070 [00:52<00:19, 15.78it/s] 71%|███████   | 761/1070 [00:52<00:19, 15.79it/s] 71%|███████▏  | 763/1070 [00:52<00:19, 15.85it/s] 71%|███████▏  | 765/1070 [00:52<00:19, 15.89it/s] 72%|███████▏  | 767/1070 [00:52<00:19, 15.81it/s] 72%|███████▏  | 769/1070 [00:52<00:19, 15.81it/s] 72%|███████▏  | 771/1070 [00:52<00:18, 15.74it/s] 72%|███████▏  | 773/1070 [00:52<00:18, 15.76it/s] 72%|███████▏  | 775/1070 [00:53<00:18, 15.79it/s] 73%|███████▎  | 777/1070 [00:53<00:18, 15.72it/s] 73%|███████▎  | 779/1070 [00:53<00:18, 15.70it/s] 73%|███████▎  | 781/1070 [00:53<00:18, 15.70it/s] 73%|███████▎  | 783/1070 [00:53<00:18, 15.72it/s] 73%|███████▎  | 785/1070 [00:53<00:18, 15.82it/s] 74%|███████▎  | 787/1070 [00:53<00:17, 15.91it/s] 74%|███████▎  | 789/1070 [00:53<00:17, 15.96it/s] 74%|███████▍  | 791/1070 [00:54<00:17, 15.95it/s] 74%|███████▍  | 793/1070 [00:54<00:17, 15.96it/s] 74%|███████▍  | 795/1070 [00:54<00:17, 15.98it/s] 74%|███████▍  | 797/1070 [00:54<00:17, 15.98it/s] 75%|███████▍  | 799/1070 [00:54<00:16, 15.96it/s] 75%|███████▍  | 801/1070 [00:54<00:16, 15.98it/s] 75%|███████▌  | 803/1070 [00:54<00:16, 16.05it/s] 75%|███████▌  | 805/1070 [00:54<00:16, 16.02it/s] 75%|███████▌  | 807/1070 [00:55<00:16, 16.01it/s] 76%|███████▌  | 809/1070 [00:55<00:16, 16.01it/s] 76%|███████▌  | 811/1070 [00:55<00:16, 16.02it/s] 76%|███████▌  | 813/1070 [00:55<00:16, 16.02it/s] 76%|███████▌  | 815/1070 [00:55<00:15, 16.01it/s] 76%|███████▋  | 817/1070 [00:55<00:15, 16.00it/s] 77%|███████▋  | 819/1070 [00:55<00:15, 15.99it/s] 77%|███████▋  | 821/1070 [00:55<00:15, 15.97it/s] 77%|███████▋  | 823/1070 [00:56<00:15, 15.91it/s] 77%|███████▋  | 825/1070 [00:56<00:15, 15.85it/s] 77%|███████▋  | 827/1070 [00:56<00:15, 15.79it/s] 77%|███████▋  | 829/1070 [00:56<00:15, 15.72it/s] 78%|███████▊  | 831/1070 [00:56<00:15, 15.74it/s] 78%|███████▊  | 833/1070 [00:56<00:15, 15.70it/s] 78%|███████▊  | 835/1070 [00:56<00:14, 15.71it/s] 78%|███████▊  | 837/1070 [00:56<00:14, 15.74it/s] 78%|███████▊  | 839/1070 [00:57<00:14, 15.79it/s] 79%|███████▊  | 841/1070 [00:57<00:14, 15.77it/s] 79%|███████▉  | 843/1070 [00:57<00:14, 15.77it/s] 79%|███████▉  | 845/1070 [00:57<00:14, 15.79it/s] 79%|███████▉  | 847/1070 [00:57<00:14, 15.80it/s] 79%|███████▉  | 849/1070 [00:57<00:13, 15.85it/s] 80%|███████▉  | 851/1070 [00:57<00:13, 15.85it/s] 80%|███████▉  | 853/1070 [00:57<00:13, 15.86it/s] 80%|███████▉  | 855/1070 [00:58<00:13, 15.83it/s]                                                   80%|████████  | 856/1070 [00:58<00:13, 15.83it/s][INFO|trainer.py:755] 2023-11-15 23:50:57,389 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 23:50:57,390 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 23:50:57,391 >>   Num examples = 760
[INFO|trainer.py:3134] 2023-11-15 23:50:57,391 >>   Batch size = 8
{'eval_loss': 0.3019368350505829, 'eval_accuracy': 0.9105263157894737, 'eval_micro_f1': 0.9105263157894739, 'eval_macro_f1': 0.9078229170364052, 'eval_runtime': 1.113, 'eval_samples_per_second': 682.817, 'eval_steps_per_second': 85.352, 'epoch': 3.0}
{'loss': 0.1638, 'learning_rate': 0.0001, 'epoch': 4.0}

  0%|          | 0/95 [00:00<?, ?it/s][A
 11%|█         | 10/95 [00:00<00:00, 94.72it/s][A
 21%|██        | 20/95 [00:00<00:00, 88.41it/s][A
 31%|███       | 29/95 [00:00<00:00, 87.23it/s][A
 40%|████      | 38/95 [00:00<00:00, 84.82it/s][A
 49%|████▉     | 47/95 [00:00<00:00, 80.84it/s][A
 59%|█████▉    | 56/95 [00:00<00:00, 80.29it/s][A
 68%|██████▊   | 65/95 [00:00<00:00, 78.49it/s][A
 77%|███████▋  | 73/95 [00:00<00:00, 77.99it/s][A
 86%|████████▋ | 82/95 [00:01<00:00, 80.68it/s][A
 96%|█████████▌| 91/95 [00:01<00:00, 82.80it/s][A                                                  
                                               [A 80%|████████  | 856/1070 [00:59<00:13, 15.83it/s]
100%|██████████| 95/95 [00:01<00:00, 82.80it/s][A
                                               [A 80%|████████  | 857/1070 [00:59<00:51,  4.14it/s] 80%|████████  | 859/1070 [00:59<00:39,  5.32it/s] 80%|████████  | 861/1070 [00:59<00:31,  6.66it/s] 81%|████████  | 863/1070 [00:59<00:25,  8.07it/s] 81%|████████  | 865/1070 [00:59<00:21,  9.49it/s] 81%|████████  | 867/1070 [00:59<00:18, 10.82it/s] 81%|████████  | 869/1070 [01:00<00:16, 11.99it/s] 81%|████████▏ | 871/1070 [01:00<00:15, 12.96it/s] 82%|████████▏ | 873/1070 [01:00<00:14, 13.75it/s] 82%|████████▏ | 875/1070 [01:00<00:13, 14.37it/s] 82%|████████▏ | 877/1070 [01:00<00:13, 14.81it/s] 82%|████████▏ | 879/1070 [01:00<00:12, 15.13it/s] 82%|████████▏ | 881/1070 [01:00<00:12, 15.37it/s] 83%|████████▎ | 883/1070 [01:00<00:12, 15.56it/s] 83%|████████▎ | 885/1070 [01:01<00:11, 15.66it/s] 83%|████████▎ | 887/1070 [01:01<00:11, 15.73it/s] 83%|████████▎ | 889/1070 [01:01<00:11, 15.83it/s] 83%|████████▎ | 891/1070 [01:01<00:11, 15.87it/s] 83%|████████▎ | 893/1070 [01:01<00:11, 15.78it/s] 84%|████████▎ | 895/1070 [01:01<00:11, 15.74it/s] 84%|████████▍ | 897/1070 [01:01<00:11, 15.71it/s] 84%|████████▍ | 899/1070 [01:02<00:10, 15.68it/s] 84%|████████▍ | 901/1070 [01:02<00:10, 15.77it/s] 84%|████████▍ | 903/1070 [01:02<00:10, 15.80it/s] 85%|████████▍ | 905/1070 [01:02<00:10, 15.76it/s] 85%|████████▍ | 907/1070 [01:02<00:10, 15.83it/s] 85%|████████▍ | 909/1070 [01:02<00:10, 15.87it/s] 85%|████████▌ | 911/1070 [01:02<00:10, 15.88it/s] 85%|████████▌ | 913/1070 [01:02<00:09, 15.82it/s] 86%|████████▌ | 915/1070 [01:03<00:09, 15.77it/s] 86%|████████▌ | 917/1070 [01:03<00:09, 15.81it/s] 86%|████████▌ | 919/1070 [01:03<00:09, 15.81it/s] 86%|████████▌ | 921/1070 [01:03<00:09, 15.82it/s] 86%|████████▋ | 923/1070 [01:03<00:09, 15.82it/s] 86%|████████▋ | 925/1070 [01:03<00:09, 15.82it/s] 87%|████████▋ | 927/1070 [01:03<00:09, 15.79it/s] 87%|████████▋ | 929/1070 [01:03<00:08, 15.82it/s] 87%|████████▋ | 931/1070 [01:04<00:08, 15.85it/s] 87%|████████▋ | 933/1070 [01:04<00:08, 15.88it/s] 87%|████████▋ | 935/1070 [01:04<00:08, 15.79it/s] 88%|████████▊ | 937/1070 [01:04<00:08, 15.72it/s] 88%|████████▊ | 939/1070 [01:04<00:08, 15.70it/s] 88%|████████▊ | 941/1070 [01:04<00:08, 15.65it/s] 88%|████████▊ | 943/1070 [01:04<00:08, 15.64it/s] 88%|████████▊ | 945/1070 [01:04<00:07, 15.71it/s] 89%|████████▊ | 947/1070 [01:05<00:07, 15.79it/s] 89%|████████▊ | 949/1070 [01:05<00:07, 15.89it/s] 89%|████████▉ | 951/1070 [01:05<00:07, 15.94it/s] 89%|████████▉ | 953/1070 [01:05<00:07, 15.96it/s] 89%|████████▉ | 955/1070 [01:05<00:07, 15.97it/s] 89%|████████▉ | 957/1070 [01:05<00:07, 15.97it/s] 90%|████████▉ | 959/1070 [01:05<00:06, 15.97it/s] 90%|████████▉ | 961/1070 [01:05<00:06, 15.95it/s] 90%|█████████ | 963/1070 [01:06<00:06, 15.97it/s] 90%|█████████ | 965/1070 [01:06<00:06, 16.01it/s] 90%|█████████ | 967/1070 [01:06<00:06, 16.02it/s] 91%|█████████ | 969/1070 [01:06<00:06, 16.01it/s] 91%|█████████ | 971/1070 [01:06<00:06, 16.01it/s] 91%|█████████ | 973/1070 [01:06<00:06, 16.04it/s] 91%|█████████ | 975/1070 [01:06<00:05, 16.04it/s] 91%|█████████▏| 977/1070 [01:06<00:05, 16.01it/s] 91%|█████████▏| 979/1070 [01:07<00:05, 16.00it/s] 92%|█████████▏| 981/1070 [01:07<00:05, 15.99it/s] 92%|█████████▏| 983/1070 [01:07<00:05, 15.90it/s] 92%|█████████▏| 985/1070 [01:07<00:05, 15.82it/s] 92%|█████████▏| 987/1070 [01:07<00:05, 15.79it/s] 92%|█████████▏| 989/1070 [01:07<00:05, 15.72it/s] 93%|█████████▎| 991/1070 [01:07<00:05, 15.69it/s] 93%|█████████▎| 993/1070 [01:07<00:04, 15.70it/s] 93%|█████████▎| 995/1070 [01:08<00:04, 15.73it/s] 93%|█████████▎| 997/1070 [01:08<00:04, 15.79it/s] 93%|█████████▎| 999/1070 [01:08<00:04, 15.79it/s] 94%|█████████▎| 1001/1070 [01:08<00:04, 15.81it/s] 94%|█████████▎| 1003/1070 [01:08<00:04, 15.78it/s] 94%|█████████▍| 1005/1070 [01:08<00:04, 15.78it/s] 94%|█████████▍| 1007/1070 [01:08<00:03, 15.81it/s] 94%|█████████▍| 1009/1070 [01:08<00:03, 15.83it/s] 94%|█████████▍| 1011/1070 [01:09<00:03, 15.87it/s] 95%|█████████▍| 1013/1070 [01:09<00:03, 15.85it/s] 95%|█████████▍| 1015/1070 [01:09<00:03, 15.82it/s] 95%|█████████▌| 1017/1070 [01:09<00:03, 15.80it/s] 95%|█████████▌| 1019/1070 [01:09<00:03, 15.80it/s] 95%|█████████▌| 1021/1070 [01:09<00:03, 15.82it/s] 96%|█████████▌| 1023/1070 [01:09<00:02, 15.77it/s] 96%|█████████▌| 1025/1070 [01:09<00:02, 15.70it/s] 96%|█████████▌| 1027/1070 [01:10<00:02, 15.61it/s] 96%|█████████▌| 1029/1070 [01:10<00:02, 15.44it/s] 96%|█████████▋| 1031/1070 [01:10<00:02, 15.55it/s] 97%|█████████▋| 1033/1070 [01:10<00:02, 15.66it/s] 97%|█████████▋| 1035/1070 [01:10<00:02, 15.75it/s] 97%|█████████▋| 1037/1070 [01:10<00:02, 15.83it/s] 97%|█████████▋| 1039/1070 [01:10<00:01, 15.87it/s] 97%|█████████▋| 1041/1070 [01:10<00:01, 15.90it/s] 97%|█████████▋| 1043/1070 [01:11<00:01, 15.91it/s] 98%|█████████▊| 1045/1070 [01:11<00:01, 15.93it/s] 98%|█████████▊| 1047/1070 [01:11<00:01, 15.95it/s] 98%|█████████▊| 1049/1070 [01:11<00:01, 15.95it/s] 98%|█████████▊| 1051/1070 [01:11<00:01, 15.95it/s] 98%|█████████▊| 1053/1070 [01:11<00:01, 15.96it/s] 99%|█████████▊| 1055/1070 [01:11<00:00, 15.95it/s] 99%|█████████▉| 1057/1070 [01:11<00:00, 15.92it/s] 99%|█████████▉| 1059/1070 [01:12<00:00, 15.94it/s] 99%|█████████▉| 1061/1070 [01:12<00:00, 15.95it/s] 99%|█████████▉| 1063/1070 [01:12<00:00, 15.96it/s]100%|█████████▉| 1065/1070 [01:12<00:00, 15.95it/s]100%|█████████▉| 1067/1070 [01:12<00:00, 15.96it/s]100%|█████████▉| 1069/1070 [01:12<00:00, 15.96it/s]                                                   100%|██████████| 1070/1070 [01:12<00:00, 15.96it/s][INFO|trainer.py:755] 2023-11-15 23:51:12,066 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 23:51:12,068 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 23:51:12,068 >>   Num examples = 760
[INFO|trainer.py:3134] 2023-11-15 23:51:12,069 >>   Batch size = 8
{'eval_loss': 0.30672580003738403, 'eval_accuracy': 0.9105263157894737, 'eval_micro_f1': 0.9105263157894739, 'eval_macro_f1': 0.9081557006804123, 'eval_runtime': 1.1978, 'eval_samples_per_second': 634.512, 'eval_steps_per_second': 79.314, 'epoch': 4.0}
{'loss': 0.129, 'learning_rate': 0.0, 'epoch': 5.0}

  0%|          | 0/95 [00:00<?, ?it/s][A
 11%|█         | 10/95 [00:00<00:00, 94.58it/s][A
 21%|██        | 20/95 [00:00<00:00, 87.31it/s][A
 31%|███       | 29/95 [00:00<00:00, 85.65it/s][A
 40%|████      | 38/95 [00:00<00:00, 83.91it/s][A
 49%|████▉     | 47/95 [00:00<00:00, 82.81it/s][A
 59%|█████▉    | 56/95 [00:00<00:00, 83.35it/s][A
 68%|██████▊   | 65/95 [00:00<00:00, 82.93it/s][A
 78%|███████▊  | 74/95 [00:00<00:00, 83.22it/s][A
 87%|████████▋ | 83/95 [00:00<00:00, 83.21it/s][A
 97%|█████████▋| 92/95 [00:01<00:00, 83.60it/s][A                                                   
                                               [A100%|██████████| 1070/1070 [01:13<00:00, 15.96it/s]
100%|██████████| 95/95 [00:01<00:00, 83.60it/s][A
                                               [A[INFO|trainer.py:1963] 2023-11-15 23:51:13,260 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                   100%|██████████| 1070/1070 [01:13<00:00, 15.96it/s]100%|██████████| 1070/1070 [01:13<00:00, 14.46it/s]
[INFO|trainer.py:2855] 2023-11-15 23:51:13,263 >> Saving model checkpoint to ./result/agnews_sup_roberta-base_seed4_adapter
[INFO|configuration_utils.py:460] 2023-11-15 23:51:13,266 >> Configuration saved in ./result/agnews_sup_roberta-base_seed4_adapter/config.json
[INFO|modeling_utils.py:1997] 2023-11-15 23:51:14,461 >> Model weights saved in ./result/agnews_sup_roberta-base_seed4_adapter/pytorch_model.bin
[INFO|tokenization_utils_base.py:2235] 2023-11-15 23:51:14,464 >> tokenizer config file saved in ./result/agnews_sup_roberta-base_seed4_adapter/tokenizer_config.json
[INFO|tokenization_utils_base.py:2242] 2023-11-15 23:51:14,466 >> Special tokens file saved in ./result/agnews_sup_roberta-base_seed4_adapter/special_tokens_map.json
{'eval_loss': 0.30845755338668823, 'eval_accuracy': 0.906578947368421, 'eval_micro_f1': 0.906578947368421, 'eval_macro_f1': 0.9040233675616132, 'eval_runtime': 1.1763, 'eval_samples_per_second': 646.085, 'eval_steps_per_second': 80.761, 'epoch': 5.0}
{'train_runtime': 73.9773, 'train_samples_per_second': 462.304, 'train_steps_per_second': 14.464, 'train_loss': 0.23547944755197686, 'epoch': 5.0}
***** train metrics *****
  epoch                    =        5.0
  train_loss               =     0.2355
  train_runtime            = 0:01:13.97
  train_samples            =       6840
  train_samples_per_second =    462.304
  train_steps_per_second   =     14.464
11/15/2023 23:51:14 - INFO - __main__ - *** Evaluate ***
[INFO|trainer.py:755] 2023-11-15 23:51:14,605 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3129] 2023-11-15 23:51:14,606 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-11-15 23:51:14,606 >>   Num examples = 760
[INFO|trainer.py:3134] 2023-11-15 23:51:14,607 >>   Batch size = 8
  0%|          | 0/95 [00:00<?, ?it/s] 11%|█         | 10/95 [00:00<00:00, 87.39it/s] 20%|██        | 19/95 [00:00<00:01, 73.00it/s] 28%|██▊       | 27/95 [00:00<00:00, 73.85it/s] 37%|███▋      | 35/95 [00:00<00:00, 74.02it/s] 45%|████▌     | 43/95 [00:00<00:00, 74.31it/s] 55%|█████▍    | 52/95 [00:00<00:00, 77.33it/s] 64%|██████▍   | 61/95 [00:00<00:00, 80.04it/s] 74%|███████▎  | 70/95 [00:00<00:00, 82.11it/s] 83%|████████▎ | 79/95 [00:00<00:00, 84.05it/s] 93%|█████████▎| 88/95 [00:01<00:00, 85.20it/s]100%|██████████| 95/95 [00:01<00:00, 79.08it/s]
***** eval metrics *****
  epoch                   =        5.0
  eval_accuracy           =     0.9066
  eval_loss               =     0.3085
  eval_macro_f1           =      0.904
  eval_micro_f1           =     0.9066
  eval_runtime            = 0:00:01.21
  eval_samples            =        760
  eval_samples_per_second =    623.601
  eval_steps_per_second   =      77.95
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                  eval/accuracy ▄▁██▄▄
wandb:                      eval/loss ▁▂▃▇██
wandb:                  eval/macro_f1 ▃▁██▄▄
wandb:                  eval/micro_f1 ▄▁██▄▄
wandb:                   eval/runtime ▅▅▁▇▅█
wandb:        eval/samples_per_second ▄▃█▂▄▁
wandb:          eval/steps_per_second ▄▃█▂▄▁
wandb:                    train/epoch ▁▁▃▃▅▅▆▆████
wandb:              train/global_step ▁▁▃▃▄▄▆▆████
wandb:            train/learning_rate █▆▅▃▁
wandb:                     train/loss █▄▃▂▁
wandb:               train/total_flos ▁
wandb:               train/train_loss ▁
wandb:            train/train_runtime ▁
wandb: train/train_samples_per_second ▁
wandb:   train/train_steps_per_second ▁
wandb: 
wandb: Run summary:
wandb:                  eval/accuracy 0.90658
wandb:                      eval/loss 0.30846
wandb:                  eval/macro_f1 0.90402
wandb:                  eval/micro_f1 0.90658
wandb:                   eval/runtime 1.2187
wandb:        eval/samples_per_second 623.601
wandb:          eval/steps_per_second 77.95
wandb:                    train/epoch 5.0
wandb:              train/global_step 1070
wandb:            train/learning_rate 0.0
wandb:                     train/loss 0.129
wandb:               train/total_flos 1136567617228800.0
wandb:               train/train_loss 0.23548
wandb:            train/train_runtime 73.9773
wandb: train/train_samples_per_second 462.304
wandb:   train/train_steps_per_second 14.464
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /ceph/home/wangyifei/a3/wandb/offline-run-20231115_234841-mxreqns5
wandb: Find logs at: ./wandb/offline-run-20231115_234841-mxreqns5/logs
